I0608 15:05:34.043076      23 e2e.go:126] Starting e2e run "ca71eedc-529b-4fa2-b741-e4cd93d22b67" on Ginkgo node 1
Jun  8 15:05:34.070: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1686236733 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jun  8 15:05:34.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:05:34.233: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun  8 15:05:34.265: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun  8 15:05:34.305: INFO: 35 / 35 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun  8 15:05:34.305: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Jun  8 15:05:34.305: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun  8 15:05:34.311: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'csi-oci-node' (0 seconds elapsed)
Jun  8 15:05:34.311: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
Jun  8 15:05:34.311: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jun  8 15:05:34.311: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'oci-cloud-controller-manager' (0 seconds elapsed)
Jun  8 15:05:34.311: INFO: e2e test version: v1.26.5
Jun  8 15:05:34.313: INFO: kube-apiserver version: v1.26.5+2.el8
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jun  8 15:05:34.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:05:34.317: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.087 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jun  8 15:05:34.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:05:34.233: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jun  8 15:05:34.265: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jun  8 15:05:34.305: INFO: 35 / 35 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jun  8 15:05:34.305: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
    Jun  8 15:05:34.305: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jun  8 15:05:34.311: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'csi-oci-node' (0 seconds elapsed)
    Jun  8 15:05:34.311: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
    Jun  8 15:05:34.311: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Jun  8 15:05:34.311: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'oci-cloud-controller-manager' (0 seconds elapsed)
    Jun  8 15:05:34.311: INFO: e2e test version: v1.26.5
    Jun  8 15:05:34.313: INFO: kube-apiserver version: v1.26.5+2.el8
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jun  8 15:05:34.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:05:34.317: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:05:34.339
Jun  8 15:05:34.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename services 06/08/23 15:05:34.34
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:05:34.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:05:34.361
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-7427 06/08/23 15:05:34.364
STEP: creating replication controller nodeport-test in namespace services-7427 06/08/23 15:05:34.386
I0608 15:05:34.395196      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7427, replica count: 2
I0608 15:05:37.448180      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0608 15:05:40.449137      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  8 15:05:40.449: INFO: Creating new exec pod
Jun  8 15:05:40.458: INFO: Waiting up to 5m0s for pod "execpodfkfmg" in namespace "services-7427" to be "running"
Jun  8 15:05:40.461: INFO: Pod "execpodfkfmg": Phase="Pending", Reason="", readiness=false. Elapsed: 3.105499ms
Jun  8 15:05:42.465: INFO: Pod "execpodfkfmg": Phase="Running", Reason="", readiness=true. Elapsed: 2.007330521s
Jun  8 15:05:42.465: INFO: Pod "execpodfkfmg" satisfied condition "running"
Jun  8 15:05:43.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-7427 exec execpodfkfmg -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Jun  8 15:05:43.666: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun  8 15:05:43.667: INFO: stdout: ""
Jun  8 15:05:43.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-7427 exec execpodfkfmg -- /bin/sh -x -c nc -v -z -w 2 10.104.42.109 80'
Jun  8 15:05:43.832: INFO: stderr: "+ nc -v -z -w 2 10.104.42.109 80\nConnection to 10.104.42.109 80 port [tcp/http] succeeded!\n"
Jun  8 15:05:43.832: INFO: stdout: ""
Jun  8 15:05:43.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-7427 exec execpodfkfmg -- /bin/sh -x -c nc -v -z -w 2 100.100.236.215 30106'
Jun  8 15:05:43.994: INFO: stderr: "+ nc -v -z -w 2 100.100.236.215 30106\nConnection to 100.100.236.215 30106 port [tcp/*] succeeded!\n"
Jun  8 15:05:43.994: INFO: stdout: ""
Jun  8 15:05:43.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-7427 exec execpodfkfmg -- /bin/sh -x -c nc -v -z -w 2 100.100.237.235 30106'
Jun  8 15:05:44.150: INFO: stderr: "+ nc -v -z -w 2 100.100.237.235 30106\nConnection to 100.100.237.235 30106 port [tcp/*] succeeded!\n"
Jun  8 15:05:44.150: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  8 15:05:44.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7427" for this suite. 06/08/23 15:05:44.158
------------------------------
• [SLOW TEST] [9.826 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:05:34.339
    Jun  8 15:05:34.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename services 06/08/23 15:05:34.34
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:05:34.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:05:34.361
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-7427 06/08/23 15:05:34.364
    STEP: creating replication controller nodeport-test in namespace services-7427 06/08/23 15:05:34.386
    I0608 15:05:34.395196      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7427, replica count: 2
    I0608 15:05:37.448180      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0608 15:05:40.449137      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun  8 15:05:40.449: INFO: Creating new exec pod
    Jun  8 15:05:40.458: INFO: Waiting up to 5m0s for pod "execpodfkfmg" in namespace "services-7427" to be "running"
    Jun  8 15:05:40.461: INFO: Pod "execpodfkfmg": Phase="Pending", Reason="", readiness=false. Elapsed: 3.105499ms
    Jun  8 15:05:42.465: INFO: Pod "execpodfkfmg": Phase="Running", Reason="", readiness=true. Elapsed: 2.007330521s
    Jun  8 15:05:42.465: INFO: Pod "execpodfkfmg" satisfied condition "running"
    Jun  8 15:05:43.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-7427 exec execpodfkfmg -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Jun  8 15:05:43.666: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun  8 15:05:43.667: INFO: stdout: ""
    Jun  8 15:05:43.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-7427 exec execpodfkfmg -- /bin/sh -x -c nc -v -z -w 2 10.104.42.109 80'
    Jun  8 15:05:43.832: INFO: stderr: "+ nc -v -z -w 2 10.104.42.109 80\nConnection to 10.104.42.109 80 port [tcp/http] succeeded!\n"
    Jun  8 15:05:43.832: INFO: stdout: ""
    Jun  8 15:05:43.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-7427 exec execpodfkfmg -- /bin/sh -x -c nc -v -z -w 2 100.100.236.215 30106'
    Jun  8 15:05:43.994: INFO: stderr: "+ nc -v -z -w 2 100.100.236.215 30106\nConnection to 100.100.236.215 30106 port [tcp/*] succeeded!\n"
    Jun  8 15:05:43.994: INFO: stdout: ""
    Jun  8 15:05:43.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-7427 exec execpodfkfmg -- /bin/sh -x -c nc -v -z -w 2 100.100.237.235 30106'
    Jun  8 15:05:44.150: INFO: stderr: "+ nc -v -z -w 2 100.100.237.235 30106\nConnection to 100.100.237.235 30106 port [tcp/*] succeeded!\n"
    Jun  8 15:05:44.150: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:05:44.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7427" for this suite. 06/08/23 15:05:44.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:05:44.169
Jun  8 15:05:44.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 15:05:44.17
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:05:44.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:05:44.189
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 06/08/23 15:05:44.192
Jun  8 15:05:44.200: INFO: Waiting up to 5m0s for pod "labelsupdateda5b6051-13e5-4dfb-bb93-4c4f475ecea2" in namespace "projected-9826" to be "running and ready"
Jun  8 15:05:44.204: INFO: Pod "labelsupdateda5b6051-13e5-4dfb-bb93-4c4f475ecea2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040714ms
Jun  8 15:05:44.205: INFO: The phase of Pod labelsupdateda5b6051-13e5-4dfb-bb93-4c4f475ecea2 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:05:46.210: INFO: Pod "labelsupdateda5b6051-13e5-4dfb-bb93-4c4f475ecea2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009818851s
Jun  8 15:05:46.210: INFO: The phase of Pod labelsupdateda5b6051-13e5-4dfb-bb93-4c4f475ecea2 is Running (Ready = true)
Jun  8 15:05:46.210: INFO: Pod "labelsupdateda5b6051-13e5-4dfb-bb93-4c4f475ecea2" satisfied condition "running and ready"
Jun  8 15:05:46.743: INFO: Successfully updated pod "labelsupdateda5b6051-13e5-4dfb-bb93-4c4f475ecea2"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  8 15:05:50.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9826" for this suite. 06/08/23 15:05:50.772
------------------------------
• [SLOW TEST] [6.609 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:05:44.169
    Jun  8 15:05:44.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 15:05:44.17
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:05:44.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:05:44.189
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 06/08/23 15:05:44.192
    Jun  8 15:05:44.200: INFO: Waiting up to 5m0s for pod "labelsupdateda5b6051-13e5-4dfb-bb93-4c4f475ecea2" in namespace "projected-9826" to be "running and ready"
    Jun  8 15:05:44.204: INFO: Pod "labelsupdateda5b6051-13e5-4dfb-bb93-4c4f475ecea2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040714ms
    Jun  8 15:05:44.205: INFO: The phase of Pod labelsupdateda5b6051-13e5-4dfb-bb93-4c4f475ecea2 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:05:46.210: INFO: Pod "labelsupdateda5b6051-13e5-4dfb-bb93-4c4f475ecea2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009818851s
    Jun  8 15:05:46.210: INFO: The phase of Pod labelsupdateda5b6051-13e5-4dfb-bb93-4c4f475ecea2 is Running (Ready = true)
    Jun  8 15:05:46.210: INFO: Pod "labelsupdateda5b6051-13e5-4dfb-bb93-4c4f475ecea2" satisfied condition "running and ready"
    Jun  8 15:05:46.743: INFO: Successfully updated pod "labelsupdateda5b6051-13e5-4dfb-bb93-4c4f475ecea2"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:05:50.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9826" for this suite. 06/08/23 15:05:50.772
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:05:50.779
Jun  8 15:05:50.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 15:05:50.78
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:05:50.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:05:50.801
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 06/08/23 15:05:50.805
Jun  8 15:05:50.813: INFO: Waiting up to 5m0s for pod "downwardapi-volume-63d65ab4-0ddf-4e31-bad2-71cd2cc5b113" in namespace "projected-801" to be "Succeeded or Failed"
Jun  8 15:05:50.816: INFO: Pod "downwardapi-volume-63d65ab4-0ddf-4e31-bad2-71cd2cc5b113": Phase="Pending", Reason="", readiness=false. Elapsed: 3.111912ms
Jun  8 15:05:52.821: INFO: Pod "downwardapi-volume-63d65ab4-0ddf-4e31-bad2-71cd2cc5b113": Phase="Running", Reason="", readiness=false. Elapsed: 2.008361251s
Jun  8 15:05:54.820: INFO: Pod "downwardapi-volume-63d65ab4-0ddf-4e31-bad2-71cd2cc5b113": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007263416s
STEP: Saw pod success 06/08/23 15:05:54.821
Jun  8 15:05:54.821: INFO: Pod "downwardapi-volume-63d65ab4-0ddf-4e31-bad2-71cd2cc5b113" satisfied condition "Succeeded or Failed"
Jun  8 15:05:54.824: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-63d65ab4-0ddf-4e31-bad2-71cd2cc5b113 container client-container: <nil>
STEP: delete the pod 06/08/23 15:05:54.831
Jun  8 15:05:54.843: INFO: Waiting for pod downwardapi-volume-63d65ab4-0ddf-4e31-bad2-71cd2cc5b113 to disappear
Jun  8 15:05:54.846: INFO: Pod downwardapi-volume-63d65ab4-0ddf-4e31-bad2-71cd2cc5b113 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  8 15:05:54.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-801" for this suite. 06/08/23 15:05:54.851
------------------------------
• [4.078 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:05:50.779
    Jun  8 15:05:50.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 15:05:50.78
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:05:50.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:05:50.801
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 06/08/23 15:05:50.805
    Jun  8 15:05:50.813: INFO: Waiting up to 5m0s for pod "downwardapi-volume-63d65ab4-0ddf-4e31-bad2-71cd2cc5b113" in namespace "projected-801" to be "Succeeded or Failed"
    Jun  8 15:05:50.816: INFO: Pod "downwardapi-volume-63d65ab4-0ddf-4e31-bad2-71cd2cc5b113": Phase="Pending", Reason="", readiness=false. Elapsed: 3.111912ms
    Jun  8 15:05:52.821: INFO: Pod "downwardapi-volume-63d65ab4-0ddf-4e31-bad2-71cd2cc5b113": Phase="Running", Reason="", readiness=false. Elapsed: 2.008361251s
    Jun  8 15:05:54.820: INFO: Pod "downwardapi-volume-63d65ab4-0ddf-4e31-bad2-71cd2cc5b113": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007263416s
    STEP: Saw pod success 06/08/23 15:05:54.821
    Jun  8 15:05:54.821: INFO: Pod "downwardapi-volume-63d65ab4-0ddf-4e31-bad2-71cd2cc5b113" satisfied condition "Succeeded or Failed"
    Jun  8 15:05:54.824: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-63d65ab4-0ddf-4e31-bad2-71cd2cc5b113 container client-container: <nil>
    STEP: delete the pod 06/08/23 15:05:54.831
    Jun  8 15:05:54.843: INFO: Waiting for pod downwardapi-volume-63d65ab4-0ddf-4e31-bad2-71cd2cc5b113 to disappear
    Jun  8 15:05:54.846: INFO: Pod downwardapi-volume-63d65ab4-0ddf-4e31-bad2-71cd2cc5b113 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:05:54.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-801" for this suite. 06/08/23 15:05:54.851
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:05:54.859
Jun  8 15:05:54.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename crd-publish-openapi 06/08/23 15:05:54.86
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:05:54.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:05:54.878
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Jun  8 15:05:54.882: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 06/08/23 15:05:56.987
Jun  8 15:05:56.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 --namespace=crd-publish-openapi-5566 create -f -'
Jun  8 15:05:57.682: INFO: stderr: ""
Jun  8 15:05:57.682: INFO: stdout: "e2e-test-crd-publish-openapi-5794-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun  8 15:05:57.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 --namespace=crd-publish-openapi-5566 delete e2e-test-crd-publish-openapi-5794-crds test-foo'
Jun  8 15:05:57.800: INFO: stderr: ""
Jun  8 15:05:57.800: INFO: stdout: "e2e-test-crd-publish-openapi-5794-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun  8 15:05:57.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 --namespace=crd-publish-openapi-5566 apply -f -'
Jun  8 15:05:58.033: INFO: stderr: ""
Jun  8 15:05:58.033: INFO: stdout: "e2e-test-crd-publish-openapi-5794-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun  8 15:05:58.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 --namespace=crd-publish-openapi-5566 delete e2e-test-crd-publish-openapi-5794-crds test-foo'
Jun  8 15:06:00.150: INFO: stderr: ""
Jun  8 15:06:00.150: INFO: stdout: "e2e-test-crd-publish-openapi-5794-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 06/08/23 15:06:00.15
Jun  8 15:06:00.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 --namespace=crd-publish-openapi-5566 create -f -'
Jun  8 15:06:00.732: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 06/08/23 15:06:00.732
Jun  8 15:06:00.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 --namespace=crd-publish-openapi-5566 create -f -'
Jun  8 15:06:00.930: INFO: rc: 1
Jun  8 15:06:00.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 --namespace=crd-publish-openapi-5566 apply -f -'
Jun  8 15:06:02.351: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 06/08/23 15:06:02.351
Jun  8 15:06:02.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 --namespace=crd-publish-openapi-5566 create -f -'
Jun  8 15:06:02.536: INFO: rc: 1
Jun  8 15:06:02.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 --namespace=crd-publish-openapi-5566 apply -f -'
Jun  8 15:06:02.729: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 06/08/23 15:06:02.729
Jun  8 15:06:02.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 explain e2e-test-crd-publish-openapi-5794-crds'
Jun  8 15:06:02.921: INFO: stderr: ""
Jun  8 15:06:02.921: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5794-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 06/08/23 15:06:02.921
Jun  8 15:06:02.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 explain e2e-test-crd-publish-openapi-5794-crds.metadata'
Jun  8 15:06:03.112: INFO: stderr: ""
Jun  8 15:06:03.112: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5794-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun  8 15:06:03.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 explain e2e-test-crd-publish-openapi-5794-crds.spec'
Jun  8 15:06:03.293: INFO: stderr: ""
Jun  8 15:06:03.293: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5794-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun  8 15:06:03.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 explain e2e-test-crd-publish-openapi-5794-crds.spec.bars'
Jun  8 15:06:03.477: INFO: stderr: ""
Jun  8 15:06:03.477: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5794-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 06/08/23 15:06:03.477
Jun  8 15:06:03.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 explain e2e-test-crd-publish-openapi-5794-crds.spec.bars2'
Jun  8 15:06:04.043: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:06:06.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5566" for this suite. 06/08/23 15:06:06.287
------------------------------
• [SLOW TEST] [11.436 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:05:54.859
    Jun  8 15:05:54.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename crd-publish-openapi 06/08/23 15:05:54.86
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:05:54.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:05:54.878
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Jun  8 15:05:54.882: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 06/08/23 15:05:56.987
    Jun  8 15:05:56.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 --namespace=crd-publish-openapi-5566 create -f -'
    Jun  8 15:05:57.682: INFO: stderr: ""
    Jun  8 15:05:57.682: INFO: stdout: "e2e-test-crd-publish-openapi-5794-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jun  8 15:05:57.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 --namespace=crd-publish-openapi-5566 delete e2e-test-crd-publish-openapi-5794-crds test-foo'
    Jun  8 15:05:57.800: INFO: stderr: ""
    Jun  8 15:05:57.800: INFO: stdout: "e2e-test-crd-publish-openapi-5794-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jun  8 15:05:57.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 --namespace=crd-publish-openapi-5566 apply -f -'
    Jun  8 15:05:58.033: INFO: stderr: ""
    Jun  8 15:05:58.033: INFO: stdout: "e2e-test-crd-publish-openapi-5794-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jun  8 15:05:58.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 --namespace=crd-publish-openapi-5566 delete e2e-test-crd-publish-openapi-5794-crds test-foo'
    Jun  8 15:06:00.150: INFO: stderr: ""
    Jun  8 15:06:00.150: INFO: stdout: "e2e-test-crd-publish-openapi-5794-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 06/08/23 15:06:00.15
    Jun  8 15:06:00.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 --namespace=crd-publish-openapi-5566 create -f -'
    Jun  8 15:06:00.732: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 06/08/23 15:06:00.732
    Jun  8 15:06:00.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 --namespace=crd-publish-openapi-5566 create -f -'
    Jun  8 15:06:00.930: INFO: rc: 1
    Jun  8 15:06:00.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 --namespace=crd-publish-openapi-5566 apply -f -'
    Jun  8 15:06:02.351: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 06/08/23 15:06:02.351
    Jun  8 15:06:02.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 --namespace=crd-publish-openapi-5566 create -f -'
    Jun  8 15:06:02.536: INFO: rc: 1
    Jun  8 15:06:02.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 --namespace=crd-publish-openapi-5566 apply -f -'
    Jun  8 15:06:02.729: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 06/08/23 15:06:02.729
    Jun  8 15:06:02.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 explain e2e-test-crd-publish-openapi-5794-crds'
    Jun  8 15:06:02.921: INFO: stderr: ""
    Jun  8 15:06:02.921: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5794-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 06/08/23 15:06:02.921
    Jun  8 15:06:02.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 explain e2e-test-crd-publish-openapi-5794-crds.metadata'
    Jun  8 15:06:03.112: INFO: stderr: ""
    Jun  8 15:06:03.112: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5794-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jun  8 15:06:03.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 explain e2e-test-crd-publish-openapi-5794-crds.spec'
    Jun  8 15:06:03.293: INFO: stderr: ""
    Jun  8 15:06:03.293: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5794-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jun  8 15:06:03.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 explain e2e-test-crd-publish-openapi-5794-crds.spec.bars'
    Jun  8 15:06:03.477: INFO: stderr: ""
    Jun  8 15:06:03.477: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5794-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 06/08/23 15:06:03.477
    Jun  8 15:06:03.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-5566 explain e2e-test-crd-publish-openapi-5794-crds.spec.bars2'
    Jun  8 15:06:04.043: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:06:06.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5566" for this suite. 06/08/23 15:06:06.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:06:06.296
Jun  8 15:06:06.297: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 15:06:06.298
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:06:06.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:06:06.317
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-223c2cee-46dd-42bb-965a-f2adc9f8f76c 06/08/23 15:06:06.32
STEP: Creating a pod to test consume secrets 06/08/23 15:06:06.325
Jun  8 15:06:06.334: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533" in namespace "projected-5671" to be "Succeeded or Failed"
Jun  8 15:06:06.339: INFO: Pod "pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533": Phase="Pending", Reason="", readiness=false. Elapsed: 5.576803ms
Jun  8 15:06:08.345: INFO: Pod "pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011654883s
Jun  8 15:06:10.346: INFO: Pod "pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011835976s
Jun  8 15:06:12.345: INFO: Pod "pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011568237s
Jun  8 15:06:14.344: INFO: Pod "pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010633435s
Jun  8 15:06:16.344: INFO: Pod "pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.010797941s
STEP: Saw pod success 06/08/23 15:06:16.345
Jun  8 15:06:16.345: INFO: Pod "pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533" satisfied condition "Succeeded or Failed"
Jun  8 15:06:16.348: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533 container projected-secret-volume-test: <nil>
STEP: delete the pod 06/08/23 15:06:16.366
Jun  8 15:06:16.380: INFO: Waiting for pod pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533 to disappear
Jun  8 15:06:16.384: INFO: Pod pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jun  8 15:06:16.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5671" for this suite. 06/08/23 15:06:16.39
------------------------------
• [SLOW TEST] [10.100 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:06:06.296
    Jun  8 15:06:06.297: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 15:06:06.298
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:06:06.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:06:06.317
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-223c2cee-46dd-42bb-965a-f2adc9f8f76c 06/08/23 15:06:06.32
    STEP: Creating a pod to test consume secrets 06/08/23 15:06:06.325
    Jun  8 15:06:06.334: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533" in namespace "projected-5671" to be "Succeeded or Failed"
    Jun  8 15:06:06.339: INFO: Pod "pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533": Phase="Pending", Reason="", readiness=false. Elapsed: 5.576803ms
    Jun  8 15:06:08.345: INFO: Pod "pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011654883s
    Jun  8 15:06:10.346: INFO: Pod "pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011835976s
    Jun  8 15:06:12.345: INFO: Pod "pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011568237s
    Jun  8 15:06:14.344: INFO: Pod "pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010633435s
    Jun  8 15:06:16.344: INFO: Pod "pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.010797941s
    STEP: Saw pod success 06/08/23 15:06:16.345
    Jun  8 15:06:16.345: INFO: Pod "pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533" satisfied condition "Succeeded or Failed"
    Jun  8 15:06:16.348: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533 container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/08/23 15:06:16.366
    Jun  8 15:06:16.380: INFO: Waiting for pod pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533 to disappear
    Jun  8 15:06:16.384: INFO: Pod pod-projected-secrets-54a6db27-5933-4252-af35-f2e046d2d533 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:06:16.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5671" for this suite. 06/08/23 15:06:16.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:06:16.399
Jun  8 15:06:16.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename pods 06/08/23 15:06:16.4
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:06:16.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:06:16.419
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 06/08/23 15:06:16.422
STEP: setting up watch 06/08/23 15:06:16.423
STEP: submitting the pod to kubernetes 06/08/23 15:06:16.528
STEP: verifying the pod is in kubernetes 06/08/23 15:06:16.538
STEP: verifying pod creation was observed 06/08/23 15:06:16.542
Jun  8 15:06:16.542: INFO: Waiting up to 5m0s for pod "pod-submit-remove-2dd898c5-b88a-41ad-9165-d09206f3a144" in namespace "pods-1871" to be "running"
Jun  8 15:06:16.547: INFO: Pod "pod-submit-remove-2dd898c5-b88a-41ad-9165-d09206f3a144": Phase="Pending", Reason="", readiness=false. Elapsed: 5.118266ms
Jun  8 15:06:18.555: INFO: Pod "pod-submit-remove-2dd898c5-b88a-41ad-9165-d09206f3a144": Phase="Running", Reason="", readiness=true. Elapsed: 2.012642003s
Jun  8 15:06:18.555: INFO: Pod "pod-submit-remove-2dd898c5-b88a-41ad-9165-d09206f3a144" satisfied condition "running"
STEP: deleting the pod gracefully 06/08/23 15:06:18.559
STEP: verifying pod deletion was observed 06/08/23 15:06:18.566
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun  8 15:06:21.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1871" for this suite. 06/08/23 15:06:21.587
------------------------------
• [SLOW TEST] [5.196 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:06:16.399
    Jun  8 15:06:16.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename pods 06/08/23 15:06:16.4
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:06:16.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:06:16.419
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 06/08/23 15:06:16.422
    STEP: setting up watch 06/08/23 15:06:16.423
    STEP: submitting the pod to kubernetes 06/08/23 15:06:16.528
    STEP: verifying the pod is in kubernetes 06/08/23 15:06:16.538
    STEP: verifying pod creation was observed 06/08/23 15:06:16.542
    Jun  8 15:06:16.542: INFO: Waiting up to 5m0s for pod "pod-submit-remove-2dd898c5-b88a-41ad-9165-d09206f3a144" in namespace "pods-1871" to be "running"
    Jun  8 15:06:16.547: INFO: Pod "pod-submit-remove-2dd898c5-b88a-41ad-9165-d09206f3a144": Phase="Pending", Reason="", readiness=false. Elapsed: 5.118266ms
    Jun  8 15:06:18.555: INFO: Pod "pod-submit-remove-2dd898c5-b88a-41ad-9165-d09206f3a144": Phase="Running", Reason="", readiness=true. Elapsed: 2.012642003s
    Jun  8 15:06:18.555: INFO: Pod "pod-submit-remove-2dd898c5-b88a-41ad-9165-d09206f3a144" satisfied condition "running"
    STEP: deleting the pod gracefully 06/08/23 15:06:18.559
    STEP: verifying pod deletion was observed 06/08/23 15:06:18.566
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:06:21.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1871" for this suite. 06/08/23 15:06:21.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:06:21.597
Jun  8 15:06:21.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename var-expansion 06/08/23 15:06:21.598
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:06:21.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:06:21.615
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 06/08/23 15:06:21.619
STEP: waiting for pod running 06/08/23 15:06:21.628
Jun  8 15:06:21.628: INFO: Waiting up to 2m0s for pod "var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73" in namespace "var-expansion-2952" to be "running"
Jun  8 15:06:21.632: INFO: Pod "var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73": Phase="Pending", Reason="", readiness=false. Elapsed: 3.877469ms
Jun  8 15:06:23.637: INFO: Pod "var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73": Phase="Running", Reason="", readiness=true. Elapsed: 2.008794695s
Jun  8 15:06:23.637: INFO: Pod "var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73" satisfied condition "running"
STEP: creating a file in subpath 06/08/23 15:06:23.637
Jun  8 15:06:23.641: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2952 PodName:var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:06:23.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:06:23.642: INFO: ExecWithOptions: Clientset creation
Jun  8 15:06:23.642: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2952/pods/var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 06/08/23 15:06:23.721
Jun  8 15:06:23.726: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2952 PodName:var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:06:23.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:06:23.727: INFO: ExecWithOptions: Clientset creation
Jun  8 15:06:23.727: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2952/pods/var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 06/08/23 15:06:23.801
Jun  8 15:06:24.316: INFO: Successfully updated pod "var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73"
STEP: waiting for annotated pod running 06/08/23 15:06:24.316
Jun  8 15:06:24.316: INFO: Waiting up to 2m0s for pod "var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73" in namespace "var-expansion-2952" to be "running"
Jun  8 15:06:24.324: INFO: Pod "var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73": Phase="Running", Reason="", readiness=true. Elapsed: 7.59237ms
Jun  8 15:06:24.324: INFO: Pod "var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73" satisfied condition "running"
STEP: deleting the pod gracefully 06/08/23 15:06:24.324
Jun  8 15:06:24.324: INFO: Deleting pod "var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73" in namespace "var-expansion-2952"
Jun  8 15:06:24.333: INFO: Wait up to 5m0s for pod "var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun  8 15:06:58.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2952" for this suite. 06/08/23 15:06:58.347
------------------------------
• [SLOW TEST] [36.757 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:06:21.597
    Jun  8 15:06:21.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename var-expansion 06/08/23 15:06:21.598
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:06:21.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:06:21.615
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 06/08/23 15:06:21.619
    STEP: waiting for pod running 06/08/23 15:06:21.628
    Jun  8 15:06:21.628: INFO: Waiting up to 2m0s for pod "var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73" in namespace "var-expansion-2952" to be "running"
    Jun  8 15:06:21.632: INFO: Pod "var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73": Phase="Pending", Reason="", readiness=false. Elapsed: 3.877469ms
    Jun  8 15:06:23.637: INFO: Pod "var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73": Phase="Running", Reason="", readiness=true. Elapsed: 2.008794695s
    Jun  8 15:06:23.637: INFO: Pod "var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73" satisfied condition "running"
    STEP: creating a file in subpath 06/08/23 15:06:23.637
    Jun  8 15:06:23.641: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2952 PodName:var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:06:23.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:06:23.642: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:06:23.642: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2952/pods/var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 06/08/23 15:06:23.721
    Jun  8 15:06:23.726: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2952 PodName:var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:06:23.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:06:23.727: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:06:23.727: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2952/pods/var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 06/08/23 15:06:23.801
    Jun  8 15:06:24.316: INFO: Successfully updated pod "var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73"
    STEP: waiting for annotated pod running 06/08/23 15:06:24.316
    Jun  8 15:06:24.316: INFO: Waiting up to 2m0s for pod "var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73" in namespace "var-expansion-2952" to be "running"
    Jun  8 15:06:24.324: INFO: Pod "var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73": Phase="Running", Reason="", readiness=true. Elapsed: 7.59237ms
    Jun  8 15:06:24.324: INFO: Pod "var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73" satisfied condition "running"
    STEP: deleting the pod gracefully 06/08/23 15:06:24.324
    Jun  8 15:06:24.324: INFO: Deleting pod "var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73" in namespace "var-expansion-2952"
    Jun  8 15:06:24.333: INFO: Wait up to 5m0s for pod "var-expansion-f00e132f-a189-4cab-9c86-b6f14d3e6e73" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:06:58.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2952" for this suite. 06/08/23 15:06:58.347
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:06:58.355
Jun  8 15:06:58.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 15:06:58.357
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:06:58.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:06:58.377
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 06/08/23 15:06:58.38
Jun  8 15:06:58.390: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3257889a-f4a5-479f-8199-61c7a8e055fe" in namespace "projected-2744" to be "Succeeded or Failed"
Jun  8 15:06:58.394: INFO: Pod "downwardapi-volume-3257889a-f4a5-479f-8199-61c7a8e055fe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.644304ms
Jun  8 15:07:00.398: INFO: Pod "downwardapi-volume-3257889a-f4a5-479f-8199-61c7a8e055fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008377724s
Jun  8 15:07:02.400: INFO: Pod "downwardapi-volume-3257889a-f4a5-479f-8199-61c7a8e055fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010355743s
STEP: Saw pod success 06/08/23 15:07:02.4
Jun  8 15:07:02.401: INFO: Pod "downwardapi-volume-3257889a-f4a5-479f-8199-61c7a8e055fe" satisfied condition "Succeeded or Failed"
Jun  8 15:07:02.404: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-3257889a-f4a5-479f-8199-61c7a8e055fe container client-container: <nil>
STEP: delete the pod 06/08/23 15:07:02.412
Jun  8 15:07:02.427: INFO: Waiting for pod downwardapi-volume-3257889a-f4a5-479f-8199-61c7a8e055fe to disappear
Jun  8 15:07:02.430: INFO: Pod downwardapi-volume-3257889a-f4a5-479f-8199-61c7a8e055fe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  8 15:07:02.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2744" for this suite. 06/08/23 15:07:02.436
------------------------------
• [4.088 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:06:58.355
    Jun  8 15:06:58.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 15:06:58.357
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:06:58.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:06:58.377
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 06/08/23 15:06:58.38
    Jun  8 15:06:58.390: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3257889a-f4a5-479f-8199-61c7a8e055fe" in namespace "projected-2744" to be "Succeeded or Failed"
    Jun  8 15:06:58.394: INFO: Pod "downwardapi-volume-3257889a-f4a5-479f-8199-61c7a8e055fe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.644304ms
    Jun  8 15:07:00.398: INFO: Pod "downwardapi-volume-3257889a-f4a5-479f-8199-61c7a8e055fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008377724s
    Jun  8 15:07:02.400: INFO: Pod "downwardapi-volume-3257889a-f4a5-479f-8199-61c7a8e055fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010355743s
    STEP: Saw pod success 06/08/23 15:07:02.4
    Jun  8 15:07:02.401: INFO: Pod "downwardapi-volume-3257889a-f4a5-479f-8199-61c7a8e055fe" satisfied condition "Succeeded or Failed"
    Jun  8 15:07:02.404: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-3257889a-f4a5-479f-8199-61c7a8e055fe container client-container: <nil>
    STEP: delete the pod 06/08/23 15:07:02.412
    Jun  8 15:07:02.427: INFO: Waiting for pod downwardapi-volume-3257889a-f4a5-479f-8199-61c7a8e055fe to disappear
    Jun  8 15:07:02.430: INFO: Pod downwardapi-volume-3257889a-f4a5-479f-8199-61c7a8e055fe no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:07:02.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2744" for this suite. 06/08/23 15:07:02.436
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:07:02.444
Jun  8 15:07:02.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename statefulset 06/08/23 15:07:02.445
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:07:02.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:07:02.462
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9705 06/08/23 15:07:02.466
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 06/08/23 15:07:02.472
Jun  8 15:07:02.485: INFO: Found 0 stateful pods, waiting for 3
Jun  8 15:07:12.491: INFO: Found 2 stateful pods, waiting for 3
Jun  8 15:07:22.493: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  8 15:07:22.493: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  8 15:07:22.493: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun  8 15:07:22.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-9705 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  8 15:07:22.676: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  8 15:07:22.676: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  8 15:07:22.676: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 06/08/23 15:07:32.698
Jun  8 15:07:32.719: INFO: Updating stateful set ss2
STEP: Creating a new revision 06/08/23 15:07:32.719
STEP: Updating Pods in reverse ordinal order 06/08/23 15:07:42.739
Jun  8 15:07:42.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-9705 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  8 15:07:42.941: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  8 15:07:42.941: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  8 15:07:42.941: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 06/08/23 15:08:02.969
Jun  8 15:08:02.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-9705 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  8 15:08:03.130: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  8 15:08:03.130: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  8 15:08:03.130: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  8 15:08:13.171: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 06/08/23 15:08:23.189
Jun  8 15:08:23.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-9705 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  8 15:08:23.352: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  8 15:08:23.352: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  8 15:08:23.352: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun  8 15:08:33.381: INFO: Deleting all statefulset in ns statefulset-9705
Jun  8 15:08:33.384: INFO: Scaling statefulset ss2 to 0
Jun  8 15:08:43.408: INFO: Waiting for statefulset status.replicas updated to 0
Jun  8 15:08:43.412: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun  8 15:08:43.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9705" for this suite. 06/08/23 15:08:43.435
------------------------------
• [SLOW TEST] [101.000 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:07:02.444
    Jun  8 15:07:02.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename statefulset 06/08/23 15:07:02.445
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:07:02.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:07:02.462
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9705 06/08/23 15:07:02.466
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 06/08/23 15:07:02.472
    Jun  8 15:07:02.485: INFO: Found 0 stateful pods, waiting for 3
    Jun  8 15:07:12.491: INFO: Found 2 stateful pods, waiting for 3
    Jun  8 15:07:22.493: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun  8 15:07:22.493: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun  8 15:07:22.493: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jun  8 15:07:22.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-9705 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun  8 15:07:22.676: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun  8 15:07:22.676: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun  8 15:07:22.676: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 06/08/23 15:07:32.698
    Jun  8 15:07:32.719: INFO: Updating stateful set ss2
    STEP: Creating a new revision 06/08/23 15:07:32.719
    STEP: Updating Pods in reverse ordinal order 06/08/23 15:07:42.739
    Jun  8 15:07:42.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-9705 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun  8 15:07:42.941: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun  8 15:07:42.941: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun  8 15:07:42.941: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 06/08/23 15:08:02.969
    Jun  8 15:08:02.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-9705 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun  8 15:08:03.130: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun  8 15:08:03.130: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun  8 15:08:03.130: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun  8 15:08:13.171: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 06/08/23 15:08:23.189
    Jun  8 15:08:23.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-9705 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun  8 15:08:23.352: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun  8 15:08:23.352: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun  8 15:08:23.352: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun  8 15:08:33.381: INFO: Deleting all statefulset in ns statefulset-9705
    Jun  8 15:08:33.384: INFO: Scaling statefulset ss2 to 0
    Jun  8 15:08:43.408: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  8 15:08:43.412: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:08:43.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9705" for this suite. 06/08/23 15:08:43.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:08:43.448
Jun  8 15:08:43.448: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename downward-api 06/08/23 15:08:43.449
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:08:43.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:08:43.467
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 06/08/23 15:08:43.47
Jun  8 15:08:43.479: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ad92d31-1905-4882-a27e-c47424e8de87" in namespace "downward-api-8861" to be "Succeeded or Failed"
Jun  8 15:08:43.483: INFO: Pod "downwardapi-volume-7ad92d31-1905-4882-a27e-c47424e8de87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.722799ms
Jun  8 15:08:45.488: INFO: Pod "downwardapi-volume-7ad92d31-1905-4882-a27e-c47424e8de87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009355633s
Jun  8 15:08:47.488: INFO: Pod "downwardapi-volume-7ad92d31-1905-4882-a27e-c47424e8de87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009153178s
STEP: Saw pod success 06/08/23 15:08:47.488
Jun  8 15:08:47.488: INFO: Pod "downwardapi-volume-7ad92d31-1905-4882-a27e-c47424e8de87" satisfied condition "Succeeded or Failed"
Jun  8 15:08:47.493: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-7ad92d31-1905-4882-a27e-c47424e8de87 container client-container: <nil>
STEP: delete the pod 06/08/23 15:08:47.508
Jun  8 15:08:47.519: INFO: Waiting for pod downwardapi-volume-7ad92d31-1905-4882-a27e-c47424e8de87 to disappear
Jun  8 15:08:47.522: INFO: Pod downwardapi-volume-7ad92d31-1905-4882-a27e-c47424e8de87 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  8 15:08:47.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8861" for this suite. 06/08/23 15:08:47.528
------------------------------
• [4.087 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:08:43.448
    Jun  8 15:08:43.448: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename downward-api 06/08/23 15:08:43.449
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:08:43.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:08:43.467
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 06/08/23 15:08:43.47
    Jun  8 15:08:43.479: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ad92d31-1905-4882-a27e-c47424e8de87" in namespace "downward-api-8861" to be "Succeeded or Failed"
    Jun  8 15:08:43.483: INFO: Pod "downwardapi-volume-7ad92d31-1905-4882-a27e-c47424e8de87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.722799ms
    Jun  8 15:08:45.488: INFO: Pod "downwardapi-volume-7ad92d31-1905-4882-a27e-c47424e8de87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009355633s
    Jun  8 15:08:47.488: INFO: Pod "downwardapi-volume-7ad92d31-1905-4882-a27e-c47424e8de87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009153178s
    STEP: Saw pod success 06/08/23 15:08:47.488
    Jun  8 15:08:47.488: INFO: Pod "downwardapi-volume-7ad92d31-1905-4882-a27e-c47424e8de87" satisfied condition "Succeeded or Failed"
    Jun  8 15:08:47.493: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-7ad92d31-1905-4882-a27e-c47424e8de87 container client-container: <nil>
    STEP: delete the pod 06/08/23 15:08:47.508
    Jun  8 15:08:47.519: INFO: Waiting for pod downwardapi-volume-7ad92d31-1905-4882-a27e-c47424e8de87 to disappear
    Jun  8 15:08:47.522: INFO: Pod downwardapi-volume-7ad92d31-1905-4882-a27e-c47424e8de87 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:08:47.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8861" for this suite. 06/08/23 15:08:47.528
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:08:47.536
Jun  8 15:08:47.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename webhook 06/08/23 15:08:47.537
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:08:47.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:08:47.554
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/08/23 15:08:47.571
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 15:08:47.945
STEP: Deploying the webhook pod 06/08/23 15:08:47.954
STEP: Wait for the deployment to be ready 06/08/23 15:08:47.966
Jun  8 15:08:47.975: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 06/08/23 15:08:49.988
STEP: Verifying the service has paired with the endpoint 06/08/23 15:08:50.012
Jun  8 15:08:51.013: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 06/08/23 15:08:51.019
STEP: create a pod that should be updated by the webhook 06/08/23 15:08:51.042
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:08:51.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2627" for this suite. 06/08/23 15:08:51.135
STEP: Destroying namespace "webhook-2627-markers" for this suite. 06/08/23 15:08:51.145
------------------------------
• [3.625 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:08:47.536
    Jun  8 15:08:47.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename webhook 06/08/23 15:08:47.537
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:08:47.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:08:47.554
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/08/23 15:08:47.571
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 15:08:47.945
    STEP: Deploying the webhook pod 06/08/23 15:08:47.954
    STEP: Wait for the deployment to be ready 06/08/23 15:08:47.966
    Jun  8 15:08:47.975: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 06/08/23 15:08:49.988
    STEP: Verifying the service has paired with the endpoint 06/08/23 15:08:50.012
    Jun  8 15:08:51.013: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 06/08/23 15:08:51.019
    STEP: create a pod that should be updated by the webhook 06/08/23 15:08:51.042
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:08:51.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2627" for this suite. 06/08/23 15:08:51.135
    STEP: Destroying namespace "webhook-2627-markers" for this suite. 06/08/23 15:08:51.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:08:51.163
Jun  8 15:08:51.163: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename container-lifecycle-hook 06/08/23 15:08:51.164
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:08:51.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:08:51.19
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 06/08/23 15:08:51.202
Jun  8 15:08:51.216: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5510" to be "running and ready"
Jun  8 15:08:51.222: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.73416ms
Jun  8 15:08:51.222: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:08:53.229: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013209797s
Jun  8 15:08:53.229: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:08:55.229: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013385776s
Jun  8 15:08:55.229: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:08:57.228: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 6.01283732s
Jun  8 15:08:57.229: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun  8 15:08:57.229: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 06/08/23 15:08:57.233
Jun  8 15:08:57.239: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-5510" to be "running and ready"
Jun  8 15:08:57.244: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.979815ms
Jun  8 15:08:57.244: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:08:59.250: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010582896s
Jun  8 15:08:59.250: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jun  8 15:08:59.250: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 06/08/23 15:08:59.254
STEP: delete the pod with lifecycle hook 06/08/23 15:08:59.27
Jun  8 15:08:59.278: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  8 15:08:59.283: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  8 15:09:01.284: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  8 15:09:01.289: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jun  8 15:09:01.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-5510" for this suite. 06/08/23 15:09:01.295
------------------------------
• [SLOW TEST] [10.140 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:08:51.163
    Jun  8 15:08:51.163: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/08/23 15:08:51.164
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:08:51.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:08:51.19
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 06/08/23 15:08:51.202
    Jun  8 15:08:51.216: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5510" to be "running and ready"
    Jun  8 15:08:51.222: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.73416ms
    Jun  8 15:08:51.222: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:08:53.229: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013209797s
    Jun  8 15:08:53.229: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:08:55.229: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013385776s
    Jun  8 15:08:55.229: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:08:57.228: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 6.01283732s
    Jun  8 15:08:57.229: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun  8 15:08:57.229: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 06/08/23 15:08:57.233
    Jun  8 15:08:57.239: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-5510" to be "running and ready"
    Jun  8 15:08:57.244: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.979815ms
    Jun  8 15:08:57.244: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:08:59.250: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010582896s
    Jun  8 15:08:59.250: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jun  8 15:08:59.250: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 06/08/23 15:08:59.254
    STEP: delete the pod with lifecycle hook 06/08/23 15:08:59.27
    Jun  8 15:08:59.278: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jun  8 15:08:59.283: INFO: Pod pod-with-poststart-exec-hook still exists
    Jun  8 15:09:01.284: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jun  8 15:09:01.289: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:09:01.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-5510" for this suite. 06/08/23 15:09:01.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:09:01.304
Jun  8 15:09:01.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename custom-resource-definition 06/08/23 15:09:01.305
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:01.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:01.329
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jun  8 15:09:01.332: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:09:04.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5937" for this suite. 06/08/23 15:09:04.486
------------------------------
• [3.190 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:09:01.304
    Jun  8 15:09:01.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename custom-resource-definition 06/08/23 15:09:01.305
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:01.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:01.329
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jun  8 15:09:01.332: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:09:04.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5937" for this suite. 06/08/23 15:09:04.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:09:04.496
Jun  8 15:09:04.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename init-container 06/08/23 15:09:04.496
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:04.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:04.515
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 06/08/23 15:09:04.518
Jun  8 15:09:04.518: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:09:09.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-3571" for this suite. 06/08/23 15:09:09.992
------------------------------
• [SLOW TEST] [5.504 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:09:04.496
    Jun  8 15:09:04.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename init-container 06/08/23 15:09:04.496
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:04.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:04.515
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 06/08/23 15:09:04.518
    Jun  8 15:09:04.518: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:09:09.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-3571" for this suite. 06/08/23 15:09:09.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:09:10.001
Jun  8 15:09:10.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename services 06/08/23 15:09:10.003
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:10.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:10.022
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3019 06/08/23 15:09:10.025
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/08/23 15:09:10.04
STEP: creating service externalsvc in namespace services-3019 06/08/23 15:09:10.04
STEP: creating replication controller externalsvc in namespace services-3019 06/08/23 15:09:10.071
I0608 15:09:10.081474      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3019, replica count: 2
I0608 15:09:13.133648      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 06/08/23 15:09:13.139
Jun  8 15:09:13.166: INFO: Creating new exec pod
Jun  8 15:09:13.176: INFO: Waiting up to 5m0s for pod "execpodzx29g" in namespace "services-3019" to be "running"
Jun  8 15:09:13.181: INFO: Pod "execpodzx29g": Phase="Pending", Reason="", readiness=false. Elapsed: 4.698286ms
Jun  8 15:09:15.187: INFO: Pod "execpodzx29g": Phase="Running", Reason="", readiness=true. Elapsed: 2.01068317s
Jun  8 15:09:15.187: INFO: Pod "execpodzx29g" satisfied condition "running"
Jun  8 15:09:15.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-3019 exec execpodzx29g -- /bin/sh -x -c nslookup clusterip-service.services-3019.svc.cluster.local'
Jun  8 15:09:15.366: INFO: stderr: "+ nslookup clusterip-service.services-3019.svc.cluster.local\n"
Jun  8 15:09:15.366: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-3019.svc.cluster.local\tcanonical name = externalsvc.services-3019.svc.cluster.local.\nName:\texternalsvc.services-3019.svc.cluster.local\nAddress: 10.103.101.237\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3019, will wait for the garbage collector to delete the pods 06/08/23 15:09:15.366
Jun  8 15:09:15.429: INFO: Deleting ReplicationController externalsvc took: 8.228081ms
Jun  8 15:09:15.530: INFO: Terminating ReplicationController externalsvc pods took: 100.906316ms
Jun  8 15:09:17.565: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  8 15:09:17.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3019" for this suite. 06/08/23 15:09:17.603
------------------------------
• [SLOW TEST] [7.612 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:09:10.001
    Jun  8 15:09:10.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename services 06/08/23 15:09:10.003
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:10.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:10.022
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3019 06/08/23 15:09:10.025
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/08/23 15:09:10.04
    STEP: creating service externalsvc in namespace services-3019 06/08/23 15:09:10.04
    STEP: creating replication controller externalsvc in namespace services-3019 06/08/23 15:09:10.071
    I0608 15:09:10.081474      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3019, replica count: 2
    I0608 15:09:13.133648      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 06/08/23 15:09:13.139
    Jun  8 15:09:13.166: INFO: Creating new exec pod
    Jun  8 15:09:13.176: INFO: Waiting up to 5m0s for pod "execpodzx29g" in namespace "services-3019" to be "running"
    Jun  8 15:09:13.181: INFO: Pod "execpodzx29g": Phase="Pending", Reason="", readiness=false. Elapsed: 4.698286ms
    Jun  8 15:09:15.187: INFO: Pod "execpodzx29g": Phase="Running", Reason="", readiness=true. Elapsed: 2.01068317s
    Jun  8 15:09:15.187: INFO: Pod "execpodzx29g" satisfied condition "running"
    Jun  8 15:09:15.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-3019 exec execpodzx29g -- /bin/sh -x -c nslookup clusterip-service.services-3019.svc.cluster.local'
    Jun  8 15:09:15.366: INFO: stderr: "+ nslookup clusterip-service.services-3019.svc.cluster.local\n"
    Jun  8 15:09:15.366: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-3019.svc.cluster.local\tcanonical name = externalsvc.services-3019.svc.cluster.local.\nName:\texternalsvc.services-3019.svc.cluster.local\nAddress: 10.103.101.237\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3019, will wait for the garbage collector to delete the pods 06/08/23 15:09:15.366
    Jun  8 15:09:15.429: INFO: Deleting ReplicationController externalsvc took: 8.228081ms
    Jun  8 15:09:15.530: INFO: Terminating ReplicationController externalsvc pods took: 100.906316ms
    Jun  8 15:09:17.565: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:09:17.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3019" for this suite. 06/08/23 15:09:17.603
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:09:17.614
Jun  8 15:09:17.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename deployment 06/08/23 15:09:17.616
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:17.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:17.644
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jun  8 15:09:17.649: INFO: Creating deployment "webserver-deployment"
Jun  8 15:09:17.657: INFO: Waiting for observed generation 1
Jun  8 15:09:19.668: INFO: Waiting for all required pods to come up
Jun  8 15:09:19.674: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 06/08/23 15:09:19.674
Jun  8 15:09:19.674: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-vsxgr" in namespace "deployment-7753" to be "running"
Jun  8 15:09:19.674: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2kzc8" in namespace "deployment-7753" to be "running"
Jun  8 15:09:19.674: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-bsfk2" in namespace "deployment-7753" to be "running"
Jun  8 15:09:19.675: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jddtz" in namespace "deployment-7753" to be "running"
Jun  8 15:09:19.675: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-slszz" in namespace "deployment-7753" to be "running"
Jun  8 15:09:19.679: INFO: Pod "webserver-deployment-7f5969cbc7-jddtz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.434501ms
Jun  8 15:09:19.680: INFO: Pod "webserver-deployment-7f5969cbc7-2kzc8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.796613ms
Jun  8 15:09:19.682: INFO: Pod "webserver-deployment-7f5969cbc7-slszz": Phase="Pending", Reason="", readiness=false. Elapsed: 7.025485ms
Jun  8 15:09:19.682: INFO: Pod "webserver-deployment-7f5969cbc7-vsxgr": Phase="Pending", Reason="", readiness=false. Elapsed: 7.296238ms
Jun  8 15:09:19.682: INFO: Pod "webserver-deployment-7f5969cbc7-bsfk2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.385124ms
Jun  8 15:09:21.694: INFO: Pod "webserver-deployment-7f5969cbc7-jddtz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019056915s
Jun  8 15:09:21.701: INFO: Pod "webserver-deployment-7f5969cbc7-2kzc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026514288s
Jun  8 15:09:21.701: INFO: Pod "webserver-deployment-7f5969cbc7-vsxgr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026643931s
Jun  8 15:09:21.701: INFO: Pod "webserver-deployment-7f5969cbc7-slszz": Phase="Running", Reason="", readiness=true. Elapsed: 2.026470895s
Jun  8 15:09:21.701: INFO: Pod "webserver-deployment-7f5969cbc7-slszz" satisfied condition "running"
Jun  8 15:09:21.704: INFO: Pod "webserver-deployment-7f5969cbc7-bsfk2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029890309s
Jun  8 15:09:23.685: INFO: Pod "webserver-deployment-7f5969cbc7-jddtz": Phase="Running", Reason="", readiness=true. Elapsed: 4.010440719s
Jun  8 15:09:23.685: INFO: Pod "webserver-deployment-7f5969cbc7-jddtz" satisfied condition "running"
Jun  8 15:09:23.686: INFO: Pod "webserver-deployment-7f5969cbc7-2kzc8": Phase="Running", Reason="", readiness=true. Elapsed: 4.011953808s
Jun  8 15:09:23.686: INFO: Pod "webserver-deployment-7f5969cbc7-2kzc8" satisfied condition "running"
Jun  8 15:09:23.688: INFO: Pod "webserver-deployment-7f5969cbc7-bsfk2": Phase="Running", Reason="", readiness=true. Elapsed: 4.013133125s
Jun  8 15:09:23.688: INFO: Pod "webserver-deployment-7f5969cbc7-vsxgr": Phase="Running", Reason="", readiness=true. Elapsed: 4.013197061s
Jun  8 15:09:23.688: INFO: Pod "webserver-deployment-7f5969cbc7-vsxgr" satisfied condition "running"
Jun  8 15:09:23.688: INFO: Pod "webserver-deployment-7f5969cbc7-bsfk2" satisfied condition "running"
Jun  8 15:09:23.688: INFO: Waiting for deployment "webserver-deployment" to complete
Jun  8 15:09:23.695: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jun  8 15:09:23.706: INFO: Updating deployment webserver-deployment
Jun  8 15:09:23.706: INFO: Waiting for observed generation 2
Jun  8 15:09:25.715: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun  8 15:09:25.719: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun  8 15:09:25.723: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun  8 15:09:25.733: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun  8 15:09:25.733: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun  8 15:09:25.736: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun  8 15:09:25.743: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun  8 15:09:25.743: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jun  8 15:09:25.753: INFO: Updating deployment webserver-deployment
Jun  8 15:09:25.753: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun  8 15:09:25.762: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun  8 15:09:25.767: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun  8 15:09:25.777: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7753  77838280-a0a8-49e0-b4d0-5044f45f8c07 18106 3 2023-06-08 15:09:17 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003dcbc88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-08 15:09:23 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-06-08 15:09:23 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jun  8 15:09:25.796: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-7753  08858992-f4b4-4848-8c89-1a74276f548a 18109 3 2023-06-08 15:09:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 77838280-a0a8-49e0-b4d0-5044f45f8c07 0xc003f8a157 0xc003f8a158}] [] [{kube-controller-manager Update apps/v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77838280-a0a8-49e0-b4d0-5044f45f8c07\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f8a1f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  8 15:09:25.796: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun  8 15:09:25.796: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-7753  d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 18107 3 2023-06-08 15:09:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 77838280-a0a8-49e0-b4d0-5044f45f8c07 0xc003f8a067 0xc003f8a068}] [] [{kube-controller-manager Update apps/v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77838280-a0a8-49e0-b4d0-5044f45f8c07\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f8a0f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jun  8 15:09:25.818: INFO: Pod "webserver-deployment-7f5969cbc7-2kzc8" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2kzc8 webserver-deployment-7f5969cbc7- deployment-7753  0dc1e61d-805d-450c-9933-7fe44a510063 18010 0 2023-06-08 15:09:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd8587 0xc003cd8588}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l5qn4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l5qn4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-control-plane-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.235,PodIP:10.244.2.2,StartTime:2023-06-08 15:09:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:09:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://566454f30de4d24b5e8e6d765bcbe27c872f069a178490b26d51a50567bda67a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.818: INFO: Pod "webserver-deployment-7f5969cbc7-662xn" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-662xn webserver-deployment-7f5969cbc7- deployment-7753  5a21b136-ff72-4a48-b249-5b608a79576c 18123 0 2023-06-08 15:09:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd8760 0xc003cd8761}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c8btx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c8btx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.215,PodIP:,StartTime:2023-06-08 15:09:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.819: INFO: Pod "webserver-deployment-7f5969cbc7-7cklh" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7cklh webserver-deployment-7f5969cbc7- deployment-7753  a523cd2f-a299-4ee4-8e85-d28844958c85 17941 0 2023-06-08 15:09:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd8917 0xc003cd8918}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p8j4g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p8j4g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-control-plane-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.165,PodIP:10.244.0.5,StartTime:2023-06-08 15:09:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:09:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://3cb28795ca658cc52931202e8af9cc151c51c6c4529e79cacf8af45d62763b07,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.820: INFO: Pod "webserver-deployment-7f5969cbc7-9dktd" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9dktd webserver-deployment-7f5969cbc7- deployment-7753  792df3ba-d355-439b-b3d9-10ec34c8b39d 17931 0 2023-06-08 15:09:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd8af0 0xc003cd8af1}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lkcft,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lkcft,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.90,PodIP:10.244.4.7,StartTime:2023-06-08 15:09:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:09:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://b2adf346592e4cfdde99f4459925b5ba02328a093e6c5d077382785587e1c1dd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.820: INFO: Pod "webserver-deployment-7f5969cbc7-bsfk2" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bsfk2 webserver-deployment-7f5969cbc7- deployment-7753  d345de5c-2ee4-4ee4-a9fb-a64f6afdef49 18013 0 2023-06-08 15:09:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd8cc0 0xc003cd8cc1}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6zb8d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6zb8d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-control-plane-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.235,PodIP:10.244.2.3,StartTime:2023-06-08 15:09:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:09:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://68461ea9e5c6978a197573c2f22eeafac808c56c0aacefa621776f9bfc17c93b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.821: INFO: Pod "webserver-deployment-7f5969cbc7-d5qc6" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-d5qc6 webserver-deployment-7f5969cbc7- deployment-7753  4fc62484-94da-4c6c-b2fe-1c0a5f180fa4 18129 0 2023-06-08 15:09:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd8e90 0xc003cd8e91}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xfkp7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xfkp7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.821: INFO: Pod "webserver-deployment-7f5969cbc7-g8pvb" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-g8pvb webserver-deployment-7f5969cbc7- deployment-7753  120d5fed-9800-41c8-9a99-81c5a7084b2b 18128 0 2023-06-08 15:09:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd8fc7 0xc003cd8fc8}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-55wxv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-55wxv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.822: INFO: Pod "webserver-deployment-7f5969cbc7-hnqq7" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hnqq7 webserver-deployment-7f5969cbc7- deployment-7753  bdb6f1ce-ea7e-42fd-9b70-5ff6868fb6d0 17934 0 2023-06-08 15:09:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd9107 0xc003cd9108}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-czvs5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-czvs5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.90,PodIP:10.244.4.8,StartTime:2023-06-08 15:09:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:09:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://bc2404c593556fee307cdd505ddcc4c3a3ca429d9d9d0d406435ef6e1e6f044b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.822: INFO: Pod "webserver-deployment-7f5969cbc7-jddtz" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jddtz webserver-deployment-7f5969cbc7- deployment-7753  3005777f-04f2-4a4e-8dad-96965c00d951 18018 0 2023-06-08 15:09:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd92e0 0xc003cd92e1}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zh5bx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zh5bx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-control-plane-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.41,PodIP:10.244.1.7,StartTime:2023-06-08 15:09:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:09:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://4aebf26c73e4fb3e1f5fb2d8893f7809e5d61cef14244184d3e19c92f99cf5ec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.823: INFO: Pod "webserver-deployment-7f5969cbc7-jrf7x" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jrf7x webserver-deployment-7f5969cbc7- deployment-7753  a02fe18a-05b8-4728-a08c-8c30a3316dec 18126 0 2023-06-08 15:09:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd94b0 0xc003cd94b1}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h64h8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h64h8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.823: INFO: Pod "webserver-deployment-7f5969cbc7-kx4h6" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kx4h6 webserver-deployment-7f5969cbc7- deployment-7753  a35b61fd-7785-4d45-9810-774dd4b6e58d 18127 0 2023-06-08 15:09:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd95e7 0xc003cd95e8}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tzjzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tzjzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.823: INFO: Pod "webserver-deployment-7f5969cbc7-lh67m" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lh67m webserver-deployment-7f5969cbc7- deployment-7753  0fc99380-773a-4402-8cd4-ba4140856ff0 18121 0 2023-06-08 15:09:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd9727 0xc003cd9728}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jfjxt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jfjxt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.824: INFO: Pod "webserver-deployment-7f5969cbc7-slszz" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-slszz webserver-deployment-7f5969cbc7- deployment-7753  44a1b527-bb7b-4e6a-9d98-3a2334712c0d 17954 0 2023-06-08 15:09:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd9880 0xc003cd9881}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t8zht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t8zht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-control-plane-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.165,PodIP:10.244.0.6,StartTime:2023-06-08 15:09:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:09:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://bec25261cba8f37fb56ed970ec15a38afd93597f2ac0840722fe071e3503fca6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.824: INFO: Pod "webserver-deployment-7f5969cbc7-sws97" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sws97 webserver-deployment-7f5969cbc7- deployment-7753  7dd27e00-1ce3-4fc2-874a-147e3b7b8442 18120 0 2023-06-08 15:09:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd9a50 0xc003cd9a51}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2xfv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2xfv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.824: INFO: Pod "webserver-deployment-7f5969cbc7-vsxgr" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vsxgr webserver-deployment-7f5969cbc7- deployment-7753  0de12494-bfdd-4b42-83ee-b2ad7408536e 17974 0 2023-06-08 15:09:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd9ba0 0xc003cd9ba1}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4rbfz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4rbfz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-control-plane-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.41,PodIP:10.244.1.6,StartTime:2023-06-08 15:09:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:09:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://67bf8a5486137547c8cc2634ce7b6af8878397b26379dde8efcf4a269b779143,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.825: INFO: Pod "webserver-deployment-d9f79cb5-4z7jd" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4z7jd webserver-deployment-d9f79cb5- deployment-7753  67b3d97d-2be7-439f-a501-b6260405886a 18122 0 2023-06-08 15:09:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 08858992-f4b4-4848-8c89-1a74276f548a 0xc003cd9d5f 0xc003cd9d70}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08858992-f4b4-4848-8c89-1a74276f548a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zvhv4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zvhv4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.825: INFO: Pod "webserver-deployment-d9f79cb5-7r4zc" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7r4zc webserver-deployment-d9f79cb5- deployment-7753  08f6c5ee-ffa6-4243-bce9-28d641405b21 18092 0 2023-06-08 15:09:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 08858992-f4b4-4848-8c89-1a74276f548a 0xc003cd9ebf 0xc003cd9ed0}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08858992-f4b4-4848-8c89-1a74276f548a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4r7v6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4r7v6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.215,PodIP:10.244.3.24,StartTime:2023-06-08 15:09:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.825: INFO: Pod "webserver-deployment-d9f79cb5-92b4m" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-92b4m webserver-deployment-d9f79cb5- deployment-7753  40580466-00c8-4cd2-b511-66237c661856 18134 0 2023-06-08 15:09:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 08858992-f4b4-4848-8c89-1a74276f548a 0xc00406a0bf 0xc00406a0d0}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08858992-f4b4-4848-8c89-1a74276f548a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zwp5s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zwp5s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-control-plane-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.826: INFO: Pod "webserver-deployment-d9f79cb5-g7cd8" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-g7cd8 webserver-deployment-d9f79cb5- deployment-7753  2c7be4ee-8dd4-47e1-afab-04205c9ef932 18105 0 2023-06-08 15:09:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 08858992-f4b4-4848-8c89-1a74276f548a 0xc00406a21f 0xc00406a230}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08858992-f4b4-4848-8c89-1a74276f548a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7wvrx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7wvrx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-control-plane-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.41,PodIP:10.244.1.8,StartTime:2023-06-08 15:09:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.826: INFO: Pod "webserver-deployment-d9f79cb5-hh946" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-hh946 webserver-deployment-d9f79cb5- deployment-7753  ccf409d9-f0a4-418b-b7f4-06ed27fa4354 18097 0 2023-06-08 15:09:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 08858992-f4b4-4848-8c89-1a74276f548a 0xc00406a41f 0xc00406a430}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08858992-f4b4-4848-8c89-1a74276f548a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sb5sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sb5sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-control-plane-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.235,PodIP:10.244.2.4,StartTime:2023-06-08 15:09:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.827: INFO: Pod "webserver-deployment-d9f79cb5-qp5bw" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-qp5bw webserver-deployment-d9f79cb5- deployment-7753  a97b8133-94f8-48be-9ff0-3a4c0f707eee 18124 0 2023-06-08 15:09:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 08858992-f4b4-4848-8c89-1a74276f548a 0xc00406a61f 0xc00406a630}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08858992-f4b4-4848-8c89-1a74276f548a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l7xb6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l7xb6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.827: INFO: Pod "webserver-deployment-d9f79cb5-ssnw2" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ssnw2 webserver-deployment-d9f79cb5- deployment-7753  cc994b9d-c88b-45af-ab36-b387ec1b576f 18044 0 2023-06-08 15:09:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 08858992-f4b4-4848-8c89-1a74276f548a 0xc00406a77f 0xc00406a790}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08858992-f4b4-4848-8c89-1a74276f548a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mwxts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mwxts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-control-plane-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.165,PodIP:,StartTime:2023-06-08 15:09:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:09:25.827: INFO: Pod "webserver-deployment-d9f79cb5-wkjw9" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wkjw9 webserver-deployment-d9f79cb5- deployment-7753  27605752-ad2d-46eb-9bb0-99cf1efe5fab 18102 0 2023-06-08 15:09:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 08858992-f4b4-4848-8c89-1a74276f548a 0xc00406a967 0xc00406a968}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08858992-f4b4-4848-8c89-1a74276f548a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sqnqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sqnqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.90,PodIP:10.244.4.9,StartTime:2023-06-08 15:09:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun  8 15:09:25.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7753" for this suite. 06/08/23 15:09:25.875
------------------------------
• [SLOW TEST] [8.301 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:09:17.614
    Jun  8 15:09:17.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename deployment 06/08/23 15:09:17.616
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:17.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:17.644
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jun  8 15:09:17.649: INFO: Creating deployment "webserver-deployment"
    Jun  8 15:09:17.657: INFO: Waiting for observed generation 1
    Jun  8 15:09:19.668: INFO: Waiting for all required pods to come up
    Jun  8 15:09:19.674: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 06/08/23 15:09:19.674
    Jun  8 15:09:19.674: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-vsxgr" in namespace "deployment-7753" to be "running"
    Jun  8 15:09:19.674: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2kzc8" in namespace "deployment-7753" to be "running"
    Jun  8 15:09:19.674: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-bsfk2" in namespace "deployment-7753" to be "running"
    Jun  8 15:09:19.675: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jddtz" in namespace "deployment-7753" to be "running"
    Jun  8 15:09:19.675: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-slszz" in namespace "deployment-7753" to be "running"
    Jun  8 15:09:19.679: INFO: Pod "webserver-deployment-7f5969cbc7-jddtz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.434501ms
    Jun  8 15:09:19.680: INFO: Pod "webserver-deployment-7f5969cbc7-2kzc8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.796613ms
    Jun  8 15:09:19.682: INFO: Pod "webserver-deployment-7f5969cbc7-slszz": Phase="Pending", Reason="", readiness=false. Elapsed: 7.025485ms
    Jun  8 15:09:19.682: INFO: Pod "webserver-deployment-7f5969cbc7-vsxgr": Phase="Pending", Reason="", readiness=false. Elapsed: 7.296238ms
    Jun  8 15:09:19.682: INFO: Pod "webserver-deployment-7f5969cbc7-bsfk2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.385124ms
    Jun  8 15:09:21.694: INFO: Pod "webserver-deployment-7f5969cbc7-jddtz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019056915s
    Jun  8 15:09:21.701: INFO: Pod "webserver-deployment-7f5969cbc7-2kzc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026514288s
    Jun  8 15:09:21.701: INFO: Pod "webserver-deployment-7f5969cbc7-vsxgr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026643931s
    Jun  8 15:09:21.701: INFO: Pod "webserver-deployment-7f5969cbc7-slszz": Phase="Running", Reason="", readiness=true. Elapsed: 2.026470895s
    Jun  8 15:09:21.701: INFO: Pod "webserver-deployment-7f5969cbc7-slszz" satisfied condition "running"
    Jun  8 15:09:21.704: INFO: Pod "webserver-deployment-7f5969cbc7-bsfk2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029890309s
    Jun  8 15:09:23.685: INFO: Pod "webserver-deployment-7f5969cbc7-jddtz": Phase="Running", Reason="", readiness=true. Elapsed: 4.010440719s
    Jun  8 15:09:23.685: INFO: Pod "webserver-deployment-7f5969cbc7-jddtz" satisfied condition "running"
    Jun  8 15:09:23.686: INFO: Pod "webserver-deployment-7f5969cbc7-2kzc8": Phase="Running", Reason="", readiness=true. Elapsed: 4.011953808s
    Jun  8 15:09:23.686: INFO: Pod "webserver-deployment-7f5969cbc7-2kzc8" satisfied condition "running"
    Jun  8 15:09:23.688: INFO: Pod "webserver-deployment-7f5969cbc7-bsfk2": Phase="Running", Reason="", readiness=true. Elapsed: 4.013133125s
    Jun  8 15:09:23.688: INFO: Pod "webserver-deployment-7f5969cbc7-vsxgr": Phase="Running", Reason="", readiness=true. Elapsed: 4.013197061s
    Jun  8 15:09:23.688: INFO: Pod "webserver-deployment-7f5969cbc7-vsxgr" satisfied condition "running"
    Jun  8 15:09:23.688: INFO: Pod "webserver-deployment-7f5969cbc7-bsfk2" satisfied condition "running"
    Jun  8 15:09:23.688: INFO: Waiting for deployment "webserver-deployment" to complete
    Jun  8 15:09:23.695: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jun  8 15:09:23.706: INFO: Updating deployment webserver-deployment
    Jun  8 15:09:23.706: INFO: Waiting for observed generation 2
    Jun  8 15:09:25.715: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jun  8 15:09:25.719: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jun  8 15:09:25.723: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jun  8 15:09:25.733: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jun  8 15:09:25.733: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jun  8 15:09:25.736: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jun  8 15:09:25.743: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jun  8 15:09:25.743: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jun  8 15:09:25.753: INFO: Updating deployment webserver-deployment
    Jun  8 15:09:25.753: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jun  8 15:09:25.762: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jun  8 15:09:25.767: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun  8 15:09:25.777: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-7753  77838280-a0a8-49e0-b4d0-5044f45f8c07 18106 3 2023-06-08 15:09:17 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003dcbc88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-08 15:09:23 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-06-08 15:09:23 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jun  8 15:09:25.796: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-7753  08858992-f4b4-4848-8c89-1a74276f548a 18109 3 2023-06-08 15:09:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 77838280-a0a8-49e0-b4d0-5044f45f8c07 0xc003f8a157 0xc003f8a158}] [] [{kube-controller-manager Update apps/v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77838280-a0a8-49e0-b4d0-5044f45f8c07\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f8a1f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun  8 15:09:25.796: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jun  8 15:09:25.796: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-7753  d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 18107 3 2023-06-08 15:09:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 77838280-a0a8-49e0-b4d0-5044f45f8c07 0xc003f8a067 0xc003f8a068}] [] [{kube-controller-manager Update apps/v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77838280-a0a8-49e0-b4d0-5044f45f8c07\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f8a0f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jun  8 15:09:25.818: INFO: Pod "webserver-deployment-7f5969cbc7-2kzc8" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2kzc8 webserver-deployment-7f5969cbc7- deployment-7753  0dc1e61d-805d-450c-9933-7fe44a510063 18010 0 2023-06-08 15:09:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd8587 0xc003cd8588}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l5qn4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l5qn4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-control-plane-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.235,PodIP:10.244.2.2,StartTime:2023-06-08 15:09:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:09:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://566454f30de4d24b5e8e6d765bcbe27c872f069a178490b26d51a50567bda67a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.818: INFO: Pod "webserver-deployment-7f5969cbc7-662xn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-662xn webserver-deployment-7f5969cbc7- deployment-7753  5a21b136-ff72-4a48-b249-5b608a79576c 18123 0 2023-06-08 15:09:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd8760 0xc003cd8761}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c8btx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c8btx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.215,PodIP:,StartTime:2023-06-08 15:09:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.819: INFO: Pod "webserver-deployment-7f5969cbc7-7cklh" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7cklh webserver-deployment-7f5969cbc7- deployment-7753  a523cd2f-a299-4ee4-8e85-d28844958c85 17941 0 2023-06-08 15:09:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd8917 0xc003cd8918}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p8j4g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p8j4g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-control-plane-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.165,PodIP:10.244.0.5,StartTime:2023-06-08 15:09:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:09:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://3cb28795ca658cc52931202e8af9cc151c51c6c4529e79cacf8af45d62763b07,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.820: INFO: Pod "webserver-deployment-7f5969cbc7-9dktd" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9dktd webserver-deployment-7f5969cbc7- deployment-7753  792df3ba-d355-439b-b3d9-10ec34c8b39d 17931 0 2023-06-08 15:09:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd8af0 0xc003cd8af1}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lkcft,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lkcft,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.90,PodIP:10.244.4.7,StartTime:2023-06-08 15:09:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:09:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://b2adf346592e4cfdde99f4459925b5ba02328a093e6c5d077382785587e1c1dd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.820: INFO: Pod "webserver-deployment-7f5969cbc7-bsfk2" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bsfk2 webserver-deployment-7f5969cbc7- deployment-7753  d345de5c-2ee4-4ee4-a9fb-a64f6afdef49 18013 0 2023-06-08 15:09:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd8cc0 0xc003cd8cc1}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6zb8d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6zb8d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-control-plane-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.235,PodIP:10.244.2.3,StartTime:2023-06-08 15:09:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:09:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://68461ea9e5c6978a197573c2f22eeafac808c56c0aacefa621776f9bfc17c93b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.821: INFO: Pod "webserver-deployment-7f5969cbc7-d5qc6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-d5qc6 webserver-deployment-7f5969cbc7- deployment-7753  4fc62484-94da-4c6c-b2fe-1c0a5f180fa4 18129 0 2023-06-08 15:09:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd8e90 0xc003cd8e91}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xfkp7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xfkp7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.821: INFO: Pod "webserver-deployment-7f5969cbc7-g8pvb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-g8pvb webserver-deployment-7f5969cbc7- deployment-7753  120d5fed-9800-41c8-9a99-81c5a7084b2b 18128 0 2023-06-08 15:09:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd8fc7 0xc003cd8fc8}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-55wxv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-55wxv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.822: INFO: Pod "webserver-deployment-7f5969cbc7-hnqq7" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hnqq7 webserver-deployment-7f5969cbc7- deployment-7753  bdb6f1ce-ea7e-42fd-9b70-5ff6868fb6d0 17934 0 2023-06-08 15:09:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd9107 0xc003cd9108}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-czvs5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-czvs5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.90,PodIP:10.244.4.8,StartTime:2023-06-08 15:09:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:09:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://bc2404c593556fee307cdd505ddcc4c3a3ca429d9d9d0d406435ef6e1e6f044b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.822: INFO: Pod "webserver-deployment-7f5969cbc7-jddtz" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jddtz webserver-deployment-7f5969cbc7- deployment-7753  3005777f-04f2-4a4e-8dad-96965c00d951 18018 0 2023-06-08 15:09:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd92e0 0xc003cd92e1}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zh5bx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zh5bx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-control-plane-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.41,PodIP:10.244.1.7,StartTime:2023-06-08 15:09:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:09:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://4aebf26c73e4fb3e1f5fb2d8893f7809e5d61cef14244184d3e19c92f99cf5ec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.823: INFO: Pod "webserver-deployment-7f5969cbc7-jrf7x" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jrf7x webserver-deployment-7f5969cbc7- deployment-7753  a02fe18a-05b8-4728-a08c-8c30a3316dec 18126 0 2023-06-08 15:09:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd94b0 0xc003cd94b1}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h64h8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h64h8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.823: INFO: Pod "webserver-deployment-7f5969cbc7-kx4h6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kx4h6 webserver-deployment-7f5969cbc7- deployment-7753  a35b61fd-7785-4d45-9810-774dd4b6e58d 18127 0 2023-06-08 15:09:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd95e7 0xc003cd95e8}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tzjzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tzjzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.823: INFO: Pod "webserver-deployment-7f5969cbc7-lh67m" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lh67m webserver-deployment-7f5969cbc7- deployment-7753  0fc99380-773a-4402-8cd4-ba4140856ff0 18121 0 2023-06-08 15:09:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd9727 0xc003cd9728}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jfjxt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jfjxt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.824: INFO: Pod "webserver-deployment-7f5969cbc7-slszz" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-slszz webserver-deployment-7f5969cbc7- deployment-7753  44a1b527-bb7b-4e6a-9d98-3a2334712c0d 17954 0 2023-06-08 15:09:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd9880 0xc003cd9881}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t8zht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t8zht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-control-plane-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.165,PodIP:10.244.0.6,StartTime:2023-06-08 15:09:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:09:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://bec25261cba8f37fb56ed970ec15a38afd93597f2ac0840722fe071e3503fca6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.824: INFO: Pod "webserver-deployment-7f5969cbc7-sws97" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sws97 webserver-deployment-7f5969cbc7- deployment-7753  7dd27e00-1ce3-4fc2-874a-147e3b7b8442 18120 0 2023-06-08 15:09:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd9a50 0xc003cd9a51}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2xfv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2xfv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.824: INFO: Pod "webserver-deployment-7f5969cbc7-vsxgr" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vsxgr webserver-deployment-7f5969cbc7- deployment-7753  0de12494-bfdd-4b42-83ee-b2ad7408536e 17974 0 2023-06-08 15:09:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 d7d688a7-69b7-44d1-ab6f-cf1fbef2143a 0xc003cd9ba0 0xc003cd9ba1}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d688a7-69b7-44d1-ab6f-cf1fbef2143a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4rbfz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4rbfz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-control-plane-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.41,PodIP:10.244.1.6,StartTime:2023-06-08 15:09:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:09:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://67bf8a5486137547c8cc2634ce7b6af8878397b26379dde8efcf4a269b779143,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.825: INFO: Pod "webserver-deployment-d9f79cb5-4z7jd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4z7jd webserver-deployment-d9f79cb5- deployment-7753  67b3d97d-2be7-439f-a501-b6260405886a 18122 0 2023-06-08 15:09:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 08858992-f4b4-4848-8c89-1a74276f548a 0xc003cd9d5f 0xc003cd9d70}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08858992-f4b4-4848-8c89-1a74276f548a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zvhv4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zvhv4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.825: INFO: Pod "webserver-deployment-d9f79cb5-7r4zc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7r4zc webserver-deployment-d9f79cb5- deployment-7753  08f6c5ee-ffa6-4243-bce9-28d641405b21 18092 0 2023-06-08 15:09:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 08858992-f4b4-4848-8c89-1a74276f548a 0xc003cd9ebf 0xc003cd9ed0}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08858992-f4b4-4848-8c89-1a74276f548a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4r7v6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4r7v6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.215,PodIP:10.244.3.24,StartTime:2023-06-08 15:09:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.825: INFO: Pod "webserver-deployment-d9f79cb5-92b4m" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-92b4m webserver-deployment-d9f79cb5- deployment-7753  40580466-00c8-4cd2-b511-66237c661856 18134 0 2023-06-08 15:09:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 08858992-f4b4-4848-8c89-1a74276f548a 0xc00406a0bf 0xc00406a0d0}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08858992-f4b4-4848-8c89-1a74276f548a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zwp5s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zwp5s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-control-plane-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.826: INFO: Pod "webserver-deployment-d9f79cb5-g7cd8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-g7cd8 webserver-deployment-d9f79cb5- deployment-7753  2c7be4ee-8dd4-47e1-afab-04205c9ef932 18105 0 2023-06-08 15:09:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 08858992-f4b4-4848-8c89-1a74276f548a 0xc00406a21f 0xc00406a230}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08858992-f4b4-4848-8c89-1a74276f548a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7wvrx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7wvrx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-control-plane-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.41,PodIP:10.244.1.8,StartTime:2023-06-08 15:09:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.826: INFO: Pod "webserver-deployment-d9f79cb5-hh946" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-hh946 webserver-deployment-d9f79cb5- deployment-7753  ccf409d9-f0a4-418b-b7f4-06ed27fa4354 18097 0 2023-06-08 15:09:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 08858992-f4b4-4848-8c89-1a74276f548a 0xc00406a41f 0xc00406a430}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08858992-f4b4-4848-8c89-1a74276f548a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sb5sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sb5sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-control-plane-003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.235,PodIP:10.244.2.4,StartTime:2023-06-08 15:09:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.827: INFO: Pod "webserver-deployment-d9f79cb5-qp5bw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-qp5bw webserver-deployment-d9f79cb5- deployment-7753  a97b8133-94f8-48be-9ff0-3a4c0f707eee 18124 0 2023-06-08 15:09:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 08858992-f4b4-4848-8c89-1a74276f548a 0xc00406a61f 0xc00406a630}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08858992-f4b4-4848-8c89-1a74276f548a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l7xb6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l7xb6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.827: INFO: Pod "webserver-deployment-d9f79cb5-ssnw2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ssnw2 webserver-deployment-d9f79cb5- deployment-7753  cc994b9d-c88b-45af-ab36-b387ec1b576f 18044 0 2023-06-08 15:09:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 08858992-f4b4-4848-8c89-1a74276f548a 0xc00406a77f 0xc00406a790}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08858992-f4b4-4848-8c89-1a74276f548a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mwxts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mwxts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-control-plane-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.165,PodIP:,StartTime:2023-06-08 15:09:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:09:25.827: INFO: Pod "webserver-deployment-d9f79cb5-wkjw9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wkjw9 webserver-deployment-d9f79cb5- deployment-7753  27605752-ad2d-46eb-9bb0-99cf1efe5fab 18102 0 2023-06-08 15:09:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 08858992-f4b4-4848-8c89-1a74276f548a 0xc00406a967 0xc00406a968}] [] [{kube-controller-manager Update v1 2023-06-08 15:09:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08858992-f4b4-4848-8c89-1a74276f548a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:09:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sqnqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sqnqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:09:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.90,PodIP:10.244.4.9,StartTime:2023-06-08 15:09:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:09:25.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7753" for this suite. 06/08/23 15:09:25.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:09:25.919
Jun  8 15:09:25.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename svcaccounts 06/08/23 15:09:25.92
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:25.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:25.997
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Jun  8 15:09:26.009: INFO: Got root ca configmap in namespace "svcaccounts-1868"
Jun  8 15:09:26.031: INFO: Deleted root ca configmap in namespace "svcaccounts-1868"
STEP: waiting for a new root ca configmap created 06/08/23 15:09:26.532
Jun  8 15:09:26.536: INFO: Recreated root ca configmap in namespace "svcaccounts-1868"
Jun  8 15:09:26.541: INFO: Updated root ca configmap in namespace "svcaccounts-1868"
STEP: waiting for the root ca configmap reconciled 06/08/23 15:09:27.042
Jun  8 15:09:27.047: INFO: Reconciled root ca configmap in namespace "svcaccounts-1868"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jun  8 15:09:27.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1868" for this suite. 06/08/23 15:09:27.054
------------------------------
• [1.156 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:09:25.919
    Jun  8 15:09:25.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename svcaccounts 06/08/23 15:09:25.92
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:25.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:25.997
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Jun  8 15:09:26.009: INFO: Got root ca configmap in namespace "svcaccounts-1868"
    Jun  8 15:09:26.031: INFO: Deleted root ca configmap in namespace "svcaccounts-1868"
    STEP: waiting for a new root ca configmap created 06/08/23 15:09:26.532
    Jun  8 15:09:26.536: INFO: Recreated root ca configmap in namespace "svcaccounts-1868"
    Jun  8 15:09:26.541: INFO: Updated root ca configmap in namespace "svcaccounts-1868"
    STEP: waiting for the root ca configmap reconciled 06/08/23 15:09:27.042
    Jun  8 15:09:27.047: INFO: Reconciled root ca configmap in namespace "svcaccounts-1868"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:09:27.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1868" for this suite. 06/08/23 15:09:27.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:09:27.077
Jun  8 15:09:27.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename runtimeclass 06/08/23 15:09:27.078
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:27.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:27.769
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jun  8 15:09:27.788: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-714 to be scheduled
Jun  8 15:09:27.792: INFO: 1 pods are not scheduled: [runtimeclass-714/test-runtimeclass-runtimeclass-714-preconfigured-handler-pwr6b(b4f33b62-25af-44f6-9771-91a5aa9bc1a2)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jun  8 15:09:29.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-714" for this suite. 06/08/23 15:09:29.81
------------------------------
• [2.741 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:09:27.077
    Jun  8 15:09:27.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename runtimeclass 06/08/23 15:09:27.078
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:27.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:27.769
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jun  8 15:09:27.788: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-714 to be scheduled
    Jun  8 15:09:27.792: INFO: 1 pods are not scheduled: [runtimeclass-714/test-runtimeclass-runtimeclass-714-preconfigured-handler-pwr6b(b4f33b62-25af-44f6-9771-91a5aa9bc1a2)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:09:29.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-714" for this suite. 06/08/23 15:09:29.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:09:29.82
Jun  8 15:09:29.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename webhook 06/08/23 15:09:29.821
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:29.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:29.844
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/08/23 15:09:29.861
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 15:09:30.863
STEP: Deploying the webhook pod 06/08/23 15:09:30.872
STEP: Wait for the deployment to be ready 06/08/23 15:09:30.885
Jun  8 15:09:30.895: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 06/08/23 15:09:32.909
STEP: Verifying the service has paired with the endpoint 06/08/23 15:09:32.924
Jun  8 15:09:33.925: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/08/23 15:09:33.93
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/08/23 15:09:33.949
STEP: Creating a dummy validating-webhook-configuration object 06/08/23 15:09:33.964
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 06/08/23 15:09:33.974
STEP: Creating a dummy mutating-webhook-configuration object 06/08/23 15:09:33.981
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 06/08/23 15:09:33.989
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:09:34.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8057" for this suite. 06/08/23 15:09:34.084
STEP: Destroying namespace "webhook-8057-markers" for this suite. 06/08/23 15:09:34.095
------------------------------
• [4.286 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:09:29.82
    Jun  8 15:09:29.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename webhook 06/08/23 15:09:29.821
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:29.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:29.844
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/08/23 15:09:29.861
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 15:09:30.863
    STEP: Deploying the webhook pod 06/08/23 15:09:30.872
    STEP: Wait for the deployment to be ready 06/08/23 15:09:30.885
    Jun  8 15:09:30.895: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 06/08/23 15:09:32.909
    STEP: Verifying the service has paired with the endpoint 06/08/23 15:09:32.924
    Jun  8 15:09:33.925: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/08/23 15:09:33.93
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/08/23 15:09:33.949
    STEP: Creating a dummy validating-webhook-configuration object 06/08/23 15:09:33.964
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 06/08/23 15:09:33.974
    STEP: Creating a dummy mutating-webhook-configuration object 06/08/23 15:09:33.981
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 06/08/23 15:09:33.989
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:09:34.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8057" for this suite. 06/08/23 15:09:34.084
    STEP: Destroying namespace "webhook-8057-markers" for this suite. 06/08/23 15:09:34.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:09:34.108
Jun  8 15:09:34.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename emptydir 06/08/23 15:09:34.11
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:34.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:34.132
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 06/08/23 15:09:34.136
Jun  8 15:09:34.154: INFO: Waiting up to 5m0s for pod "pod-f47fe9a4-1242-44a8-9837-db5f21a002cf" in namespace "emptydir-9725" to be "Succeeded or Failed"
Jun  8 15:09:34.165: INFO: Pod "pod-f47fe9a4-1242-44a8-9837-db5f21a002cf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.817874ms
Jun  8 15:09:36.175: INFO: Pod "pod-f47fe9a4-1242-44a8-9837-db5f21a002cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02043407s
Jun  8 15:09:38.171: INFO: Pod "pod-f47fe9a4-1242-44a8-9837-db5f21a002cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016745434s
STEP: Saw pod success 06/08/23 15:09:38.171
Jun  8 15:09:38.171: INFO: Pod "pod-f47fe9a4-1242-44a8-9837-db5f21a002cf" satisfied condition "Succeeded or Failed"
Jun  8 15:09:38.176: INFO: Trying to get logs from node chl8tf-worker-002 pod pod-f47fe9a4-1242-44a8-9837-db5f21a002cf container test-container: <nil>
STEP: delete the pod 06/08/23 15:09:38.192
Jun  8 15:09:38.206: INFO: Waiting for pod pod-f47fe9a4-1242-44a8-9837-db5f21a002cf to disappear
Jun  8 15:09:38.209: INFO: Pod pod-f47fe9a4-1242-44a8-9837-db5f21a002cf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  8 15:09:38.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9725" for this suite. 06/08/23 15:09:38.214
------------------------------
• [4.113 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:09:34.108
    Jun  8 15:09:34.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename emptydir 06/08/23 15:09:34.11
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:34.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:34.132
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 06/08/23 15:09:34.136
    Jun  8 15:09:34.154: INFO: Waiting up to 5m0s for pod "pod-f47fe9a4-1242-44a8-9837-db5f21a002cf" in namespace "emptydir-9725" to be "Succeeded or Failed"
    Jun  8 15:09:34.165: INFO: Pod "pod-f47fe9a4-1242-44a8-9837-db5f21a002cf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.817874ms
    Jun  8 15:09:36.175: INFO: Pod "pod-f47fe9a4-1242-44a8-9837-db5f21a002cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02043407s
    Jun  8 15:09:38.171: INFO: Pod "pod-f47fe9a4-1242-44a8-9837-db5f21a002cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016745434s
    STEP: Saw pod success 06/08/23 15:09:38.171
    Jun  8 15:09:38.171: INFO: Pod "pod-f47fe9a4-1242-44a8-9837-db5f21a002cf" satisfied condition "Succeeded or Failed"
    Jun  8 15:09:38.176: INFO: Trying to get logs from node chl8tf-worker-002 pod pod-f47fe9a4-1242-44a8-9837-db5f21a002cf container test-container: <nil>
    STEP: delete the pod 06/08/23 15:09:38.192
    Jun  8 15:09:38.206: INFO: Waiting for pod pod-f47fe9a4-1242-44a8-9837-db5f21a002cf to disappear
    Jun  8 15:09:38.209: INFO: Pod pod-f47fe9a4-1242-44a8-9837-db5f21a002cf no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:09:38.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9725" for this suite. 06/08/23 15:09:38.214
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:09:38.224
Jun  8 15:09:38.224: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename pods 06/08/23 15:09:38.225
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:38.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:38.243
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 06/08/23 15:09:38.246
Jun  8 15:09:38.255: INFO: Waiting up to 5m0s for pod "pod-hostip-58f6f093-c51c-4900-836d-9c3e24a29a18" in namespace "pods-3588" to be "running and ready"
Jun  8 15:09:38.259: INFO: Pod "pod-hostip-58f6f093-c51c-4900-836d-9c3e24a29a18": Phase="Pending", Reason="", readiness=false. Elapsed: 4.165647ms
Jun  8 15:09:38.259: INFO: The phase of Pod pod-hostip-58f6f093-c51c-4900-836d-9c3e24a29a18 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:09:40.263: INFO: Pod "pod-hostip-58f6f093-c51c-4900-836d-9c3e24a29a18": Phase="Running", Reason="", readiness=true. Elapsed: 2.008513595s
Jun  8 15:09:40.263: INFO: The phase of Pod pod-hostip-58f6f093-c51c-4900-836d-9c3e24a29a18 is Running (Ready = true)
Jun  8 15:09:40.263: INFO: Pod "pod-hostip-58f6f093-c51c-4900-836d-9c3e24a29a18" satisfied condition "running and ready"
Jun  8 15:09:40.271: INFO: Pod pod-hostip-58f6f093-c51c-4900-836d-9c3e24a29a18 has hostIP: 100.100.236.215
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun  8 15:09:40.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3588" for this suite. 06/08/23 15:09:40.277
------------------------------
• [2.060 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:09:38.224
    Jun  8 15:09:38.224: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename pods 06/08/23 15:09:38.225
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:38.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:38.243
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 06/08/23 15:09:38.246
    Jun  8 15:09:38.255: INFO: Waiting up to 5m0s for pod "pod-hostip-58f6f093-c51c-4900-836d-9c3e24a29a18" in namespace "pods-3588" to be "running and ready"
    Jun  8 15:09:38.259: INFO: Pod "pod-hostip-58f6f093-c51c-4900-836d-9c3e24a29a18": Phase="Pending", Reason="", readiness=false. Elapsed: 4.165647ms
    Jun  8 15:09:38.259: INFO: The phase of Pod pod-hostip-58f6f093-c51c-4900-836d-9c3e24a29a18 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:09:40.263: INFO: Pod "pod-hostip-58f6f093-c51c-4900-836d-9c3e24a29a18": Phase="Running", Reason="", readiness=true. Elapsed: 2.008513595s
    Jun  8 15:09:40.263: INFO: The phase of Pod pod-hostip-58f6f093-c51c-4900-836d-9c3e24a29a18 is Running (Ready = true)
    Jun  8 15:09:40.263: INFO: Pod "pod-hostip-58f6f093-c51c-4900-836d-9c3e24a29a18" satisfied condition "running and ready"
    Jun  8 15:09:40.271: INFO: Pod pod-hostip-58f6f093-c51c-4900-836d-9c3e24a29a18 has hostIP: 100.100.236.215
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:09:40.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3588" for this suite. 06/08/23 15:09:40.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:09:40.285
Jun  8 15:09:40.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 15:09:40.286
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:40.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:40.305
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-2caf674f-580a-48d3-9b94-26ce24a196d4 06/08/23 15:09:40.314
STEP: Creating secret with name s-test-opt-upd-9fc2754e-7168-42aa-ba93-6c9e36d2beea 06/08/23 15:09:40.32
STEP: Creating the pod 06/08/23 15:09:40.325
Jun  8 15:09:40.338: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3e65f72e-72e8-4b71-9a29-08c9a3eeddf9" in namespace "projected-8793" to be "running and ready"
Jun  8 15:09:40.342: INFO: Pod "pod-projected-secrets-3e65f72e-72e8-4b71-9a29-08c9a3eeddf9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.967887ms
Jun  8 15:09:40.342: INFO: The phase of Pod pod-projected-secrets-3e65f72e-72e8-4b71-9a29-08c9a3eeddf9 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:09:42.348: INFO: Pod "pod-projected-secrets-3e65f72e-72e8-4b71-9a29-08c9a3eeddf9": Phase="Running", Reason="", readiness=true. Elapsed: 2.010002312s
Jun  8 15:09:42.348: INFO: The phase of Pod pod-projected-secrets-3e65f72e-72e8-4b71-9a29-08c9a3eeddf9 is Running (Ready = true)
Jun  8 15:09:42.348: INFO: Pod "pod-projected-secrets-3e65f72e-72e8-4b71-9a29-08c9a3eeddf9" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-2caf674f-580a-48d3-9b94-26ce24a196d4 06/08/23 15:09:42.374
STEP: Updating secret s-test-opt-upd-9fc2754e-7168-42aa-ba93-6c9e36d2beea 06/08/23 15:09:42.382
STEP: Creating secret with name s-test-opt-create-9cb3f3e9-1863-4c66-9162-be3f10a9a70d 06/08/23 15:09:42.388
STEP: waiting to observe update in volume 06/08/23 15:09:42.393
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jun  8 15:09:46.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8793" for this suite. 06/08/23 15:09:46.443
------------------------------
• [SLOW TEST] [6.166 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:09:40.285
    Jun  8 15:09:40.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 15:09:40.286
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:40.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:40.305
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-2caf674f-580a-48d3-9b94-26ce24a196d4 06/08/23 15:09:40.314
    STEP: Creating secret with name s-test-opt-upd-9fc2754e-7168-42aa-ba93-6c9e36d2beea 06/08/23 15:09:40.32
    STEP: Creating the pod 06/08/23 15:09:40.325
    Jun  8 15:09:40.338: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3e65f72e-72e8-4b71-9a29-08c9a3eeddf9" in namespace "projected-8793" to be "running and ready"
    Jun  8 15:09:40.342: INFO: Pod "pod-projected-secrets-3e65f72e-72e8-4b71-9a29-08c9a3eeddf9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.967887ms
    Jun  8 15:09:40.342: INFO: The phase of Pod pod-projected-secrets-3e65f72e-72e8-4b71-9a29-08c9a3eeddf9 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:09:42.348: INFO: Pod "pod-projected-secrets-3e65f72e-72e8-4b71-9a29-08c9a3eeddf9": Phase="Running", Reason="", readiness=true. Elapsed: 2.010002312s
    Jun  8 15:09:42.348: INFO: The phase of Pod pod-projected-secrets-3e65f72e-72e8-4b71-9a29-08c9a3eeddf9 is Running (Ready = true)
    Jun  8 15:09:42.348: INFO: Pod "pod-projected-secrets-3e65f72e-72e8-4b71-9a29-08c9a3eeddf9" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-2caf674f-580a-48d3-9b94-26ce24a196d4 06/08/23 15:09:42.374
    STEP: Updating secret s-test-opt-upd-9fc2754e-7168-42aa-ba93-6c9e36d2beea 06/08/23 15:09:42.382
    STEP: Creating secret with name s-test-opt-create-9cb3f3e9-1863-4c66-9162-be3f10a9a70d 06/08/23 15:09:42.388
    STEP: waiting to observe update in volume 06/08/23 15:09:42.393
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:09:46.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8793" for this suite. 06/08/23 15:09:46.443
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:09:46.452
Jun  8 15:09:46.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename ingress 06/08/23 15:09:46.453
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:46.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:46.471
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 06/08/23 15:09:46.475
STEP: getting /apis/networking.k8s.io 06/08/23 15:09:46.478
STEP: getting /apis/networking.k8s.iov1 06/08/23 15:09:46.479
STEP: creating 06/08/23 15:09:46.481
STEP: getting 06/08/23 15:09:46.498
STEP: listing 06/08/23 15:09:46.501
STEP: watching 06/08/23 15:09:46.505
Jun  8 15:09:46.505: INFO: starting watch
STEP: cluster-wide listing 06/08/23 15:09:46.506
STEP: cluster-wide watching 06/08/23 15:09:46.51
Jun  8 15:09:46.510: INFO: starting watch
STEP: patching 06/08/23 15:09:46.511
STEP: updating 06/08/23 15:09:46.518
Jun  8 15:09:46.528: INFO: waiting for watch events with expected annotations
Jun  8 15:09:46.528: INFO: saw patched and updated annotations
STEP: patching /status 06/08/23 15:09:46.528
STEP: updating /status 06/08/23 15:09:46.534
STEP: get /status 06/08/23 15:09:46.543
STEP: deleting 06/08/23 15:09:46.547
STEP: deleting a collection 06/08/23 15:09:46.561
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Jun  8 15:09:46.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-5516" for this suite. 06/08/23 15:09:46.584
------------------------------
• [0.140 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:09:46.452
    Jun  8 15:09:46.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename ingress 06/08/23 15:09:46.453
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:46.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:46.471
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 06/08/23 15:09:46.475
    STEP: getting /apis/networking.k8s.io 06/08/23 15:09:46.478
    STEP: getting /apis/networking.k8s.iov1 06/08/23 15:09:46.479
    STEP: creating 06/08/23 15:09:46.481
    STEP: getting 06/08/23 15:09:46.498
    STEP: listing 06/08/23 15:09:46.501
    STEP: watching 06/08/23 15:09:46.505
    Jun  8 15:09:46.505: INFO: starting watch
    STEP: cluster-wide listing 06/08/23 15:09:46.506
    STEP: cluster-wide watching 06/08/23 15:09:46.51
    Jun  8 15:09:46.510: INFO: starting watch
    STEP: patching 06/08/23 15:09:46.511
    STEP: updating 06/08/23 15:09:46.518
    Jun  8 15:09:46.528: INFO: waiting for watch events with expected annotations
    Jun  8 15:09:46.528: INFO: saw patched and updated annotations
    STEP: patching /status 06/08/23 15:09:46.528
    STEP: updating /status 06/08/23 15:09:46.534
    STEP: get /status 06/08/23 15:09:46.543
    STEP: deleting 06/08/23 15:09:46.547
    STEP: deleting a collection 06/08/23 15:09:46.561
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:09:46.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-5516" for this suite. 06/08/23 15:09:46.584
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:09:46.592
Jun  8 15:09:46.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 15:09:46.593
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:46.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:46.612
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-c296aaa7-08d9-4393-9ba8-20d24225fd39 06/08/23 15:09:46.62
STEP: Creating the pod 06/08/23 15:09:46.625
Jun  8 15:09:46.635: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9a2ca605-dd0d-403d-a7c7-901beff988ba" in namespace "projected-838" to be "running and ready"
Jun  8 15:09:46.641: INFO: Pod "pod-projected-configmaps-9a2ca605-dd0d-403d-a7c7-901beff988ba": Phase="Pending", Reason="", readiness=false. Elapsed: 5.839354ms
Jun  8 15:09:46.641: INFO: The phase of Pod pod-projected-configmaps-9a2ca605-dd0d-403d-a7c7-901beff988ba is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:09:48.646: INFO: Pod "pod-projected-configmaps-9a2ca605-dd0d-403d-a7c7-901beff988ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.010832608s
Jun  8 15:09:48.646: INFO: The phase of Pod pod-projected-configmaps-9a2ca605-dd0d-403d-a7c7-901beff988ba is Running (Ready = true)
Jun  8 15:09:48.646: INFO: Pod "pod-projected-configmaps-9a2ca605-dd0d-403d-a7c7-901beff988ba" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-c296aaa7-08d9-4393-9ba8-20d24225fd39 06/08/23 15:09:48.657
STEP: waiting to observe update in volume 06/08/23 15:09:48.664
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun  8 15:11:19.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-838" for this suite. 06/08/23 15:11:19.161
------------------------------
• [SLOW TEST] [92.577 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:09:46.592
    Jun  8 15:09:46.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 15:09:46.593
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:09:46.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:09:46.612
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-c296aaa7-08d9-4393-9ba8-20d24225fd39 06/08/23 15:09:46.62
    STEP: Creating the pod 06/08/23 15:09:46.625
    Jun  8 15:09:46.635: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9a2ca605-dd0d-403d-a7c7-901beff988ba" in namespace "projected-838" to be "running and ready"
    Jun  8 15:09:46.641: INFO: Pod "pod-projected-configmaps-9a2ca605-dd0d-403d-a7c7-901beff988ba": Phase="Pending", Reason="", readiness=false. Elapsed: 5.839354ms
    Jun  8 15:09:46.641: INFO: The phase of Pod pod-projected-configmaps-9a2ca605-dd0d-403d-a7c7-901beff988ba is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:09:48.646: INFO: Pod "pod-projected-configmaps-9a2ca605-dd0d-403d-a7c7-901beff988ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.010832608s
    Jun  8 15:09:48.646: INFO: The phase of Pod pod-projected-configmaps-9a2ca605-dd0d-403d-a7c7-901beff988ba is Running (Ready = true)
    Jun  8 15:09:48.646: INFO: Pod "pod-projected-configmaps-9a2ca605-dd0d-403d-a7c7-901beff988ba" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-c296aaa7-08d9-4393-9ba8-20d24225fd39 06/08/23 15:09:48.657
    STEP: waiting to observe update in volume 06/08/23 15:09:48.664
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:11:19.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-838" for this suite. 06/08/23 15:11:19.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:11:19.17
Jun  8 15:11:19.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename resourcequota 06/08/23 15:11:19.171
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:11:19.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:11:19.19
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 06/08/23 15:11:19.193
STEP: Getting a ResourceQuota 06/08/23 15:11:19.199
STEP: Updating a ResourceQuota 06/08/23 15:11:19.203
STEP: Verifying a ResourceQuota was modified 06/08/23 15:11:19.208
STEP: Deleting a ResourceQuota 06/08/23 15:11:19.212
STEP: Verifying the deleted ResourceQuota 06/08/23 15:11:19.219
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  8 15:11:19.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2427" for this suite. 06/08/23 15:11:19.228
------------------------------
• [0.066 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:11:19.17
    Jun  8 15:11:19.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename resourcequota 06/08/23 15:11:19.171
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:11:19.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:11:19.19
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 06/08/23 15:11:19.193
    STEP: Getting a ResourceQuota 06/08/23 15:11:19.199
    STEP: Updating a ResourceQuota 06/08/23 15:11:19.203
    STEP: Verifying a ResourceQuota was modified 06/08/23 15:11:19.208
    STEP: Deleting a ResourceQuota 06/08/23 15:11:19.212
    STEP: Verifying the deleted ResourceQuota 06/08/23 15:11:19.219
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:11:19.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2427" for this suite. 06/08/23 15:11:19.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:11:19.237
Jun  8 15:11:19.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 15:11:19.238
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:11:19.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:11:19.254
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 06/08/23 15:11:19.257
Jun  8 15:11:19.267: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d8f68c8-53bf-434b-9382-76e788feb409" in namespace "projected-3712" to be "Succeeded or Failed"
Jun  8 15:11:19.270: INFO: Pod "downwardapi-volume-6d8f68c8-53bf-434b-9382-76e788feb409": Phase="Pending", Reason="", readiness=false. Elapsed: 3.73268ms
Jun  8 15:11:21.276: INFO: Pod "downwardapi-volume-6d8f68c8-53bf-434b-9382-76e788feb409": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009454556s
Jun  8 15:11:23.277: INFO: Pod "downwardapi-volume-6d8f68c8-53bf-434b-9382-76e788feb409": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009892984s
STEP: Saw pod success 06/08/23 15:11:23.277
Jun  8 15:11:23.277: INFO: Pod "downwardapi-volume-6d8f68c8-53bf-434b-9382-76e788feb409" satisfied condition "Succeeded or Failed"
Jun  8 15:11:23.281: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-6d8f68c8-53bf-434b-9382-76e788feb409 container client-container: <nil>
STEP: delete the pod 06/08/23 15:11:23.3
Jun  8 15:11:23.313: INFO: Waiting for pod downwardapi-volume-6d8f68c8-53bf-434b-9382-76e788feb409 to disappear
Jun  8 15:11:23.317: INFO: Pod downwardapi-volume-6d8f68c8-53bf-434b-9382-76e788feb409 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  8 15:11:23.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3712" for this suite. 06/08/23 15:11:23.324
------------------------------
• [4.095 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:11:19.237
    Jun  8 15:11:19.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 15:11:19.238
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:11:19.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:11:19.254
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 06/08/23 15:11:19.257
    Jun  8 15:11:19.267: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d8f68c8-53bf-434b-9382-76e788feb409" in namespace "projected-3712" to be "Succeeded or Failed"
    Jun  8 15:11:19.270: INFO: Pod "downwardapi-volume-6d8f68c8-53bf-434b-9382-76e788feb409": Phase="Pending", Reason="", readiness=false. Elapsed: 3.73268ms
    Jun  8 15:11:21.276: INFO: Pod "downwardapi-volume-6d8f68c8-53bf-434b-9382-76e788feb409": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009454556s
    Jun  8 15:11:23.277: INFO: Pod "downwardapi-volume-6d8f68c8-53bf-434b-9382-76e788feb409": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009892984s
    STEP: Saw pod success 06/08/23 15:11:23.277
    Jun  8 15:11:23.277: INFO: Pod "downwardapi-volume-6d8f68c8-53bf-434b-9382-76e788feb409" satisfied condition "Succeeded or Failed"
    Jun  8 15:11:23.281: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-6d8f68c8-53bf-434b-9382-76e788feb409 container client-container: <nil>
    STEP: delete the pod 06/08/23 15:11:23.3
    Jun  8 15:11:23.313: INFO: Waiting for pod downwardapi-volume-6d8f68c8-53bf-434b-9382-76e788feb409 to disappear
    Jun  8 15:11:23.317: INFO: Pod downwardapi-volume-6d8f68c8-53bf-434b-9382-76e788feb409 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:11:23.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3712" for this suite. 06/08/23 15:11:23.324
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:11:23.334
Jun  8 15:11:23.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename var-expansion 06/08/23 15:11:23.335
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:11:23.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:11:23.353
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Jun  8 15:11:23.365: INFO: Waiting up to 2m0s for pod "var-expansion-b1decd0d-81a0-4fe3-869d-71c7dd44d433" in namespace "var-expansion-573" to be "container 0 failed with reason CreateContainerConfigError"
Jun  8 15:11:23.369: INFO: Pod "var-expansion-b1decd0d-81a0-4fe3-869d-71c7dd44d433": Phase="Pending", Reason="", readiness=false. Elapsed: 3.950137ms
Jun  8 15:11:25.376: INFO: Pod "var-expansion-b1decd0d-81a0-4fe3-869d-71c7dd44d433": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010448317s
Jun  8 15:11:25.376: INFO: Pod "var-expansion-b1decd0d-81a0-4fe3-869d-71c7dd44d433" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jun  8 15:11:25.376: INFO: Deleting pod "var-expansion-b1decd0d-81a0-4fe3-869d-71c7dd44d433" in namespace "var-expansion-573"
Jun  8 15:11:25.384: INFO: Wait up to 5m0s for pod "var-expansion-b1decd0d-81a0-4fe3-869d-71c7dd44d433" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun  8 15:11:27.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-573" for this suite. 06/08/23 15:11:27.401
------------------------------
• [4.075 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:11:23.334
    Jun  8 15:11:23.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename var-expansion 06/08/23 15:11:23.335
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:11:23.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:11:23.353
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Jun  8 15:11:23.365: INFO: Waiting up to 2m0s for pod "var-expansion-b1decd0d-81a0-4fe3-869d-71c7dd44d433" in namespace "var-expansion-573" to be "container 0 failed with reason CreateContainerConfigError"
    Jun  8 15:11:23.369: INFO: Pod "var-expansion-b1decd0d-81a0-4fe3-869d-71c7dd44d433": Phase="Pending", Reason="", readiness=false. Elapsed: 3.950137ms
    Jun  8 15:11:25.376: INFO: Pod "var-expansion-b1decd0d-81a0-4fe3-869d-71c7dd44d433": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010448317s
    Jun  8 15:11:25.376: INFO: Pod "var-expansion-b1decd0d-81a0-4fe3-869d-71c7dd44d433" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jun  8 15:11:25.376: INFO: Deleting pod "var-expansion-b1decd0d-81a0-4fe3-869d-71c7dd44d433" in namespace "var-expansion-573"
    Jun  8 15:11:25.384: INFO: Wait up to 5m0s for pod "var-expansion-b1decd0d-81a0-4fe3-869d-71c7dd44d433" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:11:27.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-573" for this suite. 06/08/23 15:11:27.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:11:27.413
Jun  8 15:11:27.413: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename replication-controller 06/08/23 15:11:27.414
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:11:27.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:11:27.433
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a 06/08/23 15:11:27.436
Jun  8 15:11:27.447: INFO: Pod name my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a: Found 0 pods out of 1
Jun  8 15:11:32.451: INFO: Pod name my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a: Found 1 pods out of 1
Jun  8 15:11:32.452: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a" are running
Jun  8 15:11:32.452: INFO: Waiting up to 5m0s for pod "my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a-x7ql9" in namespace "replication-controller-3130" to be "running"
Jun  8 15:11:32.455: INFO: Pod "my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a-x7ql9": Phase="Running", Reason="", readiness=true. Elapsed: 3.763941ms
Jun  8 15:11:32.455: INFO: Pod "my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a-x7ql9" satisfied condition "running"
Jun  8 15:11:32.455: INFO: Pod "my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a-x7ql9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-08 15:11:27 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-08 15:11:28 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-08 15:11:28 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-08 15:11:27 +0000 UTC Reason: Message:}])
Jun  8 15:11:32.455: INFO: Trying to dial the pod
Jun  8 15:11:37.472: INFO: Controller my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a: Got expected result from replica 1 [my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a-x7ql9]: "my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a-x7ql9", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jun  8 15:11:37.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3130" for this suite. 06/08/23 15:11:37.478
------------------------------
• [SLOW TEST] [10.073 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:11:27.413
    Jun  8 15:11:27.413: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename replication-controller 06/08/23 15:11:27.414
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:11:27.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:11:27.433
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a 06/08/23 15:11:27.436
    Jun  8 15:11:27.447: INFO: Pod name my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a: Found 0 pods out of 1
    Jun  8 15:11:32.451: INFO: Pod name my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a: Found 1 pods out of 1
    Jun  8 15:11:32.452: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a" are running
    Jun  8 15:11:32.452: INFO: Waiting up to 5m0s for pod "my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a-x7ql9" in namespace "replication-controller-3130" to be "running"
    Jun  8 15:11:32.455: INFO: Pod "my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a-x7ql9": Phase="Running", Reason="", readiness=true. Elapsed: 3.763941ms
    Jun  8 15:11:32.455: INFO: Pod "my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a-x7ql9" satisfied condition "running"
    Jun  8 15:11:32.455: INFO: Pod "my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a-x7ql9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-08 15:11:27 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-08 15:11:28 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-08 15:11:28 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-08 15:11:27 +0000 UTC Reason: Message:}])
    Jun  8 15:11:32.455: INFO: Trying to dial the pod
    Jun  8 15:11:37.472: INFO: Controller my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a: Got expected result from replica 1 [my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a-x7ql9]: "my-hostname-basic-0739ee64-d12b-4227-9dd9-d3021f421d9a-x7ql9", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:11:37.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3130" for this suite. 06/08/23 15:11:37.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:11:37.487
Jun  8 15:11:37.487: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename init-container 06/08/23 15:11:37.488
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:11:37.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:11:37.506
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 06/08/23 15:11:37.51
Jun  8 15:11:37.510: INFO: PodSpec: initContainers in spec.initContainers
Jun  8 15:12:23.487: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9dc91f9d-7a0c-42d3-9247-4da24b063ab9", GenerateName:"", Namespace:"init-container-3080", SelfLink:"", UID:"1f5ae10b-9812-4111-a841-e0b53f0e5bb4", ResourceVersion:"19516", Generation:0, CreationTimestamp:time.Date(2023, time.June, 8, 15, 11, 37, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"510452433"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 8, 15, 11, 37, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00091a030), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 8, 15, 12, 23, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00091a060), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-bc57b", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0038ac6c0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bc57b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bc57b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bc57b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003f76ad8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"chl8tf-worker-001", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000a97a40), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003f76b60)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003f76b80)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003f76b88), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003f76b8c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00107a210), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 8, 15, 11, 37, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 8, 15, 11, 37, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 8, 15, 11, 37, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 8, 15, 11, 37, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"100.100.236.215", PodIP:"10.244.3.30", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.3.30"}}, StartTime:time.Date(2023, time.June, 8, 15, 11, 37, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000a97b20)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000a97b90)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://524aa91d2e2a54c25329fcc0611b48b8b0ef36b8895cd04aa7b0264266533f38", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038ac740), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038ac720), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc003f76c0f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:12:23.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-3080" for this suite. 06/08/23 15:12:23.495
------------------------------
• [SLOW TEST] [46.016 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:11:37.487
    Jun  8 15:11:37.487: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename init-container 06/08/23 15:11:37.488
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:11:37.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:11:37.506
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 06/08/23 15:11:37.51
    Jun  8 15:11:37.510: INFO: PodSpec: initContainers in spec.initContainers
    Jun  8 15:12:23.487: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9dc91f9d-7a0c-42d3-9247-4da24b063ab9", GenerateName:"", Namespace:"init-container-3080", SelfLink:"", UID:"1f5ae10b-9812-4111-a841-e0b53f0e5bb4", ResourceVersion:"19516", Generation:0, CreationTimestamp:time.Date(2023, time.June, 8, 15, 11, 37, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"510452433"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 8, 15, 11, 37, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00091a030), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 8, 15, 12, 23, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00091a060), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-bc57b", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0038ac6c0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bc57b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bc57b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bc57b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003f76ad8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"chl8tf-worker-001", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000a97a40), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003f76b60)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003f76b80)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003f76b88), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003f76b8c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00107a210), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 8, 15, 11, 37, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 8, 15, 11, 37, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 8, 15, 11, 37, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 8, 15, 11, 37, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"100.100.236.215", PodIP:"10.244.3.30", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.3.30"}}, StartTime:time.Date(2023, time.June, 8, 15, 11, 37, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000a97b20)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000a97b90)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://524aa91d2e2a54c25329fcc0611b48b8b0ef36b8895cd04aa7b0264266533f38", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038ac740), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038ac720), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc003f76c0f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:12:23.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-3080" for this suite. 06/08/23 15:12:23.495
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:12:23.505
Jun  8 15:12:23.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename csiinlinevolumes 06/08/23 15:12:23.507
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:12:23.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:12:23.526
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 06/08/23 15:12:23.529
STEP: getting 06/08/23 15:12:23.547
STEP: listing 06/08/23 15:12:23.554
STEP: deleting 06/08/23 15:12:23.557
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jun  8 15:12:23.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-6453" for this suite. 06/08/23 15:12:23.583
------------------------------
• [0.086 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:12:23.505
    Jun  8 15:12:23.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename csiinlinevolumes 06/08/23 15:12:23.507
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:12:23.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:12:23.526
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 06/08/23 15:12:23.529
    STEP: getting 06/08/23 15:12:23.547
    STEP: listing 06/08/23 15:12:23.554
    STEP: deleting 06/08/23 15:12:23.557
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:12:23.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-6453" for this suite. 06/08/23 15:12:23.583
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:12:23.591
Jun  8 15:12:23.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename secrets 06/08/23 15:12:23.592
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:12:23.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:12:23.61
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-5b58f46c-4f98-4539-aff4-1369ae00a434 06/08/23 15:12:23.613
STEP: Creating a pod to test consume secrets 06/08/23 15:12:23.618
Jun  8 15:12:23.627: INFO: Waiting up to 5m0s for pod "pod-secrets-4c5a62af-960b-4850-82c2-826f3f6f90f3" in namespace "secrets-8852" to be "Succeeded or Failed"
Jun  8 15:12:23.632: INFO: Pod "pod-secrets-4c5a62af-960b-4850-82c2-826f3f6f90f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.462088ms
Jun  8 15:12:25.637: INFO: Pod "pod-secrets-4c5a62af-960b-4850-82c2-826f3f6f90f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009818086s
Jun  8 15:12:27.638: INFO: Pod "pod-secrets-4c5a62af-960b-4850-82c2-826f3f6f90f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010486128s
STEP: Saw pod success 06/08/23 15:12:27.638
Jun  8 15:12:27.638: INFO: Pod "pod-secrets-4c5a62af-960b-4850-82c2-826f3f6f90f3" satisfied condition "Succeeded or Failed"
Jun  8 15:12:27.642: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-secrets-4c5a62af-960b-4850-82c2-826f3f6f90f3 container secret-volume-test: <nil>
STEP: delete the pod 06/08/23 15:12:27.65
Jun  8 15:12:27.665: INFO: Waiting for pod pod-secrets-4c5a62af-960b-4850-82c2-826f3f6f90f3 to disappear
Jun  8 15:12:27.668: INFO: Pod pod-secrets-4c5a62af-960b-4850-82c2-826f3f6f90f3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  8 15:12:27.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8852" for this suite. 06/08/23 15:12:27.673
------------------------------
• [4.089 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:12:23.591
    Jun  8 15:12:23.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename secrets 06/08/23 15:12:23.592
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:12:23.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:12:23.61
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-5b58f46c-4f98-4539-aff4-1369ae00a434 06/08/23 15:12:23.613
    STEP: Creating a pod to test consume secrets 06/08/23 15:12:23.618
    Jun  8 15:12:23.627: INFO: Waiting up to 5m0s for pod "pod-secrets-4c5a62af-960b-4850-82c2-826f3f6f90f3" in namespace "secrets-8852" to be "Succeeded or Failed"
    Jun  8 15:12:23.632: INFO: Pod "pod-secrets-4c5a62af-960b-4850-82c2-826f3f6f90f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.462088ms
    Jun  8 15:12:25.637: INFO: Pod "pod-secrets-4c5a62af-960b-4850-82c2-826f3f6f90f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009818086s
    Jun  8 15:12:27.638: INFO: Pod "pod-secrets-4c5a62af-960b-4850-82c2-826f3f6f90f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010486128s
    STEP: Saw pod success 06/08/23 15:12:27.638
    Jun  8 15:12:27.638: INFO: Pod "pod-secrets-4c5a62af-960b-4850-82c2-826f3f6f90f3" satisfied condition "Succeeded or Failed"
    Jun  8 15:12:27.642: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-secrets-4c5a62af-960b-4850-82c2-826f3f6f90f3 container secret-volume-test: <nil>
    STEP: delete the pod 06/08/23 15:12:27.65
    Jun  8 15:12:27.665: INFO: Waiting for pod pod-secrets-4c5a62af-960b-4850-82c2-826f3f6f90f3 to disappear
    Jun  8 15:12:27.668: INFO: Pod pod-secrets-4c5a62af-960b-4850-82c2-826f3f6f90f3 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:12:27.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8852" for this suite. 06/08/23 15:12:27.673
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:12:27.681
Jun  8 15:12:27.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename secrets 06/08/23 15:12:27.682
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:12:27.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:12:27.699
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-04fb1a96-3f33-4268-a258-c2244902f2b9 06/08/23 15:12:27.703
STEP: Creating a pod to test consume secrets 06/08/23 15:12:27.707
Jun  8 15:12:27.716: INFO: Waiting up to 5m0s for pod "pod-secrets-52195d0d-592f-4895-9d12-1eeb6e34224f" in namespace "secrets-197" to be "Succeeded or Failed"
Jun  8 15:12:27.720: INFO: Pod "pod-secrets-52195d0d-592f-4895-9d12-1eeb6e34224f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.693992ms
Jun  8 15:12:29.725: INFO: Pod "pod-secrets-52195d0d-592f-4895-9d12-1eeb6e34224f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008912213s
Jun  8 15:12:31.726: INFO: Pod "pod-secrets-52195d0d-592f-4895-9d12-1eeb6e34224f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010330883s
STEP: Saw pod success 06/08/23 15:12:31.726
Jun  8 15:12:31.726: INFO: Pod "pod-secrets-52195d0d-592f-4895-9d12-1eeb6e34224f" satisfied condition "Succeeded or Failed"
Jun  8 15:12:31.730: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-secrets-52195d0d-592f-4895-9d12-1eeb6e34224f container secret-env-test: <nil>
STEP: delete the pod 06/08/23 15:12:31.739
Jun  8 15:12:31.750: INFO: Waiting for pod pod-secrets-52195d0d-592f-4895-9d12-1eeb6e34224f to disappear
Jun  8 15:12:31.754: INFO: Pod pod-secrets-52195d0d-592f-4895-9d12-1eeb6e34224f no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  8 15:12:31.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-197" for this suite. 06/08/23 15:12:31.76
------------------------------
• [4.085 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:12:27.681
    Jun  8 15:12:27.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename secrets 06/08/23 15:12:27.682
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:12:27.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:12:27.699
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-04fb1a96-3f33-4268-a258-c2244902f2b9 06/08/23 15:12:27.703
    STEP: Creating a pod to test consume secrets 06/08/23 15:12:27.707
    Jun  8 15:12:27.716: INFO: Waiting up to 5m0s for pod "pod-secrets-52195d0d-592f-4895-9d12-1eeb6e34224f" in namespace "secrets-197" to be "Succeeded or Failed"
    Jun  8 15:12:27.720: INFO: Pod "pod-secrets-52195d0d-592f-4895-9d12-1eeb6e34224f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.693992ms
    Jun  8 15:12:29.725: INFO: Pod "pod-secrets-52195d0d-592f-4895-9d12-1eeb6e34224f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008912213s
    Jun  8 15:12:31.726: INFO: Pod "pod-secrets-52195d0d-592f-4895-9d12-1eeb6e34224f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010330883s
    STEP: Saw pod success 06/08/23 15:12:31.726
    Jun  8 15:12:31.726: INFO: Pod "pod-secrets-52195d0d-592f-4895-9d12-1eeb6e34224f" satisfied condition "Succeeded or Failed"
    Jun  8 15:12:31.730: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-secrets-52195d0d-592f-4895-9d12-1eeb6e34224f container secret-env-test: <nil>
    STEP: delete the pod 06/08/23 15:12:31.739
    Jun  8 15:12:31.750: INFO: Waiting for pod pod-secrets-52195d0d-592f-4895-9d12-1eeb6e34224f to disappear
    Jun  8 15:12:31.754: INFO: Pod pod-secrets-52195d0d-592f-4895-9d12-1eeb6e34224f no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:12:31.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-197" for this suite. 06/08/23 15:12:31.76
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:12:31.767
Jun  8 15:12:31.767: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename pod-network-test 06/08/23 15:12:31.769
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:12:31.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:12:31.787
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-8337 06/08/23 15:12:31.79
STEP: creating a selector 06/08/23 15:12:31.79
STEP: Creating the service pods in kubernetes 06/08/23 15:12:31.79
Jun  8 15:12:31.790: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun  8 15:12:31.851: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8337" to be "running and ready"
Jun  8 15:12:31.859: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.315538ms
Jun  8 15:12:31.859: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:12:33.865: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013694549s
Jun  8 15:12:33.865: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:12:35.865: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014025829s
Jun  8 15:12:35.865: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:12:37.865: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01389448s
Jun  8 15:12:37.865: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 15:12:39.865: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013786006s
Jun  8 15:12:39.865: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 15:12:41.865: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013453292s
Jun  8 15:12:41.865: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 15:12:43.866: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015129367s
Jun  8 15:12:43.866: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 15:12:45.865: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013797252s
Jun  8 15:12:45.865: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 15:12:47.865: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014386115s
Jun  8 15:12:47.865: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 15:12:49.864: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.013128392s
Jun  8 15:12:49.864: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 15:12:51.866: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014850648s
Jun  8 15:12:51.866: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 15:12:53.865: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014076782s
Jun  8 15:12:53.865: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun  8 15:12:53.865: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun  8 15:12:53.869: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8337" to be "running and ready"
Jun  8 15:12:53.873: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.530684ms
Jun  8 15:12:53.873: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun  8 15:12:53.873: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun  8 15:12:53.877: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8337" to be "running and ready"
Jun  8 15:12:53.880: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.581791ms
Jun  8 15:12:53.880: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun  8 15:12:53.880: INFO: Pod "netserver-2" satisfied condition "running and ready"
Jun  8 15:12:53.884: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-8337" to be "running and ready"
Jun  8 15:12:53.887: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.284158ms
Jun  8 15:12:53.887: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Jun  8 15:12:53.887: INFO: Pod "netserver-3" satisfied condition "running and ready"
Jun  8 15:12:53.891: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-8337" to be "running and ready"
Jun  8 15:12:53.894: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.715771ms
Jun  8 15:12:53.894: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Jun  8 15:12:53.894: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 06/08/23 15:12:53.898
Jun  8 15:12:53.910: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8337" to be "running"
Jun  8 15:12:53.914: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.616688ms
Jun  8 15:12:55.920: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009644773s
Jun  8 15:12:55.920: INFO: Pod "test-container-pod" satisfied condition "running"
Jun  8 15:12:55.924: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8337" to be "running"
Jun  8 15:12:55.927: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.729832ms
Jun  8 15:12:55.927: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jun  8 15:12:55.931: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Jun  8 15:12:55.931: INFO: Going to poll 10.244.0.8 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Jun  8 15:12:55.935: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.8:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8337 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:12:55.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:12:55.935: INFO: ExecWithOptions: Clientset creation
Jun  8 15:12:55.936: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8337/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.0.8%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun  8 15:12:56.022: INFO: Found all 1 expected endpoints: [netserver-0]
Jun  8 15:12:56.022: INFO: Going to poll 10.244.1.9 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Jun  8 15:12:56.028: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.9:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8337 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:12:56.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:12:56.028: INFO: ExecWithOptions: Clientset creation
Jun  8 15:12:56.028: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8337/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.9%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun  8 15:12:56.119: INFO: Found all 1 expected endpoints: [netserver-1]
Jun  8 15:12:56.119: INFO: Going to poll 10.244.2.5 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Jun  8 15:12:56.124: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.5:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8337 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:12:56.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:12:56.125: INFO: ExecWithOptions: Clientset creation
Jun  8 15:12:56.125: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8337/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.2.5%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun  8 15:12:56.207: INFO: Found all 1 expected endpoints: [netserver-2]
Jun  8 15:12:56.207: INFO: Going to poll 10.244.3.33 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Jun  8 15:12:56.212: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.33:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8337 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:12:56.212: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:12:56.213: INFO: ExecWithOptions: Clientset creation
Jun  8 15:12:56.213: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8337/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.3.33%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun  8 15:12:56.299: INFO: Found all 1 expected endpoints: [netserver-3]
Jun  8 15:12:56.299: INFO: Going to poll 10.244.4.13 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Jun  8 15:12:56.304: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.4.13:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8337 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:12:56.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:12:56.305: INFO: ExecWithOptions: Clientset creation
Jun  8 15:12:56.305: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8337/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.4.13%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun  8 15:12:56.387: INFO: Found all 1 expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jun  8 15:12:56.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-8337" for this suite. 06/08/23 15:12:56.394
------------------------------
• [SLOW TEST] [24.635 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:12:31.767
    Jun  8 15:12:31.767: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename pod-network-test 06/08/23 15:12:31.769
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:12:31.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:12:31.787
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-8337 06/08/23 15:12:31.79
    STEP: creating a selector 06/08/23 15:12:31.79
    STEP: Creating the service pods in kubernetes 06/08/23 15:12:31.79
    Jun  8 15:12:31.790: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jun  8 15:12:31.851: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8337" to be "running and ready"
    Jun  8 15:12:31.859: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.315538ms
    Jun  8 15:12:31.859: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:12:33.865: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013694549s
    Jun  8 15:12:33.865: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:12:35.865: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014025829s
    Jun  8 15:12:35.865: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:12:37.865: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01389448s
    Jun  8 15:12:37.865: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 15:12:39.865: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013786006s
    Jun  8 15:12:39.865: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 15:12:41.865: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013453292s
    Jun  8 15:12:41.865: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 15:12:43.866: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015129367s
    Jun  8 15:12:43.866: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 15:12:45.865: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013797252s
    Jun  8 15:12:45.865: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 15:12:47.865: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014386115s
    Jun  8 15:12:47.865: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 15:12:49.864: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.013128392s
    Jun  8 15:12:49.864: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 15:12:51.866: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014850648s
    Jun  8 15:12:51.866: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 15:12:53.865: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014076782s
    Jun  8 15:12:53.865: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun  8 15:12:53.865: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun  8 15:12:53.869: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8337" to be "running and ready"
    Jun  8 15:12:53.873: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.530684ms
    Jun  8 15:12:53.873: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun  8 15:12:53.873: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun  8 15:12:53.877: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8337" to be "running and ready"
    Jun  8 15:12:53.880: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.581791ms
    Jun  8 15:12:53.880: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun  8 15:12:53.880: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Jun  8 15:12:53.884: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-8337" to be "running and ready"
    Jun  8 15:12:53.887: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.284158ms
    Jun  8 15:12:53.887: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Jun  8 15:12:53.887: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Jun  8 15:12:53.891: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-8337" to be "running and ready"
    Jun  8 15:12:53.894: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.715771ms
    Jun  8 15:12:53.894: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Jun  8 15:12:53.894: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 06/08/23 15:12:53.898
    Jun  8 15:12:53.910: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8337" to be "running"
    Jun  8 15:12:53.914: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.616688ms
    Jun  8 15:12:55.920: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009644773s
    Jun  8 15:12:55.920: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun  8 15:12:55.924: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8337" to be "running"
    Jun  8 15:12:55.927: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.729832ms
    Jun  8 15:12:55.927: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jun  8 15:12:55.931: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Jun  8 15:12:55.931: INFO: Going to poll 10.244.0.8 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Jun  8 15:12:55.935: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.8:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8337 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:12:55.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:12:55.935: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:12:55.936: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8337/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.0.8%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun  8 15:12:56.022: INFO: Found all 1 expected endpoints: [netserver-0]
    Jun  8 15:12:56.022: INFO: Going to poll 10.244.1.9 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Jun  8 15:12:56.028: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.9:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8337 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:12:56.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:12:56.028: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:12:56.028: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8337/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.9%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun  8 15:12:56.119: INFO: Found all 1 expected endpoints: [netserver-1]
    Jun  8 15:12:56.119: INFO: Going to poll 10.244.2.5 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Jun  8 15:12:56.124: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.5:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8337 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:12:56.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:12:56.125: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:12:56.125: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8337/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.2.5%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun  8 15:12:56.207: INFO: Found all 1 expected endpoints: [netserver-2]
    Jun  8 15:12:56.207: INFO: Going to poll 10.244.3.33 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Jun  8 15:12:56.212: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.33:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8337 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:12:56.212: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:12:56.213: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:12:56.213: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8337/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.3.33%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun  8 15:12:56.299: INFO: Found all 1 expected endpoints: [netserver-3]
    Jun  8 15:12:56.299: INFO: Going to poll 10.244.4.13 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Jun  8 15:12:56.304: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.4.13:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8337 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:12:56.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:12:56.305: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:12:56.305: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8337/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.4.13%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun  8 15:12:56.387: INFO: Found all 1 expected endpoints: [netserver-4]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:12:56.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-8337" for this suite. 06/08/23 15:12:56.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:12:56.404
Jun  8 15:12:56.404: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename subpath 06/08/23 15:12:56.405
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:12:56.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:12:56.423
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/08/23 15:12:56.427
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-6sst 06/08/23 15:12:56.437
STEP: Creating a pod to test atomic-volume-subpath 06/08/23 15:12:56.437
Jun  8 15:12:56.446: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6sst" in namespace "subpath-5348" to be "Succeeded or Failed"
Jun  8 15:12:56.450: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Pending", Reason="", readiness=false. Elapsed: 3.810344ms
Jun  8 15:12:58.456: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=true. Elapsed: 2.009319203s
Jun  8 15:13:00.456: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=true. Elapsed: 4.009679447s
Jun  8 15:13:02.456: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=true. Elapsed: 6.009587204s
Jun  8 15:13:04.456: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=true. Elapsed: 8.009523723s
Jun  8 15:13:06.455: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=true. Elapsed: 10.008946385s
Jun  8 15:13:08.457: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=true. Elapsed: 12.010256274s
Jun  8 15:13:10.456: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=true. Elapsed: 14.009732355s
Jun  8 15:13:12.457: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=true. Elapsed: 16.010299591s
Jun  8 15:13:14.456: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=true. Elapsed: 18.009514679s
Jun  8 15:13:16.456: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=true. Elapsed: 20.009909259s
Jun  8 15:13:18.457: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=false. Elapsed: 22.010696057s
Jun  8 15:13:20.456: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.0100042s
STEP: Saw pod success 06/08/23 15:13:20.457
Jun  8 15:13:20.457: INFO: Pod "pod-subpath-test-configmap-6sst" satisfied condition "Succeeded or Failed"
Jun  8 15:13:20.461: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-subpath-test-configmap-6sst container test-container-subpath-configmap-6sst: <nil>
STEP: delete the pod 06/08/23 15:13:20.471
Jun  8 15:13:20.485: INFO: Waiting for pod pod-subpath-test-configmap-6sst to disappear
Jun  8 15:13:20.490: INFO: Pod pod-subpath-test-configmap-6sst no longer exists
STEP: Deleting pod pod-subpath-test-configmap-6sst 06/08/23 15:13:20.49
Jun  8 15:13:20.490: INFO: Deleting pod "pod-subpath-test-configmap-6sst" in namespace "subpath-5348"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jun  8 15:13:20.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5348" for this suite. 06/08/23 15:13:20.5
------------------------------
• [SLOW TEST] [24.103 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:12:56.404
    Jun  8 15:12:56.404: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename subpath 06/08/23 15:12:56.405
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:12:56.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:12:56.423
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/08/23 15:12:56.427
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-6sst 06/08/23 15:12:56.437
    STEP: Creating a pod to test atomic-volume-subpath 06/08/23 15:12:56.437
    Jun  8 15:12:56.446: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6sst" in namespace "subpath-5348" to be "Succeeded or Failed"
    Jun  8 15:12:56.450: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Pending", Reason="", readiness=false. Elapsed: 3.810344ms
    Jun  8 15:12:58.456: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=true. Elapsed: 2.009319203s
    Jun  8 15:13:00.456: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=true. Elapsed: 4.009679447s
    Jun  8 15:13:02.456: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=true. Elapsed: 6.009587204s
    Jun  8 15:13:04.456: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=true. Elapsed: 8.009523723s
    Jun  8 15:13:06.455: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=true. Elapsed: 10.008946385s
    Jun  8 15:13:08.457: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=true. Elapsed: 12.010256274s
    Jun  8 15:13:10.456: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=true. Elapsed: 14.009732355s
    Jun  8 15:13:12.457: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=true. Elapsed: 16.010299591s
    Jun  8 15:13:14.456: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=true. Elapsed: 18.009514679s
    Jun  8 15:13:16.456: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=true. Elapsed: 20.009909259s
    Jun  8 15:13:18.457: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Running", Reason="", readiness=false. Elapsed: 22.010696057s
    Jun  8 15:13:20.456: INFO: Pod "pod-subpath-test-configmap-6sst": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.0100042s
    STEP: Saw pod success 06/08/23 15:13:20.457
    Jun  8 15:13:20.457: INFO: Pod "pod-subpath-test-configmap-6sst" satisfied condition "Succeeded or Failed"
    Jun  8 15:13:20.461: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-subpath-test-configmap-6sst container test-container-subpath-configmap-6sst: <nil>
    STEP: delete the pod 06/08/23 15:13:20.471
    Jun  8 15:13:20.485: INFO: Waiting for pod pod-subpath-test-configmap-6sst to disappear
    Jun  8 15:13:20.490: INFO: Pod pod-subpath-test-configmap-6sst no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-6sst 06/08/23 15:13:20.49
    Jun  8 15:13:20.490: INFO: Deleting pod "pod-subpath-test-configmap-6sst" in namespace "subpath-5348"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:13:20.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5348" for this suite. 06/08/23 15:13:20.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:13:20.508
Jun  8 15:13:20.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename daemonsets 06/08/23 15:13:20.509
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:13:20.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:13:20.527
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 06/08/23 15:13:20.563
STEP: Check that daemon pods launch on every node of the cluster. 06/08/23 15:13:20.569
Jun  8 15:13:20.581: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  8 15:13:20.581: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
Jun  8 15:13:21.592: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  8 15:13:21.592: INFO: Node chl8tf-control-plane-002 is running 0 daemon pod, expected 1
Jun  8 15:13:22.593: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jun  8 15:13:22.593: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Getting /status 06/08/23 15:13:22.597
Jun  8 15:13:22.603: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 06/08/23 15:13:22.603
Jun  8 15:13:22.614: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 06/08/23 15:13:22.614
Jun  8 15:13:22.616: INFO: Observed &DaemonSet event: ADDED
Jun  8 15:13:22.616: INFO: Observed &DaemonSet event: MODIFIED
Jun  8 15:13:22.616: INFO: Observed &DaemonSet event: MODIFIED
Jun  8 15:13:22.616: INFO: Observed &DaemonSet event: MODIFIED
Jun  8 15:13:22.616: INFO: Observed &DaemonSet event: MODIFIED
Jun  8 15:13:22.617: INFO: Observed &DaemonSet event: MODIFIED
Jun  8 15:13:22.617: INFO: Observed &DaemonSet event: MODIFIED
Jun  8 15:13:22.617: INFO: Found daemon set daemon-set in namespace daemonsets-4991 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun  8 15:13:22.617: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 06/08/23 15:13:22.617
STEP: watching for the daemon set status to be patched 06/08/23 15:13:22.625
Jun  8 15:13:22.628: INFO: Observed &DaemonSet event: ADDED
Jun  8 15:13:22.628: INFO: Observed &DaemonSet event: MODIFIED
Jun  8 15:13:22.628: INFO: Observed &DaemonSet event: MODIFIED
Jun  8 15:13:22.628: INFO: Observed &DaemonSet event: MODIFIED
Jun  8 15:13:22.629: INFO: Observed &DaemonSet event: MODIFIED
Jun  8 15:13:22.629: INFO: Observed &DaemonSet event: MODIFIED
Jun  8 15:13:22.629: INFO: Observed &DaemonSet event: MODIFIED
Jun  8 15:13:22.629: INFO: Observed daemon set daemon-set in namespace daemonsets-4991 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun  8 15:13:22.629: INFO: Observed &DaemonSet event: MODIFIED
Jun  8 15:13:22.629: INFO: Found daemon set daemon-set in namespace daemonsets-4991 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jun  8 15:13:22.629: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 06/08/23 15:13:22.633
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4991, will wait for the garbage collector to delete the pods 06/08/23 15:13:22.634
Jun  8 15:13:22.695: INFO: Deleting DaemonSet.extensions daemon-set took: 6.964491ms
Jun  8 15:13:22.796: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.943129ms
Jun  8 15:13:25.300: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  8 15:13:25.300: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun  8 15:13:25.304: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20074"},"items":null}

Jun  8 15:13:25.308: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20074"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:13:25.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4991" for this suite. 06/08/23 15:13:25.338
------------------------------
• [4.837 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:13:20.508
    Jun  8 15:13:20.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename daemonsets 06/08/23 15:13:20.509
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:13:20.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:13:20.527
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 06/08/23 15:13:20.563
    STEP: Check that daemon pods launch on every node of the cluster. 06/08/23 15:13:20.569
    Jun  8 15:13:20.581: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  8 15:13:20.581: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
    Jun  8 15:13:21.592: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  8 15:13:21.592: INFO: Node chl8tf-control-plane-002 is running 0 daemon pod, expected 1
    Jun  8 15:13:22.593: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jun  8 15:13:22.593: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Getting /status 06/08/23 15:13:22.597
    Jun  8 15:13:22.603: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 06/08/23 15:13:22.603
    Jun  8 15:13:22.614: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 06/08/23 15:13:22.614
    Jun  8 15:13:22.616: INFO: Observed &DaemonSet event: ADDED
    Jun  8 15:13:22.616: INFO: Observed &DaemonSet event: MODIFIED
    Jun  8 15:13:22.616: INFO: Observed &DaemonSet event: MODIFIED
    Jun  8 15:13:22.616: INFO: Observed &DaemonSet event: MODIFIED
    Jun  8 15:13:22.616: INFO: Observed &DaemonSet event: MODIFIED
    Jun  8 15:13:22.617: INFO: Observed &DaemonSet event: MODIFIED
    Jun  8 15:13:22.617: INFO: Observed &DaemonSet event: MODIFIED
    Jun  8 15:13:22.617: INFO: Found daemon set daemon-set in namespace daemonsets-4991 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun  8 15:13:22.617: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 06/08/23 15:13:22.617
    STEP: watching for the daemon set status to be patched 06/08/23 15:13:22.625
    Jun  8 15:13:22.628: INFO: Observed &DaemonSet event: ADDED
    Jun  8 15:13:22.628: INFO: Observed &DaemonSet event: MODIFIED
    Jun  8 15:13:22.628: INFO: Observed &DaemonSet event: MODIFIED
    Jun  8 15:13:22.628: INFO: Observed &DaemonSet event: MODIFIED
    Jun  8 15:13:22.629: INFO: Observed &DaemonSet event: MODIFIED
    Jun  8 15:13:22.629: INFO: Observed &DaemonSet event: MODIFIED
    Jun  8 15:13:22.629: INFO: Observed &DaemonSet event: MODIFIED
    Jun  8 15:13:22.629: INFO: Observed daemon set daemon-set in namespace daemonsets-4991 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun  8 15:13:22.629: INFO: Observed &DaemonSet event: MODIFIED
    Jun  8 15:13:22.629: INFO: Found daemon set daemon-set in namespace daemonsets-4991 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jun  8 15:13:22.629: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 06/08/23 15:13:22.633
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4991, will wait for the garbage collector to delete the pods 06/08/23 15:13:22.634
    Jun  8 15:13:22.695: INFO: Deleting DaemonSet.extensions daemon-set took: 6.964491ms
    Jun  8 15:13:22.796: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.943129ms
    Jun  8 15:13:25.300: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  8 15:13:25.300: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun  8 15:13:25.304: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20074"},"items":null}

    Jun  8 15:13:25.308: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20074"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:13:25.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4991" for this suite. 06/08/23 15:13:25.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:13:25.346
Jun  8 15:13:25.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename downward-api 06/08/23 15:13:25.348
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:13:25.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:13:25.365
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 06/08/23 15:13:25.368
Jun  8 15:13:25.379: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0aa306eb-af34-4dc6-a823-722c0f142e5f" in namespace "downward-api-7429" to be "Succeeded or Failed"
Jun  8 15:13:25.384: INFO: Pod "downwardapi-volume-0aa306eb-af34-4dc6-a823-722c0f142e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.815157ms
Jun  8 15:13:27.389: INFO: Pod "downwardapi-volume-0aa306eb-af34-4dc6-a823-722c0f142e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010419942s
Jun  8 15:13:29.389: INFO: Pod "downwardapi-volume-0aa306eb-af34-4dc6-a823-722c0f142e5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009804548s
STEP: Saw pod success 06/08/23 15:13:29.389
Jun  8 15:13:29.389: INFO: Pod "downwardapi-volume-0aa306eb-af34-4dc6-a823-722c0f142e5f" satisfied condition "Succeeded or Failed"
Jun  8 15:13:29.393: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-0aa306eb-af34-4dc6-a823-722c0f142e5f container client-container: <nil>
STEP: delete the pod 06/08/23 15:13:29.4
Jun  8 15:13:29.412: INFO: Waiting for pod downwardapi-volume-0aa306eb-af34-4dc6-a823-722c0f142e5f to disappear
Jun  8 15:13:29.415: INFO: Pod downwardapi-volume-0aa306eb-af34-4dc6-a823-722c0f142e5f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  8 15:13:29.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7429" for this suite. 06/08/23 15:13:29.421
------------------------------
• [4.082 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:13:25.346
    Jun  8 15:13:25.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename downward-api 06/08/23 15:13:25.348
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:13:25.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:13:25.365
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 06/08/23 15:13:25.368
    Jun  8 15:13:25.379: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0aa306eb-af34-4dc6-a823-722c0f142e5f" in namespace "downward-api-7429" to be "Succeeded or Failed"
    Jun  8 15:13:25.384: INFO: Pod "downwardapi-volume-0aa306eb-af34-4dc6-a823-722c0f142e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.815157ms
    Jun  8 15:13:27.389: INFO: Pod "downwardapi-volume-0aa306eb-af34-4dc6-a823-722c0f142e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010419942s
    Jun  8 15:13:29.389: INFO: Pod "downwardapi-volume-0aa306eb-af34-4dc6-a823-722c0f142e5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009804548s
    STEP: Saw pod success 06/08/23 15:13:29.389
    Jun  8 15:13:29.389: INFO: Pod "downwardapi-volume-0aa306eb-af34-4dc6-a823-722c0f142e5f" satisfied condition "Succeeded or Failed"
    Jun  8 15:13:29.393: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-0aa306eb-af34-4dc6-a823-722c0f142e5f container client-container: <nil>
    STEP: delete the pod 06/08/23 15:13:29.4
    Jun  8 15:13:29.412: INFO: Waiting for pod downwardapi-volume-0aa306eb-af34-4dc6-a823-722c0f142e5f to disappear
    Jun  8 15:13:29.415: INFO: Pod downwardapi-volume-0aa306eb-af34-4dc6-a823-722c0f142e5f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:13:29.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7429" for this suite. 06/08/23 15:13:29.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:13:29.429
Jun  8 15:13:29.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 15:13:29.43
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:13:29.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:13:29.448
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-a5207b56-17af-4531-9c8e-1ade9728e956 06/08/23 15:13:29.451
STEP: Creating a pod to test consume secrets 06/08/23 15:13:29.456
Jun  8 15:13:29.465: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dccd38c5-5312-470f-a054-0cdcf24a834c" in namespace "projected-9501" to be "Succeeded or Failed"
Jun  8 15:13:29.469: INFO: Pod "pod-projected-secrets-dccd38c5-5312-470f-a054-0cdcf24a834c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.695898ms
Jun  8 15:13:31.475: INFO: Pod "pod-projected-secrets-dccd38c5-5312-470f-a054-0cdcf24a834c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009864616s
Jun  8 15:13:33.473: INFO: Pod "pod-projected-secrets-dccd38c5-5312-470f-a054-0cdcf24a834c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008280408s
STEP: Saw pod success 06/08/23 15:13:33.473
Jun  8 15:13:33.474: INFO: Pod "pod-projected-secrets-dccd38c5-5312-470f-a054-0cdcf24a834c" satisfied condition "Succeeded or Failed"
Jun  8 15:13:33.478: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-secrets-dccd38c5-5312-470f-a054-0cdcf24a834c container projected-secret-volume-test: <nil>
STEP: delete the pod 06/08/23 15:13:33.485
Jun  8 15:13:33.500: INFO: Waiting for pod pod-projected-secrets-dccd38c5-5312-470f-a054-0cdcf24a834c to disappear
Jun  8 15:13:33.503: INFO: Pod pod-projected-secrets-dccd38c5-5312-470f-a054-0cdcf24a834c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jun  8 15:13:33.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9501" for this suite. 06/08/23 15:13:33.509
------------------------------
• [4.087 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:13:29.429
    Jun  8 15:13:29.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 15:13:29.43
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:13:29.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:13:29.448
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-a5207b56-17af-4531-9c8e-1ade9728e956 06/08/23 15:13:29.451
    STEP: Creating a pod to test consume secrets 06/08/23 15:13:29.456
    Jun  8 15:13:29.465: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dccd38c5-5312-470f-a054-0cdcf24a834c" in namespace "projected-9501" to be "Succeeded or Failed"
    Jun  8 15:13:29.469: INFO: Pod "pod-projected-secrets-dccd38c5-5312-470f-a054-0cdcf24a834c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.695898ms
    Jun  8 15:13:31.475: INFO: Pod "pod-projected-secrets-dccd38c5-5312-470f-a054-0cdcf24a834c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009864616s
    Jun  8 15:13:33.473: INFO: Pod "pod-projected-secrets-dccd38c5-5312-470f-a054-0cdcf24a834c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008280408s
    STEP: Saw pod success 06/08/23 15:13:33.473
    Jun  8 15:13:33.474: INFO: Pod "pod-projected-secrets-dccd38c5-5312-470f-a054-0cdcf24a834c" satisfied condition "Succeeded or Failed"
    Jun  8 15:13:33.478: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-secrets-dccd38c5-5312-470f-a054-0cdcf24a834c container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/08/23 15:13:33.485
    Jun  8 15:13:33.500: INFO: Waiting for pod pod-projected-secrets-dccd38c5-5312-470f-a054-0cdcf24a834c to disappear
    Jun  8 15:13:33.503: INFO: Pod pod-projected-secrets-dccd38c5-5312-470f-a054-0cdcf24a834c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:13:33.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9501" for this suite. 06/08/23 15:13:33.509
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:13:33.517
Jun  8 15:13:33.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename downward-api 06/08/23 15:13:33.518
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:13:33.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:13:33.537
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 06/08/23 15:13:33.54
Jun  8 15:13:33.548: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c84dea8d-0b6e-4fac-9f3e-f5f998b91c68" in namespace "downward-api-6137" to be "Succeeded or Failed"
Jun  8 15:13:33.552: INFO: Pod "downwardapi-volume-c84dea8d-0b6e-4fac-9f3e-f5f998b91c68": Phase="Pending", Reason="", readiness=false. Elapsed: 3.574056ms
Jun  8 15:13:35.557: INFO: Pod "downwardapi-volume-c84dea8d-0b6e-4fac-9f3e-f5f998b91c68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008527485s
Jun  8 15:13:37.559: INFO: Pod "downwardapi-volume-c84dea8d-0b6e-4fac-9f3e-f5f998b91c68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010568038s
STEP: Saw pod success 06/08/23 15:13:37.559
Jun  8 15:13:37.559: INFO: Pod "downwardapi-volume-c84dea8d-0b6e-4fac-9f3e-f5f998b91c68" satisfied condition "Succeeded or Failed"
Jun  8 15:13:37.563: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-c84dea8d-0b6e-4fac-9f3e-f5f998b91c68 container client-container: <nil>
STEP: delete the pod 06/08/23 15:13:37.571
Jun  8 15:13:37.583: INFO: Waiting for pod downwardapi-volume-c84dea8d-0b6e-4fac-9f3e-f5f998b91c68 to disappear
Jun  8 15:13:37.587: INFO: Pod downwardapi-volume-c84dea8d-0b6e-4fac-9f3e-f5f998b91c68 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  8 15:13:37.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6137" for this suite. 06/08/23 15:13:37.592
------------------------------
• [4.083 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:13:33.517
    Jun  8 15:13:33.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename downward-api 06/08/23 15:13:33.518
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:13:33.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:13:33.537
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 06/08/23 15:13:33.54
    Jun  8 15:13:33.548: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c84dea8d-0b6e-4fac-9f3e-f5f998b91c68" in namespace "downward-api-6137" to be "Succeeded or Failed"
    Jun  8 15:13:33.552: INFO: Pod "downwardapi-volume-c84dea8d-0b6e-4fac-9f3e-f5f998b91c68": Phase="Pending", Reason="", readiness=false. Elapsed: 3.574056ms
    Jun  8 15:13:35.557: INFO: Pod "downwardapi-volume-c84dea8d-0b6e-4fac-9f3e-f5f998b91c68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008527485s
    Jun  8 15:13:37.559: INFO: Pod "downwardapi-volume-c84dea8d-0b6e-4fac-9f3e-f5f998b91c68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010568038s
    STEP: Saw pod success 06/08/23 15:13:37.559
    Jun  8 15:13:37.559: INFO: Pod "downwardapi-volume-c84dea8d-0b6e-4fac-9f3e-f5f998b91c68" satisfied condition "Succeeded or Failed"
    Jun  8 15:13:37.563: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-c84dea8d-0b6e-4fac-9f3e-f5f998b91c68 container client-container: <nil>
    STEP: delete the pod 06/08/23 15:13:37.571
    Jun  8 15:13:37.583: INFO: Waiting for pod downwardapi-volume-c84dea8d-0b6e-4fac-9f3e-f5f998b91c68 to disappear
    Jun  8 15:13:37.587: INFO: Pod downwardapi-volume-c84dea8d-0b6e-4fac-9f3e-f5f998b91c68 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:13:37.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6137" for this suite. 06/08/23 15:13:37.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:13:37.6
Jun  8 15:13:37.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename gc 06/08/23 15:13:37.601
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:13:37.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:13:37.619
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 06/08/23 15:13:37.628
STEP: create the rc2 06/08/23 15:13:37.633
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 06/08/23 15:13:42.648
STEP: delete the rc simpletest-rc-to-be-deleted 06/08/23 15:13:43.618
STEP: wait for the rc to be deleted 06/08/23 15:13:43.634
Jun  8 15:13:48.656: INFO: 70 pods remaining
Jun  8 15:13:48.656: INFO: 70 pods has nil DeletionTimestamp
Jun  8 15:13:48.656: INFO: 
STEP: Gathering metrics 06/08/23 15:13:53.654
Jun  8 15:13:53.690: INFO: Waiting up to 5m0s for pod "kube-controller-manager-chl8tf-control-plane-003" in namespace "kube-system" to be "running and ready"
Jun  8 15:13:53.694: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003": Phase="Running", Reason="", readiness=true. Elapsed: 4.887306ms
Jun  8 15:13:53.695: INFO: The phase of Pod kube-controller-manager-chl8tf-control-plane-003 is Running (Ready = true)
Jun  8 15:13:53.695: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003" satisfied condition "running and ready"
Jun  8 15:13:53.759: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun  8 15:13:53.759: INFO: Deleting pod "simpletest-rc-to-be-deleted-295v4" in namespace "gc-4145"
Jun  8 15:13:53.783: INFO: Deleting pod "simpletest-rc-to-be-deleted-296h4" in namespace "gc-4145"
Jun  8 15:13:53.794: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bspx" in namespace "gc-4145"
Jun  8 15:13:53.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gltc" in namespace "gc-4145"
Jun  8 15:13:53.817: INFO: Deleting pod "simpletest-rc-to-be-deleted-2zxnj" in namespace "gc-4145"
Jun  8 15:13:53.834: INFO: Deleting pod "simpletest-rc-to-be-deleted-4snjr" in namespace "gc-4145"
Jun  8 15:13:53.866: INFO: Deleting pod "simpletest-rc-to-be-deleted-65k2f" in namespace "gc-4145"
Jun  8 15:13:53.905: INFO: Deleting pod "simpletest-rc-to-be-deleted-68h8n" in namespace "gc-4145"
Jun  8 15:13:53.926: INFO: Deleting pod "simpletest-rc-to-be-deleted-68xqd" in namespace "gc-4145"
Jun  8 15:13:53.940: INFO: Deleting pod "simpletest-rc-to-be-deleted-6bsnj" in namespace "gc-4145"
Jun  8 15:13:53.956: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dnxq" in namespace "gc-4145"
Jun  8 15:13:53.973: INFO: Deleting pod "simpletest-rc-to-be-deleted-6l42m" in namespace "gc-4145"
Jun  8 15:13:53.988: INFO: Deleting pod "simpletest-rc-to-be-deleted-6sp6q" in namespace "gc-4145"
Jun  8 15:13:54.013: INFO: Deleting pod "simpletest-rc-to-be-deleted-76nqh" in namespace "gc-4145"
Jun  8 15:13:54.030: INFO: Deleting pod "simpletest-rc-to-be-deleted-7trbj" in namespace "gc-4145"
Jun  8 15:13:54.047: INFO: Deleting pod "simpletest-rc-to-be-deleted-82f6j" in namespace "gc-4145"
Jun  8 15:13:54.068: INFO: Deleting pod "simpletest-rc-to-be-deleted-858t7" in namespace "gc-4145"
Jun  8 15:13:54.112: INFO: Deleting pod "simpletest-rc-to-be-deleted-8csbp" in namespace "gc-4145"
Jun  8 15:13:54.131: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jtm9" in namespace "gc-4145"
Jun  8 15:13:54.156: INFO: Deleting pod "simpletest-rc-to-be-deleted-8sw7f" in namespace "gc-4145"
Jun  8 15:13:54.179: INFO: Deleting pod "simpletest-rc-to-be-deleted-8wdqw" in namespace "gc-4145"
Jun  8 15:13:54.200: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xmh9" in namespace "gc-4145"
Jun  8 15:13:54.218: INFO: Deleting pod "simpletest-rc-to-be-deleted-9md5h" in namespace "gc-4145"
Jun  8 15:13:54.239: INFO: Deleting pod "simpletest-rc-to-be-deleted-b29nx" in namespace "gc-4145"
Jun  8 15:13:54.270: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5wjp" in namespace "gc-4145"
Jun  8 15:13:54.293: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7cp9" in namespace "gc-4145"
Jun  8 15:13:54.313: INFO: Deleting pod "simpletest-rc-to-be-deleted-b98pc" in namespace "gc-4145"
Jun  8 15:13:54.355: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbl4h" in namespace "gc-4145"
Jun  8 15:13:54.372: INFO: Deleting pod "simpletest-rc-to-be-deleted-btwbc" in namespace "gc-4145"
Jun  8 15:13:54.399: INFO: Deleting pod "simpletest-rc-to-be-deleted-c78bh" in namespace "gc-4145"
Jun  8 15:13:54.438: INFO: Deleting pod "simpletest-rc-to-be-deleted-c89pz" in namespace "gc-4145"
Jun  8 15:13:54.457: INFO: Deleting pod "simpletest-rc-to-be-deleted-chfjs" in namespace "gc-4145"
Jun  8 15:13:54.477: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddlgj" in namespace "gc-4145"
Jun  8 15:13:54.498: INFO: Deleting pod "simpletest-rc-to-be-deleted-dg6gr" in namespace "gc-4145"
Jun  8 15:13:54.518: INFO: Deleting pod "simpletest-rc-to-be-deleted-dg9pd" in namespace "gc-4145"
Jun  8 15:13:54.541: INFO: Deleting pod "simpletest-rc-to-be-deleted-dhcc8" in namespace "gc-4145"
Jun  8 15:13:54.559: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnl6s" in namespace "gc-4145"
Jun  8 15:13:54.581: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbj2d" in namespace "gc-4145"
Jun  8 15:13:54.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftch2" in namespace "gc-4145"
Jun  8 15:13:54.621: INFO: Deleting pod "simpletest-rc-to-be-deleted-g99wc" in namespace "gc-4145"
Jun  8 15:13:54.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdg6f" in namespace "gc-4145"
Jun  8 15:13:54.683: INFO: Deleting pod "simpletest-rc-to-be-deleted-gf2ng" in namespace "gc-4145"
Jun  8 15:13:54.707: INFO: Deleting pod "simpletest-rc-to-be-deleted-gshmd" in namespace "gc-4145"
Jun  8 15:13:54.729: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxxv5" in namespace "gc-4145"
Jun  8 15:13:54.760: INFO: Deleting pod "simpletest-rc-to-be-deleted-hzf7f" in namespace "gc-4145"
Jun  8 15:13:54.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-j7h4g" in namespace "gc-4145"
Jun  8 15:13:54.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-j8z92" in namespace "gc-4145"
Jun  8 15:13:54.847: INFO: Deleting pod "simpletest-rc-to-be-deleted-jccqm" in namespace "gc-4145"
Jun  8 15:13:54.868: INFO: Deleting pod "simpletest-rc-to-be-deleted-jgrqx" in namespace "gc-4145"
Jun  8 15:13:54.915: INFO: Deleting pod "simpletest-rc-to-be-deleted-jh8bt" in namespace "gc-4145"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jun  8 15:13:54.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4145" for this suite. 06/08/23 15:13:54.959
------------------------------
• [SLOW TEST] [17.371 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:13:37.6
    Jun  8 15:13:37.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename gc 06/08/23 15:13:37.601
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:13:37.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:13:37.619
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 06/08/23 15:13:37.628
    STEP: create the rc2 06/08/23 15:13:37.633
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 06/08/23 15:13:42.648
    STEP: delete the rc simpletest-rc-to-be-deleted 06/08/23 15:13:43.618
    STEP: wait for the rc to be deleted 06/08/23 15:13:43.634
    Jun  8 15:13:48.656: INFO: 70 pods remaining
    Jun  8 15:13:48.656: INFO: 70 pods has nil DeletionTimestamp
    Jun  8 15:13:48.656: INFO: 
    STEP: Gathering metrics 06/08/23 15:13:53.654
    Jun  8 15:13:53.690: INFO: Waiting up to 5m0s for pod "kube-controller-manager-chl8tf-control-plane-003" in namespace "kube-system" to be "running and ready"
    Jun  8 15:13:53.694: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003": Phase="Running", Reason="", readiness=true. Elapsed: 4.887306ms
    Jun  8 15:13:53.695: INFO: The phase of Pod kube-controller-manager-chl8tf-control-plane-003 is Running (Ready = true)
    Jun  8 15:13:53.695: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003" satisfied condition "running and ready"
    Jun  8 15:13:53.759: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jun  8 15:13:53.759: INFO: Deleting pod "simpletest-rc-to-be-deleted-295v4" in namespace "gc-4145"
    Jun  8 15:13:53.783: INFO: Deleting pod "simpletest-rc-to-be-deleted-296h4" in namespace "gc-4145"
    Jun  8 15:13:53.794: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bspx" in namespace "gc-4145"
    Jun  8 15:13:53.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gltc" in namespace "gc-4145"
    Jun  8 15:13:53.817: INFO: Deleting pod "simpletest-rc-to-be-deleted-2zxnj" in namespace "gc-4145"
    Jun  8 15:13:53.834: INFO: Deleting pod "simpletest-rc-to-be-deleted-4snjr" in namespace "gc-4145"
    Jun  8 15:13:53.866: INFO: Deleting pod "simpletest-rc-to-be-deleted-65k2f" in namespace "gc-4145"
    Jun  8 15:13:53.905: INFO: Deleting pod "simpletest-rc-to-be-deleted-68h8n" in namespace "gc-4145"
    Jun  8 15:13:53.926: INFO: Deleting pod "simpletest-rc-to-be-deleted-68xqd" in namespace "gc-4145"
    Jun  8 15:13:53.940: INFO: Deleting pod "simpletest-rc-to-be-deleted-6bsnj" in namespace "gc-4145"
    Jun  8 15:13:53.956: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dnxq" in namespace "gc-4145"
    Jun  8 15:13:53.973: INFO: Deleting pod "simpletest-rc-to-be-deleted-6l42m" in namespace "gc-4145"
    Jun  8 15:13:53.988: INFO: Deleting pod "simpletest-rc-to-be-deleted-6sp6q" in namespace "gc-4145"
    Jun  8 15:13:54.013: INFO: Deleting pod "simpletest-rc-to-be-deleted-76nqh" in namespace "gc-4145"
    Jun  8 15:13:54.030: INFO: Deleting pod "simpletest-rc-to-be-deleted-7trbj" in namespace "gc-4145"
    Jun  8 15:13:54.047: INFO: Deleting pod "simpletest-rc-to-be-deleted-82f6j" in namespace "gc-4145"
    Jun  8 15:13:54.068: INFO: Deleting pod "simpletest-rc-to-be-deleted-858t7" in namespace "gc-4145"
    Jun  8 15:13:54.112: INFO: Deleting pod "simpletest-rc-to-be-deleted-8csbp" in namespace "gc-4145"
    Jun  8 15:13:54.131: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jtm9" in namespace "gc-4145"
    Jun  8 15:13:54.156: INFO: Deleting pod "simpletest-rc-to-be-deleted-8sw7f" in namespace "gc-4145"
    Jun  8 15:13:54.179: INFO: Deleting pod "simpletest-rc-to-be-deleted-8wdqw" in namespace "gc-4145"
    Jun  8 15:13:54.200: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xmh9" in namespace "gc-4145"
    Jun  8 15:13:54.218: INFO: Deleting pod "simpletest-rc-to-be-deleted-9md5h" in namespace "gc-4145"
    Jun  8 15:13:54.239: INFO: Deleting pod "simpletest-rc-to-be-deleted-b29nx" in namespace "gc-4145"
    Jun  8 15:13:54.270: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5wjp" in namespace "gc-4145"
    Jun  8 15:13:54.293: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7cp9" in namespace "gc-4145"
    Jun  8 15:13:54.313: INFO: Deleting pod "simpletest-rc-to-be-deleted-b98pc" in namespace "gc-4145"
    Jun  8 15:13:54.355: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbl4h" in namespace "gc-4145"
    Jun  8 15:13:54.372: INFO: Deleting pod "simpletest-rc-to-be-deleted-btwbc" in namespace "gc-4145"
    Jun  8 15:13:54.399: INFO: Deleting pod "simpletest-rc-to-be-deleted-c78bh" in namespace "gc-4145"
    Jun  8 15:13:54.438: INFO: Deleting pod "simpletest-rc-to-be-deleted-c89pz" in namespace "gc-4145"
    Jun  8 15:13:54.457: INFO: Deleting pod "simpletest-rc-to-be-deleted-chfjs" in namespace "gc-4145"
    Jun  8 15:13:54.477: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddlgj" in namespace "gc-4145"
    Jun  8 15:13:54.498: INFO: Deleting pod "simpletest-rc-to-be-deleted-dg6gr" in namespace "gc-4145"
    Jun  8 15:13:54.518: INFO: Deleting pod "simpletest-rc-to-be-deleted-dg9pd" in namespace "gc-4145"
    Jun  8 15:13:54.541: INFO: Deleting pod "simpletest-rc-to-be-deleted-dhcc8" in namespace "gc-4145"
    Jun  8 15:13:54.559: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnl6s" in namespace "gc-4145"
    Jun  8 15:13:54.581: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbj2d" in namespace "gc-4145"
    Jun  8 15:13:54.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftch2" in namespace "gc-4145"
    Jun  8 15:13:54.621: INFO: Deleting pod "simpletest-rc-to-be-deleted-g99wc" in namespace "gc-4145"
    Jun  8 15:13:54.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdg6f" in namespace "gc-4145"
    Jun  8 15:13:54.683: INFO: Deleting pod "simpletest-rc-to-be-deleted-gf2ng" in namespace "gc-4145"
    Jun  8 15:13:54.707: INFO: Deleting pod "simpletest-rc-to-be-deleted-gshmd" in namespace "gc-4145"
    Jun  8 15:13:54.729: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxxv5" in namespace "gc-4145"
    Jun  8 15:13:54.760: INFO: Deleting pod "simpletest-rc-to-be-deleted-hzf7f" in namespace "gc-4145"
    Jun  8 15:13:54.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-j7h4g" in namespace "gc-4145"
    Jun  8 15:13:54.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-j8z92" in namespace "gc-4145"
    Jun  8 15:13:54.847: INFO: Deleting pod "simpletest-rc-to-be-deleted-jccqm" in namespace "gc-4145"
    Jun  8 15:13:54.868: INFO: Deleting pod "simpletest-rc-to-be-deleted-jgrqx" in namespace "gc-4145"
    Jun  8 15:13:54.915: INFO: Deleting pod "simpletest-rc-to-be-deleted-jh8bt" in namespace "gc-4145"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:13:54.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4145" for this suite. 06/08/23 15:13:54.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:13:54.975
Jun  8 15:13:54.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename endpointslice 06/08/23 15:13:54.977
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:13:55.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:13:55.037
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 06/08/23 15:13:55.063
STEP: getting /apis/discovery.k8s.io 06/08/23 15:13:55.067
STEP: getting /apis/discovery.k8s.iov1 06/08/23 15:13:55.069
STEP: creating 06/08/23 15:13:55.071
STEP: getting 06/08/23 15:13:55.101
STEP: listing 06/08/23 15:13:55.116
STEP: watching 06/08/23 15:13:55.126
Jun  8 15:13:55.126: INFO: starting watch
STEP: cluster-wide listing 06/08/23 15:13:55.128
STEP: cluster-wide watching 06/08/23 15:13:55.134
Jun  8 15:13:55.134: INFO: starting watch
STEP: patching 06/08/23 15:13:55.136
STEP: updating 06/08/23 15:13:55.148
Jun  8 15:13:55.163: INFO: waiting for watch events with expected annotations
Jun  8 15:13:55.163: INFO: saw patched and updated annotations
STEP: deleting 06/08/23 15:13:55.164
STEP: deleting a collection 06/08/23 15:13:55.186
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jun  8 15:13:55.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5690" for this suite. 06/08/23 15:13:55.224
------------------------------
• [0.263 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:13:54.975
    Jun  8 15:13:54.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename endpointslice 06/08/23 15:13:54.977
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:13:55.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:13:55.037
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 06/08/23 15:13:55.063
    STEP: getting /apis/discovery.k8s.io 06/08/23 15:13:55.067
    STEP: getting /apis/discovery.k8s.iov1 06/08/23 15:13:55.069
    STEP: creating 06/08/23 15:13:55.071
    STEP: getting 06/08/23 15:13:55.101
    STEP: listing 06/08/23 15:13:55.116
    STEP: watching 06/08/23 15:13:55.126
    Jun  8 15:13:55.126: INFO: starting watch
    STEP: cluster-wide listing 06/08/23 15:13:55.128
    STEP: cluster-wide watching 06/08/23 15:13:55.134
    Jun  8 15:13:55.134: INFO: starting watch
    STEP: patching 06/08/23 15:13:55.136
    STEP: updating 06/08/23 15:13:55.148
    Jun  8 15:13:55.163: INFO: waiting for watch events with expected annotations
    Jun  8 15:13:55.163: INFO: saw patched and updated annotations
    STEP: deleting 06/08/23 15:13:55.164
    STEP: deleting a collection 06/08/23 15:13:55.186
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:13:55.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5690" for this suite. 06/08/23 15:13:55.224
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:13:55.237
Jun  8 15:13:55.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename namespaces 06/08/23 15:13:55.239
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:13:55.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:13:55.282
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 06/08/23 15:13:55.287
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:13:55.31
STEP: Creating a pod in the namespace 06/08/23 15:13:55.318
STEP: Waiting for the pod to have running status 06/08/23 15:13:55.332
Jun  8 15:13:55.332: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-1166" to be "running"
Jun  8 15:13:55.340: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009276ms
Jun  8 15:13:57.345: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013034597s
Jun  8 15:13:59.347: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014205514s
Jun  8 15:14:01.346: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013614717s
Jun  8 15:14:03.347: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.014766646s
Jun  8 15:14:03.347: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 06/08/23 15:14:03.347
STEP: Waiting for the namespace to be removed. 06/08/23 15:14:03.356
STEP: Recreating the namespace 06/08/23 15:14:14.362
STEP: Verifying there are no pods in the namespace 06/08/23 15:14:14.38
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:14:14.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-8824" for this suite. 06/08/23 15:14:14.391
STEP: Destroying namespace "nsdeletetest-1166" for this suite. 06/08/23 15:14:14.398
Jun  8 15:14:14.403: INFO: Namespace nsdeletetest-1166 was already deleted
STEP: Destroying namespace "nsdeletetest-8089" for this suite. 06/08/23 15:14:14.403
------------------------------
• [SLOW TEST] [19.174 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:13:55.237
    Jun  8 15:13:55.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename namespaces 06/08/23 15:13:55.239
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:13:55.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:13:55.282
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 06/08/23 15:13:55.287
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:13:55.31
    STEP: Creating a pod in the namespace 06/08/23 15:13:55.318
    STEP: Waiting for the pod to have running status 06/08/23 15:13:55.332
    Jun  8 15:13:55.332: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-1166" to be "running"
    Jun  8 15:13:55.340: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009276ms
    Jun  8 15:13:57.345: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013034597s
    Jun  8 15:13:59.347: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014205514s
    Jun  8 15:14:01.346: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013614717s
    Jun  8 15:14:03.347: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.014766646s
    Jun  8 15:14:03.347: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 06/08/23 15:14:03.347
    STEP: Waiting for the namespace to be removed. 06/08/23 15:14:03.356
    STEP: Recreating the namespace 06/08/23 15:14:14.362
    STEP: Verifying there are no pods in the namespace 06/08/23 15:14:14.38
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:14:14.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-8824" for this suite. 06/08/23 15:14:14.391
    STEP: Destroying namespace "nsdeletetest-1166" for this suite. 06/08/23 15:14:14.398
    Jun  8 15:14:14.403: INFO: Namespace nsdeletetest-1166 was already deleted
    STEP: Destroying namespace "nsdeletetest-8089" for this suite. 06/08/23 15:14:14.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:14:14.412
Jun  8 15:14:14.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename runtimeclass 06/08/23 15:14:14.413
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:14:14.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:14:14.434
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 06/08/23 15:14:14.437
STEP: getting /apis/node.k8s.io 06/08/23 15:14:14.44
STEP: getting /apis/node.k8s.io/v1 06/08/23 15:14:14.442
STEP: creating 06/08/23 15:14:14.443
STEP: watching 06/08/23 15:14:14.463
Jun  8 15:14:14.463: INFO: starting watch
STEP: getting 06/08/23 15:14:14.469
STEP: listing 06/08/23 15:14:14.473
STEP: patching 06/08/23 15:14:14.477
STEP: updating 06/08/23 15:14:14.484
Jun  8 15:14:14.490: INFO: waiting for watch events with expected annotations
STEP: deleting 06/08/23 15:14:14.491
STEP: deleting a collection 06/08/23 15:14:14.506
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jun  8 15:14:14.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4740" for this suite. 06/08/23 15:14:14.534
------------------------------
• [0.131 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:14:14.412
    Jun  8 15:14:14.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename runtimeclass 06/08/23 15:14:14.413
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:14:14.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:14:14.434
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 06/08/23 15:14:14.437
    STEP: getting /apis/node.k8s.io 06/08/23 15:14:14.44
    STEP: getting /apis/node.k8s.io/v1 06/08/23 15:14:14.442
    STEP: creating 06/08/23 15:14:14.443
    STEP: watching 06/08/23 15:14:14.463
    Jun  8 15:14:14.463: INFO: starting watch
    STEP: getting 06/08/23 15:14:14.469
    STEP: listing 06/08/23 15:14:14.473
    STEP: patching 06/08/23 15:14:14.477
    STEP: updating 06/08/23 15:14:14.484
    Jun  8 15:14:14.490: INFO: waiting for watch events with expected annotations
    STEP: deleting 06/08/23 15:14:14.491
    STEP: deleting a collection 06/08/23 15:14:14.506
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:14:14.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4740" for this suite. 06/08/23 15:14:14.534
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:14:14.544
Jun  8 15:14:14.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename configmap 06/08/23 15:14:14.545
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:14:14.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:14:14.566
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-86d712b0-924d-4566-bc5c-fecdca976741 06/08/23 15:14:14.569
STEP: Creating a pod to test consume configMaps 06/08/23 15:14:14.577
Jun  8 15:14:14.588: INFO: Waiting up to 5m0s for pod "pod-configmaps-35c843ef-32b3-4aa3-8b52-a5e34c986529" in namespace "configmap-8819" to be "Succeeded or Failed"
Jun  8 15:14:14.592: INFO: Pod "pod-configmaps-35c843ef-32b3-4aa3-8b52-a5e34c986529": Phase="Pending", Reason="", readiness=false. Elapsed: 4.11275ms
Jun  8 15:14:16.598: INFO: Pod "pod-configmaps-35c843ef-32b3-4aa3-8b52-a5e34c986529": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010041975s
Jun  8 15:14:18.598: INFO: Pod "pod-configmaps-35c843ef-32b3-4aa3-8b52-a5e34c986529": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009895996s
STEP: Saw pod success 06/08/23 15:14:18.598
Jun  8 15:14:18.598: INFO: Pod "pod-configmaps-35c843ef-32b3-4aa3-8b52-a5e34c986529" satisfied condition "Succeeded or Failed"
Jun  8 15:14:18.602: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-configmaps-35c843ef-32b3-4aa3-8b52-a5e34c986529 container configmap-volume-test: <nil>
STEP: delete the pod 06/08/23 15:14:18.61
Jun  8 15:14:18.622: INFO: Waiting for pod pod-configmaps-35c843ef-32b3-4aa3-8b52-a5e34c986529 to disappear
Jun  8 15:14:18.625: INFO: Pod pod-configmaps-35c843ef-32b3-4aa3-8b52-a5e34c986529 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  8 15:14:18.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8819" for this suite. 06/08/23 15:14:18.632
------------------------------
• [4.095 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:14:14.544
    Jun  8 15:14:14.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename configmap 06/08/23 15:14:14.545
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:14:14.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:14:14.566
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-86d712b0-924d-4566-bc5c-fecdca976741 06/08/23 15:14:14.569
    STEP: Creating a pod to test consume configMaps 06/08/23 15:14:14.577
    Jun  8 15:14:14.588: INFO: Waiting up to 5m0s for pod "pod-configmaps-35c843ef-32b3-4aa3-8b52-a5e34c986529" in namespace "configmap-8819" to be "Succeeded or Failed"
    Jun  8 15:14:14.592: INFO: Pod "pod-configmaps-35c843ef-32b3-4aa3-8b52-a5e34c986529": Phase="Pending", Reason="", readiness=false. Elapsed: 4.11275ms
    Jun  8 15:14:16.598: INFO: Pod "pod-configmaps-35c843ef-32b3-4aa3-8b52-a5e34c986529": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010041975s
    Jun  8 15:14:18.598: INFO: Pod "pod-configmaps-35c843ef-32b3-4aa3-8b52-a5e34c986529": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009895996s
    STEP: Saw pod success 06/08/23 15:14:18.598
    Jun  8 15:14:18.598: INFO: Pod "pod-configmaps-35c843ef-32b3-4aa3-8b52-a5e34c986529" satisfied condition "Succeeded or Failed"
    Jun  8 15:14:18.602: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-configmaps-35c843ef-32b3-4aa3-8b52-a5e34c986529 container configmap-volume-test: <nil>
    STEP: delete the pod 06/08/23 15:14:18.61
    Jun  8 15:14:18.622: INFO: Waiting for pod pod-configmaps-35c843ef-32b3-4aa3-8b52-a5e34c986529 to disappear
    Jun  8 15:14:18.625: INFO: Pod pod-configmaps-35c843ef-32b3-4aa3-8b52-a5e34c986529 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:14:18.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8819" for this suite. 06/08/23 15:14:18.632
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:14:18.639
Jun  8 15:14:18.639: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename webhook 06/08/23 15:14:18.641
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:14:18.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:14:18.658
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/08/23 15:14:18.675
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 15:14:19.106
STEP: Deploying the webhook pod 06/08/23 15:14:19.116
STEP: Wait for the deployment to be ready 06/08/23 15:14:19.127
Jun  8 15:14:19.136: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/08/23 15:14:21.15
STEP: Verifying the service has paired with the endpoint 06/08/23 15:14:21.167
Jun  8 15:14:22.168: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 06/08/23 15:14:22.173
STEP: create a pod 06/08/23 15:14:22.19
Jun  8 15:14:22.196: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1675" to be "running"
Jun  8 15:14:22.200: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.085017ms
Jun  8 15:14:24.205: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009000935s
Jun  8 15:14:24.205: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 06/08/23 15:14:24.205
Jun  8 15:14:24.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=webhook-1675 attach --namespace=webhook-1675 to-be-attached-pod -i -c=container1'
Jun  8 15:14:24.305: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:14:24.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1675" for this suite. 06/08/23 15:14:24.402
STEP: Destroying namespace "webhook-1675-markers" for this suite. 06/08/23 15:14:24.424
------------------------------
• [SLOW TEST] [5.808 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:14:18.639
    Jun  8 15:14:18.639: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename webhook 06/08/23 15:14:18.641
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:14:18.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:14:18.658
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/08/23 15:14:18.675
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 15:14:19.106
    STEP: Deploying the webhook pod 06/08/23 15:14:19.116
    STEP: Wait for the deployment to be ready 06/08/23 15:14:19.127
    Jun  8 15:14:19.136: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/08/23 15:14:21.15
    STEP: Verifying the service has paired with the endpoint 06/08/23 15:14:21.167
    Jun  8 15:14:22.168: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 06/08/23 15:14:22.173
    STEP: create a pod 06/08/23 15:14:22.19
    Jun  8 15:14:22.196: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1675" to be "running"
    Jun  8 15:14:22.200: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.085017ms
    Jun  8 15:14:24.205: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009000935s
    Jun  8 15:14:24.205: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 06/08/23 15:14:24.205
    Jun  8 15:14:24.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=webhook-1675 attach --namespace=webhook-1675 to-be-attached-pod -i -c=container1'
    Jun  8 15:14:24.305: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:14:24.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1675" for this suite. 06/08/23 15:14:24.402
    STEP: Destroying namespace "webhook-1675-markers" for this suite. 06/08/23 15:14:24.424
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:14:24.448
Jun  8 15:14:24.448: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename secrets 06/08/23 15:14:24.45
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:14:24.474
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:14:24.48
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-3c0057a2-4414-4958-91cd-ecd602028a3e 06/08/23 15:14:24.489
STEP: Creating a pod to test consume secrets 06/08/23 15:14:24.497
Jun  8 15:14:24.515: INFO: Waiting up to 5m0s for pod "pod-secrets-e83dd0c7-539e-4d0a-83a4-46c3a1891e9b" in namespace "secrets-3089" to be "Succeeded or Failed"
Jun  8 15:14:24.524: INFO: Pod "pod-secrets-e83dd0c7-539e-4d0a-83a4-46c3a1891e9b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.052271ms
Jun  8 15:14:26.530: INFO: Pod "pod-secrets-e83dd0c7-539e-4d0a-83a4-46c3a1891e9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014793761s
Jun  8 15:14:28.531: INFO: Pod "pod-secrets-e83dd0c7-539e-4d0a-83a4-46c3a1891e9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01531586s
STEP: Saw pod success 06/08/23 15:14:28.531
Jun  8 15:14:28.531: INFO: Pod "pod-secrets-e83dd0c7-539e-4d0a-83a4-46c3a1891e9b" satisfied condition "Succeeded or Failed"
Jun  8 15:14:28.535: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-secrets-e83dd0c7-539e-4d0a-83a4-46c3a1891e9b container secret-volume-test: <nil>
STEP: delete the pod 06/08/23 15:14:28.542
Jun  8 15:14:28.557: INFO: Waiting for pod pod-secrets-e83dd0c7-539e-4d0a-83a4-46c3a1891e9b to disappear
Jun  8 15:14:28.563: INFO: Pod pod-secrets-e83dd0c7-539e-4d0a-83a4-46c3a1891e9b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  8 15:14:28.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3089" for this suite. 06/08/23 15:14:28.569
------------------------------
• [4.128 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:14:24.448
    Jun  8 15:14:24.448: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename secrets 06/08/23 15:14:24.45
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:14:24.474
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:14:24.48
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-3c0057a2-4414-4958-91cd-ecd602028a3e 06/08/23 15:14:24.489
    STEP: Creating a pod to test consume secrets 06/08/23 15:14:24.497
    Jun  8 15:14:24.515: INFO: Waiting up to 5m0s for pod "pod-secrets-e83dd0c7-539e-4d0a-83a4-46c3a1891e9b" in namespace "secrets-3089" to be "Succeeded or Failed"
    Jun  8 15:14:24.524: INFO: Pod "pod-secrets-e83dd0c7-539e-4d0a-83a4-46c3a1891e9b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.052271ms
    Jun  8 15:14:26.530: INFO: Pod "pod-secrets-e83dd0c7-539e-4d0a-83a4-46c3a1891e9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014793761s
    Jun  8 15:14:28.531: INFO: Pod "pod-secrets-e83dd0c7-539e-4d0a-83a4-46c3a1891e9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01531586s
    STEP: Saw pod success 06/08/23 15:14:28.531
    Jun  8 15:14:28.531: INFO: Pod "pod-secrets-e83dd0c7-539e-4d0a-83a4-46c3a1891e9b" satisfied condition "Succeeded or Failed"
    Jun  8 15:14:28.535: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-secrets-e83dd0c7-539e-4d0a-83a4-46c3a1891e9b container secret-volume-test: <nil>
    STEP: delete the pod 06/08/23 15:14:28.542
    Jun  8 15:14:28.557: INFO: Waiting for pod pod-secrets-e83dd0c7-539e-4d0a-83a4-46c3a1891e9b to disappear
    Jun  8 15:14:28.563: INFO: Pod pod-secrets-e83dd0c7-539e-4d0a-83a4-46c3a1891e9b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:14:28.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3089" for this suite. 06/08/23 15:14:28.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:14:28.579
Jun  8 15:14:28.579: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubelet-test 06/08/23 15:14:28.58
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:14:28.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:14:28.598
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jun  8 15:14:28.610: INFO: Waiting up to 5m0s for pod "busybox-scheduling-80648aa4-43d0-46d3-84fc-47821625a928" in namespace "kubelet-test-5792" to be "running and ready"
Jun  8 15:14:28.614: INFO: Pod "busybox-scheduling-80648aa4-43d0-46d3-84fc-47821625a928": Phase="Pending", Reason="", readiness=false. Elapsed: 4.141184ms
Jun  8 15:14:28.614: INFO: The phase of Pod busybox-scheduling-80648aa4-43d0-46d3-84fc-47821625a928 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:14:30.619: INFO: Pod "busybox-scheduling-80648aa4-43d0-46d3-84fc-47821625a928": Phase="Running", Reason="", readiness=true. Elapsed: 2.009413388s
Jun  8 15:14:30.619: INFO: The phase of Pod busybox-scheduling-80648aa4-43d0-46d3-84fc-47821625a928 is Running (Ready = true)
Jun  8 15:14:30.619: INFO: Pod "busybox-scheduling-80648aa4-43d0-46d3-84fc-47821625a928" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jun  8 15:14:30.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5792" for this suite. 06/08/23 15:14:30.636
------------------------------
• [2.065 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:14:28.579
    Jun  8 15:14:28.579: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubelet-test 06/08/23 15:14:28.58
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:14:28.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:14:28.598
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jun  8 15:14:28.610: INFO: Waiting up to 5m0s for pod "busybox-scheduling-80648aa4-43d0-46d3-84fc-47821625a928" in namespace "kubelet-test-5792" to be "running and ready"
    Jun  8 15:14:28.614: INFO: Pod "busybox-scheduling-80648aa4-43d0-46d3-84fc-47821625a928": Phase="Pending", Reason="", readiness=false. Elapsed: 4.141184ms
    Jun  8 15:14:28.614: INFO: The phase of Pod busybox-scheduling-80648aa4-43d0-46d3-84fc-47821625a928 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:14:30.619: INFO: Pod "busybox-scheduling-80648aa4-43d0-46d3-84fc-47821625a928": Phase="Running", Reason="", readiness=true. Elapsed: 2.009413388s
    Jun  8 15:14:30.619: INFO: The phase of Pod busybox-scheduling-80648aa4-43d0-46d3-84fc-47821625a928 is Running (Ready = true)
    Jun  8 15:14:30.619: INFO: Pod "busybox-scheduling-80648aa4-43d0-46d3-84fc-47821625a928" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:14:30.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5792" for this suite. 06/08/23 15:14:30.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:14:30.644
Jun  8 15:14:30.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename aggregator 06/08/23 15:14:30.645
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:14:30.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:14:30.662
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jun  8 15:14:30.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 06/08/23 15:14:30.666
Jun  8 15:14:31.342: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun  8 15:14:33.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  8 15:14:35.399: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  8 15:14:37.400: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  8 15:14:39.399: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  8 15:14:41.399: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  8 15:14:43.400: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  8 15:14:45.399: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  8 15:14:47.399: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  8 15:14:49.398: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  8 15:14:51.400: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  8 15:14:53.400: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  8 15:14:55.531: INFO: Waited 124.497555ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 06/08/23 15:14:55.585
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 06/08/23 15:14:55.589
STEP: List APIServices 06/08/23 15:14:55.597
Jun  8 15:14:55.605: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Jun  8 15:14:55.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-1219" for this suite. 06/08/23 15:14:55.786
------------------------------
• [SLOW TEST] [25.152 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:14:30.644
    Jun  8 15:14:30.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename aggregator 06/08/23 15:14:30.645
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:14:30.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:14:30.662
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jun  8 15:14:30.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 06/08/23 15:14:30.666
    Jun  8 15:14:31.342: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jun  8 15:14:33.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  8 15:14:35.399: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  8 15:14:37.400: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  8 15:14:39.399: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  8 15:14:41.399: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  8 15:14:43.400: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  8 15:14:45.399: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  8 15:14:47.399: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  8 15:14:49.398: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  8 15:14:51.400: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  8 15:14:53.400: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  8 15:14:55.531: INFO: Waited 124.497555ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 06/08/23 15:14:55.585
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 06/08/23 15:14:55.589
    STEP: List APIServices 06/08/23 15:14:55.597
    Jun  8 15:14:55.605: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:14:55.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-1219" for this suite. 06/08/23 15:14:55.786
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:14:55.798
Jun  8 15:14:55.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename emptydir 06/08/23 15:14:55.8
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:14:55.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:14:55.83
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 06/08/23 15:14:55.836
Jun  8 15:14:55.849: INFO: Waiting up to 5m0s for pod "pod-94881da7-e4a3-4477-bd5c-72eb55a8c2cd" in namespace "emptydir-1208" to be "Succeeded or Failed"
Jun  8 15:14:55.856: INFO: Pod "pod-94881da7-e4a3-4477-bd5c-72eb55a8c2cd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.522725ms
Jun  8 15:14:57.861: INFO: Pod "pod-94881da7-e4a3-4477-bd5c-72eb55a8c2cd": Phase="Running", Reason="", readiness=false. Elapsed: 2.011956993s
Jun  8 15:14:59.861: INFO: Pod "pod-94881da7-e4a3-4477-bd5c-72eb55a8c2cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011884142s
STEP: Saw pod success 06/08/23 15:14:59.861
Jun  8 15:14:59.862: INFO: Pod "pod-94881da7-e4a3-4477-bd5c-72eb55a8c2cd" satisfied condition "Succeeded or Failed"
Jun  8 15:14:59.865: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-94881da7-e4a3-4477-bd5c-72eb55a8c2cd container test-container: <nil>
STEP: delete the pod 06/08/23 15:14:59.873
Jun  8 15:14:59.887: INFO: Waiting for pod pod-94881da7-e4a3-4477-bd5c-72eb55a8c2cd to disappear
Jun  8 15:14:59.891: INFO: Pod pod-94881da7-e4a3-4477-bd5c-72eb55a8c2cd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  8 15:14:59.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1208" for this suite. 06/08/23 15:14:59.896
------------------------------
• [4.105 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:14:55.798
    Jun  8 15:14:55.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename emptydir 06/08/23 15:14:55.8
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:14:55.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:14:55.83
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 06/08/23 15:14:55.836
    Jun  8 15:14:55.849: INFO: Waiting up to 5m0s for pod "pod-94881da7-e4a3-4477-bd5c-72eb55a8c2cd" in namespace "emptydir-1208" to be "Succeeded or Failed"
    Jun  8 15:14:55.856: INFO: Pod "pod-94881da7-e4a3-4477-bd5c-72eb55a8c2cd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.522725ms
    Jun  8 15:14:57.861: INFO: Pod "pod-94881da7-e4a3-4477-bd5c-72eb55a8c2cd": Phase="Running", Reason="", readiness=false. Elapsed: 2.011956993s
    Jun  8 15:14:59.861: INFO: Pod "pod-94881da7-e4a3-4477-bd5c-72eb55a8c2cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011884142s
    STEP: Saw pod success 06/08/23 15:14:59.861
    Jun  8 15:14:59.862: INFO: Pod "pod-94881da7-e4a3-4477-bd5c-72eb55a8c2cd" satisfied condition "Succeeded or Failed"
    Jun  8 15:14:59.865: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-94881da7-e4a3-4477-bd5c-72eb55a8c2cd container test-container: <nil>
    STEP: delete the pod 06/08/23 15:14:59.873
    Jun  8 15:14:59.887: INFO: Waiting for pod pod-94881da7-e4a3-4477-bd5c-72eb55a8c2cd to disappear
    Jun  8 15:14:59.891: INFO: Pod pod-94881da7-e4a3-4477-bd5c-72eb55a8c2cd no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:14:59.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1208" for this suite. 06/08/23 15:14:59.896
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:14:59.904
Jun  8 15:14:59.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename subpath 06/08/23 15:14:59.905
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:14:59.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:14:59.923
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/08/23 15:14:59.926
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-j65b 06/08/23 15:14:59.937
STEP: Creating a pod to test atomic-volume-subpath 06/08/23 15:14:59.937
Jun  8 15:14:59.946: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-j65b" in namespace "subpath-3171" to be "Succeeded or Failed"
Jun  8 15:14:59.950: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.21063ms
Jun  8 15:15:01.956: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010379582s
Jun  8 15:15:03.956: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=true. Elapsed: 4.009778983s
Jun  8 15:15:05.956: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=true. Elapsed: 6.010233246s
Jun  8 15:15:07.957: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=true. Elapsed: 8.011023877s
Jun  8 15:15:09.957: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=true. Elapsed: 10.010601934s
Jun  8 15:15:11.956: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=true. Elapsed: 12.009856436s
Jun  8 15:15:13.956: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=true. Elapsed: 14.010110775s
Jun  8 15:15:15.957: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=true. Elapsed: 16.011022427s
Jun  8 15:15:17.956: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=true. Elapsed: 18.009678645s
Jun  8 15:15:19.955: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=true. Elapsed: 20.009190274s
Jun  8 15:15:21.957: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=false. Elapsed: 22.011022955s
Jun  8 15:15:23.955: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009385102s
STEP: Saw pod success 06/08/23 15:15:23.955
Jun  8 15:15:23.956: INFO: Pod "pod-subpath-test-configmap-j65b" satisfied condition "Succeeded or Failed"
Jun  8 15:15:23.960: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-subpath-test-configmap-j65b container test-container-subpath-configmap-j65b: <nil>
STEP: delete the pod 06/08/23 15:15:23.971
Jun  8 15:15:23.986: INFO: Waiting for pod pod-subpath-test-configmap-j65b to disappear
Jun  8 15:15:23.989: INFO: Pod pod-subpath-test-configmap-j65b no longer exists
STEP: Deleting pod pod-subpath-test-configmap-j65b 06/08/23 15:15:23.989
Jun  8 15:15:23.989: INFO: Deleting pod "pod-subpath-test-configmap-j65b" in namespace "subpath-3171"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jun  8 15:15:23.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3171" for this suite. 06/08/23 15:15:23.999
------------------------------
• [SLOW TEST] [24.102 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:14:59.904
    Jun  8 15:14:59.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename subpath 06/08/23 15:14:59.905
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:14:59.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:14:59.923
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/08/23 15:14:59.926
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-j65b 06/08/23 15:14:59.937
    STEP: Creating a pod to test atomic-volume-subpath 06/08/23 15:14:59.937
    Jun  8 15:14:59.946: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-j65b" in namespace "subpath-3171" to be "Succeeded or Failed"
    Jun  8 15:14:59.950: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.21063ms
    Jun  8 15:15:01.956: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010379582s
    Jun  8 15:15:03.956: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=true. Elapsed: 4.009778983s
    Jun  8 15:15:05.956: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=true. Elapsed: 6.010233246s
    Jun  8 15:15:07.957: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=true. Elapsed: 8.011023877s
    Jun  8 15:15:09.957: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=true. Elapsed: 10.010601934s
    Jun  8 15:15:11.956: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=true. Elapsed: 12.009856436s
    Jun  8 15:15:13.956: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=true. Elapsed: 14.010110775s
    Jun  8 15:15:15.957: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=true. Elapsed: 16.011022427s
    Jun  8 15:15:17.956: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=true. Elapsed: 18.009678645s
    Jun  8 15:15:19.955: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=true. Elapsed: 20.009190274s
    Jun  8 15:15:21.957: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Running", Reason="", readiness=false. Elapsed: 22.011022955s
    Jun  8 15:15:23.955: INFO: Pod "pod-subpath-test-configmap-j65b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009385102s
    STEP: Saw pod success 06/08/23 15:15:23.955
    Jun  8 15:15:23.956: INFO: Pod "pod-subpath-test-configmap-j65b" satisfied condition "Succeeded or Failed"
    Jun  8 15:15:23.960: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-subpath-test-configmap-j65b container test-container-subpath-configmap-j65b: <nil>
    STEP: delete the pod 06/08/23 15:15:23.971
    Jun  8 15:15:23.986: INFO: Waiting for pod pod-subpath-test-configmap-j65b to disappear
    Jun  8 15:15:23.989: INFO: Pod pod-subpath-test-configmap-j65b no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-j65b 06/08/23 15:15:23.989
    Jun  8 15:15:23.989: INFO: Deleting pod "pod-subpath-test-configmap-j65b" in namespace "subpath-3171"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:15:23.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3171" for this suite. 06/08/23 15:15:23.999
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:15:24.006
Jun  8 15:15:24.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename custom-resource-definition 06/08/23 15:15:24.008
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:15:24.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:15:24.028
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jun  8 15:15:24.032: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:15:25.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6987" for this suite. 06/08/23 15:15:25.064
------------------------------
• [1.066 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:15:24.006
    Jun  8 15:15:24.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename custom-resource-definition 06/08/23 15:15:24.008
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:15:24.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:15:24.028
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jun  8 15:15:24.032: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:15:25.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6987" for this suite. 06/08/23 15:15:25.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:15:25.073
Jun  8 15:15:25.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 15:15:25.074
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:15:25.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:15:25.092
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-9fcd5a14-14dc-440c-80ca-65b5f4788104 06/08/23 15:15:25.095
STEP: Creating a pod to test consume configMaps 06/08/23 15:15:25.1
Jun  8 15:15:25.109: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9600740c-108a-4e7f-be6a-ca649dd58aac" in namespace "projected-5111" to be "Succeeded or Failed"
Jun  8 15:15:25.113: INFO: Pod "pod-projected-configmaps-9600740c-108a-4e7f-be6a-ca649dd58aac": Phase="Pending", Reason="", readiness=false. Elapsed: 3.454374ms
Jun  8 15:15:27.121: INFO: Pod "pod-projected-configmaps-9600740c-108a-4e7f-be6a-ca649dd58aac": Phase="Running", Reason="", readiness=false. Elapsed: 2.01147275s
Jun  8 15:15:29.118: INFO: Pod "pod-projected-configmaps-9600740c-108a-4e7f-be6a-ca649dd58aac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008841727s
STEP: Saw pod success 06/08/23 15:15:29.118
Jun  8 15:15:29.118: INFO: Pod "pod-projected-configmaps-9600740c-108a-4e7f-be6a-ca649dd58aac" satisfied condition "Succeeded or Failed"
Jun  8 15:15:29.122: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-configmaps-9600740c-108a-4e7f-be6a-ca649dd58aac container agnhost-container: <nil>
STEP: delete the pod 06/08/23 15:15:29.131
Jun  8 15:15:29.147: INFO: Waiting for pod pod-projected-configmaps-9600740c-108a-4e7f-be6a-ca649dd58aac to disappear
Jun  8 15:15:29.151: INFO: Pod pod-projected-configmaps-9600740c-108a-4e7f-be6a-ca649dd58aac no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun  8 15:15:29.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5111" for this suite. 06/08/23 15:15:29.157
------------------------------
• [4.091 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:15:25.073
    Jun  8 15:15:25.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 15:15:25.074
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:15:25.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:15:25.092
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-9fcd5a14-14dc-440c-80ca-65b5f4788104 06/08/23 15:15:25.095
    STEP: Creating a pod to test consume configMaps 06/08/23 15:15:25.1
    Jun  8 15:15:25.109: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9600740c-108a-4e7f-be6a-ca649dd58aac" in namespace "projected-5111" to be "Succeeded or Failed"
    Jun  8 15:15:25.113: INFO: Pod "pod-projected-configmaps-9600740c-108a-4e7f-be6a-ca649dd58aac": Phase="Pending", Reason="", readiness=false. Elapsed: 3.454374ms
    Jun  8 15:15:27.121: INFO: Pod "pod-projected-configmaps-9600740c-108a-4e7f-be6a-ca649dd58aac": Phase="Running", Reason="", readiness=false. Elapsed: 2.01147275s
    Jun  8 15:15:29.118: INFO: Pod "pod-projected-configmaps-9600740c-108a-4e7f-be6a-ca649dd58aac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008841727s
    STEP: Saw pod success 06/08/23 15:15:29.118
    Jun  8 15:15:29.118: INFO: Pod "pod-projected-configmaps-9600740c-108a-4e7f-be6a-ca649dd58aac" satisfied condition "Succeeded or Failed"
    Jun  8 15:15:29.122: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-configmaps-9600740c-108a-4e7f-be6a-ca649dd58aac container agnhost-container: <nil>
    STEP: delete the pod 06/08/23 15:15:29.131
    Jun  8 15:15:29.147: INFO: Waiting for pod pod-projected-configmaps-9600740c-108a-4e7f-be6a-ca649dd58aac to disappear
    Jun  8 15:15:29.151: INFO: Pod pod-projected-configmaps-9600740c-108a-4e7f-be6a-ca649dd58aac no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:15:29.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5111" for this suite. 06/08/23 15:15:29.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:15:29.166
Jun  8 15:15:29.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename crd-publish-openapi 06/08/23 15:15:29.167
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:15:29.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:15:29.186
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 06/08/23 15:15:29.189
Jun  8 15:15:29.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:15:31.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:15:39.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2087" for this suite. 06/08/23 15:15:39.523
------------------------------
• [SLOW TEST] [10.365 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:15:29.166
    Jun  8 15:15:29.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename crd-publish-openapi 06/08/23 15:15:29.167
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:15:29.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:15:29.186
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 06/08/23 15:15:29.189
    Jun  8 15:15:29.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:15:31.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:15:39.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2087" for this suite. 06/08/23 15:15:39.523
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:15:39.532
Jun  8 15:15:39.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename emptydir-wrapper 06/08/23 15:15:39.533
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:15:39.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:15:39.559
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 06/08/23 15:15:39.562
STEP: Creating RC which spawns configmap-volume pods 06/08/23 15:15:39.828
Jun  8 15:15:39.891: INFO: Pod name wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347: Found 3 pods out of 5
Jun  8 15:15:44.901: INFO: Pod name wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347: Found 5 pods out of 5
STEP: Ensuring each pod is running 06/08/23 15:15:44.901
Jun  8 15:15:44.901: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-2w5xw" in namespace "emptydir-wrapper-7272" to be "running"
Jun  8 15:15:44.906: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-2w5xw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.994665ms
Jun  8 15:15:46.912: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-2w5xw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010871095s
Jun  8 15:15:48.913: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-2w5xw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011924328s
Jun  8 15:15:50.912: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-2w5xw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010394524s
Jun  8 15:15:52.913: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-2w5xw": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011501582s
Jun  8 15:15:54.913: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-2w5xw": Phase="Running", Reason="", readiness=true. Elapsed: 10.011696375s
Jun  8 15:15:54.913: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-2w5xw" satisfied condition "running"
Jun  8 15:15:54.913: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-4cfxq" in namespace "emptydir-wrapper-7272" to be "running"
Jun  8 15:15:54.918: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-4cfxq": Phase="Running", Reason="", readiness=true. Elapsed: 4.569726ms
Jun  8 15:15:54.918: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-4cfxq" satisfied condition "running"
Jun  8 15:15:54.918: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-76h5z" in namespace "emptydir-wrapper-7272" to be "running"
Jun  8 15:15:54.922: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-76h5z": Phase="Running", Reason="", readiness=true. Elapsed: 4.726987ms
Jun  8 15:15:54.922: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-76h5z" satisfied condition "running"
Jun  8 15:15:54.922: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-bd8zr" in namespace "emptydir-wrapper-7272" to be "running"
Jun  8 15:15:54.927: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-bd8zr": Phase="Running", Reason="", readiness=true. Elapsed: 4.344389ms
Jun  8 15:15:54.927: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-bd8zr" satisfied condition "running"
Jun  8 15:15:54.927: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-r76jt" in namespace "emptydir-wrapper-7272" to be "running"
Jun  8 15:15:54.931: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-r76jt": Phase="Running", Reason="", readiness=true. Elapsed: 4.427286ms
Jun  8 15:15:54.931: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-r76jt" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347 in namespace emptydir-wrapper-7272, will wait for the garbage collector to delete the pods 06/08/23 15:15:54.931
Jun  8 15:15:54.995: INFO: Deleting ReplicationController wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347 took: 9.064624ms
Jun  8 15:15:55.096: INFO: Terminating ReplicationController wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347 pods took: 100.683997ms
STEP: Creating RC which spawns configmap-volume pods 06/08/23 15:15:58.102
Jun  8 15:15:58.117: INFO: Pod name wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563: Found 0 pods out of 5
Jun  8 15:16:03.125: INFO: Pod name wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563: Found 5 pods out of 5
STEP: Ensuring each pod is running 06/08/23 15:16:03.125
Jun  8 15:16:03.126: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-27tkd" in namespace "emptydir-wrapper-7272" to be "running"
Jun  8 15:16:03.130: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-27tkd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.537075ms
Jun  8 15:16:05.140: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-27tkd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014133591s
Jun  8 15:16:07.136: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-27tkd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010310503s
Jun  8 15:16:09.136: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-27tkd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010472863s
Jun  8 15:16:11.136: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-27tkd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010636601s
Jun  8 15:16:13.138: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-27tkd": Phase="Running", Reason="", readiness=true. Elapsed: 10.011963501s
Jun  8 15:16:13.138: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-27tkd" satisfied condition "running"
Jun  8 15:16:13.138: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-464tn" in namespace "emptydir-wrapper-7272" to be "running"
Jun  8 15:16:13.142: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-464tn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.402255ms
Jun  8 15:16:15.149: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-464tn": Phase="Running", Reason="", readiness=true. Elapsed: 2.01097167s
Jun  8 15:16:15.149: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-464tn" satisfied condition "running"
Jun  8 15:16:15.149: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-5z29t" in namespace "emptydir-wrapper-7272" to be "running"
Jun  8 15:16:15.153: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-5z29t": Phase="Running", Reason="", readiness=true. Elapsed: 4.35228ms
Jun  8 15:16:15.153: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-5z29t" satisfied condition "running"
Jun  8 15:16:15.153: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-hrkvr" in namespace "emptydir-wrapper-7272" to be "running"
Jun  8 15:16:15.158: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-hrkvr": Phase="Running", Reason="", readiness=true. Elapsed: 4.531854ms
Jun  8 15:16:15.158: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-hrkvr" satisfied condition "running"
Jun  8 15:16:15.158: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-kkhqc" in namespace "emptydir-wrapper-7272" to be "running"
Jun  8 15:16:15.162: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-kkhqc": Phase="Running", Reason="", readiness=true. Elapsed: 4.554782ms
Jun  8 15:16:15.162: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-kkhqc" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563 in namespace emptydir-wrapper-7272, will wait for the garbage collector to delete the pods 06/08/23 15:16:15.162
Jun  8 15:16:15.226: INFO: Deleting ReplicationController wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563 took: 8.453916ms
Jun  8 15:16:15.326: INFO: Terminating ReplicationController wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563 pods took: 100.254892ms
STEP: Creating RC which spawns configmap-volume pods 06/08/23 15:16:17.834
Jun  8 15:16:17.860: INFO: Pod name wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576: Found 0 pods out of 5
Jun  8 15:16:22.868: INFO: Pod name wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576: Found 5 pods out of 5
STEP: Ensuring each pod is running 06/08/23 15:16:22.868
Jun  8 15:16:22.868: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-2jh9d" in namespace "emptydir-wrapper-7272" to be "running"
Jun  8 15:16:22.872: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-2jh9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046838ms
Jun  8 15:16:24.878: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-2jh9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009875634s
Jun  8 15:16:26.879: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-2jh9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01051694s
Jun  8 15:16:28.879: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-2jh9d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010405264s
Jun  8 15:16:30.879: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-2jh9d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010601713s
Jun  8 15:16:32.879: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-2jh9d": Phase="Running", Reason="", readiness=true. Elapsed: 10.010360668s
Jun  8 15:16:32.879: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-2jh9d" satisfied condition "running"
Jun  8 15:16:32.879: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-59kfm" in namespace "emptydir-wrapper-7272" to be "running"
Jun  8 15:16:32.883: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-59kfm": Phase="Running", Reason="", readiness=true. Elapsed: 4.353815ms
Jun  8 15:16:32.883: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-59kfm" satisfied condition "running"
Jun  8 15:16:32.883: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-bpqqn" in namespace "emptydir-wrapper-7272" to be "running"
Jun  8 15:16:32.888: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-bpqqn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.588805ms
Jun  8 15:16:34.897: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-bpqqn": Phase="Running", Reason="", readiness=true. Elapsed: 2.01361563s
Jun  8 15:16:34.897: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-bpqqn" satisfied condition "running"
Jun  8 15:16:34.897: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-lnwpp" in namespace "emptydir-wrapper-7272" to be "running"
Jun  8 15:16:34.902: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-lnwpp": Phase="Running", Reason="", readiness=true. Elapsed: 4.6119ms
Jun  8 15:16:34.902: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-lnwpp" satisfied condition "running"
Jun  8 15:16:34.902: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-sj4hc" in namespace "emptydir-wrapper-7272" to be "running"
Jun  8 15:16:34.906: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-sj4hc": Phase="Running", Reason="", readiness=true. Elapsed: 4.701294ms
Jun  8 15:16:34.906: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-sj4hc" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576 in namespace emptydir-wrapper-7272, will wait for the garbage collector to delete the pods 06/08/23 15:16:34.906
Jun  8 15:16:34.970: INFO: Deleting ReplicationController wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576 took: 8.332844ms
Jun  8 15:16:35.070: INFO: Terminating ReplicationController wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576 pods took: 100.125484ms
STEP: Cleaning up the configMaps 06/08/23 15:16:37.97
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jun  8 15:16:38.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-7272" for this suite. 06/08/23 15:16:38.296
------------------------------
• [SLOW TEST] [58.770 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:15:39.532
    Jun  8 15:15:39.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename emptydir-wrapper 06/08/23 15:15:39.533
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:15:39.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:15:39.559
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 06/08/23 15:15:39.562
    STEP: Creating RC which spawns configmap-volume pods 06/08/23 15:15:39.828
    Jun  8 15:15:39.891: INFO: Pod name wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347: Found 3 pods out of 5
    Jun  8 15:15:44.901: INFO: Pod name wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347: Found 5 pods out of 5
    STEP: Ensuring each pod is running 06/08/23 15:15:44.901
    Jun  8 15:15:44.901: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-2w5xw" in namespace "emptydir-wrapper-7272" to be "running"
    Jun  8 15:15:44.906: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-2w5xw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.994665ms
    Jun  8 15:15:46.912: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-2w5xw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010871095s
    Jun  8 15:15:48.913: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-2w5xw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011924328s
    Jun  8 15:15:50.912: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-2w5xw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010394524s
    Jun  8 15:15:52.913: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-2w5xw": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011501582s
    Jun  8 15:15:54.913: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-2w5xw": Phase="Running", Reason="", readiness=true. Elapsed: 10.011696375s
    Jun  8 15:15:54.913: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-2w5xw" satisfied condition "running"
    Jun  8 15:15:54.913: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-4cfxq" in namespace "emptydir-wrapper-7272" to be "running"
    Jun  8 15:15:54.918: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-4cfxq": Phase="Running", Reason="", readiness=true. Elapsed: 4.569726ms
    Jun  8 15:15:54.918: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-4cfxq" satisfied condition "running"
    Jun  8 15:15:54.918: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-76h5z" in namespace "emptydir-wrapper-7272" to be "running"
    Jun  8 15:15:54.922: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-76h5z": Phase="Running", Reason="", readiness=true. Elapsed: 4.726987ms
    Jun  8 15:15:54.922: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-76h5z" satisfied condition "running"
    Jun  8 15:15:54.922: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-bd8zr" in namespace "emptydir-wrapper-7272" to be "running"
    Jun  8 15:15:54.927: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-bd8zr": Phase="Running", Reason="", readiness=true. Elapsed: 4.344389ms
    Jun  8 15:15:54.927: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-bd8zr" satisfied condition "running"
    Jun  8 15:15:54.927: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-r76jt" in namespace "emptydir-wrapper-7272" to be "running"
    Jun  8 15:15:54.931: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-r76jt": Phase="Running", Reason="", readiness=true. Elapsed: 4.427286ms
    Jun  8 15:15:54.931: INFO: Pod "wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347-r76jt" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347 in namespace emptydir-wrapper-7272, will wait for the garbage collector to delete the pods 06/08/23 15:15:54.931
    Jun  8 15:15:54.995: INFO: Deleting ReplicationController wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347 took: 9.064624ms
    Jun  8 15:15:55.096: INFO: Terminating ReplicationController wrapped-volume-race-b81b4e1b-265d-4bfb-8b3c-13edf4a92347 pods took: 100.683997ms
    STEP: Creating RC which spawns configmap-volume pods 06/08/23 15:15:58.102
    Jun  8 15:15:58.117: INFO: Pod name wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563: Found 0 pods out of 5
    Jun  8 15:16:03.125: INFO: Pod name wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563: Found 5 pods out of 5
    STEP: Ensuring each pod is running 06/08/23 15:16:03.125
    Jun  8 15:16:03.126: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-27tkd" in namespace "emptydir-wrapper-7272" to be "running"
    Jun  8 15:16:03.130: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-27tkd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.537075ms
    Jun  8 15:16:05.140: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-27tkd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014133591s
    Jun  8 15:16:07.136: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-27tkd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010310503s
    Jun  8 15:16:09.136: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-27tkd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010472863s
    Jun  8 15:16:11.136: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-27tkd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010636601s
    Jun  8 15:16:13.138: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-27tkd": Phase="Running", Reason="", readiness=true. Elapsed: 10.011963501s
    Jun  8 15:16:13.138: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-27tkd" satisfied condition "running"
    Jun  8 15:16:13.138: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-464tn" in namespace "emptydir-wrapper-7272" to be "running"
    Jun  8 15:16:13.142: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-464tn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.402255ms
    Jun  8 15:16:15.149: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-464tn": Phase="Running", Reason="", readiness=true. Elapsed: 2.01097167s
    Jun  8 15:16:15.149: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-464tn" satisfied condition "running"
    Jun  8 15:16:15.149: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-5z29t" in namespace "emptydir-wrapper-7272" to be "running"
    Jun  8 15:16:15.153: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-5z29t": Phase="Running", Reason="", readiness=true. Elapsed: 4.35228ms
    Jun  8 15:16:15.153: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-5z29t" satisfied condition "running"
    Jun  8 15:16:15.153: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-hrkvr" in namespace "emptydir-wrapper-7272" to be "running"
    Jun  8 15:16:15.158: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-hrkvr": Phase="Running", Reason="", readiness=true. Elapsed: 4.531854ms
    Jun  8 15:16:15.158: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-hrkvr" satisfied condition "running"
    Jun  8 15:16:15.158: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-kkhqc" in namespace "emptydir-wrapper-7272" to be "running"
    Jun  8 15:16:15.162: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-kkhqc": Phase="Running", Reason="", readiness=true. Elapsed: 4.554782ms
    Jun  8 15:16:15.162: INFO: Pod "wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563-kkhqc" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563 in namespace emptydir-wrapper-7272, will wait for the garbage collector to delete the pods 06/08/23 15:16:15.162
    Jun  8 15:16:15.226: INFO: Deleting ReplicationController wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563 took: 8.453916ms
    Jun  8 15:16:15.326: INFO: Terminating ReplicationController wrapped-volume-race-bbe816d0-cb2f-4245-b410-6c1a73e55563 pods took: 100.254892ms
    STEP: Creating RC which spawns configmap-volume pods 06/08/23 15:16:17.834
    Jun  8 15:16:17.860: INFO: Pod name wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576: Found 0 pods out of 5
    Jun  8 15:16:22.868: INFO: Pod name wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576: Found 5 pods out of 5
    STEP: Ensuring each pod is running 06/08/23 15:16:22.868
    Jun  8 15:16:22.868: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-2jh9d" in namespace "emptydir-wrapper-7272" to be "running"
    Jun  8 15:16:22.872: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-2jh9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046838ms
    Jun  8 15:16:24.878: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-2jh9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009875634s
    Jun  8 15:16:26.879: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-2jh9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01051694s
    Jun  8 15:16:28.879: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-2jh9d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010405264s
    Jun  8 15:16:30.879: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-2jh9d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010601713s
    Jun  8 15:16:32.879: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-2jh9d": Phase="Running", Reason="", readiness=true. Elapsed: 10.010360668s
    Jun  8 15:16:32.879: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-2jh9d" satisfied condition "running"
    Jun  8 15:16:32.879: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-59kfm" in namespace "emptydir-wrapper-7272" to be "running"
    Jun  8 15:16:32.883: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-59kfm": Phase="Running", Reason="", readiness=true. Elapsed: 4.353815ms
    Jun  8 15:16:32.883: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-59kfm" satisfied condition "running"
    Jun  8 15:16:32.883: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-bpqqn" in namespace "emptydir-wrapper-7272" to be "running"
    Jun  8 15:16:32.888: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-bpqqn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.588805ms
    Jun  8 15:16:34.897: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-bpqqn": Phase="Running", Reason="", readiness=true. Elapsed: 2.01361563s
    Jun  8 15:16:34.897: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-bpqqn" satisfied condition "running"
    Jun  8 15:16:34.897: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-lnwpp" in namespace "emptydir-wrapper-7272" to be "running"
    Jun  8 15:16:34.902: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-lnwpp": Phase="Running", Reason="", readiness=true. Elapsed: 4.6119ms
    Jun  8 15:16:34.902: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-lnwpp" satisfied condition "running"
    Jun  8 15:16:34.902: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-sj4hc" in namespace "emptydir-wrapper-7272" to be "running"
    Jun  8 15:16:34.906: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-sj4hc": Phase="Running", Reason="", readiness=true. Elapsed: 4.701294ms
    Jun  8 15:16:34.906: INFO: Pod "wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576-sj4hc" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576 in namespace emptydir-wrapper-7272, will wait for the garbage collector to delete the pods 06/08/23 15:16:34.906
    Jun  8 15:16:34.970: INFO: Deleting ReplicationController wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576 took: 8.332844ms
    Jun  8 15:16:35.070: INFO: Terminating ReplicationController wrapped-volume-race-160e9de7-874a-47ed-a0e6-a78062549576 pods took: 100.125484ms
    STEP: Cleaning up the configMaps 06/08/23 15:16:37.97
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:16:38.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-7272" for this suite. 06/08/23 15:16:38.296
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:16:38.305
Jun  8 15:16:38.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename resourcequota 06/08/23 15:16:38.306
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:16:38.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:16:38.327
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-whf5j" 06/08/23 15:16:38.334
Jun  8 15:16:38.342: INFO: Resource quota "e2e-rq-status-whf5j" reports spec: hard cpu limit of 500m
Jun  8 15:16:38.343: INFO: Resource quota "e2e-rq-status-whf5j" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-whf5j" /status 06/08/23 15:16:38.343
STEP: Confirm /status for "e2e-rq-status-whf5j" resourceQuota via watch 06/08/23 15:16:38.352
Jun  8 15:16:38.354: INFO: observed resourceQuota "e2e-rq-status-whf5j" in namespace "resourcequota-2474" with hard status: v1.ResourceList(nil)
Jun  8 15:16:38.354: INFO: Found resourceQuota "e2e-rq-status-whf5j" in namespace "resourcequota-2474" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jun  8 15:16:38.354: INFO: ResourceQuota "e2e-rq-status-whf5j" /status was updated
STEP: Patching hard spec values for cpu & memory 06/08/23 15:16:38.358
Jun  8 15:16:38.364: INFO: Resource quota "e2e-rq-status-whf5j" reports spec: hard cpu limit of 1
Jun  8 15:16:38.364: INFO: Resource quota "e2e-rq-status-whf5j" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-whf5j" /status 06/08/23 15:16:38.364
STEP: Confirm /status for "e2e-rq-status-whf5j" resourceQuota via watch 06/08/23 15:16:38.371
Jun  8 15:16:38.373: INFO: observed resourceQuota "e2e-rq-status-whf5j" in namespace "resourcequota-2474" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jun  8 15:16:38.373: INFO: Found resourceQuota "e2e-rq-status-whf5j" in namespace "resourcequota-2474" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Jun  8 15:16:38.373: INFO: ResourceQuota "e2e-rq-status-whf5j" /status was patched
STEP: Get "e2e-rq-status-whf5j" /status 06/08/23 15:16:38.373
Jun  8 15:16:38.378: INFO: Resourcequota "e2e-rq-status-whf5j" reports status: hard cpu of 1
Jun  8 15:16:38.378: INFO: Resourcequota "e2e-rq-status-whf5j" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-whf5j" /status before checking Spec is unchanged 06/08/23 15:16:38.381
Jun  8 15:16:38.388: INFO: Resourcequota "e2e-rq-status-whf5j" reports status: hard cpu of 2
Jun  8 15:16:38.388: INFO: Resourcequota "e2e-rq-status-whf5j" reports status: hard memory of 2Gi
Jun  8 15:16:38.389: INFO: Found resourceQuota "e2e-rq-status-whf5j" in namespace "resourcequota-2474" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Jun  8 15:17:43.399: INFO: ResourceQuota "e2e-rq-status-whf5j" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  8 15:17:43.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2474" for this suite. 06/08/23 15:17:43.405
------------------------------
• [SLOW TEST] [65.109 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:16:38.305
    Jun  8 15:16:38.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename resourcequota 06/08/23 15:16:38.306
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:16:38.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:16:38.327
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-whf5j" 06/08/23 15:16:38.334
    Jun  8 15:16:38.342: INFO: Resource quota "e2e-rq-status-whf5j" reports spec: hard cpu limit of 500m
    Jun  8 15:16:38.343: INFO: Resource quota "e2e-rq-status-whf5j" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-whf5j" /status 06/08/23 15:16:38.343
    STEP: Confirm /status for "e2e-rq-status-whf5j" resourceQuota via watch 06/08/23 15:16:38.352
    Jun  8 15:16:38.354: INFO: observed resourceQuota "e2e-rq-status-whf5j" in namespace "resourcequota-2474" with hard status: v1.ResourceList(nil)
    Jun  8 15:16:38.354: INFO: Found resourceQuota "e2e-rq-status-whf5j" in namespace "resourcequota-2474" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jun  8 15:16:38.354: INFO: ResourceQuota "e2e-rq-status-whf5j" /status was updated
    STEP: Patching hard spec values for cpu & memory 06/08/23 15:16:38.358
    Jun  8 15:16:38.364: INFO: Resource quota "e2e-rq-status-whf5j" reports spec: hard cpu limit of 1
    Jun  8 15:16:38.364: INFO: Resource quota "e2e-rq-status-whf5j" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-whf5j" /status 06/08/23 15:16:38.364
    STEP: Confirm /status for "e2e-rq-status-whf5j" resourceQuota via watch 06/08/23 15:16:38.371
    Jun  8 15:16:38.373: INFO: observed resourceQuota "e2e-rq-status-whf5j" in namespace "resourcequota-2474" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jun  8 15:16:38.373: INFO: Found resourceQuota "e2e-rq-status-whf5j" in namespace "resourcequota-2474" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Jun  8 15:16:38.373: INFO: ResourceQuota "e2e-rq-status-whf5j" /status was patched
    STEP: Get "e2e-rq-status-whf5j" /status 06/08/23 15:16:38.373
    Jun  8 15:16:38.378: INFO: Resourcequota "e2e-rq-status-whf5j" reports status: hard cpu of 1
    Jun  8 15:16:38.378: INFO: Resourcequota "e2e-rq-status-whf5j" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-whf5j" /status before checking Spec is unchanged 06/08/23 15:16:38.381
    Jun  8 15:16:38.388: INFO: Resourcequota "e2e-rq-status-whf5j" reports status: hard cpu of 2
    Jun  8 15:16:38.388: INFO: Resourcequota "e2e-rq-status-whf5j" reports status: hard memory of 2Gi
    Jun  8 15:16:38.389: INFO: Found resourceQuota "e2e-rq-status-whf5j" in namespace "resourcequota-2474" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Jun  8 15:17:43.399: INFO: ResourceQuota "e2e-rq-status-whf5j" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:17:43.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2474" for this suite. 06/08/23 15:17:43.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:17:43.416
Jun  8 15:17:43.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename dns 06/08/23 15:17:43.417
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:17:43.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:17:43.438
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 06/08/23 15:17:43.442
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4921.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local; sleep 1; done
 06/08/23 15:17:43.447
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4921.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local; sleep 1; done
 06/08/23 15:17:43.447
STEP: creating a pod to probe DNS 06/08/23 15:17:43.447
STEP: submitting the pod to kubernetes 06/08/23 15:17:43.447
Jun  8 15:17:43.459: INFO: Waiting up to 15m0s for pod "dns-test-b0ed80e6-be2d-488b-8519-da514ddaff29" in namespace "dns-4921" to be "running"
Jun  8 15:17:43.464: INFO: Pod "dns-test-b0ed80e6-be2d-488b-8519-da514ddaff29": Phase="Pending", Reason="", readiness=false. Elapsed: 5.188716ms
Jun  8 15:17:45.470: INFO: Pod "dns-test-b0ed80e6-be2d-488b-8519-da514ddaff29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011024647s
Jun  8 15:17:47.469: INFO: Pod "dns-test-b0ed80e6-be2d-488b-8519-da514ddaff29": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010132746s
Jun  8 15:17:49.469: INFO: Pod "dns-test-b0ed80e6-be2d-488b-8519-da514ddaff29": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010195564s
Jun  8 15:17:51.470: INFO: Pod "dns-test-b0ed80e6-be2d-488b-8519-da514ddaff29": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011460881s
Jun  8 15:17:53.470: INFO: Pod "dns-test-b0ed80e6-be2d-488b-8519-da514ddaff29": Phase="Running", Reason="", readiness=true. Elapsed: 10.011515887s
Jun  8 15:17:53.470: INFO: Pod "dns-test-b0ed80e6-be2d-488b-8519-da514ddaff29" satisfied condition "running"
STEP: retrieving the pod 06/08/23 15:17:53.47
STEP: looking for the results for each expected name from probers 06/08/23 15:17:53.475
Jun  8 15:17:53.485: INFO: DNS probes using dns-test-b0ed80e6-be2d-488b-8519-da514ddaff29 succeeded

STEP: deleting the pod 06/08/23 15:17:53.485
STEP: changing the externalName to bar.example.com 06/08/23 15:17:53.498
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4921.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local; sleep 1; done
 06/08/23 15:17:53.507
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4921.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local; sleep 1; done
 06/08/23 15:17:53.507
STEP: creating a second pod to probe DNS 06/08/23 15:17:53.507
STEP: submitting the pod to kubernetes 06/08/23 15:17:53.508
Jun  8 15:17:53.514: INFO: Waiting up to 15m0s for pod "dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05" in namespace "dns-4921" to be "running"
Jun  8 15:17:53.517: INFO: Pod "dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05": Phase="Pending", Reason="", readiness=false. Elapsed: 3.44825ms
Jun  8 15:17:55.523: INFO: Pod "dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05": Phase="Running", Reason="", readiness=true. Elapsed: 2.009052588s
Jun  8 15:17:55.523: INFO: Pod "dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05" satisfied condition "running"
STEP: retrieving the pod 06/08/23 15:17:55.523
STEP: looking for the results for each expected name from probers 06/08/23 15:17:55.527
Jun  8 15:17:55.533: INFO: File wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local from pod  dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  8 15:17:55.538: INFO: File jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local from pod  dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  8 15:17:55.538: INFO: Lookups using dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 failed for: [wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local]

Jun  8 15:18:00.546: INFO: File wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local from pod  dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  8 15:18:00.551: INFO: File jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local from pod  dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  8 15:18:00.551: INFO: Lookups using dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 failed for: [wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local]

Jun  8 15:18:05.546: INFO: File wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local from pod  dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  8 15:18:05.551: INFO: File jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local from pod  dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  8 15:18:05.551: INFO: Lookups using dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 failed for: [wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local]

Jun  8 15:18:10.544: INFO: File wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local from pod  dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  8 15:18:10.549: INFO: File jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local from pod  dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  8 15:18:10.549: INFO: Lookups using dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 failed for: [wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local]

Jun  8 15:18:15.544: INFO: File wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local from pod  dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  8 15:18:15.549: INFO: File jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local from pod  dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  8 15:18:15.549: INFO: Lookups using dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 failed for: [wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local]

Jun  8 15:18:20.550: INFO: DNS probes using dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 succeeded

STEP: deleting the pod 06/08/23 15:18:20.55
STEP: changing the service to type=ClusterIP 06/08/23 15:18:20.563
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4921.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local; sleep 1; done
 06/08/23 15:18:20.586
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4921.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local; sleep 1; done
 06/08/23 15:18:20.586
STEP: creating a third pod to probe DNS 06/08/23 15:18:20.586
STEP: submitting the pod to kubernetes 06/08/23 15:18:20.591
Jun  8 15:18:20.602: INFO: Waiting up to 15m0s for pod "dns-test-9c46012e-2620-46fd-9b3d-2c8778765995" in namespace "dns-4921" to be "running"
Jun  8 15:18:20.609: INFO: Pod "dns-test-9c46012e-2620-46fd-9b3d-2c8778765995": Phase="Pending", Reason="", readiness=false. Elapsed: 6.581033ms
Jun  8 15:18:22.615: INFO: Pod "dns-test-9c46012e-2620-46fd-9b3d-2c8778765995": Phase="Running", Reason="", readiness=true. Elapsed: 2.01297726s
Jun  8 15:18:22.615: INFO: Pod "dns-test-9c46012e-2620-46fd-9b3d-2c8778765995" satisfied condition "running"
STEP: retrieving the pod 06/08/23 15:18:22.615
STEP: looking for the results for each expected name from probers 06/08/23 15:18:22.619
Jun  8 15:18:22.630: INFO: DNS probes using dns-test-9c46012e-2620-46fd-9b3d-2c8778765995 succeeded

STEP: deleting the pod 06/08/23 15:18:22.63
STEP: deleting the test externalName service 06/08/23 15:18:22.642
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun  8 15:18:22.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4921" for this suite. 06/08/23 15:18:22.669
------------------------------
• [SLOW TEST] [39.262 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:17:43.416
    Jun  8 15:17:43.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename dns 06/08/23 15:17:43.417
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:17:43.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:17:43.438
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 06/08/23 15:17:43.442
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4921.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local; sleep 1; done
     06/08/23 15:17:43.447
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4921.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local; sleep 1; done
     06/08/23 15:17:43.447
    STEP: creating a pod to probe DNS 06/08/23 15:17:43.447
    STEP: submitting the pod to kubernetes 06/08/23 15:17:43.447
    Jun  8 15:17:43.459: INFO: Waiting up to 15m0s for pod "dns-test-b0ed80e6-be2d-488b-8519-da514ddaff29" in namespace "dns-4921" to be "running"
    Jun  8 15:17:43.464: INFO: Pod "dns-test-b0ed80e6-be2d-488b-8519-da514ddaff29": Phase="Pending", Reason="", readiness=false. Elapsed: 5.188716ms
    Jun  8 15:17:45.470: INFO: Pod "dns-test-b0ed80e6-be2d-488b-8519-da514ddaff29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011024647s
    Jun  8 15:17:47.469: INFO: Pod "dns-test-b0ed80e6-be2d-488b-8519-da514ddaff29": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010132746s
    Jun  8 15:17:49.469: INFO: Pod "dns-test-b0ed80e6-be2d-488b-8519-da514ddaff29": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010195564s
    Jun  8 15:17:51.470: INFO: Pod "dns-test-b0ed80e6-be2d-488b-8519-da514ddaff29": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011460881s
    Jun  8 15:17:53.470: INFO: Pod "dns-test-b0ed80e6-be2d-488b-8519-da514ddaff29": Phase="Running", Reason="", readiness=true. Elapsed: 10.011515887s
    Jun  8 15:17:53.470: INFO: Pod "dns-test-b0ed80e6-be2d-488b-8519-da514ddaff29" satisfied condition "running"
    STEP: retrieving the pod 06/08/23 15:17:53.47
    STEP: looking for the results for each expected name from probers 06/08/23 15:17:53.475
    Jun  8 15:17:53.485: INFO: DNS probes using dns-test-b0ed80e6-be2d-488b-8519-da514ddaff29 succeeded

    STEP: deleting the pod 06/08/23 15:17:53.485
    STEP: changing the externalName to bar.example.com 06/08/23 15:17:53.498
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4921.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local; sleep 1; done
     06/08/23 15:17:53.507
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4921.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local; sleep 1; done
     06/08/23 15:17:53.507
    STEP: creating a second pod to probe DNS 06/08/23 15:17:53.507
    STEP: submitting the pod to kubernetes 06/08/23 15:17:53.508
    Jun  8 15:17:53.514: INFO: Waiting up to 15m0s for pod "dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05" in namespace "dns-4921" to be "running"
    Jun  8 15:17:53.517: INFO: Pod "dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05": Phase="Pending", Reason="", readiness=false. Elapsed: 3.44825ms
    Jun  8 15:17:55.523: INFO: Pod "dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05": Phase="Running", Reason="", readiness=true. Elapsed: 2.009052588s
    Jun  8 15:17:55.523: INFO: Pod "dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05" satisfied condition "running"
    STEP: retrieving the pod 06/08/23 15:17:55.523
    STEP: looking for the results for each expected name from probers 06/08/23 15:17:55.527
    Jun  8 15:17:55.533: INFO: File wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local from pod  dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  8 15:17:55.538: INFO: File jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local from pod  dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  8 15:17:55.538: INFO: Lookups using dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 failed for: [wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local]

    Jun  8 15:18:00.546: INFO: File wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local from pod  dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  8 15:18:00.551: INFO: File jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local from pod  dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  8 15:18:00.551: INFO: Lookups using dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 failed for: [wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local]

    Jun  8 15:18:05.546: INFO: File wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local from pod  dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  8 15:18:05.551: INFO: File jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local from pod  dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  8 15:18:05.551: INFO: Lookups using dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 failed for: [wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local]

    Jun  8 15:18:10.544: INFO: File wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local from pod  dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  8 15:18:10.549: INFO: File jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local from pod  dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  8 15:18:10.549: INFO: Lookups using dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 failed for: [wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local]

    Jun  8 15:18:15.544: INFO: File wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local from pod  dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  8 15:18:15.549: INFO: File jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local from pod  dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  8 15:18:15.549: INFO: Lookups using dns-4921/dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 failed for: [wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local]

    Jun  8 15:18:20.550: INFO: DNS probes using dns-test-6f57f6e6-7bf1-4c7e-b90f-b59bc73d6b05 succeeded

    STEP: deleting the pod 06/08/23 15:18:20.55
    STEP: changing the service to type=ClusterIP 06/08/23 15:18:20.563
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4921.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4921.svc.cluster.local; sleep 1; done
     06/08/23 15:18:20.586
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4921.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4921.svc.cluster.local; sleep 1; done
     06/08/23 15:18:20.586
    STEP: creating a third pod to probe DNS 06/08/23 15:18:20.586
    STEP: submitting the pod to kubernetes 06/08/23 15:18:20.591
    Jun  8 15:18:20.602: INFO: Waiting up to 15m0s for pod "dns-test-9c46012e-2620-46fd-9b3d-2c8778765995" in namespace "dns-4921" to be "running"
    Jun  8 15:18:20.609: INFO: Pod "dns-test-9c46012e-2620-46fd-9b3d-2c8778765995": Phase="Pending", Reason="", readiness=false. Elapsed: 6.581033ms
    Jun  8 15:18:22.615: INFO: Pod "dns-test-9c46012e-2620-46fd-9b3d-2c8778765995": Phase="Running", Reason="", readiness=true. Elapsed: 2.01297726s
    Jun  8 15:18:22.615: INFO: Pod "dns-test-9c46012e-2620-46fd-9b3d-2c8778765995" satisfied condition "running"
    STEP: retrieving the pod 06/08/23 15:18:22.615
    STEP: looking for the results for each expected name from probers 06/08/23 15:18:22.619
    Jun  8 15:18:22.630: INFO: DNS probes using dns-test-9c46012e-2620-46fd-9b3d-2c8778765995 succeeded

    STEP: deleting the pod 06/08/23 15:18:22.63
    STEP: deleting the test externalName service 06/08/23 15:18:22.642
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:18:22.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4921" for this suite. 06/08/23 15:18:22.669
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:18:22.679
Jun  8 15:18:22.679: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename svcaccounts 06/08/23 15:18:22.68
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:18:22.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:18:22.706
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  06/08/23 15:18:22.71
Jun  8 15:18:22.723: INFO: Waiting up to 5m0s for pod "test-pod-ed82b2c2-60ee-45b7-9ac4-513cf6060ca1" in namespace "svcaccounts-2000" to be "Succeeded or Failed"
Jun  8 15:18:22.727: INFO: Pod "test-pod-ed82b2c2-60ee-45b7-9ac4-513cf6060ca1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.645493ms
Jun  8 15:18:24.733: INFO: Pod "test-pod-ed82b2c2-60ee-45b7-9ac4-513cf6060ca1": Phase="Running", Reason="", readiness=false. Elapsed: 2.010016886s
Jun  8 15:18:26.733: INFO: Pod "test-pod-ed82b2c2-60ee-45b7-9ac4-513cf6060ca1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010674268s
STEP: Saw pod success 06/08/23 15:18:26.734
Jun  8 15:18:26.734: INFO: Pod "test-pod-ed82b2c2-60ee-45b7-9ac4-513cf6060ca1" satisfied condition "Succeeded or Failed"
Jun  8 15:18:26.737: INFO: Trying to get logs from node chl8tf-worker-001 pod test-pod-ed82b2c2-60ee-45b7-9ac4-513cf6060ca1 container agnhost-container: <nil>
STEP: delete the pod 06/08/23 15:18:26.755
Jun  8 15:18:26.768: INFO: Waiting for pod test-pod-ed82b2c2-60ee-45b7-9ac4-513cf6060ca1 to disappear
Jun  8 15:18:26.771: INFO: Pod test-pod-ed82b2c2-60ee-45b7-9ac4-513cf6060ca1 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jun  8 15:18:26.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2000" for this suite. 06/08/23 15:18:26.777
------------------------------
• [4.106 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:18:22.679
    Jun  8 15:18:22.679: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename svcaccounts 06/08/23 15:18:22.68
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:18:22.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:18:22.706
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  06/08/23 15:18:22.71
    Jun  8 15:18:22.723: INFO: Waiting up to 5m0s for pod "test-pod-ed82b2c2-60ee-45b7-9ac4-513cf6060ca1" in namespace "svcaccounts-2000" to be "Succeeded or Failed"
    Jun  8 15:18:22.727: INFO: Pod "test-pod-ed82b2c2-60ee-45b7-9ac4-513cf6060ca1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.645493ms
    Jun  8 15:18:24.733: INFO: Pod "test-pod-ed82b2c2-60ee-45b7-9ac4-513cf6060ca1": Phase="Running", Reason="", readiness=false. Elapsed: 2.010016886s
    Jun  8 15:18:26.733: INFO: Pod "test-pod-ed82b2c2-60ee-45b7-9ac4-513cf6060ca1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010674268s
    STEP: Saw pod success 06/08/23 15:18:26.734
    Jun  8 15:18:26.734: INFO: Pod "test-pod-ed82b2c2-60ee-45b7-9ac4-513cf6060ca1" satisfied condition "Succeeded or Failed"
    Jun  8 15:18:26.737: INFO: Trying to get logs from node chl8tf-worker-001 pod test-pod-ed82b2c2-60ee-45b7-9ac4-513cf6060ca1 container agnhost-container: <nil>
    STEP: delete the pod 06/08/23 15:18:26.755
    Jun  8 15:18:26.768: INFO: Waiting for pod test-pod-ed82b2c2-60ee-45b7-9ac4-513cf6060ca1 to disappear
    Jun  8 15:18:26.771: INFO: Pod test-pod-ed82b2c2-60ee-45b7-9ac4-513cf6060ca1 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:18:26.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2000" for this suite. 06/08/23 15:18:26.777
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:18:26.787
Jun  8 15:18:26.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename security-context 06/08/23 15:18:26.788
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:18:26.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:18:26.807
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/08/23 15:18:26.81
Jun  8 15:18:26.819: INFO: Waiting up to 5m0s for pod "security-context-102fcb0a-774c-48a6-864e-dede5766ade8" in namespace "security-context-1345" to be "Succeeded or Failed"
Jun  8 15:18:26.825: INFO: Pod "security-context-102fcb0a-774c-48a6-864e-dede5766ade8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.068201ms
Jun  8 15:18:28.830: INFO: Pod "security-context-102fcb0a-774c-48a6-864e-dede5766ade8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010050938s
Jun  8 15:18:30.831: INFO: Pod "security-context-102fcb0a-774c-48a6-864e-dede5766ade8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011131138s
STEP: Saw pod success 06/08/23 15:18:30.831
Jun  8 15:18:30.831: INFO: Pod "security-context-102fcb0a-774c-48a6-864e-dede5766ade8" satisfied condition "Succeeded or Failed"
Jun  8 15:18:30.835: INFO: Trying to get logs from node chl8tf-worker-001 pod security-context-102fcb0a-774c-48a6-864e-dede5766ade8 container test-container: <nil>
STEP: delete the pod 06/08/23 15:18:30.843
Jun  8 15:18:30.855: INFO: Waiting for pod security-context-102fcb0a-774c-48a6-864e-dede5766ade8 to disappear
Jun  8 15:18:30.859: INFO: Pod security-context-102fcb0a-774c-48a6-864e-dede5766ade8 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jun  8 15:18:30.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-1345" for this suite. 06/08/23 15:18:30.865
------------------------------
• [4.086 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:18:26.787
    Jun  8 15:18:26.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename security-context 06/08/23 15:18:26.788
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:18:26.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:18:26.807
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/08/23 15:18:26.81
    Jun  8 15:18:26.819: INFO: Waiting up to 5m0s for pod "security-context-102fcb0a-774c-48a6-864e-dede5766ade8" in namespace "security-context-1345" to be "Succeeded or Failed"
    Jun  8 15:18:26.825: INFO: Pod "security-context-102fcb0a-774c-48a6-864e-dede5766ade8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.068201ms
    Jun  8 15:18:28.830: INFO: Pod "security-context-102fcb0a-774c-48a6-864e-dede5766ade8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010050938s
    Jun  8 15:18:30.831: INFO: Pod "security-context-102fcb0a-774c-48a6-864e-dede5766ade8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011131138s
    STEP: Saw pod success 06/08/23 15:18:30.831
    Jun  8 15:18:30.831: INFO: Pod "security-context-102fcb0a-774c-48a6-864e-dede5766ade8" satisfied condition "Succeeded or Failed"
    Jun  8 15:18:30.835: INFO: Trying to get logs from node chl8tf-worker-001 pod security-context-102fcb0a-774c-48a6-864e-dede5766ade8 container test-container: <nil>
    STEP: delete the pod 06/08/23 15:18:30.843
    Jun  8 15:18:30.855: INFO: Waiting for pod security-context-102fcb0a-774c-48a6-864e-dede5766ade8 to disappear
    Jun  8 15:18:30.859: INFO: Pod security-context-102fcb0a-774c-48a6-864e-dede5766ade8 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:18:30.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-1345" for this suite. 06/08/23 15:18:30.865
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:18:30.873
Jun  8 15:18:30.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename downward-api 06/08/23 15:18:30.874
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:18:30.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:18:30.899
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 06/08/23 15:18:30.902
Jun  8 15:18:30.912: INFO: Waiting up to 5m0s for pod "downward-api-8d6dab04-96e6-413f-a519-55d820226eaa" in namespace "downward-api-9126" to be "Succeeded or Failed"
Jun  8 15:18:30.916: INFO: Pod "downward-api-8d6dab04-96e6-413f-a519-55d820226eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.905587ms
Jun  8 15:18:32.920: INFO: Pod "downward-api-8d6dab04-96e6-413f-a519-55d820226eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008415678s
Jun  8 15:18:34.921: INFO: Pod "downward-api-8d6dab04-96e6-413f-a519-55d820226eaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009486676s
STEP: Saw pod success 06/08/23 15:18:34.921
Jun  8 15:18:34.921: INFO: Pod "downward-api-8d6dab04-96e6-413f-a519-55d820226eaa" satisfied condition "Succeeded or Failed"
Jun  8 15:18:34.925: INFO: Trying to get logs from node chl8tf-worker-001 pod downward-api-8d6dab04-96e6-413f-a519-55d820226eaa container dapi-container: <nil>
STEP: delete the pod 06/08/23 15:18:34.935
Jun  8 15:18:34.946: INFO: Waiting for pod downward-api-8d6dab04-96e6-413f-a519-55d820226eaa to disappear
Jun  8 15:18:34.950: INFO: Pod downward-api-8d6dab04-96e6-413f-a519-55d820226eaa no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jun  8 15:18:34.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9126" for this suite. 06/08/23 15:18:34.955
------------------------------
• [4.089 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:18:30.873
    Jun  8 15:18:30.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename downward-api 06/08/23 15:18:30.874
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:18:30.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:18:30.899
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 06/08/23 15:18:30.902
    Jun  8 15:18:30.912: INFO: Waiting up to 5m0s for pod "downward-api-8d6dab04-96e6-413f-a519-55d820226eaa" in namespace "downward-api-9126" to be "Succeeded or Failed"
    Jun  8 15:18:30.916: INFO: Pod "downward-api-8d6dab04-96e6-413f-a519-55d820226eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.905587ms
    Jun  8 15:18:32.920: INFO: Pod "downward-api-8d6dab04-96e6-413f-a519-55d820226eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008415678s
    Jun  8 15:18:34.921: INFO: Pod "downward-api-8d6dab04-96e6-413f-a519-55d820226eaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009486676s
    STEP: Saw pod success 06/08/23 15:18:34.921
    Jun  8 15:18:34.921: INFO: Pod "downward-api-8d6dab04-96e6-413f-a519-55d820226eaa" satisfied condition "Succeeded or Failed"
    Jun  8 15:18:34.925: INFO: Trying to get logs from node chl8tf-worker-001 pod downward-api-8d6dab04-96e6-413f-a519-55d820226eaa container dapi-container: <nil>
    STEP: delete the pod 06/08/23 15:18:34.935
    Jun  8 15:18:34.946: INFO: Waiting for pod downward-api-8d6dab04-96e6-413f-a519-55d820226eaa to disappear
    Jun  8 15:18:34.950: INFO: Pod downward-api-8d6dab04-96e6-413f-a519-55d820226eaa no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:18:34.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9126" for this suite. 06/08/23 15:18:34.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:18:34.962
Jun  8 15:18:34.963: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename job 06/08/23 15:18:34.964
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:18:34.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:18:34.985
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 06/08/23 15:18:34.988
STEP: Ensuring active pods == parallelism 06/08/23 15:18:34.995
STEP: Orphaning one of the Job's Pods 06/08/23 15:18:37
Jun  8 15:18:37.519: INFO: Successfully updated pod "adopt-release-5g2ww"
STEP: Checking that the Job readopts the Pod 06/08/23 15:18:37.519
Jun  8 15:18:37.519: INFO: Waiting up to 15m0s for pod "adopt-release-5g2ww" in namespace "job-10" to be "adopted"
Jun  8 15:18:37.523: INFO: Pod "adopt-release-5g2ww": Phase="Running", Reason="", readiness=true. Elapsed: 3.800345ms
Jun  8 15:18:39.530: INFO: Pod "adopt-release-5g2ww": Phase="Running", Reason="", readiness=true. Elapsed: 2.010172961s
Jun  8 15:18:39.530: INFO: Pod "adopt-release-5g2ww" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 06/08/23 15:18:39.53
Jun  8 15:18:40.043: INFO: Successfully updated pod "adopt-release-5g2ww"
STEP: Checking that the Job releases the Pod 06/08/23 15:18:40.043
Jun  8 15:18:40.044: INFO: Waiting up to 15m0s for pod "adopt-release-5g2ww" in namespace "job-10" to be "released"
Jun  8 15:18:40.048: INFO: Pod "adopt-release-5g2ww": Phase="Running", Reason="", readiness=true. Elapsed: 4.051235ms
Jun  8 15:18:42.053: INFO: Pod "adopt-release-5g2ww": Phase="Running", Reason="", readiness=true. Elapsed: 2.009630684s
Jun  8 15:18:42.053: INFO: Pod "adopt-release-5g2ww" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jun  8 15:18:42.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-10" for this suite. 06/08/23 15:18:42.059
------------------------------
• [SLOW TEST] [7.104 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:18:34.962
    Jun  8 15:18:34.963: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename job 06/08/23 15:18:34.964
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:18:34.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:18:34.985
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 06/08/23 15:18:34.988
    STEP: Ensuring active pods == parallelism 06/08/23 15:18:34.995
    STEP: Orphaning one of the Job's Pods 06/08/23 15:18:37
    Jun  8 15:18:37.519: INFO: Successfully updated pod "adopt-release-5g2ww"
    STEP: Checking that the Job readopts the Pod 06/08/23 15:18:37.519
    Jun  8 15:18:37.519: INFO: Waiting up to 15m0s for pod "adopt-release-5g2ww" in namespace "job-10" to be "adopted"
    Jun  8 15:18:37.523: INFO: Pod "adopt-release-5g2ww": Phase="Running", Reason="", readiness=true. Elapsed: 3.800345ms
    Jun  8 15:18:39.530: INFO: Pod "adopt-release-5g2ww": Phase="Running", Reason="", readiness=true. Elapsed: 2.010172961s
    Jun  8 15:18:39.530: INFO: Pod "adopt-release-5g2ww" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 06/08/23 15:18:39.53
    Jun  8 15:18:40.043: INFO: Successfully updated pod "adopt-release-5g2ww"
    STEP: Checking that the Job releases the Pod 06/08/23 15:18:40.043
    Jun  8 15:18:40.044: INFO: Waiting up to 15m0s for pod "adopt-release-5g2ww" in namespace "job-10" to be "released"
    Jun  8 15:18:40.048: INFO: Pod "adopt-release-5g2ww": Phase="Running", Reason="", readiness=true. Elapsed: 4.051235ms
    Jun  8 15:18:42.053: INFO: Pod "adopt-release-5g2ww": Phase="Running", Reason="", readiness=true. Elapsed: 2.009630684s
    Jun  8 15:18:42.053: INFO: Pod "adopt-release-5g2ww" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:18:42.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-10" for this suite. 06/08/23 15:18:42.059
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:18:42.068
Jun  8 15:18:42.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename replication-controller 06/08/23 15:18:42.069
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:18:42.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:18:42.09
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Jun  8 15:18:42.094: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 06/08/23 15:18:43.107
STEP: Checking rc "condition-test" has the desired failure condition set 06/08/23 15:18:43.114
STEP: Scaling down rc "condition-test" to satisfy pod quota 06/08/23 15:18:44.124
Jun  8 15:18:44.137: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 06/08/23 15:18:44.137
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jun  8 15:18:45.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9495" for this suite. 06/08/23 15:18:45.153
------------------------------
• [3.094 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:18:42.068
    Jun  8 15:18:42.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename replication-controller 06/08/23 15:18:42.069
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:18:42.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:18:42.09
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Jun  8 15:18:42.094: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 06/08/23 15:18:43.107
    STEP: Checking rc "condition-test" has the desired failure condition set 06/08/23 15:18:43.114
    STEP: Scaling down rc "condition-test" to satisfy pod quota 06/08/23 15:18:44.124
    Jun  8 15:18:44.137: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 06/08/23 15:18:44.137
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:18:45.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9495" for this suite. 06/08/23 15:18:45.153
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:18:45.164
Jun  8 15:18:45.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename limitrange 06/08/23 15:18:45.166
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:18:45.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:18:45.188
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-pw4bl" in namespace "limitrange-8176" 06/08/23 15:18:45.191
STEP: Creating another limitRange in another namespace 06/08/23 15:18:45.198
Jun  8 15:18:45.215: INFO: Namespace "e2e-limitrange-pw4bl-3332" created
Jun  8 15:18:45.215: INFO: Creating LimitRange "e2e-limitrange-pw4bl" in namespace "e2e-limitrange-pw4bl-3332"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-pw4bl" 06/08/23 15:18:45.221
Jun  8 15:18:45.225: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-pw4bl" in "limitrange-8176" namespace 06/08/23 15:18:45.225
Jun  8 15:18:45.233: INFO: LimitRange "e2e-limitrange-pw4bl" has been patched
STEP: Delete LimitRange "e2e-limitrange-pw4bl" by Collection with labelSelector: "e2e-limitrange-pw4bl=patched" 06/08/23 15:18:45.233
STEP: Confirm that the limitRange "e2e-limitrange-pw4bl" has been deleted 06/08/23 15:18:45.242
Jun  8 15:18:45.242: INFO: Requesting list of LimitRange to confirm quantity
Jun  8 15:18:45.245: INFO: Found 0 LimitRange with label "e2e-limitrange-pw4bl=patched"
Jun  8 15:18:45.245: INFO: LimitRange "e2e-limitrange-pw4bl" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-pw4bl" 06/08/23 15:18:45.245
Jun  8 15:18:45.249: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jun  8 15:18:45.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-8176" for this suite. 06/08/23 15:18:45.255
STEP: Destroying namespace "e2e-limitrange-pw4bl-3332" for this suite. 06/08/23 15:18:45.261
------------------------------
• [0.105 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:18:45.164
    Jun  8 15:18:45.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename limitrange 06/08/23 15:18:45.166
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:18:45.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:18:45.188
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-pw4bl" in namespace "limitrange-8176" 06/08/23 15:18:45.191
    STEP: Creating another limitRange in another namespace 06/08/23 15:18:45.198
    Jun  8 15:18:45.215: INFO: Namespace "e2e-limitrange-pw4bl-3332" created
    Jun  8 15:18:45.215: INFO: Creating LimitRange "e2e-limitrange-pw4bl" in namespace "e2e-limitrange-pw4bl-3332"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-pw4bl" 06/08/23 15:18:45.221
    Jun  8 15:18:45.225: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-pw4bl" in "limitrange-8176" namespace 06/08/23 15:18:45.225
    Jun  8 15:18:45.233: INFO: LimitRange "e2e-limitrange-pw4bl" has been patched
    STEP: Delete LimitRange "e2e-limitrange-pw4bl" by Collection with labelSelector: "e2e-limitrange-pw4bl=patched" 06/08/23 15:18:45.233
    STEP: Confirm that the limitRange "e2e-limitrange-pw4bl" has been deleted 06/08/23 15:18:45.242
    Jun  8 15:18:45.242: INFO: Requesting list of LimitRange to confirm quantity
    Jun  8 15:18:45.245: INFO: Found 0 LimitRange with label "e2e-limitrange-pw4bl=patched"
    Jun  8 15:18:45.245: INFO: LimitRange "e2e-limitrange-pw4bl" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-pw4bl" 06/08/23 15:18:45.245
    Jun  8 15:18:45.249: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:18:45.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-8176" for this suite. 06/08/23 15:18:45.255
    STEP: Destroying namespace "e2e-limitrange-pw4bl-3332" for this suite. 06/08/23 15:18:45.261
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:18:45.269
Jun  8 15:18:45.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename webhook 06/08/23 15:18:45.27
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:18:45.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:18:45.291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/08/23 15:18:45.313
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 15:18:45.686
STEP: Deploying the webhook pod 06/08/23 15:18:45.698
STEP: Wait for the deployment to be ready 06/08/23 15:18:45.713
Jun  8 15:18:45.722: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/08/23 15:18:47.735
STEP: Verifying the service has paired with the endpoint 06/08/23 15:18:47.75
Jun  8 15:18:48.751: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Jun  8 15:18:48.756: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2460-crds.webhook.example.com via the AdmissionRegistration API 06/08/23 15:18:49.272
STEP: Creating a custom resource while v1 is storage version 06/08/23 15:18:49.29
STEP: Patching Custom Resource Definition to set v2 as storage 06/08/23 15:18:51.353
STEP: Patching the custom resource while v2 is storage version 06/08/23 15:18:51.372
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:18:51.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8755" for this suite. 06/08/23 15:18:52.051
STEP: Destroying namespace "webhook-8755-markers" for this suite. 06/08/23 15:18:52.061
------------------------------
• [SLOW TEST] [6.802 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:18:45.269
    Jun  8 15:18:45.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename webhook 06/08/23 15:18:45.27
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:18:45.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:18:45.291
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/08/23 15:18:45.313
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 15:18:45.686
    STEP: Deploying the webhook pod 06/08/23 15:18:45.698
    STEP: Wait for the deployment to be ready 06/08/23 15:18:45.713
    Jun  8 15:18:45.722: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/08/23 15:18:47.735
    STEP: Verifying the service has paired with the endpoint 06/08/23 15:18:47.75
    Jun  8 15:18:48.751: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Jun  8 15:18:48.756: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2460-crds.webhook.example.com via the AdmissionRegistration API 06/08/23 15:18:49.272
    STEP: Creating a custom resource while v1 is storage version 06/08/23 15:18:49.29
    STEP: Patching Custom Resource Definition to set v2 as storage 06/08/23 15:18:51.353
    STEP: Patching the custom resource while v2 is storage version 06/08/23 15:18:51.372
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:18:51.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8755" for this suite. 06/08/23 15:18:52.051
    STEP: Destroying namespace "webhook-8755-markers" for this suite. 06/08/23 15:18:52.061
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:18:52.072
Jun  8 15:18:52.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename var-expansion 06/08/23 15:18:52.073
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:18:52.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:18:52.101
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 06/08/23 15:18:52.105
Jun  8 15:18:52.115: INFO: Waiting up to 5m0s for pod "var-expansion-cbf81a64-26ae-4847-b8d8-4d82a94d1215" in namespace "var-expansion-1809" to be "Succeeded or Failed"
Jun  8 15:18:52.119: INFO: Pod "var-expansion-cbf81a64-26ae-4847-b8d8-4d82a94d1215": Phase="Pending", Reason="", readiness=false. Elapsed: 3.947553ms
Jun  8 15:18:54.124: INFO: Pod "var-expansion-cbf81a64-26ae-4847-b8d8-4d82a94d1215": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009321777s
Jun  8 15:18:56.125: INFO: Pod "var-expansion-cbf81a64-26ae-4847-b8d8-4d82a94d1215": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01022267s
STEP: Saw pod success 06/08/23 15:18:56.125
Jun  8 15:18:56.125: INFO: Pod "var-expansion-cbf81a64-26ae-4847-b8d8-4d82a94d1215" satisfied condition "Succeeded or Failed"
Jun  8 15:18:56.131: INFO: Trying to get logs from node chl8tf-worker-002 pod var-expansion-cbf81a64-26ae-4847-b8d8-4d82a94d1215 container dapi-container: <nil>
STEP: delete the pod 06/08/23 15:18:56.151
Jun  8 15:18:56.164: INFO: Waiting for pod var-expansion-cbf81a64-26ae-4847-b8d8-4d82a94d1215 to disappear
Jun  8 15:18:56.169: INFO: Pod var-expansion-cbf81a64-26ae-4847-b8d8-4d82a94d1215 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun  8 15:18:56.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1809" for this suite. 06/08/23 15:18:56.176
------------------------------
• [4.112 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:18:52.072
    Jun  8 15:18:52.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename var-expansion 06/08/23 15:18:52.073
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:18:52.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:18:52.101
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 06/08/23 15:18:52.105
    Jun  8 15:18:52.115: INFO: Waiting up to 5m0s for pod "var-expansion-cbf81a64-26ae-4847-b8d8-4d82a94d1215" in namespace "var-expansion-1809" to be "Succeeded or Failed"
    Jun  8 15:18:52.119: INFO: Pod "var-expansion-cbf81a64-26ae-4847-b8d8-4d82a94d1215": Phase="Pending", Reason="", readiness=false. Elapsed: 3.947553ms
    Jun  8 15:18:54.124: INFO: Pod "var-expansion-cbf81a64-26ae-4847-b8d8-4d82a94d1215": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009321777s
    Jun  8 15:18:56.125: INFO: Pod "var-expansion-cbf81a64-26ae-4847-b8d8-4d82a94d1215": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01022267s
    STEP: Saw pod success 06/08/23 15:18:56.125
    Jun  8 15:18:56.125: INFO: Pod "var-expansion-cbf81a64-26ae-4847-b8d8-4d82a94d1215" satisfied condition "Succeeded or Failed"
    Jun  8 15:18:56.131: INFO: Trying to get logs from node chl8tf-worker-002 pod var-expansion-cbf81a64-26ae-4847-b8d8-4d82a94d1215 container dapi-container: <nil>
    STEP: delete the pod 06/08/23 15:18:56.151
    Jun  8 15:18:56.164: INFO: Waiting for pod var-expansion-cbf81a64-26ae-4847-b8d8-4d82a94d1215 to disappear
    Jun  8 15:18:56.169: INFO: Pod var-expansion-cbf81a64-26ae-4847-b8d8-4d82a94d1215 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:18:56.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1809" for this suite. 06/08/23 15:18:56.176
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:18:56.185
Jun  8 15:18:56.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename emptydir 06/08/23 15:18:56.186
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:18:56.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:18:56.209
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 06/08/23 15:18:56.212
Jun  8 15:18:56.223: INFO: Waiting up to 5m0s for pod "pod-990e73d4-da2f-49f8-9de2-32811de24c57" in namespace "emptydir-4355" to be "Succeeded or Failed"
Jun  8 15:18:56.228: INFO: Pod "pod-990e73d4-da2f-49f8-9de2-32811de24c57": Phase="Pending", Reason="", readiness=false. Elapsed: 5.060985ms
Jun  8 15:18:58.234: INFO: Pod "pod-990e73d4-da2f-49f8-9de2-32811de24c57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010536838s
Jun  8 15:19:00.233: INFO: Pod "pod-990e73d4-da2f-49f8-9de2-32811de24c57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009858966s
STEP: Saw pod success 06/08/23 15:19:00.233
Jun  8 15:19:00.233: INFO: Pod "pod-990e73d4-da2f-49f8-9de2-32811de24c57" satisfied condition "Succeeded or Failed"
Jun  8 15:19:00.237: INFO: Trying to get logs from node chl8tf-worker-002 pod pod-990e73d4-da2f-49f8-9de2-32811de24c57 container test-container: <nil>
STEP: delete the pod 06/08/23 15:19:00.245
Jun  8 15:19:00.260: INFO: Waiting for pod pod-990e73d4-da2f-49f8-9de2-32811de24c57 to disappear
Jun  8 15:19:00.263: INFO: Pod pod-990e73d4-da2f-49f8-9de2-32811de24c57 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  8 15:19:00.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4355" for this suite. 06/08/23 15:19:00.268
------------------------------
• [4.090 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:18:56.185
    Jun  8 15:18:56.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename emptydir 06/08/23 15:18:56.186
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:18:56.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:18:56.209
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 06/08/23 15:18:56.212
    Jun  8 15:18:56.223: INFO: Waiting up to 5m0s for pod "pod-990e73d4-da2f-49f8-9de2-32811de24c57" in namespace "emptydir-4355" to be "Succeeded or Failed"
    Jun  8 15:18:56.228: INFO: Pod "pod-990e73d4-da2f-49f8-9de2-32811de24c57": Phase="Pending", Reason="", readiness=false. Elapsed: 5.060985ms
    Jun  8 15:18:58.234: INFO: Pod "pod-990e73d4-da2f-49f8-9de2-32811de24c57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010536838s
    Jun  8 15:19:00.233: INFO: Pod "pod-990e73d4-da2f-49f8-9de2-32811de24c57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009858966s
    STEP: Saw pod success 06/08/23 15:19:00.233
    Jun  8 15:19:00.233: INFO: Pod "pod-990e73d4-da2f-49f8-9de2-32811de24c57" satisfied condition "Succeeded or Failed"
    Jun  8 15:19:00.237: INFO: Trying to get logs from node chl8tf-worker-002 pod pod-990e73d4-da2f-49f8-9de2-32811de24c57 container test-container: <nil>
    STEP: delete the pod 06/08/23 15:19:00.245
    Jun  8 15:19:00.260: INFO: Waiting for pod pod-990e73d4-da2f-49f8-9de2-32811de24c57 to disappear
    Jun  8 15:19:00.263: INFO: Pod pod-990e73d4-da2f-49f8-9de2-32811de24c57 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:19:00.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4355" for this suite. 06/08/23 15:19:00.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:19:00.276
Jun  8 15:19:00.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubectl 06/08/23 15:19:00.278
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:00.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:00.299
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 06/08/23 15:19:00.302
Jun  8 15:19:00.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8183 create -f -'
Jun  8 15:19:01.218: INFO: stderr: ""
Jun  8 15:19:01.218: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 06/08/23 15:19:01.218
Jun  8 15:19:02.225: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  8 15:19:02.225: INFO: Found 0 / 1
Jun  8 15:19:03.224: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  8 15:19:03.224: INFO: Found 1 / 1
Jun  8 15:19:03.224: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 06/08/23 15:19:03.224
Jun  8 15:19:03.228: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  8 15:19:03.228: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun  8 15:19:03.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8183 patch pod agnhost-primary-zcwp8 -p {"metadata":{"annotations":{"x":"y"}}}'
Jun  8 15:19:03.313: INFO: stderr: ""
Jun  8 15:19:03.313: INFO: stdout: "pod/agnhost-primary-zcwp8 patched\n"
STEP: checking annotations 06/08/23 15:19:03.313
Jun  8 15:19:03.317: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  8 15:19:03.317: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  8 15:19:03.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8183" for this suite. 06/08/23 15:19:03.323
------------------------------
• [3.054 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:19:00.276
    Jun  8 15:19:00.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubectl 06/08/23 15:19:00.278
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:00.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:00.299
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 06/08/23 15:19:00.302
    Jun  8 15:19:00.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8183 create -f -'
    Jun  8 15:19:01.218: INFO: stderr: ""
    Jun  8 15:19:01.218: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 06/08/23 15:19:01.218
    Jun  8 15:19:02.225: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun  8 15:19:02.225: INFO: Found 0 / 1
    Jun  8 15:19:03.224: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun  8 15:19:03.224: INFO: Found 1 / 1
    Jun  8 15:19:03.224: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 06/08/23 15:19:03.224
    Jun  8 15:19:03.228: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun  8 15:19:03.228: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jun  8 15:19:03.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8183 patch pod agnhost-primary-zcwp8 -p {"metadata":{"annotations":{"x":"y"}}}'
    Jun  8 15:19:03.313: INFO: stderr: ""
    Jun  8 15:19:03.313: INFO: stdout: "pod/agnhost-primary-zcwp8 patched\n"
    STEP: checking annotations 06/08/23 15:19:03.313
    Jun  8 15:19:03.317: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun  8 15:19:03.317: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:19:03.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8183" for this suite. 06/08/23 15:19:03.323
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:19:03.33
Jun  8 15:19:03.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubectl 06/08/23 15:19:03.332
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:03.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:03.352
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 06/08/23 15:19:03.355
Jun  8 15:19:03.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8495 create -f -'
Jun  8 15:19:04.179: INFO: stderr: ""
Jun  8 15:19:04.179: INFO: stdout: "pod/pause created\n"
Jun  8 15:19:04.179: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun  8 15:19:04.179: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8495" to be "running and ready"
Jun  8 15:19:04.183: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.149909ms
Jun  8 15:19:04.183: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'chl8tf-worker-002' to be 'Running' but was 'Pending'
Jun  8 15:19:06.188: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009182616s
Jun  8 15:19:06.188: INFO: Pod "pause" satisfied condition "running and ready"
Jun  8 15:19:06.188: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 06/08/23 15:19:06.188
Jun  8 15:19:06.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8495 label pods pause testing-label=testing-label-value'
Jun  8 15:19:06.277: INFO: stderr: ""
Jun  8 15:19:06.277: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 06/08/23 15:19:06.277
Jun  8 15:19:06.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8495 get pod pause -L testing-label'
Jun  8 15:19:06.355: INFO: stderr: ""
Jun  8 15:19:06.355: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 06/08/23 15:19:06.355
Jun  8 15:19:06.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8495 label pods pause testing-label-'
Jun  8 15:19:06.440: INFO: stderr: ""
Jun  8 15:19:06.440: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 06/08/23 15:19:06.44
Jun  8 15:19:06.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8495 get pod pause -L testing-label'
Jun  8 15:19:06.519: INFO: stderr: ""
Jun  8 15:19:06.519: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 06/08/23 15:19:06.519
Jun  8 15:19:06.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8495 delete --grace-period=0 --force -f -'
Jun  8 15:19:06.608: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  8 15:19:06.608: INFO: stdout: "pod \"pause\" force deleted\n"
Jun  8 15:19:06.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8495 get rc,svc -l name=pause --no-headers'
Jun  8 15:19:06.696: INFO: stderr: "No resources found in kubectl-8495 namespace.\n"
Jun  8 15:19:06.696: INFO: stdout: ""
Jun  8 15:19:06.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8495 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  8 15:19:06.772: INFO: stderr: ""
Jun  8 15:19:06.772: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  8 15:19:06.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8495" for this suite. 06/08/23 15:19:06.779
------------------------------
• [3.455 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:19:03.33
    Jun  8 15:19:03.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubectl 06/08/23 15:19:03.332
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:03.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:03.352
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 06/08/23 15:19:03.355
    Jun  8 15:19:03.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8495 create -f -'
    Jun  8 15:19:04.179: INFO: stderr: ""
    Jun  8 15:19:04.179: INFO: stdout: "pod/pause created\n"
    Jun  8 15:19:04.179: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jun  8 15:19:04.179: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8495" to be "running and ready"
    Jun  8 15:19:04.183: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.149909ms
    Jun  8 15:19:04.183: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'chl8tf-worker-002' to be 'Running' but was 'Pending'
    Jun  8 15:19:06.188: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009182616s
    Jun  8 15:19:06.188: INFO: Pod "pause" satisfied condition "running and ready"
    Jun  8 15:19:06.188: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 06/08/23 15:19:06.188
    Jun  8 15:19:06.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8495 label pods pause testing-label=testing-label-value'
    Jun  8 15:19:06.277: INFO: stderr: ""
    Jun  8 15:19:06.277: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 06/08/23 15:19:06.277
    Jun  8 15:19:06.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8495 get pod pause -L testing-label'
    Jun  8 15:19:06.355: INFO: stderr: ""
    Jun  8 15:19:06.355: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 06/08/23 15:19:06.355
    Jun  8 15:19:06.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8495 label pods pause testing-label-'
    Jun  8 15:19:06.440: INFO: stderr: ""
    Jun  8 15:19:06.440: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 06/08/23 15:19:06.44
    Jun  8 15:19:06.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8495 get pod pause -L testing-label'
    Jun  8 15:19:06.519: INFO: stderr: ""
    Jun  8 15:19:06.519: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 06/08/23 15:19:06.519
    Jun  8 15:19:06.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8495 delete --grace-period=0 --force -f -'
    Jun  8 15:19:06.608: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun  8 15:19:06.608: INFO: stdout: "pod \"pause\" force deleted\n"
    Jun  8 15:19:06.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8495 get rc,svc -l name=pause --no-headers'
    Jun  8 15:19:06.696: INFO: stderr: "No resources found in kubectl-8495 namespace.\n"
    Jun  8 15:19:06.696: INFO: stdout: ""
    Jun  8 15:19:06.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8495 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jun  8 15:19:06.772: INFO: stderr: ""
    Jun  8 15:19:06.772: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:19:06.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8495" for this suite. 06/08/23 15:19:06.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:19:06.788
Jun  8 15:19:06.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename events 06/08/23 15:19:06.79
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:06.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:06.811
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 06/08/23 15:19:06.814
Jun  8 15:19:06.822: INFO: created test-event-1
Jun  8 15:19:06.827: INFO: created test-event-2
Jun  8 15:19:06.832: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 06/08/23 15:19:06.832
STEP: delete collection of events 06/08/23 15:19:06.836
Jun  8 15:19:06.836: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 06/08/23 15:19:06.86
Jun  8 15:19:06.861: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jun  8 15:19:06.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7397" for this suite. 06/08/23 15:19:06.87
------------------------------
• [0.093 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:19:06.788
    Jun  8 15:19:06.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename events 06/08/23 15:19:06.79
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:06.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:06.811
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 06/08/23 15:19:06.814
    Jun  8 15:19:06.822: INFO: created test-event-1
    Jun  8 15:19:06.827: INFO: created test-event-2
    Jun  8 15:19:06.832: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 06/08/23 15:19:06.832
    STEP: delete collection of events 06/08/23 15:19:06.836
    Jun  8 15:19:06.836: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 06/08/23 15:19:06.86
    Jun  8 15:19:06.861: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:19:06.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7397" for this suite. 06/08/23 15:19:06.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:19:06.883
Jun  8 15:19:06.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename security-context-test 06/08/23 15:19:06.884
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:06.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:06.904
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Jun  8 15:19:06.918: INFO: Waiting up to 5m0s for pod "busybox-user-65534-51d27ce1-896d-4ac9-8831-968c05fd88ca" in namespace "security-context-test-2615" to be "Succeeded or Failed"
Jun  8 15:19:06.921: INFO: Pod "busybox-user-65534-51d27ce1-896d-4ac9-8831-968c05fd88ca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.68108ms
Jun  8 15:19:08.927: INFO: Pod "busybox-user-65534-51d27ce1-896d-4ac9-8831-968c05fd88ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009074161s
Jun  8 15:19:10.927: INFO: Pod "busybox-user-65534-51d27ce1-896d-4ac9-8831-968c05fd88ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009100421s
Jun  8 15:19:10.927: INFO: Pod "busybox-user-65534-51d27ce1-896d-4ac9-8831-968c05fd88ca" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jun  8 15:19:10.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-2615" for this suite. 06/08/23 15:19:10.933
------------------------------
• [4.058 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:19:06.883
    Jun  8 15:19:06.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename security-context-test 06/08/23 15:19:06.884
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:06.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:06.904
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Jun  8 15:19:06.918: INFO: Waiting up to 5m0s for pod "busybox-user-65534-51d27ce1-896d-4ac9-8831-968c05fd88ca" in namespace "security-context-test-2615" to be "Succeeded or Failed"
    Jun  8 15:19:06.921: INFO: Pod "busybox-user-65534-51d27ce1-896d-4ac9-8831-968c05fd88ca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.68108ms
    Jun  8 15:19:08.927: INFO: Pod "busybox-user-65534-51d27ce1-896d-4ac9-8831-968c05fd88ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009074161s
    Jun  8 15:19:10.927: INFO: Pod "busybox-user-65534-51d27ce1-896d-4ac9-8831-968c05fd88ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009100421s
    Jun  8 15:19:10.927: INFO: Pod "busybox-user-65534-51d27ce1-896d-4ac9-8831-968c05fd88ca" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:19:10.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-2615" for this suite. 06/08/23 15:19:10.933
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:19:10.942
Jun  8 15:19:10.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename emptydir 06/08/23 15:19:10.944
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:10.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:10.964
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 06/08/23 15:19:10.967
Jun  8 15:19:10.977: INFO: Waiting up to 5m0s for pod "pod-049e7070-0f67-4ead-b420-accd183ffa9e" in namespace "emptydir-2358" to be "Succeeded or Failed"
Jun  8 15:19:10.982: INFO: Pod "pod-049e7070-0f67-4ead-b420-accd183ffa9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.066675ms
Jun  8 15:19:12.987: INFO: Pod "pod-049e7070-0f67-4ead-b420-accd183ffa9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01022384s
STEP: Saw pod success 06/08/23 15:19:12.987
Jun  8 15:19:12.988: INFO: Pod "pod-049e7070-0f67-4ead-b420-accd183ffa9e" satisfied condition "Succeeded or Failed"
Jun  8 15:19:12.992: INFO: Trying to get logs from node chl8tf-worker-002 pod pod-049e7070-0f67-4ead-b420-accd183ffa9e container test-container: <nil>
STEP: delete the pod 06/08/23 15:19:12.999
Jun  8 15:19:13.010: INFO: Waiting for pod pod-049e7070-0f67-4ead-b420-accd183ffa9e to disappear
Jun  8 15:19:13.014: INFO: Pod pod-049e7070-0f67-4ead-b420-accd183ffa9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  8 15:19:13.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2358" for this suite. 06/08/23 15:19:13.02
------------------------------
• [2.084 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:19:10.942
    Jun  8 15:19:10.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename emptydir 06/08/23 15:19:10.944
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:10.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:10.964
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 06/08/23 15:19:10.967
    Jun  8 15:19:10.977: INFO: Waiting up to 5m0s for pod "pod-049e7070-0f67-4ead-b420-accd183ffa9e" in namespace "emptydir-2358" to be "Succeeded or Failed"
    Jun  8 15:19:10.982: INFO: Pod "pod-049e7070-0f67-4ead-b420-accd183ffa9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.066675ms
    Jun  8 15:19:12.987: INFO: Pod "pod-049e7070-0f67-4ead-b420-accd183ffa9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01022384s
    STEP: Saw pod success 06/08/23 15:19:12.987
    Jun  8 15:19:12.988: INFO: Pod "pod-049e7070-0f67-4ead-b420-accd183ffa9e" satisfied condition "Succeeded or Failed"
    Jun  8 15:19:12.992: INFO: Trying to get logs from node chl8tf-worker-002 pod pod-049e7070-0f67-4ead-b420-accd183ffa9e container test-container: <nil>
    STEP: delete the pod 06/08/23 15:19:12.999
    Jun  8 15:19:13.010: INFO: Waiting for pod pod-049e7070-0f67-4ead-b420-accd183ffa9e to disappear
    Jun  8 15:19:13.014: INFO: Pod pod-049e7070-0f67-4ead-b420-accd183ffa9e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:19:13.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2358" for this suite. 06/08/23 15:19:13.02
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:19:13.028
Jun  8 15:19:13.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename svcaccounts 06/08/23 15:19:13.029
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:13.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:13.051
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-44rbr"  06/08/23 15:19:13.054
Jun  8 15:19:13.060: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-44rbr"  06/08/23 15:19:13.06
Jun  8 15:19:13.074: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jun  8 15:19:13.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3887" for this suite. 06/08/23 15:19:13.081
------------------------------
• [0.063 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:19:13.028
    Jun  8 15:19:13.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename svcaccounts 06/08/23 15:19:13.029
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:13.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:13.051
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-44rbr"  06/08/23 15:19:13.054
    Jun  8 15:19:13.060: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-44rbr"  06/08/23 15:19:13.06
    Jun  8 15:19:13.074: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:19:13.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3887" for this suite. 06/08/23 15:19:13.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:19:13.092
Jun  8 15:19:13.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 15:19:13.093
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:13.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:13.116
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-bfc4dd1f-fed0-4831-8b8b-b22cb2d660d5 06/08/23 15:19:13.12
STEP: Creating a pod to test consume secrets 06/08/23 15:19:13.126
Jun  8 15:19:13.138: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d69c118c-13bb-4df7-9765-1a9154743a9b" in namespace "projected-7075" to be "Succeeded or Failed"
Jun  8 15:19:13.145: INFO: Pod "pod-projected-secrets-d69c118c-13bb-4df7-9765-1a9154743a9b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.058985ms
Jun  8 15:19:15.151: INFO: Pod "pod-projected-secrets-d69c118c-13bb-4df7-9765-1a9154743a9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012696969s
Jun  8 15:19:17.151: INFO: Pod "pod-projected-secrets-d69c118c-13bb-4df7-9765-1a9154743a9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013073709s
STEP: Saw pod success 06/08/23 15:19:17.151
Jun  8 15:19:17.151: INFO: Pod "pod-projected-secrets-d69c118c-13bb-4df7-9765-1a9154743a9b" satisfied condition "Succeeded or Failed"
Jun  8 15:19:17.155: INFO: Trying to get logs from node chl8tf-worker-002 pod pod-projected-secrets-d69c118c-13bb-4df7-9765-1a9154743a9b container projected-secret-volume-test: <nil>
STEP: delete the pod 06/08/23 15:19:17.163
Jun  8 15:19:17.174: INFO: Waiting for pod pod-projected-secrets-d69c118c-13bb-4df7-9765-1a9154743a9b to disappear
Jun  8 15:19:17.177: INFO: Pod pod-projected-secrets-d69c118c-13bb-4df7-9765-1a9154743a9b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jun  8 15:19:17.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7075" for this suite. 06/08/23 15:19:17.183
------------------------------
• [4.099 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:19:13.092
    Jun  8 15:19:13.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 15:19:13.093
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:13.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:13.116
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-bfc4dd1f-fed0-4831-8b8b-b22cb2d660d5 06/08/23 15:19:13.12
    STEP: Creating a pod to test consume secrets 06/08/23 15:19:13.126
    Jun  8 15:19:13.138: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d69c118c-13bb-4df7-9765-1a9154743a9b" in namespace "projected-7075" to be "Succeeded or Failed"
    Jun  8 15:19:13.145: INFO: Pod "pod-projected-secrets-d69c118c-13bb-4df7-9765-1a9154743a9b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.058985ms
    Jun  8 15:19:15.151: INFO: Pod "pod-projected-secrets-d69c118c-13bb-4df7-9765-1a9154743a9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012696969s
    Jun  8 15:19:17.151: INFO: Pod "pod-projected-secrets-d69c118c-13bb-4df7-9765-1a9154743a9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013073709s
    STEP: Saw pod success 06/08/23 15:19:17.151
    Jun  8 15:19:17.151: INFO: Pod "pod-projected-secrets-d69c118c-13bb-4df7-9765-1a9154743a9b" satisfied condition "Succeeded or Failed"
    Jun  8 15:19:17.155: INFO: Trying to get logs from node chl8tf-worker-002 pod pod-projected-secrets-d69c118c-13bb-4df7-9765-1a9154743a9b container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/08/23 15:19:17.163
    Jun  8 15:19:17.174: INFO: Waiting for pod pod-projected-secrets-d69c118c-13bb-4df7-9765-1a9154743a9b to disappear
    Jun  8 15:19:17.177: INFO: Pod pod-projected-secrets-d69c118c-13bb-4df7-9765-1a9154743a9b no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:19:17.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7075" for this suite. 06/08/23 15:19:17.183
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:19:17.191
Jun  8 15:19:17.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename configmap 06/08/23 15:19:17.192
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:17.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:17.214
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  8 15:19:17.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-338" for this suite. 06/08/23 15:19:17.263
------------------------------
• [0.079 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:19:17.191
    Jun  8 15:19:17.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename configmap 06/08/23 15:19:17.192
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:17.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:17.214
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:19:17.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-338" for this suite. 06/08/23 15:19:17.263
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:19:17.271
Jun  8 15:19:17.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename watch 06/08/23 15:19:17.272
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:17.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:17.292
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 06/08/23 15:19:17.296
STEP: modifying the configmap once 06/08/23 15:19:17.301
STEP: modifying the configmap a second time 06/08/23 15:19:17.312
STEP: deleting the configmap 06/08/23 15:19:17.321
STEP: creating a watch on configmaps from the resource version returned by the first update 06/08/23 15:19:17.327
STEP: Expecting to observe notifications for all changes to the configmap after the first update 06/08/23 15:19:17.329
Jun  8 15:19:17.329: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9551  a0dabb40-1ceb-4cfb-8735-eb0e9975e9a8 25280 0 2023-06-08 15:19:17 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-08 15:19:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  8 15:19:17.329: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9551  a0dabb40-1ceb-4cfb-8735-eb0e9975e9a8 25281 0 2023-06-08 15:19:17 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-08 15:19:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jun  8 15:19:17.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9551" for this suite. 06/08/23 15:19:17.335
------------------------------
• [0.071 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:19:17.271
    Jun  8 15:19:17.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename watch 06/08/23 15:19:17.272
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:17.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:17.292
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 06/08/23 15:19:17.296
    STEP: modifying the configmap once 06/08/23 15:19:17.301
    STEP: modifying the configmap a second time 06/08/23 15:19:17.312
    STEP: deleting the configmap 06/08/23 15:19:17.321
    STEP: creating a watch on configmaps from the resource version returned by the first update 06/08/23 15:19:17.327
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 06/08/23 15:19:17.329
    Jun  8 15:19:17.329: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9551  a0dabb40-1ceb-4cfb-8735-eb0e9975e9a8 25280 0 2023-06-08 15:19:17 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-08 15:19:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  8 15:19:17.329: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9551  a0dabb40-1ceb-4cfb-8735-eb0e9975e9a8 25281 0 2023-06-08 15:19:17 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-08 15:19:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:19:17.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9551" for this suite. 06/08/23 15:19:17.335
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:19:17.342
Jun  8 15:19:17.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename services 06/08/23 15:19:17.343
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:17.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:17.362
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 06/08/23 15:19:17.369
STEP: waiting for available Endpoint 06/08/23 15:19:17.375
STEP: listing all Endpoints 06/08/23 15:19:17.377
STEP: updating the Endpoint 06/08/23 15:19:17.381
STEP: fetching the Endpoint 06/08/23 15:19:17.388
STEP: patching the Endpoint 06/08/23 15:19:17.392
STEP: fetching the Endpoint 06/08/23 15:19:17.403
STEP: deleting the Endpoint by Collection 06/08/23 15:19:17.407
STEP: waiting for Endpoint deletion 06/08/23 15:19:17.417
STEP: fetching the Endpoint 06/08/23 15:19:17.419
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  8 15:19:17.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4041" for this suite. 06/08/23 15:19:17.432
------------------------------
• [0.098 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:19:17.342
    Jun  8 15:19:17.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename services 06/08/23 15:19:17.343
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:17.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:17.362
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 06/08/23 15:19:17.369
    STEP: waiting for available Endpoint 06/08/23 15:19:17.375
    STEP: listing all Endpoints 06/08/23 15:19:17.377
    STEP: updating the Endpoint 06/08/23 15:19:17.381
    STEP: fetching the Endpoint 06/08/23 15:19:17.388
    STEP: patching the Endpoint 06/08/23 15:19:17.392
    STEP: fetching the Endpoint 06/08/23 15:19:17.403
    STEP: deleting the Endpoint by Collection 06/08/23 15:19:17.407
    STEP: waiting for Endpoint deletion 06/08/23 15:19:17.417
    STEP: fetching the Endpoint 06/08/23 15:19:17.419
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:19:17.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4041" for this suite. 06/08/23 15:19:17.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:19:17.447
Jun  8 15:19:17.447: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename webhook 06/08/23 15:19:17.449
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:17.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:17.474
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/08/23 15:19:17.492
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 15:19:18.104
STEP: Deploying the webhook pod 06/08/23 15:19:18.113
STEP: Wait for the deployment to be ready 06/08/23 15:19:18.127
Jun  8 15:19:18.136: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 06/08/23 15:19:20.149
STEP: Verifying the service has paired with the endpoint 06/08/23 15:19:20.166
Jun  8 15:19:21.167: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 06/08/23 15:19:21.172
STEP: Registering slow webhook via the AdmissionRegistration API 06/08/23 15:19:21.173
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 06/08/23 15:19:21.192
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 06/08/23 15:19:22.203
STEP: Registering slow webhook via the AdmissionRegistration API 06/08/23 15:19:22.204
STEP: Having no error when timeout is longer than webhook latency 06/08/23 15:19:23.233
STEP: Registering slow webhook via the AdmissionRegistration API 06/08/23 15:19:23.233
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 06/08/23 15:19:28.27
STEP: Registering slow webhook via the AdmissionRegistration API 06/08/23 15:19:28.27
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:19:33.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9642" for this suite. 06/08/23 15:19:33.362
STEP: Destroying namespace "webhook-9642-markers" for this suite. 06/08/23 15:19:33.372
------------------------------
• [SLOW TEST] [15.937 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:19:17.447
    Jun  8 15:19:17.447: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename webhook 06/08/23 15:19:17.449
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:17.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:17.474
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/08/23 15:19:17.492
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 15:19:18.104
    STEP: Deploying the webhook pod 06/08/23 15:19:18.113
    STEP: Wait for the deployment to be ready 06/08/23 15:19:18.127
    Jun  8 15:19:18.136: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 06/08/23 15:19:20.149
    STEP: Verifying the service has paired with the endpoint 06/08/23 15:19:20.166
    Jun  8 15:19:21.167: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 06/08/23 15:19:21.172
    STEP: Registering slow webhook via the AdmissionRegistration API 06/08/23 15:19:21.173
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 06/08/23 15:19:21.192
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 06/08/23 15:19:22.203
    STEP: Registering slow webhook via the AdmissionRegistration API 06/08/23 15:19:22.204
    STEP: Having no error when timeout is longer than webhook latency 06/08/23 15:19:23.233
    STEP: Registering slow webhook via the AdmissionRegistration API 06/08/23 15:19:23.233
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 06/08/23 15:19:28.27
    STEP: Registering slow webhook via the AdmissionRegistration API 06/08/23 15:19:28.27
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:19:33.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9642" for this suite. 06/08/23 15:19:33.362
    STEP: Destroying namespace "webhook-9642-markers" for this suite. 06/08/23 15:19:33.372
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:19:33.386
Jun  8 15:19:33.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename secrets 06/08/23 15:19:33.387
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:33.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:33.415
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-b1c4aaac-79e7-413b-a24e-e82cd6c266bb 06/08/23 15:19:33.447
STEP: Creating a pod to test consume secrets 06/08/23 15:19:33.454
Jun  8 15:19:33.467: INFO: Waiting up to 5m0s for pod "pod-secrets-d1c5e678-a19f-4786-ad12-266c111cf4dd" in namespace "secrets-8454" to be "Succeeded or Failed"
Jun  8 15:19:33.476: INFO: Pod "pod-secrets-d1c5e678-a19f-4786-ad12-266c111cf4dd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.147568ms
Jun  8 15:19:35.481: INFO: Pod "pod-secrets-d1c5e678-a19f-4786-ad12-266c111cf4dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01409191s
Jun  8 15:19:37.482: INFO: Pod "pod-secrets-d1c5e678-a19f-4786-ad12-266c111cf4dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014939182s
STEP: Saw pod success 06/08/23 15:19:37.482
Jun  8 15:19:37.482: INFO: Pod "pod-secrets-d1c5e678-a19f-4786-ad12-266c111cf4dd" satisfied condition "Succeeded or Failed"
Jun  8 15:19:37.486: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-secrets-d1c5e678-a19f-4786-ad12-266c111cf4dd container secret-volume-test: <nil>
STEP: delete the pod 06/08/23 15:19:37.494
Jun  8 15:19:37.509: INFO: Waiting for pod pod-secrets-d1c5e678-a19f-4786-ad12-266c111cf4dd to disappear
Jun  8 15:19:37.512: INFO: Pod pod-secrets-d1c5e678-a19f-4786-ad12-266c111cf4dd no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  8 15:19:37.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8454" for this suite. 06/08/23 15:19:37.517
STEP: Destroying namespace "secret-namespace-1255" for this suite. 06/08/23 15:19:37.524
------------------------------
• [4.145 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:19:33.386
    Jun  8 15:19:33.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename secrets 06/08/23 15:19:33.387
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:33.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:33.415
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-b1c4aaac-79e7-413b-a24e-e82cd6c266bb 06/08/23 15:19:33.447
    STEP: Creating a pod to test consume secrets 06/08/23 15:19:33.454
    Jun  8 15:19:33.467: INFO: Waiting up to 5m0s for pod "pod-secrets-d1c5e678-a19f-4786-ad12-266c111cf4dd" in namespace "secrets-8454" to be "Succeeded or Failed"
    Jun  8 15:19:33.476: INFO: Pod "pod-secrets-d1c5e678-a19f-4786-ad12-266c111cf4dd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.147568ms
    Jun  8 15:19:35.481: INFO: Pod "pod-secrets-d1c5e678-a19f-4786-ad12-266c111cf4dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01409191s
    Jun  8 15:19:37.482: INFO: Pod "pod-secrets-d1c5e678-a19f-4786-ad12-266c111cf4dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014939182s
    STEP: Saw pod success 06/08/23 15:19:37.482
    Jun  8 15:19:37.482: INFO: Pod "pod-secrets-d1c5e678-a19f-4786-ad12-266c111cf4dd" satisfied condition "Succeeded or Failed"
    Jun  8 15:19:37.486: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-secrets-d1c5e678-a19f-4786-ad12-266c111cf4dd container secret-volume-test: <nil>
    STEP: delete the pod 06/08/23 15:19:37.494
    Jun  8 15:19:37.509: INFO: Waiting for pod pod-secrets-d1c5e678-a19f-4786-ad12-266c111cf4dd to disappear
    Jun  8 15:19:37.512: INFO: Pod pod-secrets-d1c5e678-a19f-4786-ad12-266c111cf4dd no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:19:37.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8454" for this suite. 06/08/23 15:19:37.517
    STEP: Destroying namespace "secret-namespace-1255" for this suite. 06/08/23 15:19:37.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:19:37.533
Jun  8 15:19:37.533: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubectl 06/08/23 15:19:37.534
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:37.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:37.555
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 06/08/23 15:19:37.559
Jun  8 15:19:37.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-654 api-versions'
Jun  8 15:19:37.638: INFO: stderr: ""
Jun  8 15:19:37.638: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  8 15:19:37.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-654" for this suite. 06/08/23 15:19:37.645
------------------------------
• [0.119 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:19:37.533
    Jun  8 15:19:37.533: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubectl 06/08/23 15:19:37.534
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:37.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:37.555
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 06/08/23 15:19:37.559
    Jun  8 15:19:37.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-654 api-versions'
    Jun  8 15:19:37.638: INFO: stderr: ""
    Jun  8 15:19:37.638: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:19:37.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-654" for this suite. 06/08/23 15:19:37.645
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:19:37.653
Jun  8 15:19:37.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 06/08/23 15:19:37.654
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:37.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:37.675
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 06/08/23 15:19:37.678
STEP: Creating hostNetwork=false pod 06/08/23 15:19:37.678
Jun  8 15:19:37.688: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-266" to be "running and ready"
Jun  8 15:19:37.692: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.598039ms
Jun  8 15:19:37.692: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:19:39.697: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008635807s
Jun  8 15:19:39.697: INFO: The phase of Pod test-pod is Running (Ready = true)
Jun  8 15:19:39.697: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 06/08/23 15:19:39.701
Jun  8 15:19:39.708: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-266" to be "running and ready"
Jun  8 15:19:39.712: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.935654ms
Jun  8 15:19:39.712: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:19:41.719: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.0102416s
Jun  8 15:19:41.719: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jun  8 15:19:41.719: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 06/08/23 15:19:41.722
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 06/08/23 15:19:41.723
Jun  8 15:19:41.723: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-266 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:19:41.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:19:41.723: INFO: ExecWithOptions: Clientset creation
Jun  8 15:19:41.723: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun  8 15:19:41.814: INFO: Exec stderr: ""
Jun  8 15:19:41.814: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-266 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:19:41.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:19:41.815: INFO: ExecWithOptions: Clientset creation
Jun  8 15:19:41.815: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun  8 15:19:41.898: INFO: Exec stderr: ""
Jun  8 15:19:41.898: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-266 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:19:41.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:19:41.899: INFO: ExecWithOptions: Clientset creation
Jun  8 15:19:41.899: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun  8 15:19:41.974: INFO: Exec stderr: ""
Jun  8 15:19:41.974: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-266 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:19:41.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:19:41.975: INFO: ExecWithOptions: Clientset creation
Jun  8 15:19:41.975: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun  8 15:19:42.060: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 06/08/23 15:19:42.06
Jun  8 15:19:42.060: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-266 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:19:42.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:19:42.061: INFO: ExecWithOptions: Clientset creation
Jun  8 15:19:42.061: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jun  8 15:19:42.134: INFO: Exec stderr: ""
Jun  8 15:19:42.134: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-266 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:19:42.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:19:42.135: INFO: ExecWithOptions: Clientset creation
Jun  8 15:19:42.135: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jun  8 15:19:42.208: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 06/08/23 15:19:42.208
Jun  8 15:19:42.209: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-266 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:19:42.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:19:42.209: INFO: ExecWithOptions: Clientset creation
Jun  8 15:19:42.209: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-266/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun  8 15:19:42.294: INFO: Exec stderr: ""
Jun  8 15:19:42.294: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-266 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:19:42.294: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:19:42.295: INFO: ExecWithOptions: Clientset creation
Jun  8 15:19:42.295: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-266/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun  8 15:19:42.377: INFO: Exec stderr: ""
Jun  8 15:19:42.377: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-266 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:19:42.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:19:42.378: INFO: ExecWithOptions: Clientset creation
Jun  8 15:19:42.378: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-266/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun  8 15:19:42.458: INFO: Exec stderr: ""
Jun  8 15:19:42.458: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-266 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:19:42.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:19:42.459: INFO: ExecWithOptions: Clientset creation
Jun  8 15:19:42.459: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-266/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun  8 15:19:42.536: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Jun  8 15:19:42.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-266" for this suite. 06/08/23 15:19:42.546
------------------------------
• [4.901 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:19:37.653
    Jun  8 15:19:37.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 06/08/23 15:19:37.654
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:37.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:37.675
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 06/08/23 15:19:37.678
    STEP: Creating hostNetwork=false pod 06/08/23 15:19:37.678
    Jun  8 15:19:37.688: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-266" to be "running and ready"
    Jun  8 15:19:37.692: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.598039ms
    Jun  8 15:19:37.692: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:19:39.697: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008635807s
    Jun  8 15:19:39.697: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jun  8 15:19:39.697: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 06/08/23 15:19:39.701
    Jun  8 15:19:39.708: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-266" to be "running and ready"
    Jun  8 15:19:39.712: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.935654ms
    Jun  8 15:19:39.712: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:19:41.719: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.0102416s
    Jun  8 15:19:41.719: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jun  8 15:19:41.719: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 06/08/23 15:19:41.722
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 06/08/23 15:19:41.723
    Jun  8 15:19:41.723: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-266 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:19:41.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:19:41.723: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:19:41.723: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun  8 15:19:41.814: INFO: Exec stderr: ""
    Jun  8 15:19:41.814: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-266 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:19:41.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:19:41.815: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:19:41.815: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun  8 15:19:41.898: INFO: Exec stderr: ""
    Jun  8 15:19:41.898: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-266 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:19:41.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:19:41.899: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:19:41.899: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun  8 15:19:41.974: INFO: Exec stderr: ""
    Jun  8 15:19:41.974: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-266 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:19:41.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:19:41.975: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:19:41.975: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun  8 15:19:42.060: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 06/08/23 15:19:42.06
    Jun  8 15:19:42.060: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-266 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:19:42.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:19:42.061: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:19:42.061: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jun  8 15:19:42.134: INFO: Exec stderr: ""
    Jun  8 15:19:42.134: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-266 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:19:42.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:19:42.135: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:19:42.135: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-266/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jun  8 15:19:42.208: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 06/08/23 15:19:42.208
    Jun  8 15:19:42.209: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-266 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:19:42.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:19:42.209: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:19:42.209: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-266/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun  8 15:19:42.294: INFO: Exec stderr: ""
    Jun  8 15:19:42.294: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-266 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:19:42.294: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:19:42.295: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:19:42.295: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-266/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun  8 15:19:42.377: INFO: Exec stderr: ""
    Jun  8 15:19:42.377: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-266 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:19:42.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:19:42.378: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:19:42.378: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-266/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun  8 15:19:42.458: INFO: Exec stderr: ""
    Jun  8 15:19:42.458: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-266 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:19:42.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:19:42.459: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:19:42.459: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-266/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun  8 15:19:42.536: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:19:42.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-266" for this suite. 06/08/23 15:19:42.546
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:19:42.555
Jun  8 15:19:42.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename downward-api 06/08/23 15:19:42.556
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:42.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:42.577
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 06/08/23 15:19:42.581
Jun  8 15:19:42.591: INFO: Waiting up to 5m0s for pod "downward-api-ae013d55-feaa-4ce6-8b5e-06412ef570ea" in namespace "downward-api-4512" to be "Succeeded or Failed"
Jun  8 15:19:42.601: INFO: Pod "downward-api-ae013d55-feaa-4ce6-8b5e-06412ef570ea": Phase="Pending", Reason="", readiness=false. Elapsed: 9.978662ms
Jun  8 15:19:44.607: INFO: Pod "downward-api-ae013d55-feaa-4ce6-8b5e-06412ef570ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016560239s
Jun  8 15:19:46.607: INFO: Pod "downward-api-ae013d55-feaa-4ce6-8b5e-06412ef570ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015997019s
STEP: Saw pod success 06/08/23 15:19:46.607
Jun  8 15:19:46.607: INFO: Pod "downward-api-ae013d55-feaa-4ce6-8b5e-06412ef570ea" satisfied condition "Succeeded or Failed"
Jun  8 15:19:46.611: INFO: Trying to get logs from node chl8tf-worker-001 pod downward-api-ae013d55-feaa-4ce6-8b5e-06412ef570ea container dapi-container: <nil>
STEP: delete the pod 06/08/23 15:19:46.619
Jun  8 15:19:46.630: INFO: Waiting for pod downward-api-ae013d55-feaa-4ce6-8b5e-06412ef570ea to disappear
Jun  8 15:19:46.634: INFO: Pod downward-api-ae013d55-feaa-4ce6-8b5e-06412ef570ea no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jun  8 15:19:46.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4512" for this suite. 06/08/23 15:19:46.64
------------------------------
• [4.093 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:19:42.555
    Jun  8 15:19:42.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename downward-api 06/08/23 15:19:42.556
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:42.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:42.577
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 06/08/23 15:19:42.581
    Jun  8 15:19:42.591: INFO: Waiting up to 5m0s for pod "downward-api-ae013d55-feaa-4ce6-8b5e-06412ef570ea" in namespace "downward-api-4512" to be "Succeeded or Failed"
    Jun  8 15:19:42.601: INFO: Pod "downward-api-ae013d55-feaa-4ce6-8b5e-06412ef570ea": Phase="Pending", Reason="", readiness=false. Elapsed: 9.978662ms
    Jun  8 15:19:44.607: INFO: Pod "downward-api-ae013d55-feaa-4ce6-8b5e-06412ef570ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016560239s
    Jun  8 15:19:46.607: INFO: Pod "downward-api-ae013d55-feaa-4ce6-8b5e-06412ef570ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015997019s
    STEP: Saw pod success 06/08/23 15:19:46.607
    Jun  8 15:19:46.607: INFO: Pod "downward-api-ae013d55-feaa-4ce6-8b5e-06412ef570ea" satisfied condition "Succeeded or Failed"
    Jun  8 15:19:46.611: INFO: Trying to get logs from node chl8tf-worker-001 pod downward-api-ae013d55-feaa-4ce6-8b5e-06412ef570ea container dapi-container: <nil>
    STEP: delete the pod 06/08/23 15:19:46.619
    Jun  8 15:19:46.630: INFO: Waiting for pod downward-api-ae013d55-feaa-4ce6-8b5e-06412ef570ea to disappear
    Jun  8 15:19:46.634: INFO: Pod downward-api-ae013d55-feaa-4ce6-8b5e-06412ef570ea no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:19:46.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4512" for this suite. 06/08/23 15:19:46.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:19:46.648
Jun  8 15:19:46.648: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename svcaccounts 06/08/23 15:19:46.65
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:46.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:46.67
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Jun  8 15:19:46.688: INFO: created pod
Jun  8 15:19:46.688: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4261" to be "Succeeded or Failed"
Jun  8 15:19:46.692: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.099699ms
Jun  8 15:19:48.698: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010062296s
Jun  8 15:19:50.698: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009845424s
STEP: Saw pod success 06/08/23 15:19:50.698
Jun  8 15:19:50.698: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jun  8 15:20:20.699: INFO: polling logs
Jun  8 15:20:20.708: INFO: Pod logs: 
I0608 15:19:47.295902       1 log.go:198] OK: Got token
I0608 15:19:47.295965       1 log.go:198] validating with in-cluster discovery
I0608 15:19:47.296441       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0608 15:19:47.296492       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4261:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1686238186, NotBefore:1686237586, IssuedAt:1686237586, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4261", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3fba7b67-a0a8-4190-8111-dbe52b49822a"}}}
I0608 15:19:47.317323       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0608 15:19:47.324985       1 log.go:198] OK: Validated signature on JWT
I0608 15:19:47.325121       1 log.go:198] OK: Got valid claims from token!
I0608 15:19:47.325156       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4261:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1686238186, NotBefore:1686237586, IssuedAt:1686237586, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4261", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3fba7b67-a0a8-4190-8111-dbe52b49822a"}}}

Jun  8 15:20:20.708: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jun  8 15:20:20.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4261" for this suite. 06/08/23 15:20:20.722
------------------------------
• [SLOW TEST] [34.081 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:19:46.648
    Jun  8 15:19:46.648: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename svcaccounts 06/08/23 15:19:46.65
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:19:46.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:19:46.67
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Jun  8 15:19:46.688: INFO: created pod
    Jun  8 15:19:46.688: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4261" to be "Succeeded or Failed"
    Jun  8 15:19:46.692: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.099699ms
    Jun  8 15:19:48.698: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010062296s
    Jun  8 15:19:50.698: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009845424s
    STEP: Saw pod success 06/08/23 15:19:50.698
    Jun  8 15:19:50.698: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jun  8 15:20:20.699: INFO: polling logs
    Jun  8 15:20:20.708: INFO: Pod logs: 
    I0608 15:19:47.295902       1 log.go:198] OK: Got token
    I0608 15:19:47.295965       1 log.go:198] validating with in-cluster discovery
    I0608 15:19:47.296441       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0608 15:19:47.296492       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4261:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1686238186, NotBefore:1686237586, IssuedAt:1686237586, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4261", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3fba7b67-a0a8-4190-8111-dbe52b49822a"}}}
    I0608 15:19:47.317323       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0608 15:19:47.324985       1 log.go:198] OK: Validated signature on JWT
    I0608 15:19:47.325121       1 log.go:198] OK: Got valid claims from token!
    I0608 15:19:47.325156       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4261:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1686238186, NotBefore:1686237586, IssuedAt:1686237586, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4261", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3fba7b67-a0a8-4190-8111-dbe52b49822a"}}}

    Jun  8 15:20:20.708: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:20:20.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4261" for this suite. 06/08/23 15:20:20.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:20:20.732
Jun  8 15:20:20.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 15:20:20.734
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:20:20.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:20:20.755
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-c1f0c7dd-afc8-4e2e-93bb-cae9be474213 06/08/23 15:20:20.758
STEP: Creating a pod to test consume configMaps 06/08/23 15:20:20.764
Jun  8 15:20:20.773: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7d00e1b7-bc1d-4d88-ab3a-1f03bdf16c82" in namespace "projected-1079" to be "Succeeded or Failed"
Jun  8 15:20:20.777: INFO: Pod "pod-projected-configmaps-7d00e1b7-bc1d-4d88-ab3a-1f03bdf16c82": Phase="Pending", Reason="", readiness=false. Elapsed: 3.73437ms
Jun  8 15:20:22.783: INFO: Pod "pod-projected-configmaps-7d00e1b7-bc1d-4d88-ab3a-1f03bdf16c82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009522057s
Jun  8 15:20:24.782: INFO: Pod "pod-projected-configmaps-7d00e1b7-bc1d-4d88-ab3a-1f03bdf16c82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009153068s
STEP: Saw pod success 06/08/23 15:20:24.782
Jun  8 15:20:24.783: INFO: Pod "pod-projected-configmaps-7d00e1b7-bc1d-4d88-ab3a-1f03bdf16c82" satisfied condition "Succeeded or Failed"
Jun  8 15:20:24.787: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-configmaps-7d00e1b7-bc1d-4d88-ab3a-1f03bdf16c82 container agnhost-container: <nil>
STEP: delete the pod 06/08/23 15:20:24.795
Jun  8 15:20:24.809: INFO: Waiting for pod pod-projected-configmaps-7d00e1b7-bc1d-4d88-ab3a-1f03bdf16c82 to disappear
Jun  8 15:20:24.812: INFO: Pod pod-projected-configmaps-7d00e1b7-bc1d-4d88-ab3a-1f03bdf16c82 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun  8 15:20:24.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1079" for this suite. 06/08/23 15:20:24.818
------------------------------
• [4.092 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:20:20.732
    Jun  8 15:20:20.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 15:20:20.734
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:20:20.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:20:20.755
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-c1f0c7dd-afc8-4e2e-93bb-cae9be474213 06/08/23 15:20:20.758
    STEP: Creating a pod to test consume configMaps 06/08/23 15:20:20.764
    Jun  8 15:20:20.773: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7d00e1b7-bc1d-4d88-ab3a-1f03bdf16c82" in namespace "projected-1079" to be "Succeeded or Failed"
    Jun  8 15:20:20.777: INFO: Pod "pod-projected-configmaps-7d00e1b7-bc1d-4d88-ab3a-1f03bdf16c82": Phase="Pending", Reason="", readiness=false. Elapsed: 3.73437ms
    Jun  8 15:20:22.783: INFO: Pod "pod-projected-configmaps-7d00e1b7-bc1d-4d88-ab3a-1f03bdf16c82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009522057s
    Jun  8 15:20:24.782: INFO: Pod "pod-projected-configmaps-7d00e1b7-bc1d-4d88-ab3a-1f03bdf16c82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009153068s
    STEP: Saw pod success 06/08/23 15:20:24.782
    Jun  8 15:20:24.783: INFO: Pod "pod-projected-configmaps-7d00e1b7-bc1d-4d88-ab3a-1f03bdf16c82" satisfied condition "Succeeded or Failed"
    Jun  8 15:20:24.787: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-configmaps-7d00e1b7-bc1d-4d88-ab3a-1f03bdf16c82 container agnhost-container: <nil>
    STEP: delete the pod 06/08/23 15:20:24.795
    Jun  8 15:20:24.809: INFO: Waiting for pod pod-projected-configmaps-7d00e1b7-bc1d-4d88-ab3a-1f03bdf16c82 to disappear
    Jun  8 15:20:24.812: INFO: Pod pod-projected-configmaps-7d00e1b7-bc1d-4d88-ab3a-1f03bdf16c82 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:20:24.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1079" for this suite. 06/08/23 15:20:24.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:20:24.826
Jun  8 15:20:24.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename crd-watch 06/08/23 15:20:24.827
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:20:24.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:20:24.854
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jun  8 15:20:24.857: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Creating first CR  06/08/23 15:20:27.415
Jun  8 15:20:27.420: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-08T15:20:27Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-08T15:20:27Z]] name:name1 resourceVersion:25857 uid:542ead1e-8b91-48d7-ab23-22e8993857e5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 06/08/23 15:20:37.422
Jun  8 15:20:37.431: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-08T15:20:37Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-08T15:20:37Z]] name:name2 resourceVersion:25907 uid:8c824900-685d-4b0a-baaf-c8909eb6d17f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 06/08/23 15:20:47.433
Jun  8 15:20:47.441: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-08T15:20:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-08T15:20:47Z]] name:name1 resourceVersion:25947 uid:542ead1e-8b91-48d7-ab23-22e8993857e5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 06/08/23 15:20:57.442
Jun  8 15:20:57.450: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-08T15:20:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-08T15:20:57Z]] name:name2 resourceVersion:25987 uid:8c824900-685d-4b0a-baaf-c8909eb6d17f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 06/08/23 15:21:07.451
Jun  8 15:21:07.460: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-08T15:20:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-08T15:20:47Z]] name:name1 resourceVersion:26028 uid:542ead1e-8b91-48d7-ab23-22e8993857e5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 06/08/23 15:21:17.461
Jun  8 15:21:17.471: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-08T15:20:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-08T15:20:57Z]] name:name2 resourceVersion:26068 uid:8c824900-685d-4b0a-baaf-c8909eb6d17f] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:21:27.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-1240" for this suite. 06/08/23 15:21:27.992
------------------------------
• [SLOW TEST] [63.174 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:20:24.826
    Jun  8 15:20:24.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename crd-watch 06/08/23 15:20:24.827
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:20:24.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:20:24.854
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jun  8 15:20:24.857: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Creating first CR  06/08/23 15:20:27.415
    Jun  8 15:20:27.420: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-08T15:20:27Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-08T15:20:27Z]] name:name1 resourceVersion:25857 uid:542ead1e-8b91-48d7-ab23-22e8993857e5] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 06/08/23 15:20:37.422
    Jun  8 15:20:37.431: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-08T15:20:37Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-08T15:20:37Z]] name:name2 resourceVersion:25907 uid:8c824900-685d-4b0a-baaf-c8909eb6d17f] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 06/08/23 15:20:47.433
    Jun  8 15:20:47.441: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-08T15:20:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-08T15:20:47Z]] name:name1 resourceVersion:25947 uid:542ead1e-8b91-48d7-ab23-22e8993857e5] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 06/08/23 15:20:57.442
    Jun  8 15:20:57.450: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-08T15:20:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-08T15:20:57Z]] name:name2 resourceVersion:25987 uid:8c824900-685d-4b0a-baaf-c8909eb6d17f] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 06/08/23 15:21:07.451
    Jun  8 15:21:07.460: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-08T15:20:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-08T15:20:47Z]] name:name1 resourceVersion:26028 uid:542ead1e-8b91-48d7-ab23-22e8993857e5] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 06/08/23 15:21:17.461
    Jun  8 15:21:17.471: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-08T15:20:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-08T15:20:57Z]] name:name2 resourceVersion:26068 uid:8c824900-685d-4b0a-baaf-c8909eb6d17f] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:21:27.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-1240" for this suite. 06/08/23 15:21:27.992
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:21:28
Jun  8 15:21:28.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename services 06/08/23 15:21:28.002
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:21:28.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:21:28.035
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-462 06/08/23 15:21:28.038
STEP: creating service affinity-clusterip in namespace services-462 06/08/23 15:21:28.038
STEP: creating replication controller affinity-clusterip in namespace services-462 06/08/23 15:21:28.074
I0608 15:21:28.084346      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-462, replica count: 3
I0608 15:21:31.135618      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  8 15:21:31.143: INFO: Creating new exec pod
Jun  8 15:21:31.151: INFO: Waiting up to 5m0s for pod "execpod-affinity44ctg" in namespace "services-462" to be "running"
Jun  8 15:21:31.155: INFO: Pod "execpod-affinity44ctg": Phase="Pending", Reason="", readiness=false. Elapsed: 3.586144ms
Jun  8 15:21:33.160: INFO: Pod "execpod-affinity44ctg": Phase="Running", Reason="", readiness=true. Elapsed: 2.008743653s
Jun  8 15:21:33.160: INFO: Pod "execpod-affinity44ctg" satisfied condition "running"
Jun  8 15:21:34.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-462 exec execpod-affinity44ctg -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Jun  8 15:21:34.335: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jun  8 15:21:34.335: INFO: stdout: ""
Jun  8 15:21:34.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-462 exec execpod-affinity44ctg -- /bin/sh -x -c nc -v -z -w 2 10.107.208.119 80'
Jun  8 15:21:34.502: INFO: stderr: "+ nc -v -z -w 2 10.107.208.119 80\nConnection to 10.107.208.119 80 port [tcp/http] succeeded!\n"
Jun  8 15:21:34.502: INFO: stdout: ""
Jun  8 15:21:34.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-462 exec execpod-affinity44ctg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.208.119:80/ ; done'
Jun  8 15:21:34.760: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n"
Jun  8 15:21:34.760: INFO: stdout: "\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj"
Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
Jun  8 15:21:34.760: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-462, will wait for the garbage collector to delete the pods 06/08/23 15:21:34.774
Jun  8 15:21:34.836: INFO: Deleting ReplicationController affinity-clusterip took: 7.780837ms
Jun  8 15:21:34.938: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.034015ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  8 15:21:36.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-462" for this suite. 06/08/23 15:21:36.981
------------------------------
• [SLOW TEST] [8.990 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:21:28
    Jun  8 15:21:28.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename services 06/08/23 15:21:28.002
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:21:28.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:21:28.035
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-462 06/08/23 15:21:28.038
    STEP: creating service affinity-clusterip in namespace services-462 06/08/23 15:21:28.038
    STEP: creating replication controller affinity-clusterip in namespace services-462 06/08/23 15:21:28.074
    I0608 15:21:28.084346      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-462, replica count: 3
    I0608 15:21:31.135618      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun  8 15:21:31.143: INFO: Creating new exec pod
    Jun  8 15:21:31.151: INFO: Waiting up to 5m0s for pod "execpod-affinity44ctg" in namespace "services-462" to be "running"
    Jun  8 15:21:31.155: INFO: Pod "execpod-affinity44ctg": Phase="Pending", Reason="", readiness=false. Elapsed: 3.586144ms
    Jun  8 15:21:33.160: INFO: Pod "execpod-affinity44ctg": Phase="Running", Reason="", readiness=true. Elapsed: 2.008743653s
    Jun  8 15:21:33.160: INFO: Pod "execpod-affinity44ctg" satisfied condition "running"
    Jun  8 15:21:34.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-462 exec execpod-affinity44ctg -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Jun  8 15:21:34.335: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jun  8 15:21:34.335: INFO: stdout: ""
    Jun  8 15:21:34.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-462 exec execpod-affinity44ctg -- /bin/sh -x -c nc -v -z -w 2 10.107.208.119 80'
    Jun  8 15:21:34.502: INFO: stderr: "+ nc -v -z -w 2 10.107.208.119 80\nConnection to 10.107.208.119 80 port [tcp/http] succeeded!\n"
    Jun  8 15:21:34.502: INFO: stdout: ""
    Jun  8 15:21:34.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-462 exec execpod-affinity44ctg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.208.119:80/ ; done'
    Jun  8 15:21:34.760: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.208.119:80/\n"
    Jun  8 15:21:34.760: INFO: stdout: "\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj\naffinity-clusterip-5vhmj"
    Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
    Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
    Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
    Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
    Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
    Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
    Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
    Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
    Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
    Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
    Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
    Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
    Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
    Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
    Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
    Jun  8 15:21:34.760: INFO: Received response from host: affinity-clusterip-5vhmj
    Jun  8 15:21:34.760: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-462, will wait for the garbage collector to delete the pods 06/08/23 15:21:34.774
    Jun  8 15:21:34.836: INFO: Deleting ReplicationController affinity-clusterip took: 7.780837ms
    Jun  8 15:21:34.938: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.034015ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:21:36.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-462" for this suite. 06/08/23 15:21:36.981
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:21:36.991
Jun  8 15:21:36.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename services 06/08/23 15:21:36.993
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:21:37.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:21:37.021
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-1101 06/08/23 15:21:37.025
STEP: creating service affinity-nodeport in namespace services-1101 06/08/23 15:21:37.025
STEP: creating replication controller affinity-nodeport in namespace services-1101 06/08/23 15:21:37.051
I0608 15:21:37.059557      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1101, replica count: 3
I0608 15:21:40.111604      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  8 15:21:40.126: INFO: Creating new exec pod
Jun  8 15:21:40.136: INFO: Waiting up to 5m0s for pod "execpod-affinity6xxxh" in namespace "services-1101" to be "running"
Jun  8 15:21:40.139: INFO: Pod "execpod-affinity6xxxh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.396384ms
Jun  8 15:21:42.145: INFO: Pod "execpod-affinity6xxxh": Phase="Running", Reason="", readiness=true. Elapsed: 2.009078779s
Jun  8 15:21:42.145: INFO: Pod "execpod-affinity6xxxh" satisfied condition "running"
Jun  8 15:21:43.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1101 exec execpod-affinity6xxxh -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Jun  8 15:21:43.362: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jun  8 15:21:43.362: INFO: stdout: ""
Jun  8 15:21:43.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1101 exec execpod-affinity6xxxh -- /bin/sh -x -c nc -v -z -w 2 10.100.79.93 80'
Jun  8 15:21:43.568: INFO: stderr: "+ nc -v -z -w 2 10.100.79.93 80\nConnection to 10.100.79.93 80 port [tcp/http] succeeded!\n"
Jun  8 15:21:43.568: INFO: stdout: ""
Jun  8 15:21:43.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1101 exec execpod-affinity6xxxh -- /bin/sh -x -c nc -v -z -w 2 100.100.237.165 30772'
Jun  8 15:21:43.742: INFO: stderr: "+ nc -v -z -w 2 100.100.237.165 30772\nConnection to 100.100.237.165 30772 port [tcp/*] succeeded!\n"
Jun  8 15:21:43.743: INFO: stdout: ""
Jun  8 15:21:43.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1101 exec execpod-affinity6xxxh -- /bin/sh -x -c nc -v -z -w 2 100.100.236.41 30772'
Jun  8 15:21:43.921: INFO: stderr: "+ nc -v -z -w 2 100.100.236.41 30772\nConnection to 100.100.236.41 30772 port [tcp/*] succeeded!\n"
Jun  8 15:21:43.921: INFO: stdout: ""
Jun  8 15:21:43.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1101 exec execpod-affinity6xxxh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.237.165:30772/ ; done'
Jun  8 15:21:44.210: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n"
Jun  8 15:21:44.211: INFO: stdout: "\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v"
Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
Jun  8 15:21:44.211: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-1101, will wait for the garbage collector to delete the pods 06/08/23 15:21:44.224
Jun  8 15:21:44.288: INFO: Deleting ReplicationController affinity-nodeport took: 8.350099ms
Jun  8 15:21:44.388: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.517012ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  8 15:21:46.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1101" for this suite. 06/08/23 15:21:46.34
------------------------------
• [SLOW TEST] [9.361 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:21:36.991
    Jun  8 15:21:36.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename services 06/08/23 15:21:36.993
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:21:37.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:21:37.021
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-1101 06/08/23 15:21:37.025
    STEP: creating service affinity-nodeport in namespace services-1101 06/08/23 15:21:37.025
    STEP: creating replication controller affinity-nodeport in namespace services-1101 06/08/23 15:21:37.051
    I0608 15:21:37.059557      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1101, replica count: 3
    I0608 15:21:40.111604      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun  8 15:21:40.126: INFO: Creating new exec pod
    Jun  8 15:21:40.136: INFO: Waiting up to 5m0s for pod "execpod-affinity6xxxh" in namespace "services-1101" to be "running"
    Jun  8 15:21:40.139: INFO: Pod "execpod-affinity6xxxh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.396384ms
    Jun  8 15:21:42.145: INFO: Pod "execpod-affinity6xxxh": Phase="Running", Reason="", readiness=true. Elapsed: 2.009078779s
    Jun  8 15:21:42.145: INFO: Pod "execpod-affinity6xxxh" satisfied condition "running"
    Jun  8 15:21:43.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1101 exec execpod-affinity6xxxh -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Jun  8 15:21:43.362: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jun  8 15:21:43.362: INFO: stdout: ""
    Jun  8 15:21:43.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1101 exec execpod-affinity6xxxh -- /bin/sh -x -c nc -v -z -w 2 10.100.79.93 80'
    Jun  8 15:21:43.568: INFO: stderr: "+ nc -v -z -w 2 10.100.79.93 80\nConnection to 10.100.79.93 80 port [tcp/http] succeeded!\n"
    Jun  8 15:21:43.568: INFO: stdout: ""
    Jun  8 15:21:43.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1101 exec execpod-affinity6xxxh -- /bin/sh -x -c nc -v -z -w 2 100.100.237.165 30772'
    Jun  8 15:21:43.742: INFO: stderr: "+ nc -v -z -w 2 100.100.237.165 30772\nConnection to 100.100.237.165 30772 port [tcp/*] succeeded!\n"
    Jun  8 15:21:43.743: INFO: stdout: ""
    Jun  8 15:21:43.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1101 exec execpod-affinity6xxxh -- /bin/sh -x -c nc -v -z -w 2 100.100.236.41 30772'
    Jun  8 15:21:43.921: INFO: stderr: "+ nc -v -z -w 2 100.100.236.41 30772\nConnection to 100.100.236.41 30772 port [tcp/*] succeeded!\n"
    Jun  8 15:21:43.921: INFO: stdout: ""
    Jun  8 15:21:43.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1101 exec execpod-affinity6xxxh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.237.165:30772/ ; done'
    Jun  8 15:21:44.210: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30772/\n"
    Jun  8 15:21:44.211: INFO: stdout: "\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v\naffinity-nodeport-jg58v"
    Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
    Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
    Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
    Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
    Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
    Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
    Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
    Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
    Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
    Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
    Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
    Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
    Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
    Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
    Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
    Jun  8 15:21:44.211: INFO: Received response from host: affinity-nodeport-jg58v
    Jun  8 15:21:44.211: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-1101, will wait for the garbage collector to delete the pods 06/08/23 15:21:44.224
    Jun  8 15:21:44.288: INFO: Deleting ReplicationController affinity-nodeport took: 8.350099ms
    Jun  8 15:21:44.388: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.517012ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:21:46.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1101" for this suite. 06/08/23 15:21:46.34
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:21:46.353
Jun  8 15:21:46.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename sysctl 06/08/23 15:21:46.354
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:21:46.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:21:46.382
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 06/08/23 15:21:46.386
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:21:46.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-9604" for this suite. 06/08/23 15:21:46.402
------------------------------
• [0.062 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:21:46.353
    Jun  8 15:21:46.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename sysctl 06/08/23 15:21:46.354
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:21:46.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:21:46.382
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 06/08/23 15:21:46.386
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:21:46.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-9604" for this suite. 06/08/23 15:21:46.402
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:21:46.416
Jun  8 15:21:46.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename job 06/08/23 15:21:46.417
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:21:46.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:21:46.447
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 06/08/23 15:21:46.451
STEP: Ensuring job reaches completions 06/08/23 15:21:46.459
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jun  8 15:21:58.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8117" for this suite. 06/08/23 15:21:58.471
------------------------------
• [SLOW TEST] [12.063 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:21:46.416
    Jun  8 15:21:46.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename job 06/08/23 15:21:46.417
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:21:46.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:21:46.447
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 06/08/23 15:21:46.451
    STEP: Ensuring job reaches completions 06/08/23 15:21:46.459
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:21:58.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8117" for this suite. 06/08/23 15:21:58.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:21:58.479
Jun  8 15:21:58.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename resourcequota 06/08/23 15:21:58.481
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:21:58.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:21:58.501
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 06/08/23 15:22:15.509
STEP: Creating a ResourceQuota 06/08/23 15:22:20.514
STEP: Ensuring resource quota status is calculated 06/08/23 15:22:20.52
STEP: Creating a ConfigMap 06/08/23 15:22:22.526
STEP: Ensuring resource quota status captures configMap creation 06/08/23 15:22:22.541
STEP: Deleting a ConfigMap 06/08/23 15:22:24.546
STEP: Ensuring resource quota status released usage 06/08/23 15:22:24.553
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  8 15:22:26.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2157" for this suite. 06/08/23 15:22:26.565
------------------------------
• [SLOW TEST] [28.094 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:21:58.479
    Jun  8 15:21:58.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename resourcequota 06/08/23 15:21:58.481
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:21:58.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:21:58.501
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 06/08/23 15:22:15.509
    STEP: Creating a ResourceQuota 06/08/23 15:22:20.514
    STEP: Ensuring resource quota status is calculated 06/08/23 15:22:20.52
    STEP: Creating a ConfigMap 06/08/23 15:22:22.526
    STEP: Ensuring resource quota status captures configMap creation 06/08/23 15:22:22.541
    STEP: Deleting a ConfigMap 06/08/23 15:22:24.546
    STEP: Ensuring resource quota status released usage 06/08/23 15:22:24.553
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:22:26.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2157" for this suite. 06/08/23 15:22:26.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:22:26.574
Jun  8 15:22:26.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename var-expansion 06/08/23 15:22:26.575
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:22:26.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:22:26.601
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 06/08/23 15:22:26.604
Jun  8 15:22:26.613: INFO: Waiting up to 5m0s for pod "var-expansion-0714219e-7c5b-4bc2-ab3a-45b27edd607d" in namespace "var-expansion-7363" to be "Succeeded or Failed"
Jun  8 15:22:26.617: INFO: Pod "var-expansion-0714219e-7c5b-4bc2-ab3a-45b27edd607d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.603593ms
Jun  8 15:22:28.622: INFO: Pod "var-expansion-0714219e-7c5b-4bc2-ab3a-45b27edd607d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00884094s
Jun  8 15:22:30.622: INFO: Pod "var-expansion-0714219e-7c5b-4bc2-ab3a-45b27edd607d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008481348s
STEP: Saw pod success 06/08/23 15:22:30.622
Jun  8 15:22:30.622: INFO: Pod "var-expansion-0714219e-7c5b-4bc2-ab3a-45b27edd607d" satisfied condition "Succeeded or Failed"
Jun  8 15:22:30.626: INFO: Trying to get logs from node chl8tf-worker-001 pod var-expansion-0714219e-7c5b-4bc2-ab3a-45b27edd607d container dapi-container: <nil>
STEP: delete the pod 06/08/23 15:22:30.643
Jun  8 15:22:30.659: INFO: Waiting for pod var-expansion-0714219e-7c5b-4bc2-ab3a-45b27edd607d to disappear
Jun  8 15:22:30.662: INFO: Pod var-expansion-0714219e-7c5b-4bc2-ab3a-45b27edd607d no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun  8 15:22:30.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7363" for this suite. 06/08/23 15:22:30.668
------------------------------
• [4.101 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:22:26.574
    Jun  8 15:22:26.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename var-expansion 06/08/23 15:22:26.575
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:22:26.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:22:26.601
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 06/08/23 15:22:26.604
    Jun  8 15:22:26.613: INFO: Waiting up to 5m0s for pod "var-expansion-0714219e-7c5b-4bc2-ab3a-45b27edd607d" in namespace "var-expansion-7363" to be "Succeeded or Failed"
    Jun  8 15:22:26.617: INFO: Pod "var-expansion-0714219e-7c5b-4bc2-ab3a-45b27edd607d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.603593ms
    Jun  8 15:22:28.622: INFO: Pod "var-expansion-0714219e-7c5b-4bc2-ab3a-45b27edd607d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00884094s
    Jun  8 15:22:30.622: INFO: Pod "var-expansion-0714219e-7c5b-4bc2-ab3a-45b27edd607d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008481348s
    STEP: Saw pod success 06/08/23 15:22:30.622
    Jun  8 15:22:30.622: INFO: Pod "var-expansion-0714219e-7c5b-4bc2-ab3a-45b27edd607d" satisfied condition "Succeeded or Failed"
    Jun  8 15:22:30.626: INFO: Trying to get logs from node chl8tf-worker-001 pod var-expansion-0714219e-7c5b-4bc2-ab3a-45b27edd607d container dapi-container: <nil>
    STEP: delete the pod 06/08/23 15:22:30.643
    Jun  8 15:22:30.659: INFO: Waiting for pod var-expansion-0714219e-7c5b-4bc2-ab3a-45b27edd607d to disappear
    Jun  8 15:22:30.662: INFO: Pod var-expansion-0714219e-7c5b-4bc2-ab3a-45b27edd607d no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:22:30.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7363" for this suite. 06/08/23 15:22:30.668
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:22:30.675
Jun  8 15:22:30.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename crd-publish-openapi 06/08/23 15:22:30.677
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:22:30.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:22:30.697
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 06/08/23 15:22:30.701
Jun  8 15:22:30.701: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: mark a version not serverd 06/08/23 15:22:35.076
STEP: check the unserved version gets removed 06/08/23 15:22:35.099
STEP: check the other version is not changed 06/08/23 15:22:37.196
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:22:40.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1146" for this suite. 06/08/23 15:22:40.854
------------------------------
• [SLOW TEST] [10.186 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:22:30.675
    Jun  8 15:22:30.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename crd-publish-openapi 06/08/23 15:22:30.677
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:22:30.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:22:30.697
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 06/08/23 15:22:30.701
    Jun  8 15:22:30.701: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: mark a version not serverd 06/08/23 15:22:35.076
    STEP: check the unserved version gets removed 06/08/23 15:22:35.099
    STEP: check the other version is not changed 06/08/23 15:22:37.196
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:22:40.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1146" for this suite. 06/08/23 15:22:40.854
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:22:40.863
Jun  8 15:22:40.863: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename cronjob 06/08/23 15:22:40.864
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:22:40.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:22:40.884
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 06/08/23 15:22:40.887
STEP: Ensuring a job is scheduled 06/08/23 15:22:40.896
STEP: Ensuring exactly one is scheduled 06/08/23 15:23:00.901
STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/08/23 15:23:00.905
STEP: Ensuring no more jobs are scheduled 06/08/23 15:23:00.909
STEP: Removing cronjob 06/08/23 15:28:00.918
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jun  8 15:28:00.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4856" for this suite. 06/08/23 15:28:00.931
------------------------------
• [SLOW TEST] [320.075 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:22:40.863
    Jun  8 15:22:40.863: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename cronjob 06/08/23 15:22:40.864
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:22:40.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:22:40.884
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 06/08/23 15:22:40.887
    STEP: Ensuring a job is scheduled 06/08/23 15:22:40.896
    STEP: Ensuring exactly one is scheduled 06/08/23 15:23:00.901
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/08/23 15:23:00.905
    STEP: Ensuring no more jobs are scheduled 06/08/23 15:23:00.909
    STEP: Removing cronjob 06/08/23 15:28:00.918
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:28:00.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4856" for this suite. 06/08/23 15:28:00.931
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:28:00.938
Jun  8 15:28:00.938: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename container-lifecycle-hook 06/08/23 15:28:00.94
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:28:00.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:28:00.967
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 06/08/23 15:28:00.976
Jun  8 15:28:00.986: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3108" to be "running and ready"
Jun  8 15:28:00.990: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.728013ms
Jun  8 15:28:00.990: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:28:02.995: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009112482s
Jun  8 15:28:02.995: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun  8 15:28:02.995: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 06/08/23 15:28:02.999
Jun  8 15:28:03.005: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-3108" to be "running and ready"
Jun  8 15:28:03.009: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.976626ms
Jun  8 15:28:03.009: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:28:05.014: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00877547s
Jun  8 15:28:05.014: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jun  8 15:28:05.014: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 06/08/23 15:28:05.018
Jun  8 15:28:05.025: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  8 15:28:05.029: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  8 15:28:07.030: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  8 15:28:07.034: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 06/08/23 15:28:07.034
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jun  8 15:28:07.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-3108" for this suite. 06/08/23 15:28:07.057
------------------------------
• [SLOW TEST] [6.126 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:28:00.938
    Jun  8 15:28:00.938: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/08/23 15:28:00.94
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:28:00.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:28:00.967
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 06/08/23 15:28:00.976
    Jun  8 15:28:00.986: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3108" to be "running and ready"
    Jun  8 15:28:00.990: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.728013ms
    Jun  8 15:28:00.990: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:28:02.995: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009112482s
    Jun  8 15:28:02.995: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun  8 15:28:02.995: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 06/08/23 15:28:02.999
    Jun  8 15:28:03.005: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-3108" to be "running and ready"
    Jun  8 15:28:03.009: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.976626ms
    Jun  8 15:28:03.009: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:28:05.014: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00877547s
    Jun  8 15:28:05.014: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jun  8 15:28:05.014: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 06/08/23 15:28:05.018
    Jun  8 15:28:05.025: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jun  8 15:28:05.029: INFO: Pod pod-with-prestop-exec-hook still exists
    Jun  8 15:28:07.030: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jun  8 15:28:07.034: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 06/08/23 15:28:07.034
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:28:07.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-3108" for this suite. 06/08/23 15:28:07.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:28:07.067
Jun  8 15:28:07.067: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename pods 06/08/23 15:28:07.068
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:28:07.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:28:07.088
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 06/08/23 15:28:07.099
STEP: watching for Pod to be ready 06/08/23 15:28:07.108
Jun  8 15:28:07.110: INFO: observed Pod pod-test in namespace pods-1910 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jun  8 15:28:07.113: INFO: observed Pod pod-test in namespace pods-1910 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:28:07 +0000 UTC  }]
Jun  8 15:28:07.127: INFO: observed Pod pod-test in namespace pods-1910 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:28:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:28:07 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:28:07 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:28:07 +0000 UTC  }]
Jun  8 15:28:07.798: INFO: Found Pod pod-test in namespace pods-1910 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:28:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:28:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:28:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:28:07 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 06/08/23 15:28:07.802
STEP: getting the Pod and ensuring that it's patched 06/08/23 15:28:07.812
STEP: replacing the Pod's status Ready condition to False 06/08/23 15:28:07.815
STEP: check the Pod again to ensure its Ready conditions are False 06/08/23 15:28:07.829
STEP: deleting the Pod via a Collection with a LabelSelector 06/08/23 15:28:07.829
STEP: watching for the Pod to be deleted 06/08/23 15:28:07.839
Jun  8 15:28:07.840: INFO: observed event type MODIFIED
Jun  8 15:28:09.803: INFO: observed event type MODIFIED
Jun  8 15:28:10.807: INFO: observed event type MODIFIED
Jun  8 15:28:10.815: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun  8 15:28:10.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1910" for this suite. 06/08/23 15:28:10.827
------------------------------
• [3.768 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:28:07.067
    Jun  8 15:28:07.067: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename pods 06/08/23 15:28:07.068
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:28:07.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:28:07.088
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 06/08/23 15:28:07.099
    STEP: watching for Pod to be ready 06/08/23 15:28:07.108
    Jun  8 15:28:07.110: INFO: observed Pod pod-test in namespace pods-1910 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jun  8 15:28:07.113: INFO: observed Pod pod-test in namespace pods-1910 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:28:07 +0000 UTC  }]
    Jun  8 15:28:07.127: INFO: observed Pod pod-test in namespace pods-1910 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:28:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:28:07 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:28:07 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:28:07 +0000 UTC  }]
    Jun  8 15:28:07.798: INFO: Found Pod pod-test in namespace pods-1910 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:28:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:28:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:28:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:28:07 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 06/08/23 15:28:07.802
    STEP: getting the Pod and ensuring that it's patched 06/08/23 15:28:07.812
    STEP: replacing the Pod's status Ready condition to False 06/08/23 15:28:07.815
    STEP: check the Pod again to ensure its Ready conditions are False 06/08/23 15:28:07.829
    STEP: deleting the Pod via a Collection with a LabelSelector 06/08/23 15:28:07.829
    STEP: watching for the Pod to be deleted 06/08/23 15:28:07.839
    Jun  8 15:28:07.840: INFO: observed event type MODIFIED
    Jun  8 15:28:09.803: INFO: observed event type MODIFIED
    Jun  8 15:28:10.807: INFO: observed event type MODIFIED
    Jun  8 15:28:10.815: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:28:10.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1910" for this suite. 06/08/23 15:28:10.827
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:28:10.837
Jun  8 15:28:10.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename services 06/08/23 15:28:10.838
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:28:10.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:28:10.858
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3971 06/08/23 15:28:10.862
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/08/23 15:28:10.885
STEP: creating service externalsvc in namespace services-3971 06/08/23 15:28:10.885
STEP: creating replication controller externalsvc in namespace services-3971 06/08/23 15:28:10.909
I0608 15:28:10.921652      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3971, replica count: 2
I0608 15:28:13.973362      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 06/08/23 15:28:13.978
Jun  8 15:28:14.002: INFO: Creating new exec pod
Jun  8 15:28:14.015: INFO: Waiting up to 5m0s for pod "execpod8bqmx" in namespace "services-3971" to be "running"
Jun  8 15:28:14.020: INFO: Pod "execpod8bqmx": Phase="Pending", Reason="", readiness=false. Elapsed: 5.665169ms
Jun  8 15:28:16.027: INFO: Pod "execpod8bqmx": Phase="Running", Reason="", readiness=true. Elapsed: 2.011920957s
Jun  8 15:28:16.027: INFO: Pod "execpod8bqmx" satisfied condition "running"
Jun  8 15:28:16.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-3971 exec execpod8bqmx -- /bin/sh -x -c nslookup nodeport-service.services-3971.svc.cluster.local'
Jun  8 15:28:16.232: INFO: stderr: "+ nslookup nodeport-service.services-3971.svc.cluster.local\n"
Jun  8 15:28:16.232: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-3971.svc.cluster.local\tcanonical name = externalsvc.services-3971.svc.cluster.local.\nName:\texternalsvc.services-3971.svc.cluster.local\nAddress: 10.107.219.113\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3971, will wait for the garbage collector to delete the pods 06/08/23 15:28:16.232
Jun  8 15:28:16.295: INFO: Deleting ReplicationController externalsvc took: 7.741983ms
Jun  8 15:28:16.396: INFO: Terminating ReplicationController externalsvc pods took: 100.483905ms
Jun  8 15:28:18.129: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  8 15:28:18.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3971" for this suite. 06/08/23 15:28:18.154
------------------------------
• [SLOW TEST] [7.328 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:28:10.837
    Jun  8 15:28:10.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename services 06/08/23 15:28:10.838
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:28:10.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:28:10.858
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-3971 06/08/23 15:28:10.862
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/08/23 15:28:10.885
    STEP: creating service externalsvc in namespace services-3971 06/08/23 15:28:10.885
    STEP: creating replication controller externalsvc in namespace services-3971 06/08/23 15:28:10.909
    I0608 15:28:10.921652      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3971, replica count: 2
    I0608 15:28:13.973362      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 06/08/23 15:28:13.978
    Jun  8 15:28:14.002: INFO: Creating new exec pod
    Jun  8 15:28:14.015: INFO: Waiting up to 5m0s for pod "execpod8bqmx" in namespace "services-3971" to be "running"
    Jun  8 15:28:14.020: INFO: Pod "execpod8bqmx": Phase="Pending", Reason="", readiness=false. Elapsed: 5.665169ms
    Jun  8 15:28:16.027: INFO: Pod "execpod8bqmx": Phase="Running", Reason="", readiness=true. Elapsed: 2.011920957s
    Jun  8 15:28:16.027: INFO: Pod "execpod8bqmx" satisfied condition "running"
    Jun  8 15:28:16.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-3971 exec execpod8bqmx -- /bin/sh -x -c nslookup nodeport-service.services-3971.svc.cluster.local'
    Jun  8 15:28:16.232: INFO: stderr: "+ nslookup nodeport-service.services-3971.svc.cluster.local\n"
    Jun  8 15:28:16.232: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-3971.svc.cluster.local\tcanonical name = externalsvc.services-3971.svc.cluster.local.\nName:\texternalsvc.services-3971.svc.cluster.local\nAddress: 10.107.219.113\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3971, will wait for the garbage collector to delete the pods 06/08/23 15:28:16.232
    Jun  8 15:28:16.295: INFO: Deleting ReplicationController externalsvc took: 7.741983ms
    Jun  8 15:28:16.396: INFO: Terminating ReplicationController externalsvc pods took: 100.483905ms
    Jun  8 15:28:18.129: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:28:18.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3971" for this suite. 06/08/23 15:28:18.154
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:28:18.166
Jun  8 15:28:18.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename containers 06/08/23 15:28:18.167
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:28:18.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:28:18.195
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 06/08/23 15:28:18.199
Jun  8 15:28:18.212: INFO: Waiting up to 5m0s for pod "client-containers-718b738c-6c2a-4e22-a6f3-3cd73a0b4ade" in namespace "containers-5013" to be "Succeeded or Failed"
Jun  8 15:28:18.217: INFO: Pod "client-containers-718b738c-6c2a-4e22-a6f3-3cd73a0b4ade": Phase="Pending", Reason="", readiness=false. Elapsed: 4.997707ms
Jun  8 15:28:20.222: INFO: Pod "client-containers-718b738c-6c2a-4e22-a6f3-3cd73a0b4ade": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010393386s
Jun  8 15:28:22.222: INFO: Pod "client-containers-718b738c-6c2a-4e22-a6f3-3cd73a0b4ade": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010018139s
STEP: Saw pod success 06/08/23 15:28:22.222
Jun  8 15:28:22.222: INFO: Pod "client-containers-718b738c-6c2a-4e22-a6f3-3cd73a0b4ade" satisfied condition "Succeeded or Failed"
Jun  8 15:28:22.227: INFO: Trying to get logs from node chl8tf-worker-001 pod client-containers-718b738c-6c2a-4e22-a6f3-3cd73a0b4ade container agnhost-container: <nil>
STEP: delete the pod 06/08/23 15:28:22.244
Jun  8 15:28:22.258: INFO: Waiting for pod client-containers-718b738c-6c2a-4e22-a6f3-3cd73a0b4ade to disappear
Jun  8 15:28:22.262: INFO: Pod client-containers-718b738c-6c2a-4e22-a6f3-3cd73a0b4ade no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jun  8 15:28:22.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-5013" for this suite. 06/08/23 15:28:22.268
------------------------------
• [4.110 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:28:18.166
    Jun  8 15:28:18.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename containers 06/08/23 15:28:18.167
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:28:18.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:28:18.195
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 06/08/23 15:28:18.199
    Jun  8 15:28:18.212: INFO: Waiting up to 5m0s for pod "client-containers-718b738c-6c2a-4e22-a6f3-3cd73a0b4ade" in namespace "containers-5013" to be "Succeeded or Failed"
    Jun  8 15:28:18.217: INFO: Pod "client-containers-718b738c-6c2a-4e22-a6f3-3cd73a0b4ade": Phase="Pending", Reason="", readiness=false. Elapsed: 4.997707ms
    Jun  8 15:28:20.222: INFO: Pod "client-containers-718b738c-6c2a-4e22-a6f3-3cd73a0b4ade": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010393386s
    Jun  8 15:28:22.222: INFO: Pod "client-containers-718b738c-6c2a-4e22-a6f3-3cd73a0b4ade": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010018139s
    STEP: Saw pod success 06/08/23 15:28:22.222
    Jun  8 15:28:22.222: INFO: Pod "client-containers-718b738c-6c2a-4e22-a6f3-3cd73a0b4ade" satisfied condition "Succeeded or Failed"
    Jun  8 15:28:22.227: INFO: Trying to get logs from node chl8tf-worker-001 pod client-containers-718b738c-6c2a-4e22-a6f3-3cd73a0b4ade container agnhost-container: <nil>
    STEP: delete the pod 06/08/23 15:28:22.244
    Jun  8 15:28:22.258: INFO: Waiting for pod client-containers-718b738c-6c2a-4e22-a6f3-3cd73a0b4ade to disappear
    Jun  8 15:28:22.262: INFO: Pod client-containers-718b738c-6c2a-4e22-a6f3-3cd73a0b4ade no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:28:22.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-5013" for this suite. 06/08/23 15:28:22.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:28:22.277
Jun  8 15:28:22.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename daemonsets 06/08/23 15:28:22.278
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:28:22.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:28:22.298
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Jun  8 15:28:22.337: INFO: Create a RollingUpdate DaemonSet
Jun  8 15:28:22.343: INFO: Check that daemon pods launch on every node of the cluster
Jun  8 15:28:22.352: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  8 15:28:22.352: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
Jun  8 15:28:23.365: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  8 15:28:23.365: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
Jun  8 15:28:24.365: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jun  8 15:28:24.365: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
Jun  8 15:28:24.365: INFO: Update the DaemonSet to trigger a rollout
Jun  8 15:28:24.376: INFO: Updating DaemonSet daemon-set
Jun  8 15:28:27.400: INFO: Roll back the DaemonSet before rollout is complete
Jun  8 15:28:27.412: INFO: Updating DaemonSet daemon-set
Jun  8 15:28:27.412: INFO: Make sure DaemonSet rollback is complete
Jun  8 15:28:27.417: INFO: Wrong image for pod: daemon-set-s2x8x. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Jun  8 15:28:27.417: INFO: Pod daemon-set-s2x8x is not available
Jun  8 15:28:29.429: INFO: Pod daemon-set-sz7wp is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 06/08/23 15:28:29.444
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8002, will wait for the garbage collector to delete the pods 06/08/23 15:28:29.444
Jun  8 15:28:29.521: INFO: Deleting DaemonSet.extensions daemon-set took: 22.559093ms
Jun  8 15:28:29.622: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.454344ms
Jun  8 15:28:31.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  8 15:28:31.127: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun  8 15:28:31.131: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28551"},"items":null}

Jun  8 15:28:31.134: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28551"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:28:31.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8002" for this suite. 06/08/23 15:28:31.165
------------------------------
• [SLOW TEST] [8.895 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:28:22.277
    Jun  8 15:28:22.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename daemonsets 06/08/23 15:28:22.278
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:28:22.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:28:22.298
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Jun  8 15:28:22.337: INFO: Create a RollingUpdate DaemonSet
    Jun  8 15:28:22.343: INFO: Check that daemon pods launch on every node of the cluster
    Jun  8 15:28:22.352: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  8 15:28:22.352: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
    Jun  8 15:28:23.365: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun  8 15:28:23.365: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
    Jun  8 15:28:24.365: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jun  8 15:28:24.365: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    Jun  8 15:28:24.365: INFO: Update the DaemonSet to trigger a rollout
    Jun  8 15:28:24.376: INFO: Updating DaemonSet daemon-set
    Jun  8 15:28:27.400: INFO: Roll back the DaemonSet before rollout is complete
    Jun  8 15:28:27.412: INFO: Updating DaemonSet daemon-set
    Jun  8 15:28:27.412: INFO: Make sure DaemonSet rollback is complete
    Jun  8 15:28:27.417: INFO: Wrong image for pod: daemon-set-s2x8x. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Jun  8 15:28:27.417: INFO: Pod daemon-set-s2x8x is not available
    Jun  8 15:28:29.429: INFO: Pod daemon-set-sz7wp is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 06/08/23 15:28:29.444
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8002, will wait for the garbage collector to delete the pods 06/08/23 15:28:29.444
    Jun  8 15:28:29.521: INFO: Deleting DaemonSet.extensions daemon-set took: 22.559093ms
    Jun  8 15:28:29.622: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.454344ms
    Jun  8 15:28:31.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  8 15:28:31.127: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun  8 15:28:31.131: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28551"},"items":null}

    Jun  8 15:28:31.134: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28551"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:28:31.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8002" for this suite. 06/08/23 15:28:31.165
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:28:31.172
Jun  8 15:28:31.172: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubelet-test 06/08/23 15:28:31.174
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:28:31.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:28:31.194
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 06/08/23 15:28:31.206
Jun  8 15:28:31.206: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases8aea1721-8ae4-4c7a-92f2-a81f02d48df6" in namespace "kubelet-test-4041" to be "completed"
Jun  8 15:28:31.211: INFO: Pod "agnhost-host-aliases8aea1721-8ae4-4c7a-92f2-a81f02d48df6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.324469ms
Jun  8 15:28:33.216: INFO: Pod "agnhost-host-aliases8aea1721-8ae4-4c7a-92f2-a81f02d48df6": Phase="Running", Reason="", readiness=false. Elapsed: 2.009976477s
Jun  8 15:28:35.216: INFO: Pod "agnhost-host-aliases8aea1721-8ae4-4c7a-92f2-a81f02d48df6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00996838s
Jun  8 15:28:35.216: INFO: Pod "agnhost-host-aliases8aea1721-8ae4-4c7a-92f2-a81f02d48df6" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jun  8 15:28:35.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4041" for this suite. 06/08/23 15:28:35.231
------------------------------
• [4.065 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:28:31.172
    Jun  8 15:28:31.172: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubelet-test 06/08/23 15:28:31.174
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:28:31.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:28:31.194
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 06/08/23 15:28:31.206
    Jun  8 15:28:31.206: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases8aea1721-8ae4-4c7a-92f2-a81f02d48df6" in namespace "kubelet-test-4041" to be "completed"
    Jun  8 15:28:31.211: INFO: Pod "agnhost-host-aliases8aea1721-8ae4-4c7a-92f2-a81f02d48df6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.324469ms
    Jun  8 15:28:33.216: INFO: Pod "agnhost-host-aliases8aea1721-8ae4-4c7a-92f2-a81f02d48df6": Phase="Running", Reason="", readiness=false. Elapsed: 2.009976477s
    Jun  8 15:28:35.216: INFO: Pod "agnhost-host-aliases8aea1721-8ae4-4c7a-92f2-a81f02d48df6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00996838s
    Jun  8 15:28:35.216: INFO: Pod "agnhost-host-aliases8aea1721-8ae4-4c7a-92f2-a81f02d48df6" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:28:35.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4041" for this suite. 06/08/23 15:28:35.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:28:35.242
Jun  8 15:28:35.242: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename emptydir 06/08/23 15:28:35.243
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:28:35.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:28:35.264
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 06/08/23 15:28:35.267
Jun  8 15:28:35.277: INFO: Waiting up to 5m0s for pod "pod-ace7e936-0251-4b9d-af11-ddd06df80472" in namespace "emptydir-7814" to be "Succeeded or Failed"
Jun  8 15:28:35.281: INFO: Pod "pod-ace7e936-0251-4b9d-af11-ddd06df80472": Phase="Pending", Reason="", readiness=false. Elapsed: 3.678954ms
Jun  8 15:28:37.286: INFO: Pod "pod-ace7e936-0251-4b9d-af11-ddd06df80472": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009113814s
Jun  8 15:28:39.286: INFO: Pod "pod-ace7e936-0251-4b9d-af11-ddd06df80472": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008998441s
STEP: Saw pod success 06/08/23 15:28:39.286
Jun  8 15:28:39.287: INFO: Pod "pod-ace7e936-0251-4b9d-af11-ddd06df80472" satisfied condition "Succeeded or Failed"
Jun  8 15:28:39.291: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-ace7e936-0251-4b9d-af11-ddd06df80472 container test-container: <nil>
STEP: delete the pod 06/08/23 15:28:39.298
Jun  8 15:28:39.313: INFO: Waiting for pod pod-ace7e936-0251-4b9d-af11-ddd06df80472 to disappear
Jun  8 15:28:39.317: INFO: Pod pod-ace7e936-0251-4b9d-af11-ddd06df80472 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  8 15:28:39.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7814" for this suite. 06/08/23 15:28:39.323
------------------------------
• [4.088 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:28:35.242
    Jun  8 15:28:35.242: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename emptydir 06/08/23 15:28:35.243
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:28:35.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:28:35.264
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 06/08/23 15:28:35.267
    Jun  8 15:28:35.277: INFO: Waiting up to 5m0s for pod "pod-ace7e936-0251-4b9d-af11-ddd06df80472" in namespace "emptydir-7814" to be "Succeeded or Failed"
    Jun  8 15:28:35.281: INFO: Pod "pod-ace7e936-0251-4b9d-af11-ddd06df80472": Phase="Pending", Reason="", readiness=false. Elapsed: 3.678954ms
    Jun  8 15:28:37.286: INFO: Pod "pod-ace7e936-0251-4b9d-af11-ddd06df80472": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009113814s
    Jun  8 15:28:39.286: INFO: Pod "pod-ace7e936-0251-4b9d-af11-ddd06df80472": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008998441s
    STEP: Saw pod success 06/08/23 15:28:39.286
    Jun  8 15:28:39.287: INFO: Pod "pod-ace7e936-0251-4b9d-af11-ddd06df80472" satisfied condition "Succeeded or Failed"
    Jun  8 15:28:39.291: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-ace7e936-0251-4b9d-af11-ddd06df80472 container test-container: <nil>
    STEP: delete the pod 06/08/23 15:28:39.298
    Jun  8 15:28:39.313: INFO: Waiting for pod pod-ace7e936-0251-4b9d-af11-ddd06df80472 to disappear
    Jun  8 15:28:39.317: INFO: Pod pod-ace7e936-0251-4b9d-af11-ddd06df80472 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:28:39.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7814" for this suite. 06/08/23 15:28:39.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:28:39.331
Jun  8 15:28:39.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename statefulset 06/08/23 15:28:39.332
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:28:39.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:28:39.353
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6641 06/08/23 15:28:39.356
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-6641 06/08/23 15:28:39.366
Jun  8 15:28:39.377: INFO: Found 0 stateful pods, waiting for 1
Jun  8 15:28:49.383: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 06/08/23 15:28:49.39
STEP: Getting /status 06/08/23 15:28:49.404
Jun  8 15:28:49.409: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 06/08/23 15:28:49.409
Jun  8 15:28:49.418: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 06/08/23 15:28:49.418
Jun  8 15:28:49.420: INFO: Observed &StatefulSet event: ADDED
Jun  8 15:28:49.421: INFO: Found Statefulset ss in namespace statefulset-6641 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun  8 15:28:49.421: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 06/08/23 15:28:49.421
Jun  8 15:28:49.421: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun  8 15:28:49.429: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 06/08/23 15:28:49.429
Jun  8 15:28:49.431: INFO: Observed &StatefulSet event: ADDED
Jun  8 15:28:49.431: INFO: Observed Statefulset ss in namespace statefulset-6641 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun  8 15:28:49.431: INFO: Observed &StatefulSet event: MODIFIED
Jun  8 15:28:49.431: INFO: Found Statefulset ss in namespace statefulset-6641 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun  8 15:28:49.431: INFO: Deleting all statefulset in ns statefulset-6641
Jun  8 15:28:49.435: INFO: Scaling statefulset ss to 0
Jun  8 15:28:59.455: INFO: Waiting for statefulset status.replicas updated to 0
Jun  8 15:28:59.458: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun  8 15:28:59.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6641" for this suite. 06/08/23 15:28:59.479
------------------------------
• [SLOW TEST] [20.155 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:28:39.331
    Jun  8 15:28:39.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename statefulset 06/08/23 15:28:39.332
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:28:39.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:28:39.353
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6641 06/08/23 15:28:39.356
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-6641 06/08/23 15:28:39.366
    Jun  8 15:28:39.377: INFO: Found 0 stateful pods, waiting for 1
    Jun  8 15:28:49.383: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 06/08/23 15:28:49.39
    STEP: Getting /status 06/08/23 15:28:49.404
    Jun  8 15:28:49.409: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 06/08/23 15:28:49.409
    Jun  8 15:28:49.418: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 06/08/23 15:28:49.418
    Jun  8 15:28:49.420: INFO: Observed &StatefulSet event: ADDED
    Jun  8 15:28:49.421: INFO: Found Statefulset ss in namespace statefulset-6641 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun  8 15:28:49.421: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 06/08/23 15:28:49.421
    Jun  8 15:28:49.421: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jun  8 15:28:49.429: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 06/08/23 15:28:49.429
    Jun  8 15:28:49.431: INFO: Observed &StatefulSet event: ADDED
    Jun  8 15:28:49.431: INFO: Observed Statefulset ss in namespace statefulset-6641 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun  8 15:28:49.431: INFO: Observed &StatefulSet event: MODIFIED
    Jun  8 15:28:49.431: INFO: Found Statefulset ss in namespace statefulset-6641 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun  8 15:28:49.431: INFO: Deleting all statefulset in ns statefulset-6641
    Jun  8 15:28:49.435: INFO: Scaling statefulset ss to 0
    Jun  8 15:28:59.455: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  8 15:28:59.458: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:28:59.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6641" for this suite. 06/08/23 15:28:59.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:28:59.488
Jun  8 15:28:59.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename replicaset 06/08/23 15:28:59.489
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:28:59.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:28:59.508
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jun  8 15:28:59.511: INFO: Creating ReplicaSet my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866
Jun  8 15:28:59.523: INFO: Pod name my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866: Found 0 pods out of 1
Jun  8 15:29:04.528: INFO: Pod name my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866: Found 1 pods out of 1
Jun  8 15:29:04.528: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866" is running
Jun  8 15:29:04.528: INFO: Waiting up to 5m0s for pod "my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866-dgjj9" in namespace "replicaset-158" to be "running"
Jun  8 15:29:04.532: INFO: Pod "my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866-dgjj9": Phase="Running", Reason="", readiness=true. Elapsed: 4.035451ms
Jun  8 15:29:04.532: INFO: Pod "my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866-dgjj9" satisfied condition "running"
Jun  8 15:29:04.532: INFO: Pod "my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866-dgjj9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-08 15:28:59 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-08 15:29:00 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-08 15:29:00 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-08 15:28:59 +0000 UTC Reason: Message:}])
Jun  8 15:29:04.533: INFO: Trying to dial the pod
Jun  8 15:29:09.554: INFO: Controller my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866: Got expected result from replica 1 [my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866-dgjj9]: "my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866-dgjj9", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jun  8 15:29:09.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-158" for this suite. 06/08/23 15:29:09.56
------------------------------
• [SLOW TEST] [10.080 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:28:59.488
    Jun  8 15:28:59.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename replicaset 06/08/23 15:28:59.489
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:28:59.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:28:59.508
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jun  8 15:28:59.511: INFO: Creating ReplicaSet my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866
    Jun  8 15:28:59.523: INFO: Pod name my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866: Found 0 pods out of 1
    Jun  8 15:29:04.528: INFO: Pod name my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866: Found 1 pods out of 1
    Jun  8 15:29:04.528: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866" is running
    Jun  8 15:29:04.528: INFO: Waiting up to 5m0s for pod "my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866-dgjj9" in namespace "replicaset-158" to be "running"
    Jun  8 15:29:04.532: INFO: Pod "my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866-dgjj9": Phase="Running", Reason="", readiness=true. Elapsed: 4.035451ms
    Jun  8 15:29:04.532: INFO: Pod "my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866-dgjj9" satisfied condition "running"
    Jun  8 15:29:04.532: INFO: Pod "my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866-dgjj9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-08 15:28:59 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-08 15:29:00 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-08 15:29:00 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-08 15:28:59 +0000 UTC Reason: Message:}])
    Jun  8 15:29:04.533: INFO: Trying to dial the pod
    Jun  8 15:29:09.554: INFO: Controller my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866: Got expected result from replica 1 [my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866-dgjj9]: "my-hostname-basic-a020b6bb-3fce-4212-a0e7-f5c9e1381866-dgjj9", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:29:09.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-158" for this suite. 06/08/23 15:29:09.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:29:09.569
Jun  8 15:29:09.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename svcaccounts 06/08/23 15:29:09.57
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:29:09.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:29:09.599
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Jun  8 15:29:09.621: INFO: created pod pod-service-account-defaultsa
Jun  8 15:29:09.621: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun  8 15:29:09.628: INFO: created pod pod-service-account-mountsa
Jun  8 15:29:09.628: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun  8 15:29:09.635: INFO: created pod pod-service-account-nomountsa
Jun  8 15:29:09.635: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun  8 15:29:09.642: INFO: created pod pod-service-account-defaultsa-mountspec
Jun  8 15:29:09.642: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun  8 15:29:09.649: INFO: created pod pod-service-account-mountsa-mountspec
Jun  8 15:29:09.649: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun  8 15:29:09.657: INFO: created pod pod-service-account-nomountsa-mountspec
Jun  8 15:29:09.657: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun  8 15:29:09.665: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun  8 15:29:09.665: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun  8 15:29:09.671: INFO: created pod pod-service-account-mountsa-nomountspec
Jun  8 15:29:09.672: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun  8 15:29:09.679: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun  8 15:29:09.679: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jun  8 15:29:09.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3679" for this suite. 06/08/23 15:29:09.686
------------------------------
• [0.129 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:29:09.569
    Jun  8 15:29:09.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename svcaccounts 06/08/23 15:29:09.57
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:29:09.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:29:09.599
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Jun  8 15:29:09.621: INFO: created pod pod-service-account-defaultsa
    Jun  8 15:29:09.621: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jun  8 15:29:09.628: INFO: created pod pod-service-account-mountsa
    Jun  8 15:29:09.628: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jun  8 15:29:09.635: INFO: created pod pod-service-account-nomountsa
    Jun  8 15:29:09.635: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jun  8 15:29:09.642: INFO: created pod pod-service-account-defaultsa-mountspec
    Jun  8 15:29:09.642: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jun  8 15:29:09.649: INFO: created pod pod-service-account-mountsa-mountspec
    Jun  8 15:29:09.649: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jun  8 15:29:09.657: INFO: created pod pod-service-account-nomountsa-mountspec
    Jun  8 15:29:09.657: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jun  8 15:29:09.665: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jun  8 15:29:09.665: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jun  8 15:29:09.671: INFO: created pod pod-service-account-mountsa-nomountspec
    Jun  8 15:29:09.672: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jun  8 15:29:09.679: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jun  8 15:29:09.679: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:29:09.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3679" for this suite. 06/08/23 15:29:09.686
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:29:09.698
Jun  8 15:29:09.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename container-probe 06/08/23 15:29:09.7
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:29:09.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:29:09.728
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun  8 15:30:09.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9557" for this suite. 06/08/23 15:30:09.761
------------------------------
• [SLOW TEST] [60.070 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:29:09.698
    Jun  8 15:29:09.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename container-probe 06/08/23 15:29:09.7
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:29:09.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:29:09.728
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:30:09.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9557" for this suite. 06/08/23 15:30:09.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:30:09.769
Jun  8 15:30:09.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename statefulset 06/08/23 15:30:09.77
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:30:09.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:30:09.797
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5727 06/08/23 15:30:09.8
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-5727 06/08/23 15:30:09.806
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5727 06/08/23 15:30:09.812
Jun  8 15:30:09.817: INFO: Found 0 stateful pods, waiting for 1
Jun  8 15:30:19.823: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 06/08/23 15:30:19.823
Jun  8 15:30:19.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-5727 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  8 15:30:20.022: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  8 15:30:20.022: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  8 15:30:20.022: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  8 15:30:20.027: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun  8 15:30:30.033: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  8 15:30:30.033: INFO: Waiting for statefulset status.replicas updated to 0
Jun  8 15:30:30.052: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
Jun  8 15:30:30.052: INFO: ss-0  chl8tf-worker-001  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:09 +0000 UTC  }]
Jun  8 15:30:30.052: INFO: 
Jun  8 15:30:30.052: INFO: StatefulSet ss has not reached scale 3, at 1
Jun  8 15:30:31.058: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995135518s
Jun  8 15:30:32.064: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989529326s
Jun  8 15:30:33.070: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98389058s
Jun  8 15:30:34.075: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978349478s
Jun  8 15:30:35.080: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.973134372s
Jun  8 15:30:36.086: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967784928s
Jun  8 15:30:37.091: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.962399901s
Jun  8 15:30:38.097: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.95664017s
Jun  8 15:30:39.103: INFO: Verifying statefulset ss doesn't scale past 3 for another 950.491203ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5727 06/08/23 15:30:40.104
Jun  8 15:30:40.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-5727 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  8 15:30:40.272: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  8 15:30:40.272: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  8 15:30:40.272: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  8 15:30:40.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-5727 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  8 15:30:40.448: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun  8 15:30:40.448: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  8 15:30:40.448: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  8 15:30:40.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-5727 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  8 15:30:40.661: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun  8 15:30:40.661: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  8 15:30:40.661: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  8 15:30:40.666: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jun  8 15:30:50.672: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  8 15:30:50.672: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  8 15:30:50.672: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 06/08/23 15:30:50.672
Jun  8 15:30:50.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-5727 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  8 15:30:50.864: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  8 15:30:50.864: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  8 15:30:50.864: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  8 15:30:50.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-5727 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  8 15:30:51.024: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  8 15:30:51.024: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  8 15:30:51.024: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  8 15:30:51.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-5727 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  8 15:30:51.193: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  8 15:30:51.193: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  8 15:30:51.193: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  8 15:30:51.193: INFO: Waiting for statefulset status.replicas updated to 0
Jun  8 15:30:51.197: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jun  8 15:31:01.207: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  8 15:31:01.207: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun  8 15:31:01.207: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun  8 15:31:01.221: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Jun  8 15:31:01.221: INFO: ss-0  chl8tf-worker-001         Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:09 +0000 UTC  }]
Jun  8 15:31:01.221: INFO: ss-1  chl8tf-worker-002         Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:30 +0000 UTC  }]
Jun  8 15:31:01.221: INFO: ss-2  chl8tf-control-plane-001  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:30 +0000 UTC  }]
Jun  8 15:31:01.221: INFO: 
Jun  8 15:31:01.221: INFO: StatefulSet ss has not reached scale 0, at 3
Jun  8 15:31:02.227: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Jun  8 15:31:02.227: INFO: ss-0  chl8tf-worker-001         Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:09 +0000 UTC  }]
Jun  8 15:31:02.227: INFO: ss-1  chl8tf-worker-002         Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:30 +0000 UTC  }]
Jun  8 15:31:02.227: INFO: ss-2  chl8tf-control-plane-001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:30 +0000 UTC  }]
Jun  8 15:31:02.227: INFO: 
Jun  8 15:31:02.227: INFO: StatefulSet ss has not reached scale 0, at 3
Jun  8 15:31:03.232: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.988859563s
Jun  8 15:31:04.237: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.98399445s
Jun  8 15:31:05.241: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.979528444s
Jun  8 15:31:06.246: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.975125794s
Jun  8 15:31:07.251: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.97033875s
Jun  8 15:31:08.256: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.965141701s
Jun  8 15:31:09.261: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.960282797s
Jun  8 15:31:10.266: INFO: Verifying statefulset ss doesn't scale past 0 for another 955.451848ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5727 06/08/23 15:31:11.266
Jun  8 15:31:11.272: INFO: Scaling statefulset ss to 0
Jun  8 15:31:11.284: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun  8 15:31:11.288: INFO: Deleting all statefulset in ns statefulset-5727
Jun  8 15:31:11.291: INFO: Scaling statefulset ss to 0
Jun  8 15:31:11.303: INFO: Waiting for statefulset status.replicas updated to 0
Jun  8 15:31:11.306: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun  8 15:31:11.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5727" for this suite. 06/08/23 15:31:11.326
------------------------------
• [SLOW TEST] [61.563 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:30:09.769
    Jun  8 15:30:09.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename statefulset 06/08/23 15:30:09.77
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:30:09.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:30:09.797
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5727 06/08/23 15:30:09.8
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-5727 06/08/23 15:30:09.806
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5727 06/08/23 15:30:09.812
    Jun  8 15:30:09.817: INFO: Found 0 stateful pods, waiting for 1
    Jun  8 15:30:19.823: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 06/08/23 15:30:19.823
    Jun  8 15:30:19.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-5727 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun  8 15:30:20.022: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun  8 15:30:20.022: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun  8 15:30:20.022: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun  8 15:30:20.027: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jun  8 15:30:30.033: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun  8 15:30:30.033: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  8 15:30:30.052: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
    Jun  8 15:30:30.052: INFO: ss-0  chl8tf-worker-001  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:09 +0000 UTC  }]
    Jun  8 15:30:30.052: INFO: 
    Jun  8 15:30:30.052: INFO: StatefulSet ss has not reached scale 3, at 1
    Jun  8 15:30:31.058: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995135518s
    Jun  8 15:30:32.064: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989529326s
    Jun  8 15:30:33.070: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98389058s
    Jun  8 15:30:34.075: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978349478s
    Jun  8 15:30:35.080: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.973134372s
    Jun  8 15:30:36.086: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967784928s
    Jun  8 15:30:37.091: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.962399901s
    Jun  8 15:30:38.097: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.95664017s
    Jun  8 15:30:39.103: INFO: Verifying statefulset ss doesn't scale past 3 for another 950.491203ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5727 06/08/23 15:30:40.104
    Jun  8 15:30:40.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-5727 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun  8 15:30:40.272: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun  8 15:30:40.272: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun  8 15:30:40.272: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun  8 15:30:40.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-5727 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun  8 15:30:40.448: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jun  8 15:30:40.448: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun  8 15:30:40.448: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun  8 15:30:40.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-5727 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun  8 15:30:40.661: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jun  8 15:30:40.661: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun  8 15:30:40.661: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun  8 15:30:40.666: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Jun  8 15:30:50.672: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun  8 15:30:50.672: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun  8 15:30:50.672: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 06/08/23 15:30:50.672
    Jun  8 15:30:50.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-5727 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun  8 15:30:50.864: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun  8 15:30:50.864: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun  8 15:30:50.864: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun  8 15:30:50.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-5727 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun  8 15:30:51.024: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun  8 15:30:51.024: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun  8 15:30:51.024: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun  8 15:30:51.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-5727 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun  8 15:30:51.193: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun  8 15:30:51.193: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun  8 15:30:51.193: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun  8 15:30:51.193: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  8 15:30:51.197: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jun  8 15:31:01.207: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun  8 15:31:01.207: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jun  8 15:31:01.207: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jun  8 15:31:01.221: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
    Jun  8 15:31:01.221: INFO: ss-0  chl8tf-worker-001         Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:09 +0000 UTC  }]
    Jun  8 15:31:01.221: INFO: ss-1  chl8tf-worker-002         Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:30 +0000 UTC  }]
    Jun  8 15:31:01.221: INFO: ss-2  chl8tf-control-plane-001  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:30 +0000 UTC  }]
    Jun  8 15:31:01.221: INFO: 
    Jun  8 15:31:01.221: INFO: StatefulSet ss has not reached scale 0, at 3
    Jun  8 15:31:02.227: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
    Jun  8 15:31:02.227: INFO: ss-0  chl8tf-worker-001         Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:09 +0000 UTC  }]
    Jun  8 15:31:02.227: INFO: ss-1  chl8tf-worker-002         Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:30 +0000 UTC  }]
    Jun  8 15:31:02.227: INFO: ss-2  chl8tf-control-plane-001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:30:30 +0000 UTC  }]
    Jun  8 15:31:02.227: INFO: 
    Jun  8 15:31:02.227: INFO: StatefulSet ss has not reached scale 0, at 3
    Jun  8 15:31:03.232: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.988859563s
    Jun  8 15:31:04.237: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.98399445s
    Jun  8 15:31:05.241: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.979528444s
    Jun  8 15:31:06.246: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.975125794s
    Jun  8 15:31:07.251: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.97033875s
    Jun  8 15:31:08.256: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.965141701s
    Jun  8 15:31:09.261: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.960282797s
    Jun  8 15:31:10.266: INFO: Verifying statefulset ss doesn't scale past 0 for another 955.451848ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5727 06/08/23 15:31:11.266
    Jun  8 15:31:11.272: INFO: Scaling statefulset ss to 0
    Jun  8 15:31:11.284: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun  8 15:31:11.288: INFO: Deleting all statefulset in ns statefulset-5727
    Jun  8 15:31:11.291: INFO: Scaling statefulset ss to 0
    Jun  8 15:31:11.303: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  8 15:31:11.306: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:31:11.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5727" for this suite. 06/08/23 15:31:11.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:31:11.334
Jun  8 15:31:11.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename secrets 06/08/23 15:31:11.335
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:11.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:11.356
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-51658fdb-6064-4a31-86c9-382eb6c7dff4 06/08/23 15:31:11.359
STEP: Creating a pod to test consume secrets 06/08/23 15:31:11.364
Jun  8 15:31:11.374: INFO: Waiting up to 5m0s for pod "pod-secrets-917ea43c-ecd7-4151-8f68-7dedfb7537b7" in namespace "secrets-3941" to be "Succeeded or Failed"
Jun  8 15:31:11.378: INFO: Pod "pod-secrets-917ea43c-ecd7-4151-8f68-7dedfb7537b7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.333693ms
Jun  8 15:31:13.383: INFO: Pod "pod-secrets-917ea43c-ecd7-4151-8f68-7dedfb7537b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009010061s
Jun  8 15:31:15.383: INFO: Pod "pod-secrets-917ea43c-ecd7-4151-8f68-7dedfb7537b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009139741s
STEP: Saw pod success 06/08/23 15:31:15.383
Jun  8 15:31:15.384: INFO: Pod "pod-secrets-917ea43c-ecd7-4151-8f68-7dedfb7537b7" satisfied condition "Succeeded or Failed"
Jun  8 15:31:15.387: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-secrets-917ea43c-ecd7-4151-8f68-7dedfb7537b7 container secret-volume-test: <nil>
STEP: delete the pod 06/08/23 15:31:15.404
Jun  8 15:31:15.417: INFO: Waiting for pod pod-secrets-917ea43c-ecd7-4151-8f68-7dedfb7537b7 to disappear
Jun  8 15:31:15.420: INFO: Pod pod-secrets-917ea43c-ecd7-4151-8f68-7dedfb7537b7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  8 15:31:15.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3941" for this suite. 06/08/23 15:31:15.425
------------------------------
• [4.098 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:31:11.334
    Jun  8 15:31:11.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename secrets 06/08/23 15:31:11.335
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:11.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:11.356
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-51658fdb-6064-4a31-86c9-382eb6c7dff4 06/08/23 15:31:11.359
    STEP: Creating a pod to test consume secrets 06/08/23 15:31:11.364
    Jun  8 15:31:11.374: INFO: Waiting up to 5m0s for pod "pod-secrets-917ea43c-ecd7-4151-8f68-7dedfb7537b7" in namespace "secrets-3941" to be "Succeeded or Failed"
    Jun  8 15:31:11.378: INFO: Pod "pod-secrets-917ea43c-ecd7-4151-8f68-7dedfb7537b7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.333693ms
    Jun  8 15:31:13.383: INFO: Pod "pod-secrets-917ea43c-ecd7-4151-8f68-7dedfb7537b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009010061s
    Jun  8 15:31:15.383: INFO: Pod "pod-secrets-917ea43c-ecd7-4151-8f68-7dedfb7537b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009139741s
    STEP: Saw pod success 06/08/23 15:31:15.383
    Jun  8 15:31:15.384: INFO: Pod "pod-secrets-917ea43c-ecd7-4151-8f68-7dedfb7537b7" satisfied condition "Succeeded or Failed"
    Jun  8 15:31:15.387: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-secrets-917ea43c-ecd7-4151-8f68-7dedfb7537b7 container secret-volume-test: <nil>
    STEP: delete the pod 06/08/23 15:31:15.404
    Jun  8 15:31:15.417: INFO: Waiting for pod pod-secrets-917ea43c-ecd7-4151-8f68-7dedfb7537b7 to disappear
    Jun  8 15:31:15.420: INFO: Pod pod-secrets-917ea43c-ecd7-4151-8f68-7dedfb7537b7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:31:15.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3941" for this suite. 06/08/23 15:31:15.425
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:31:15.432
Jun  8 15:31:15.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename configmap 06/08/23 15:31:15.433
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:15.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:15.455
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-6dccdf80-73d3-409e-b55c-35ce6172b67b 06/08/23 15:31:15.459
STEP: Creating a pod to test consume configMaps 06/08/23 15:31:15.464
Jun  8 15:31:15.473: INFO: Waiting up to 5m0s for pod "pod-configmaps-83b804e3-1d2c-4d33-bc0f-45d870f39a0b" in namespace "configmap-5609" to be "Succeeded or Failed"
Jun  8 15:31:15.480: INFO: Pod "pod-configmaps-83b804e3-1d2c-4d33-bc0f-45d870f39a0b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.638898ms
Jun  8 15:31:17.485: INFO: Pod "pod-configmaps-83b804e3-1d2c-4d33-bc0f-45d870f39a0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01215316s
Jun  8 15:31:19.485: INFO: Pod "pod-configmaps-83b804e3-1d2c-4d33-bc0f-45d870f39a0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011899383s
STEP: Saw pod success 06/08/23 15:31:19.485
Jun  8 15:31:19.485: INFO: Pod "pod-configmaps-83b804e3-1d2c-4d33-bc0f-45d870f39a0b" satisfied condition "Succeeded or Failed"
Jun  8 15:31:19.489: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-configmaps-83b804e3-1d2c-4d33-bc0f-45d870f39a0b container agnhost-container: <nil>
STEP: delete the pod 06/08/23 15:31:19.497
Jun  8 15:31:19.508: INFO: Waiting for pod pod-configmaps-83b804e3-1d2c-4d33-bc0f-45d870f39a0b to disappear
Jun  8 15:31:19.511: INFO: Pod pod-configmaps-83b804e3-1d2c-4d33-bc0f-45d870f39a0b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  8 15:31:19.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5609" for this suite. 06/08/23 15:31:19.517
------------------------------
• [4.092 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:31:15.432
    Jun  8 15:31:15.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename configmap 06/08/23 15:31:15.433
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:15.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:15.455
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-6dccdf80-73d3-409e-b55c-35ce6172b67b 06/08/23 15:31:15.459
    STEP: Creating a pod to test consume configMaps 06/08/23 15:31:15.464
    Jun  8 15:31:15.473: INFO: Waiting up to 5m0s for pod "pod-configmaps-83b804e3-1d2c-4d33-bc0f-45d870f39a0b" in namespace "configmap-5609" to be "Succeeded or Failed"
    Jun  8 15:31:15.480: INFO: Pod "pod-configmaps-83b804e3-1d2c-4d33-bc0f-45d870f39a0b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.638898ms
    Jun  8 15:31:17.485: INFO: Pod "pod-configmaps-83b804e3-1d2c-4d33-bc0f-45d870f39a0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01215316s
    Jun  8 15:31:19.485: INFO: Pod "pod-configmaps-83b804e3-1d2c-4d33-bc0f-45d870f39a0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011899383s
    STEP: Saw pod success 06/08/23 15:31:19.485
    Jun  8 15:31:19.485: INFO: Pod "pod-configmaps-83b804e3-1d2c-4d33-bc0f-45d870f39a0b" satisfied condition "Succeeded or Failed"
    Jun  8 15:31:19.489: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-configmaps-83b804e3-1d2c-4d33-bc0f-45d870f39a0b container agnhost-container: <nil>
    STEP: delete the pod 06/08/23 15:31:19.497
    Jun  8 15:31:19.508: INFO: Waiting for pod pod-configmaps-83b804e3-1d2c-4d33-bc0f-45d870f39a0b to disappear
    Jun  8 15:31:19.511: INFO: Pod pod-configmaps-83b804e3-1d2c-4d33-bc0f-45d870f39a0b no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:31:19.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5609" for this suite. 06/08/23 15:31:19.517
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:31:19.525
Jun  8 15:31:19.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename sysctl 06/08/23 15:31:19.526
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:19.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:19.55
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 06/08/23 15:31:19.553
STEP: Watching for error events or started pod 06/08/23 15:31:19.562
STEP: Waiting for pod completion 06/08/23 15:31:21.567
Jun  8 15:31:21.567: INFO: Waiting up to 3m0s for pod "sysctl-84e1e8a9-8487-45bf-9a60-a209cc3c6ce2" in namespace "sysctl-7349" to be "completed"
Jun  8 15:31:21.571: INFO: Pod "sysctl-84e1e8a9-8487-45bf-9a60-a209cc3c6ce2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.846789ms
Jun  8 15:31:23.575: INFO: Pod "sysctl-84e1e8a9-8487-45bf-9a60-a209cc3c6ce2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007936478s
Jun  8 15:31:23.575: INFO: Pod "sysctl-84e1e8a9-8487-45bf-9a60-a209cc3c6ce2" satisfied condition "completed"
STEP: Checking that the pod succeeded 06/08/23 15:31:23.579
STEP: Getting logs from the pod 06/08/23 15:31:23.579
STEP: Checking that the sysctl is actually updated 06/08/23 15:31:23.586
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:31:23.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-7349" for this suite. 06/08/23 15:31:23.592
------------------------------
• [4.074 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:31:19.525
    Jun  8 15:31:19.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename sysctl 06/08/23 15:31:19.526
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:19.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:19.55
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 06/08/23 15:31:19.553
    STEP: Watching for error events or started pod 06/08/23 15:31:19.562
    STEP: Waiting for pod completion 06/08/23 15:31:21.567
    Jun  8 15:31:21.567: INFO: Waiting up to 3m0s for pod "sysctl-84e1e8a9-8487-45bf-9a60-a209cc3c6ce2" in namespace "sysctl-7349" to be "completed"
    Jun  8 15:31:21.571: INFO: Pod "sysctl-84e1e8a9-8487-45bf-9a60-a209cc3c6ce2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.846789ms
    Jun  8 15:31:23.575: INFO: Pod "sysctl-84e1e8a9-8487-45bf-9a60-a209cc3c6ce2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007936478s
    Jun  8 15:31:23.575: INFO: Pod "sysctl-84e1e8a9-8487-45bf-9a60-a209cc3c6ce2" satisfied condition "completed"
    STEP: Checking that the pod succeeded 06/08/23 15:31:23.579
    STEP: Getting logs from the pod 06/08/23 15:31:23.579
    STEP: Checking that the sysctl is actually updated 06/08/23 15:31:23.586
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:31:23.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-7349" for this suite. 06/08/23 15:31:23.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:31:23.601
Jun  8 15:31:23.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename pods 06/08/23 15:31:23.602
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:23.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:23.622
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Jun  8 15:31:23.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: creating the pod 06/08/23 15:31:23.626
STEP: submitting the pod to kubernetes 06/08/23 15:31:23.626
Jun  8 15:31:23.635: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-4f3842fc-bd46-4c62-88eb-fe6b79da6962" in namespace "pods-6234" to be "running and ready"
Jun  8 15:31:23.639: INFO: Pod "pod-exec-websocket-4f3842fc-bd46-4c62-88eb-fe6b79da6962": Phase="Pending", Reason="", readiness=false. Elapsed: 3.500646ms
Jun  8 15:31:23.639: INFO: The phase of Pod pod-exec-websocket-4f3842fc-bd46-4c62-88eb-fe6b79da6962 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:31:25.644: INFO: Pod "pod-exec-websocket-4f3842fc-bd46-4c62-88eb-fe6b79da6962": Phase="Running", Reason="", readiness=true. Elapsed: 2.008104464s
Jun  8 15:31:25.644: INFO: The phase of Pod pod-exec-websocket-4f3842fc-bd46-4c62-88eb-fe6b79da6962 is Running (Ready = true)
Jun  8 15:31:25.644: INFO: Pod "pod-exec-websocket-4f3842fc-bd46-4c62-88eb-fe6b79da6962" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun  8 15:31:25.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6234" for this suite. 06/08/23 15:31:25.771
------------------------------
• [2.178 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:31:23.601
    Jun  8 15:31:23.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename pods 06/08/23 15:31:23.602
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:23.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:23.622
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Jun  8 15:31:23.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: creating the pod 06/08/23 15:31:23.626
    STEP: submitting the pod to kubernetes 06/08/23 15:31:23.626
    Jun  8 15:31:23.635: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-4f3842fc-bd46-4c62-88eb-fe6b79da6962" in namespace "pods-6234" to be "running and ready"
    Jun  8 15:31:23.639: INFO: Pod "pod-exec-websocket-4f3842fc-bd46-4c62-88eb-fe6b79da6962": Phase="Pending", Reason="", readiness=false. Elapsed: 3.500646ms
    Jun  8 15:31:23.639: INFO: The phase of Pod pod-exec-websocket-4f3842fc-bd46-4c62-88eb-fe6b79da6962 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:31:25.644: INFO: Pod "pod-exec-websocket-4f3842fc-bd46-4c62-88eb-fe6b79da6962": Phase="Running", Reason="", readiness=true. Elapsed: 2.008104464s
    Jun  8 15:31:25.644: INFO: The phase of Pod pod-exec-websocket-4f3842fc-bd46-4c62-88eb-fe6b79da6962 is Running (Ready = true)
    Jun  8 15:31:25.644: INFO: Pod "pod-exec-websocket-4f3842fc-bd46-4c62-88eb-fe6b79da6962" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:31:25.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6234" for this suite. 06/08/23 15:31:25.771
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:31:25.779
Jun  8 15:31:25.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename resourcequota 06/08/23 15:31:25.78
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:25.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:25.8
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 06/08/23 15:31:25.803
STEP: Counting existing ResourceQuota 06/08/23 15:31:30.808
STEP: Creating a ResourceQuota 06/08/23 15:31:35.814
STEP: Ensuring resource quota status is calculated 06/08/23 15:31:35.82
STEP: Creating a Secret 06/08/23 15:31:37.825
STEP: Ensuring resource quota status captures secret creation 06/08/23 15:31:37.84
STEP: Deleting a secret 06/08/23 15:31:39.845
STEP: Ensuring resource quota status released usage 06/08/23 15:31:39.853
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  8 15:31:41.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5874" for this suite. 06/08/23 15:31:41.866
------------------------------
• [SLOW TEST] [16.094 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:31:25.779
    Jun  8 15:31:25.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename resourcequota 06/08/23 15:31:25.78
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:25.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:25.8
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 06/08/23 15:31:25.803
    STEP: Counting existing ResourceQuota 06/08/23 15:31:30.808
    STEP: Creating a ResourceQuota 06/08/23 15:31:35.814
    STEP: Ensuring resource quota status is calculated 06/08/23 15:31:35.82
    STEP: Creating a Secret 06/08/23 15:31:37.825
    STEP: Ensuring resource quota status captures secret creation 06/08/23 15:31:37.84
    STEP: Deleting a secret 06/08/23 15:31:39.845
    STEP: Ensuring resource quota status released usage 06/08/23 15:31:39.853
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:31:41.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5874" for this suite. 06/08/23 15:31:41.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:31:41.873
Jun  8 15:31:41.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename events 06/08/23 15:31:41.875
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:41.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:41.899
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 06/08/23 15:31:41.902
STEP: listing all events in all namespaces 06/08/23 15:31:41.911
STEP: patching the test event 06/08/23 15:31:41.921
STEP: fetching the test event 06/08/23 15:31:41.929
STEP: updating the test event 06/08/23 15:31:41.933
STEP: getting the test event 06/08/23 15:31:41.943
STEP: deleting the test event 06/08/23 15:31:41.947
STEP: listing all events in all namespaces 06/08/23 15:31:41.955
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jun  8 15:31:41.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7206" for this suite. 06/08/23 15:31:41.971
------------------------------
• [0.105 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:31:41.873
    Jun  8 15:31:41.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename events 06/08/23 15:31:41.875
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:41.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:41.899
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 06/08/23 15:31:41.902
    STEP: listing all events in all namespaces 06/08/23 15:31:41.911
    STEP: patching the test event 06/08/23 15:31:41.921
    STEP: fetching the test event 06/08/23 15:31:41.929
    STEP: updating the test event 06/08/23 15:31:41.933
    STEP: getting the test event 06/08/23 15:31:41.943
    STEP: deleting the test event 06/08/23 15:31:41.947
    STEP: listing all events in all namespaces 06/08/23 15:31:41.955
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:31:41.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7206" for this suite. 06/08/23 15:31:41.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:31:41.98
Jun  8 15:31:41.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename discovery 06/08/23 15:31:41.981
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:41.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:42.002
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 06/08/23 15:31:42.006
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jun  8 15:31:42.665: INFO: Checking APIGroup: apiregistration.k8s.io
Jun  8 15:31:42.666: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jun  8 15:31:42.666: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jun  8 15:31:42.666: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jun  8 15:31:42.666: INFO: Checking APIGroup: apps
Jun  8 15:31:42.668: INFO: PreferredVersion.GroupVersion: apps/v1
Jun  8 15:31:42.668: INFO: Versions found [{apps/v1 v1}]
Jun  8 15:31:42.668: INFO: apps/v1 matches apps/v1
Jun  8 15:31:42.668: INFO: Checking APIGroup: events.k8s.io
Jun  8 15:31:42.669: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jun  8 15:31:42.669: INFO: Versions found [{events.k8s.io/v1 v1}]
Jun  8 15:31:42.669: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jun  8 15:31:42.669: INFO: Checking APIGroup: authentication.k8s.io
Jun  8 15:31:42.670: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jun  8 15:31:42.670: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jun  8 15:31:42.670: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jun  8 15:31:42.670: INFO: Checking APIGroup: authorization.k8s.io
Jun  8 15:31:42.671: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jun  8 15:31:42.671: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jun  8 15:31:42.671: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jun  8 15:31:42.671: INFO: Checking APIGroup: autoscaling
Jun  8 15:31:42.673: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jun  8 15:31:42.673: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Jun  8 15:31:42.673: INFO: autoscaling/v2 matches autoscaling/v2
Jun  8 15:31:42.673: INFO: Checking APIGroup: batch
Jun  8 15:31:42.674: INFO: PreferredVersion.GroupVersion: batch/v1
Jun  8 15:31:42.674: INFO: Versions found [{batch/v1 v1}]
Jun  8 15:31:42.674: INFO: batch/v1 matches batch/v1
Jun  8 15:31:42.674: INFO: Checking APIGroup: certificates.k8s.io
Jun  8 15:31:42.676: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jun  8 15:31:42.676: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jun  8 15:31:42.676: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jun  8 15:31:42.676: INFO: Checking APIGroup: networking.k8s.io
Jun  8 15:31:42.677: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jun  8 15:31:42.677: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jun  8 15:31:42.677: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jun  8 15:31:42.677: INFO: Checking APIGroup: policy
Jun  8 15:31:42.678: INFO: PreferredVersion.GroupVersion: policy/v1
Jun  8 15:31:42.678: INFO: Versions found [{policy/v1 v1}]
Jun  8 15:31:42.678: INFO: policy/v1 matches policy/v1
Jun  8 15:31:42.678: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jun  8 15:31:42.679: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jun  8 15:31:42.679: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jun  8 15:31:42.679: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jun  8 15:31:42.679: INFO: Checking APIGroup: storage.k8s.io
Jun  8 15:31:42.680: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jun  8 15:31:42.681: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jun  8 15:31:42.681: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jun  8 15:31:42.681: INFO: Checking APIGroup: admissionregistration.k8s.io
Jun  8 15:31:42.682: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jun  8 15:31:42.682: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jun  8 15:31:42.682: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jun  8 15:31:42.682: INFO: Checking APIGroup: apiextensions.k8s.io
Jun  8 15:31:42.683: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jun  8 15:31:42.683: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jun  8 15:31:42.683: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jun  8 15:31:42.683: INFO: Checking APIGroup: scheduling.k8s.io
Jun  8 15:31:42.684: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jun  8 15:31:42.684: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jun  8 15:31:42.684: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jun  8 15:31:42.684: INFO: Checking APIGroup: coordination.k8s.io
Jun  8 15:31:42.686: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jun  8 15:31:42.686: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jun  8 15:31:42.686: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jun  8 15:31:42.686: INFO: Checking APIGroup: node.k8s.io
Jun  8 15:31:42.687: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jun  8 15:31:42.687: INFO: Versions found [{node.k8s.io/v1 v1}]
Jun  8 15:31:42.687: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jun  8 15:31:42.687: INFO: Checking APIGroup: discovery.k8s.io
Jun  8 15:31:42.688: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jun  8 15:31:42.688: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jun  8 15:31:42.688: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jun  8 15:31:42.688: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jun  8 15:31:42.689: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Jun  8 15:31:42.689: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Jun  8 15:31:42.689: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Jun  8 15:31:42.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-5535" for this suite. 06/08/23 15:31:42.696
------------------------------
• [0.724 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:31:41.98
    Jun  8 15:31:41.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename discovery 06/08/23 15:31:41.981
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:41.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:42.002
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 06/08/23 15:31:42.006
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jun  8 15:31:42.665: INFO: Checking APIGroup: apiregistration.k8s.io
    Jun  8 15:31:42.666: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jun  8 15:31:42.666: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jun  8 15:31:42.666: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jun  8 15:31:42.666: INFO: Checking APIGroup: apps
    Jun  8 15:31:42.668: INFO: PreferredVersion.GroupVersion: apps/v1
    Jun  8 15:31:42.668: INFO: Versions found [{apps/v1 v1}]
    Jun  8 15:31:42.668: INFO: apps/v1 matches apps/v1
    Jun  8 15:31:42.668: INFO: Checking APIGroup: events.k8s.io
    Jun  8 15:31:42.669: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jun  8 15:31:42.669: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jun  8 15:31:42.669: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jun  8 15:31:42.669: INFO: Checking APIGroup: authentication.k8s.io
    Jun  8 15:31:42.670: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jun  8 15:31:42.670: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jun  8 15:31:42.670: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jun  8 15:31:42.670: INFO: Checking APIGroup: authorization.k8s.io
    Jun  8 15:31:42.671: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jun  8 15:31:42.671: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jun  8 15:31:42.671: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jun  8 15:31:42.671: INFO: Checking APIGroup: autoscaling
    Jun  8 15:31:42.673: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jun  8 15:31:42.673: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Jun  8 15:31:42.673: INFO: autoscaling/v2 matches autoscaling/v2
    Jun  8 15:31:42.673: INFO: Checking APIGroup: batch
    Jun  8 15:31:42.674: INFO: PreferredVersion.GroupVersion: batch/v1
    Jun  8 15:31:42.674: INFO: Versions found [{batch/v1 v1}]
    Jun  8 15:31:42.674: INFO: batch/v1 matches batch/v1
    Jun  8 15:31:42.674: INFO: Checking APIGroup: certificates.k8s.io
    Jun  8 15:31:42.676: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jun  8 15:31:42.676: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jun  8 15:31:42.676: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jun  8 15:31:42.676: INFO: Checking APIGroup: networking.k8s.io
    Jun  8 15:31:42.677: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jun  8 15:31:42.677: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jun  8 15:31:42.677: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jun  8 15:31:42.677: INFO: Checking APIGroup: policy
    Jun  8 15:31:42.678: INFO: PreferredVersion.GroupVersion: policy/v1
    Jun  8 15:31:42.678: INFO: Versions found [{policy/v1 v1}]
    Jun  8 15:31:42.678: INFO: policy/v1 matches policy/v1
    Jun  8 15:31:42.678: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jun  8 15:31:42.679: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jun  8 15:31:42.679: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jun  8 15:31:42.679: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jun  8 15:31:42.679: INFO: Checking APIGroup: storage.k8s.io
    Jun  8 15:31:42.680: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jun  8 15:31:42.681: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jun  8 15:31:42.681: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jun  8 15:31:42.681: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jun  8 15:31:42.682: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jun  8 15:31:42.682: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jun  8 15:31:42.682: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jun  8 15:31:42.682: INFO: Checking APIGroup: apiextensions.k8s.io
    Jun  8 15:31:42.683: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jun  8 15:31:42.683: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jun  8 15:31:42.683: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jun  8 15:31:42.683: INFO: Checking APIGroup: scheduling.k8s.io
    Jun  8 15:31:42.684: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jun  8 15:31:42.684: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jun  8 15:31:42.684: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jun  8 15:31:42.684: INFO: Checking APIGroup: coordination.k8s.io
    Jun  8 15:31:42.686: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jun  8 15:31:42.686: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jun  8 15:31:42.686: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jun  8 15:31:42.686: INFO: Checking APIGroup: node.k8s.io
    Jun  8 15:31:42.687: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jun  8 15:31:42.687: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jun  8 15:31:42.687: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jun  8 15:31:42.687: INFO: Checking APIGroup: discovery.k8s.io
    Jun  8 15:31:42.688: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jun  8 15:31:42.688: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jun  8 15:31:42.688: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jun  8 15:31:42.688: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jun  8 15:31:42.689: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Jun  8 15:31:42.689: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Jun  8 15:31:42.689: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:31:42.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-5535" for this suite. 06/08/23 15:31:42.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:31:42.705
Jun  8 15:31:42.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename dns 06/08/23 15:31:42.707
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:42.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:42.729
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1015.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1015.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 06/08/23 15:31:42.733
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1015.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1015.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 06/08/23 15:31:42.733
STEP: creating a pod to probe /etc/hosts 06/08/23 15:31:42.733
STEP: submitting the pod to kubernetes 06/08/23 15:31:42.733
Jun  8 15:31:42.744: INFO: Waiting up to 15m0s for pod "dns-test-4d3fbc51-580e-48f5-84f8-1977905e1a50" in namespace "dns-1015" to be "running"
Jun  8 15:31:42.748: INFO: Pod "dns-test-4d3fbc51-580e-48f5-84f8-1977905e1a50": Phase="Pending", Reason="", readiness=false. Elapsed: 3.766289ms
Jun  8 15:31:44.754: INFO: Pod "dns-test-4d3fbc51-580e-48f5-84f8-1977905e1a50": Phase="Running", Reason="", readiness=true. Elapsed: 2.010178893s
Jun  8 15:31:44.754: INFO: Pod "dns-test-4d3fbc51-580e-48f5-84f8-1977905e1a50" satisfied condition "running"
STEP: retrieving the pod 06/08/23 15:31:44.754
STEP: looking for the results for each expected name from probers 06/08/23 15:31:44.759
Jun  8 15:31:44.780: INFO: DNS probes using dns-1015/dns-test-4d3fbc51-580e-48f5-84f8-1977905e1a50 succeeded

STEP: deleting the pod 06/08/23 15:31:44.78
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun  8 15:31:44.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1015" for this suite. 06/08/23 15:31:44.8
------------------------------
• [2.102 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:31:42.705
    Jun  8 15:31:42.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename dns 06/08/23 15:31:42.707
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:42.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:42.729
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1015.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1015.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     06/08/23 15:31:42.733
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1015.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1015.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     06/08/23 15:31:42.733
    STEP: creating a pod to probe /etc/hosts 06/08/23 15:31:42.733
    STEP: submitting the pod to kubernetes 06/08/23 15:31:42.733
    Jun  8 15:31:42.744: INFO: Waiting up to 15m0s for pod "dns-test-4d3fbc51-580e-48f5-84f8-1977905e1a50" in namespace "dns-1015" to be "running"
    Jun  8 15:31:42.748: INFO: Pod "dns-test-4d3fbc51-580e-48f5-84f8-1977905e1a50": Phase="Pending", Reason="", readiness=false. Elapsed: 3.766289ms
    Jun  8 15:31:44.754: INFO: Pod "dns-test-4d3fbc51-580e-48f5-84f8-1977905e1a50": Phase="Running", Reason="", readiness=true. Elapsed: 2.010178893s
    Jun  8 15:31:44.754: INFO: Pod "dns-test-4d3fbc51-580e-48f5-84f8-1977905e1a50" satisfied condition "running"
    STEP: retrieving the pod 06/08/23 15:31:44.754
    STEP: looking for the results for each expected name from probers 06/08/23 15:31:44.759
    Jun  8 15:31:44.780: INFO: DNS probes using dns-1015/dns-test-4d3fbc51-580e-48f5-84f8-1977905e1a50 succeeded

    STEP: deleting the pod 06/08/23 15:31:44.78
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:31:44.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1015" for this suite. 06/08/23 15:31:44.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:31:44.808
Jun  8 15:31:44.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename dns 06/08/23 15:31:44.81
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:44.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:44.832
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 06/08/23 15:31:44.835
Jun  8 15:31:44.845: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5137  fde96856-e595-4ebd-940c-ad00896ac492 29954 0 2023-06-08 15:31:44 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-06-08 15:31:44 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sc2gj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sc2gj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:31:44.845: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5137" to be "running and ready"
Jun  8 15:31:44.851: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 5.424678ms
Jun  8 15:31:44.851: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:31:46.857: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.011636277s
Jun  8 15:31:46.857: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jun  8 15:31:46.857: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 06/08/23 15:31:46.857
Jun  8 15:31:46.857: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5137 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:31:46.857: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:31:46.858: INFO: ExecWithOptions: Clientset creation
Jun  8 15:31:46.858: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-5137/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 06/08/23 15:31:46.966
Jun  8 15:31:46.966: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5137 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:31:46.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:31:46.967: INFO: ExecWithOptions: Clientset creation
Jun  8 15:31:46.967: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-5137/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun  8 15:31:47.064: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun  8 15:31:47.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5137" for this suite. 06/08/23 15:31:47.089
------------------------------
• [2.289 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:31:44.808
    Jun  8 15:31:44.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename dns 06/08/23 15:31:44.81
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:44.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:44.832
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 06/08/23 15:31:44.835
    Jun  8 15:31:44.845: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5137  fde96856-e595-4ebd-940c-ad00896ac492 29954 0 2023-06-08 15:31:44 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-06-08 15:31:44 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sc2gj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sc2gj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:31:44.845: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5137" to be "running and ready"
    Jun  8 15:31:44.851: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 5.424678ms
    Jun  8 15:31:44.851: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:31:46.857: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.011636277s
    Jun  8 15:31:46.857: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jun  8 15:31:46.857: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 06/08/23 15:31:46.857
    Jun  8 15:31:46.857: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5137 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:31:46.857: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:31:46.858: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:31:46.858: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-5137/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 06/08/23 15:31:46.966
    Jun  8 15:31:46.966: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5137 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:31:46.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:31:46.967: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:31:46.967: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-5137/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun  8 15:31:47.064: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:31:47.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5137" for this suite. 06/08/23 15:31:47.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:31:47.099
Jun  8 15:31:47.099: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename limitrange 06/08/23 15:31:47.1
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:47.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:47.123
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 06/08/23 15:31:47.127
STEP: Setting up watch 06/08/23 15:31:47.127
STEP: Submitting a LimitRange 06/08/23 15:31:47.231
STEP: Verifying LimitRange creation was observed 06/08/23 15:31:47.238
STEP: Fetching the LimitRange to ensure it has proper values 06/08/23 15:31:47.238
Jun  8 15:31:47.243: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun  8 15:31:47.243: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 06/08/23 15:31:47.243
STEP: Ensuring Pod has resource requirements applied from LimitRange 06/08/23 15:31:47.25
Jun  8 15:31:47.255: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun  8 15:31:47.255: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 06/08/23 15:31:47.255
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 06/08/23 15:31:47.263
Jun  8 15:31:47.271: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jun  8 15:31:47.271: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 06/08/23 15:31:47.271
STEP: Failing to create a Pod with more than max resources 06/08/23 15:31:47.274
STEP: Updating a LimitRange 06/08/23 15:31:47.277
STEP: Verifying LimitRange updating is effective 06/08/23 15:31:47.284
STEP: Creating a Pod with less than former min resources 06/08/23 15:31:49.29
STEP: Failing to create a Pod with more than max resources 06/08/23 15:31:49.296
STEP: Deleting a LimitRange 06/08/23 15:31:49.298
STEP: Verifying the LimitRange was deleted 06/08/23 15:31:49.307
Jun  8 15:31:54.313: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 06/08/23 15:31:54.313
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jun  8 15:31:54.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-7834" for this suite. 06/08/23 15:31:54.33
------------------------------
• [SLOW TEST] [7.239 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:31:47.099
    Jun  8 15:31:47.099: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename limitrange 06/08/23 15:31:47.1
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:47.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:47.123
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 06/08/23 15:31:47.127
    STEP: Setting up watch 06/08/23 15:31:47.127
    STEP: Submitting a LimitRange 06/08/23 15:31:47.231
    STEP: Verifying LimitRange creation was observed 06/08/23 15:31:47.238
    STEP: Fetching the LimitRange to ensure it has proper values 06/08/23 15:31:47.238
    Jun  8 15:31:47.243: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jun  8 15:31:47.243: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 06/08/23 15:31:47.243
    STEP: Ensuring Pod has resource requirements applied from LimitRange 06/08/23 15:31:47.25
    Jun  8 15:31:47.255: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jun  8 15:31:47.255: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 06/08/23 15:31:47.255
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 06/08/23 15:31:47.263
    Jun  8 15:31:47.271: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jun  8 15:31:47.271: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 06/08/23 15:31:47.271
    STEP: Failing to create a Pod with more than max resources 06/08/23 15:31:47.274
    STEP: Updating a LimitRange 06/08/23 15:31:47.277
    STEP: Verifying LimitRange updating is effective 06/08/23 15:31:47.284
    STEP: Creating a Pod with less than former min resources 06/08/23 15:31:49.29
    STEP: Failing to create a Pod with more than max resources 06/08/23 15:31:49.296
    STEP: Deleting a LimitRange 06/08/23 15:31:49.298
    STEP: Verifying the LimitRange was deleted 06/08/23 15:31:49.307
    Jun  8 15:31:54.313: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 06/08/23 15:31:54.313
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:31:54.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-7834" for this suite. 06/08/23 15:31:54.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:31:54.34
Jun  8 15:31:54.341: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename containers 06/08/23 15:31:54.342
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:54.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:54.363
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Jun  8 15:31:54.376: INFO: Waiting up to 5m0s for pod "client-containers-b3cf74ad-a053-4aca-9270-6c47445b7676" in namespace "containers-9374" to be "running"
Jun  8 15:31:54.380: INFO: Pod "client-containers-b3cf74ad-a053-4aca-9270-6c47445b7676": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007998ms
Jun  8 15:31:56.386: INFO: Pod "client-containers-b3cf74ad-a053-4aca-9270-6c47445b7676": Phase="Running", Reason="", readiness=true. Elapsed: 2.009797561s
Jun  8 15:31:56.386: INFO: Pod "client-containers-b3cf74ad-a053-4aca-9270-6c47445b7676" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jun  8 15:31:56.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9374" for this suite. 06/08/23 15:31:56.4
------------------------------
• [2.068 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:31:54.34
    Jun  8 15:31:54.341: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename containers 06/08/23 15:31:54.342
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:54.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:54.363
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Jun  8 15:31:54.376: INFO: Waiting up to 5m0s for pod "client-containers-b3cf74ad-a053-4aca-9270-6c47445b7676" in namespace "containers-9374" to be "running"
    Jun  8 15:31:54.380: INFO: Pod "client-containers-b3cf74ad-a053-4aca-9270-6c47445b7676": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007998ms
    Jun  8 15:31:56.386: INFO: Pod "client-containers-b3cf74ad-a053-4aca-9270-6c47445b7676": Phase="Running", Reason="", readiness=true. Elapsed: 2.009797561s
    Jun  8 15:31:56.386: INFO: Pod "client-containers-b3cf74ad-a053-4aca-9270-6c47445b7676" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:31:56.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9374" for this suite. 06/08/23 15:31:56.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:31:56.409
Jun  8 15:31:56.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename services 06/08/23 15:31:56.41
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:56.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:56.431
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-1950 06/08/23 15:31:56.435
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1950 to expose endpoints map[] 06/08/23 15:31:56.451
Jun  8 15:31:56.455: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jun  8 15:31:57.466: INFO: successfully validated that service endpoint-test2 in namespace services-1950 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1950 06/08/23 15:31:57.466
Jun  8 15:31:57.475: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1950" to be "running and ready"
Jun  8 15:31:57.479: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.861093ms
Jun  8 15:31:57.479: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:31:59.484: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008779314s
Jun  8 15:31:59.484: INFO: The phase of Pod pod1 is Running (Ready = true)
Jun  8 15:31:59.484: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1950 to expose endpoints map[pod1:[80]] 06/08/23 15:31:59.487
Jun  8 15:31:59.499: INFO: successfully validated that service endpoint-test2 in namespace services-1950 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 06/08/23 15:31:59.499
Jun  8 15:31:59.500: INFO: Creating new exec pod
Jun  8 15:31:59.505: INFO: Waiting up to 5m0s for pod "execpodkhgpb" in namespace "services-1950" to be "running"
Jun  8 15:31:59.509: INFO: Pod "execpodkhgpb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.653952ms
Jun  8 15:32:01.514: INFO: Pod "execpodkhgpb": Phase="Running", Reason="", readiness=true. Elapsed: 2.008798133s
Jun  8 15:32:01.514: INFO: Pod "execpodkhgpb" satisfied condition "running"
Jun  8 15:32:02.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1950 exec execpodkhgpb -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jun  8 15:32:02.684: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun  8 15:32:02.684: INFO: stdout: ""
Jun  8 15:32:02.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1950 exec execpodkhgpb -- /bin/sh -x -c nc -v -z -w 2 10.110.252.124 80'
Jun  8 15:32:02.846: INFO: stderr: "+ nc -v -z -w 2 10.110.252.124 80\nConnection to 10.110.252.124 80 port [tcp/http] succeeded!\n"
Jun  8 15:32:02.846: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-1950 06/08/23 15:32:02.846
Jun  8 15:32:02.854: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1950" to be "running and ready"
Jun  8 15:32:02.858: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.856643ms
Jun  8 15:32:02.858: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:32:04.865: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011593355s
Jun  8 15:32:04.865: INFO: The phase of Pod pod2 is Running (Ready = true)
Jun  8 15:32:04.865: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1950 to expose endpoints map[pod1:[80] pod2:[80]] 06/08/23 15:32:04.869
Jun  8 15:32:04.885: INFO: successfully validated that service endpoint-test2 in namespace services-1950 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 06/08/23 15:32:04.885
Jun  8 15:32:05.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1950 exec execpodkhgpb -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jun  8 15:32:06.055: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun  8 15:32:06.055: INFO: stdout: ""
Jun  8 15:32:06.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1950 exec execpodkhgpb -- /bin/sh -x -c nc -v -z -w 2 10.110.252.124 80'
Jun  8 15:32:06.218: INFO: stderr: "+ nc -v -z -w 2 10.110.252.124 80\nConnection to 10.110.252.124 80 port [tcp/http] succeeded!\n"
Jun  8 15:32:06.218: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-1950 06/08/23 15:32:06.218
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1950 to expose endpoints map[pod2:[80]] 06/08/23 15:32:06.236
Jun  8 15:32:06.250: INFO: successfully validated that service endpoint-test2 in namespace services-1950 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 06/08/23 15:32:06.25
Jun  8 15:32:07.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1950 exec execpodkhgpb -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jun  8 15:32:07.429: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun  8 15:32:07.429: INFO: stdout: ""
Jun  8 15:32:07.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1950 exec execpodkhgpb -- /bin/sh -x -c nc -v -z -w 2 10.110.252.124 80'
Jun  8 15:32:07.603: INFO: stderr: "+ nc -v -z -w 2 10.110.252.124 80\nConnection to 10.110.252.124 80 port [tcp/http] succeeded!\n"
Jun  8 15:32:07.603: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-1950 06/08/23 15:32:07.603
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1950 to expose endpoints map[] 06/08/23 15:32:07.616
Jun  8 15:32:08.637: INFO: successfully validated that service endpoint-test2 in namespace services-1950 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  8 15:32:08.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1950" for this suite. 06/08/23 15:32:08.668
------------------------------
• [SLOW TEST] [12.269 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:31:56.409
    Jun  8 15:31:56.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename services 06/08/23 15:31:56.41
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:31:56.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:31:56.431
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-1950 06/08/23 15:31:56.435
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1950 to expose endpoints map[] 06/08/23 15:31:56.451
    Jun  8 15:31:56.455: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Jun  8 15:31:57.466: INFO: successfully validated that service endpoint-test2 in namespace services-1950 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-1950 06/08/23 15:31:57.466
    Jun  8 15:31:57.475: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1950" to be "running and ready"
    Jun  8 15:31:57.479: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.861093ms
    Jun  8 15:31:57.479: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:31:59.484: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008779314s
    Jun  8 15:31:59.484: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jun  8 15:31:59.484: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1950 to expose endpoints map[pod1:[80]] 06/08/23 15:31:59.487
    Jun  8 15:31:59.499: INFO: successfully validated that service endpoint-test2 in namespace services-1950 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 06/08/23 15:31:59.499
    Jun  8 15:31:59.500: INFO: Creating new exec pod
    Jun  8 15:31:59.505: INFO: Waiting up to 5m0s for pod "execpodkhgpb" in namespace "services-1950" to be "running"
    Jun  8 15:31:59.509: INFO: Pod "execpodkhgpb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.653952ms
    Jun  8 15:32:01.514: INFO: Pod "execpodkhgpb": Phase="Running", Reason="", readiness=true. Elapsed: 2.008798133s
    Jun  8 15:32:01.514: INFO: Pod "execpodkhgpb" satisfied condition "running"
    Jun  8 15:32:02.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1950 exec execpodkhgpb -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jun  8 15:32:02.684: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jun  8 15:32:02.684: INFO: stdout: ""
    Jun  8 15:32:02.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1950 exec execpodkhgpb -- /bin/sh -x -c nc -v -z -w 2 10.110.252.124 80'
    Jun  8 15:32:02.846: INFO: stderr: "+ nc -v -z -w 2 10.110.252.124 80\nConnection to 10.110.252.124 80 port [tcp/http] succeeded!\n"
    Jun  8 15:32:02.846: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-1950 06/08/23 15:32:02.846
    Jun  8 15:32:02.854: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1950" to be "running and ready"
    Jun  8 15:32:02.858: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.856643ms
    Jun  8 15:32:02.858: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:32:04.865: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011593355s
    Jun  8 15:32:04.865: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jun  8 15:32:04.865: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1950 to expose endpoints map[pod1:[80] pod2:[80]] 06/08/23 15:32:04.869
    Jun  8 15:32:04.885: INFO: successfully validated that service endpoint-test2 in namespace services-1950 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 06/08/23 15:32:04.885
    Jun  8 15:32:05.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1950 exec execpodkhgpb -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jun  8 15:32:06.055: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jun  8 15:32:06.055: INFO: stdout: ""
    Jun  8 15:32:06.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1950 exec execpodkhgpb -- /bin/sh -x -c nc -v -z -w 2 10.110.252.124 80'
    Jun  8 15:32:06.218: INFO: stderr: "+ nc -v -z -w 2 10.110.252.124 80\nConnection to 10.110.252.124 80 port [tcp/http] succeeded!\n"
    Jun  8 15:32:06.218: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-1950 06/08/23 15:32:06.218
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1950 to expose endpoints map[pod2:[80]] 06/08/23 15:32:06.236
    Jun  8 15:32:06.250: INFO: successfully validated that service endpoint-test2 in namespace services-1950 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 06/08/23 15:32:06.25
    Jun  8 15:32:07.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1950 exec execpodkhgpb -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jun  8 15:32:07.429: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jun  8 15:32:07.429: INFO: stdout: ""
    Jun  8 15:32:07.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-1950 exec execpodkhgpb -- /bin/sh -x -c nc -v -z -w 2 10.110.252.124 80'
    Jun  8 15:32:07.603: INFO: stderr: "+ nc -v -z -w 2 10.110.252.124 80\nConnection to 10.110.252.124 80 port [tcp/http] succeeded!\n"
    Jun  8 15:32:07.603: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-1950 06/08/23 15:32:07.603
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1950 to expose endpoints map[] 06/08/23 15:32:07.616
    Jun  8 15:32:08.637: INFO: successfully validated that service endpoint-test2 in namespace services-1950 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:32:08.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1950" for this suite. 06/08/23 15:32:08.668
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:32:08.678
Jun  8 15:32:08.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename runtimeclass 06/08/23 15:32:08.679
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:08.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:08.71
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-53-delete-me 06/08/23 15:32:08.721
STEP: Waiting for the RuntimeClass to disappear 06/08/23 15:32:08.729
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jun  8 15:32:08.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-53" for this suite. 06/08/23 15:32:08.751
------------------------------
• [0.090 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:32:08.678
    Jun  8 15:32:08.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename runtimeclass 06/08/23 15:32:08.679
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:08.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:08.71
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-53-delete-me 06/08/23 15:32:08.721
    STEP: Waiting for the RuntimeClass to disappear 06/08/23 15:32:08.729
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:32:08.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-53" for this suite. 06/08/23 15:32:08.751
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:32:08.768
Jun  8 15:32:08.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename secrets 06/08/23 15:32:08.77
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:08.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:08.804
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  8 15:32:08.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7673" for this suite. 06/08/23 15:32:08.872
------------------------------
• [0.112 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:32:08.768
    Jun  8 15:32:08.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename secrets 06/08/23 15:32:08.77
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:08.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:08.804
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:32:08.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7673" for this suite. 06/08/23 15:32:08.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:32:08.883
Jun  8 15:32:08.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename configmap 06/08/23 15:32:08.884
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:08.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:08.909
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-10347422-e305-458d-8b0e-604f52cdb261 06/08/23 15:32:08.922
STEP: Creating the pod 06/08/23 15:32:08.929
Jun  8 15:32:08.941: INFO: Waiting up to 5m0s for pod "pod-configmaps-52ec7b7a-fbdb-4f7a-81e7-0110434b3a22" in namespace "configmap-1434" to be "running"
Jun  8 15:32:08.946: INFO: Pod "pod-configmaps-52ec7b7a-fbdb-4f7a-81e7-0110434b3a22": Phase="Pending", Reason="", readiness=false. Elapsed: 5.228246ms
Jun  8 15:32:10.953: INFO: Pod "pod-configmaps-52ec7b7a-fbdb-4f7a-81e7-0110434b3a22": Phase="Running", Reason="", readiness=false. Elapsed: 2.012369278s
Jun  8 15:32:10.953: INFO: Pod "pod-configmaps-52ec7b7a-fbdb-4f7a-81e7-0110434b3a22" satisfied condition "running"
STEP: Waiting for pod with text data 06/08/23 15:32:10.953
STEP: Waiting for pod with binary data 06/08/23 15:32:10.962
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  8 15:32:10.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1434" for this suite. 06/08/23 15:32:10.975
------------------------------
• [2.101 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:32:08.883
    Jun  8 15:32:08.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename configmap 06/08/23 15:32:08.884
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:08.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:08.909
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-10347422-e305-458d-8b0e-604f52cdb261 06/08/23 15:32:08.922
    STEP: Creating the pod 06/08/23 15:32:08.929
    Jun  8 15:32:08.941: INFO: Waiting up to 5m0s for pod "pod-configmaps-52ec7b7a-fbdb-4f7a-81e7-0110434b3a22" in namespace "configmap-1434" to be "running"
    Jun  8 15:32:08.946: INFO: Pod "pod-configmaps-52ec7b7a-fbdb-4f7a-81e7-0110434b3a22": Phase="Pending", Reason="", readiness=false. Elapsed: 5.228246ms
    Jun  8 15:32:10.953: INFO: Pod "pod-configmaps-52ec7b7a-fbdb-4f7a-81e7-0110434b3a22": Phase="Running", Reason="", readiness=false. Elapsed: 2.012369278s
    Jun  8 15:32:10.953: INFO: Pod "pod-configmaps-52ec7b7a-fbdb-4f7a-81e7-0110434b3a22" satisfied condition "running"
    STEP: Waiting for pod with text data 06/08/23 15:32:10.953
    STEP: Waiting for pod with binary data 06/08/23 15:32:10.962
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:32:10.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1434" for this suite. 06/08/23 15:32:10.975
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:32:10.984
Jun  8 15:32:10.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename var-expansion 06/08/23 15:32:10.985
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:11.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:11.006
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Jun  8 15:32:11.027: INFO: Waiting up to 2m0s for pod "var-expansion-6d0b6b29-1fb5-4073-8413-bb4e15baa4b9" in namespace "var-expansion-5292" to be "container 0 failed with reason CreateContainerConfigError"
Jun  8 15:32:11.032: INFO: Pod "var-expansion-6d0b6b29-1fb5-4073-8413-bb4e15baa4b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.305058ms
Jun  8 15:32:13.037: INFO: Pod "var-expansion-6d0b6b29-1fb5-4073-8413-bb4e15baa4b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009264146s
Jun  8 15:32:13.037: INFO: Pod "var-expansion-6d0b6b29-1fb5-4073-8413-bb4e15baa4b9" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jun  8 15:32:13.037: INFO: Deleting pod "var-expansion-6d0b6b29-1fb5-4073-8413-bb4e15baa4b9" in namespace "var-expansion-5292"
Jun  8 15:32:13.044: INFO: Wait up to 5m0s for pod "var-expansion-6d0b6b29-1fb5-4073-8413-bb4e15baa4b9" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun  8 15:32:15.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5292" for this suite. 06/08/23 15:32:15.06
------------------------------
• [4.085 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:32:10.984
    Jun  8 15:32:10.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename var-expansion 06/08/23 15:32:10.985
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:11.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:11.006
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Jun  8 15:32:11.027: INFO: Waiting up to 2m0s for pod "var-expansion-6d0b6b29-1fb5-4073-8413-bb4e15baa4b9" in namespace "var-expansion-5292" to be "container 0 failed with reason CreateContainerConfigError"
    Jun  8 15:32:11.032: INFO: Pod "var-expansion-6d0b6b29-1fb5-4073-8413-bb4e15baa4b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.305058ms
    Jun  8 15:32:13.037: INFO: Pod "var-expansion-6d0b6b29-1fb5-4073-8413-bb4e15baa4b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009264146s
    Jun  8 15:32:13.037: INFO: Pod "var-expansion-6d0b6b29-1fb5-4073-8413-bb4e15baa4b9" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jun  8 15:32:13.037: INFO: Deleting pod "var-expansion-6d0b6b29-1fb5-4073-8413-bb4e15baa4b9" in namespace "var-expansion-5292"
    Jun  8 15:32:13.044: INFO: Wait up to 5m0s for pod "var-expansion-6d0b6b29-1fb5-4073-8413-bb4e15baa4b9" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:32:15.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5292" for this suite. 06/08/23 15:32:15.06
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:32:15.071
Jun  8 15:32:15.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 15:32:15.072
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:15.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:15.092
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 06/08/23 15:32:15.095
Jun  8 15:32:15.105: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92733cf2-ca64-4b36-a4da-61d1fee4304c" in namespace "projected-903" to be "Succeeded or Failed"
Jun  8 15:32:15.109: INFO: Pod "downwardapi-volume-92733cf2-ca64-4b36-a4da-61d1fee4304c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.435084ms
Jun  8 15:32:17.114: INFO: Pod "downwardapi-volume-92733cf2-ca64-4b36-a4da-61d1fee4304c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009227384s
Jun  8 15:32:19.113: INFO: Pod "downwardapi-volume-92733cf2-ca64-4b36-a4da-61d1fee4304c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008041135s
STEP: Saw pod success 06/08/23 15:32:19.113
Jun  8 15:32:19.113: INFO: Pod "downwardapi-volume-92733cf2-ca64-4b36-a4da-61d1fee4304c" satisfied condition "Succeeded or Failed"
Jun  8 15:32:19.117: INFO: Trying to get logs from node chl8tf-worker-002 pod downwardapi-volume-92733cf2-ca64-4b36-a4da-61d1fee4304c container client-container: <nil>
STEP: delete the pod 06/08/23 15:32:19.134
Jun  8 15:32:19.149: INFO: Waiting for pod downwardapi-volume-92733cf2-ca64-4b36-a4da-61d1fee4304c to disappear
Jun  8 15:32:19.152: INFO: Pod downwardapi-volume-92733cf2-ca64-4b36-a4da-61d1fee4304c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  8 15:32:19.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-903" for this suite. 06/08/23 15:32:19.158
------------------------------
• [4.094 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:32:15.071
    Jun  8 15:32:15.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 15:32:15.072
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:15.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:15.092
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 06/08/23 15:32:15.095
    Jun  8 15:32:15.105: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92733cf2-ca64-4b36-a4da-61d1fee4304c" in namespace "projected-903" to be "Succeeded or Failed"
    Jun  8 15:32:15.109: INFO: Pod "downwardapi-volume-92733cf2-ca64-4b36-a4da-61d1fee4304c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.435084ms
    Jun  8 15:32:17.114: INFO: Pod "downwardapi-volume-92733cf2-ca64-4b36-a4da-61d1fee4304c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009227384s
    Jun  8 15:32:19.113: INFO: Pod "downwardapi-volume-92733cf2-ca64-4b36-a4da-61d1fee4304c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008041135s
    STEP: Saw pod success 06/08/23 15:32:19.113
    Jun  8 15:32:19.113: INFO: Pod "downwardapi-volume-92733cf2-ca64-4b36-a4da-61d1fee4304c" satisfied condition "Succeeded or Failed"
    Jun  8 15:32:19.117: INFO: Trying to get logs from node chl8tf-worker-002 pod downwardapi-volume-92733cf2-ca64-4b36-a4da-61d1fee4304c container client-container: <nil>
    STEP: delete the pod 06/08/23 15:32:19.134
    Jun  8 15:32:19.149: INFO: Waiting for pod downwardapi-volume-92733cf2-ca64-4b36-a4da-61d1fee4304c to disappear
    Jun  8 15:32:19.152: INFO: Pod downwardapi-volume-92733cf2-ca64-4b36-a4da-61d1fee4304c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:32:19.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-903" for this suite. 06/08/23 15:32:19.158
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:32:19.166
Jun  8 15:32:19.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubelet-test 06/08/23 15:32:19.167
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:19.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:19.189
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jun  8 15:32:23.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7313" for this suite. 06/08/23 15:32:23.218
------------------------------
• [4.060 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:32:19.166
    Jun  8 15:32:19.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubelet-test 06/08/23 15:32:19.167
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:19.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:19.189
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:32:23.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7313" for this suite. 06/08/23 15:32:23.218
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:32:23.226
Jun  8 15:32:23.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename webhook 06/08/23 15:32:23.228
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:23.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:23.251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/08/23 15:32:23.268
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 15:32:23.584
STEP: Deploying the webhook pod 06/08/23 15:32:23.595
STEP: Wait for the deployment to be ready 06/08/23 15:32:23.609
Jun  8 15:32:23.617: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/08/23 15:32:25.629
STEP: Verifying the service has paired with the endpoint 06/08/23 15:32:25.644
Jun  8 15:32:26.645: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Jun  8 15:32:26.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Registering the custom resource webhook via the AdmissionRegistration API 06/08/23 15:32:27.159
STEP: Creating a custom resource that should be denied by the webhook 06/08/23 15:32:27.175
STEP: Creating a custom resource whose deletion would be denied by the webhook 06/08/23 15:32:29.213
STEP: Updating the custom resource with disallowed data should be denied 06/08/23 15:32:29.221
STEP: Deleting the custom resource should be denied 06/08/23 15:32:29.231
STEP: Remove the offending key and value from the custom resource data 06/08/23 15:32:29.239
STEP: Deleting the updated custom resource should be successful 06/08/23 15:32:29.249
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:32:29.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-676" for this suite. 06/08/23 15:32:29.837
STEP: Destroying namespace "webhook-676-markers" for this suite. 06/08/23 15:32:29.847
------------------------------
• [SLOW TEST] [6.636 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:32:23.226
    Jun  8 15:32:23.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename webhook 06/08/23 15:32:23.228
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:23.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:23.251
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/08/23 15:32:23.268
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 15:32:23.584
    STEP: Deploying the webhook pod 06/08/23 15:32:23.595
    STEP: Wait for the deployment to be ready 06/08/23 15:32:23.609
    Jun  8 15:32:23.617: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/08/23 15:32:25.629
    STEP: Verifying the service has paired with the endpoint 06/08/23 15:32:25.644
    Jun  8 15:32:26.645: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Jun  8 15:32:26.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 06/08/23 15:32:27.159
    STEP: Creating a custom resource that should be denied by the webhook 06/08/23 15:32:27.175
    STEP: Creating a custom resource whose deletion would be denied by the webhook 06/08/23 15:32:29.213
    STEP: Updating the custom resource with disallowed data should be denied 06/08/23 15:32:29.221
    STEP: Deleting the custom resource should be denied 06/08/23 15:32:29.231
    STEP: Remove the offending key and value from the custom resource data 06/08/23 15:32:29.239
    STEP: Deleting the updated custom resource should be successful 06/08/23 15:32:29.249
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:32:29.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-676" for this suite. 06/08/23 15:32:29.837
    STEP: Destroying namespace "webhook-676-markers" for this suite. 06/08/23 15:32:29.847
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:32:29.863
Jun  8 15:32:29.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename job 06/08/23 15:32:29.865
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:29.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:29.892
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 06/08/23 15:32:29.896
STEP: Ensuring job reaches completions 06/08/23 15:32:29.902
STEP: Ensuring pods with index for job exist 06/08/23 15:32:39.907
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jun  8 15:32:39.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2172" for this suite. 06/08/23 15:32:39.918
------------------------------
• [SLOW TEST] [10.062 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:32:29.863
    Jun  8 15:32:29.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename job 06/08/23 15:32:29.865
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:29.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:29.892
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 06/08/23 15:32:29.896
    STEP: Ensuring job reaches completions 06/08/23 15:32:29.902
    STEP: Ensuring pods with index for job exist 06/08/23 15:32:39.907
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:32:39.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2172" for this suite. 06/08/23 15:32:39.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:32:39.928
Jun  8 15:32:39.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename pods 06/08/23 15:32:39.928
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:39.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:39.95
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Jun  8 15:32:39.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: creating the pod 06/08/23 15:32:39.954
STEP: submitting the pod to kubernetes 06/08/23 15:32:39.954
Jun  8 15:32:39.963: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-5d329234-eff9-490c-af70-8bdfff22fbdd" in namespace "pods-8355" to be "running and ready"
Jun  8 15:32:39.968: INFO: Pod "pod-logs-websocket-5d329234-eff9-490c-af70-8bdfff22fbdd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.689842ms
Jun  8 15:32:39.968: INFO: The phase of Pod pod-logs-websocket-5d329234-eff9-490c-af70-8bdfff22fbdd is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:32:41.973: INFO: Pod "pod-logs-websocket-5d329234-eff9-490c-af70-8bdfff22fbdd": Phase="Running", Reason="", readiness=true. Elapsed: 2.009817657s
Jun  8 15:32:41.973: INFO: The phase of Pod pod-logs-websocket-5d329234-eff9-490c-af70-8bdfff22fbdd is Running (Ready = true)
Jun  8 15:32:41.973: INFO: Pod "pod-logs-websocket-5d329234-eff9-490c-af70-8bdfff22fbdd" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun  8 15:32:41.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8355" for this suite. 06/08/23 15:32:42.004
------------------------------
• [2.085 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:32:39.928
    Jun  8 15:32:39.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename pods 06/08/23 15:32:39.928
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:39.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:39.95
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Jun  8 15:32:39.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: creating the pod 06/08/23 15:32:39.954
    STEP: submitting the pod to kubernetes 06/08/23 15:32:39.954
    Jun  8 15:32:39.963: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-5d329234-eff9-490c-af70-8bdfff22fbdd" in namespace "pods-8355" to be "running and ready"
    Jun  8 15:32:39.968: INFO: Pod "pod-logs-websocket-5d329234-eff9-490c-af70-8bdfff22fbdd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.689842ms
    Jun  8 15:32:39.968: INFO: The phase of Pod pod-logs-websocket-5d329234-eff9-490c-af70-8bdfff22fbdd is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:32:41.973: INFO: Pod "pod-logs-websocket-5d329234-eff9-490c-af70-8bdfff22fbdd": Phase="Running", Reason="", readiness=true. Elapsed: 2.009817657s
    Jun  8 15:32:41.973: INFO: The phase of Pod pod-logs-websocket-5d329234-eff9-490c-af70-8bdfff22fbdd is Running (Ready = true)
    Jun  8 15:32:41.973: INFO: Pod "pod-logs-websocket-5d329234-eff9-490c-af70-8bdfff22fbdd" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:32:41.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8355" for this suite. 06/08/23 15:32:42.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:32:42.014
Jun  8 15:32:42.014: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename lease-test 06/08/23 15:32:42.015
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:42.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:42.036
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Jun  8 15:32:42.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-5053" for this suite. 06/08/23 15:32:42.107
------------------------------
• [0.101 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:32:42.014
    Jun  8 15:32:42.014: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename lease-test 06/08/23 15:32:42.015
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:42.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:42.036
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:32:42.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-5053" for this suite. 06/08/23 15:32:42.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:32:42.116
Jun  8 15:32:42.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename watch 06/08/23 15:32:42.117
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:42.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:42.137
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 06/08/23 15:32:42.14
STEP: creating a new configmap 06/08/23 15:32:42.142
STEP: modifying the configmap once 06/08/23 15:32:42.148
STEP: changing the label value of the configmap 06/08/23 15:32:42.156
STEP: Expecting to observe a delete notification for the watched object 06/08/23 15:32:42.165
Jun  8 15:32:42.165: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1207  a7835d9d-8fd9-40da-82a3-21df2da109ea 30654 0 2023-06-08 15:32:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-08 15:32:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  8 15:32:42.165: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1207  a7835d9d-8fd9-40da-82a3-21df2da109ea 30655 0 2023-06-08 15:32:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-08 15:32:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  8 15:32:42.165: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1207  a7835d9d-8fd9-40da-82a3-21df2da109ea 30656 0 2023-06-08 15:32:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-08 15:32:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 06/08/23 15:32:42.165
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 06/08/23 15:32:42.173
STEP: changing the label value of the configmap back 06/08/23 15:32:52.174
STEP: modifying the configmap a third time 06/08/23 15:32:52.185
STEP: deleting the configmap 06/08/23 15:32:52.193
STEP: Expecting to observe an add notification for the watched object when the label value was restored 06/08/23 15:32:52.2
Jun  8 15:32:52.200: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1207  a7835d9d-8fd9-40da-82a3-21df2da109ea 30744 0 2023-06-08 15:32:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-08 15:32:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  8 15:32:52.201: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1207  a7835d9d-8fd9-40da-82a3-21df2da109ea 30745 0 2023-06-08 15:32:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-08 15:32:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  8 15:32:52.201: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1207  a7835d9d-8fd9-40da-82a3-21df2da109ea 30746 0 2023-06-08 15:32:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-08 15:32:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jun  8 15:32:52.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1207" for this suite. 06/08/23 15:32:52.207
------------------------------
• [SLOW TEST] [10.098 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:32:42.116
    Jun  8 15:32:42.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename watch 06/08/23 15:32:42.117
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:42.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:42.137
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 06/08/23 15:32:42.14
    STEP: creating a new configmap 06/08/23 15:32:42.142
    STEP: modifying the configmap once 06/08/23 15:32:42.148
    STEP: changing the label value of the configmap 06/08/23 15:32:42.156
    STEP: Expecting to observe a delete notification for the watched object 06/08/23 15:32:42.165
    Jun  8 15:32:42.165: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1207  a7835d9d-8fd9-40da-82a3-21df2da109ea 30654 0 2023-06-08 15:32:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-08 15:32:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  8 15:32:42.165: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1207  a7835d9d-8fd9-40da-82a3-21df2da109ea 30655 0 2023-06-08 15:32:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-08 15:32:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  8 15:32:42.165: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1207  a7835d9d-8fd9-40da-82a3-21df2da109ea 30656 0 2023-06-08 15:32:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-08 15:32:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 06/08/23 15:32:42.165
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 06/08/23 15:32:42.173
    STEP: changing the label value of the configmap back 06/08/23 15:32:52.174
    STEP: modifying the configmap a third time 06/08/23 15:32:52.185
    STEP: deleting the configmap 06/08/23 15:32:52.193
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 06/08/23 15:32:52.2
    Jun  8 15:32:52.200: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1207  a7835d9d-8fd9-40da-82a3-21df2da109ea 30744 0 2023-06-08 15:32:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-08 15:32:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  8 15:32:52.201: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1207  a7835d9d-8fd9-40da-82a3-21df2da109ea 30745 0 2023-06-08 15:32:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-08 15:32:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  8 15:32:52.201: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1207  a7835d9d-8fd9-40da-82a3-21df2da109ea 30746 0 2023-06-08 15:32:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-08 15:32:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:32:52.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1207" for this suite. 06/08/23 15:32:52.207
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:32:52.215
Jun  8 15:32:52.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename subpath 06/08/23 15:32:52.216
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:52.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:52.236
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/08/23 15:32:52.24
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-fkz9 06/08/23 15:32:52.25
STEP: Creating a pod to test atomic-volume-subpath 06/08/23 15:32:52.25
Jun  8 15:32:52.259: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fkz9" in namespace "subpath-2198" to be "Succeeded or Failed"
Jun  8 15:32:52.262: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.664851ms
Jun  8 15:32:54.268: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009249029s
Jun  8 15:32:56.269: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=true. Elapsed: 4.009875814s
Jun  8 15:32:58.268: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=true. Elapsed: 6.009508682s
Jun  8 15:33:00.268: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=true. Elapsed: 8.009067603s
Jun  8 15:33:02.268: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=true. Elapsed: 10.009574482s
Jun  8 15:33:04.268: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=true. Elapsed: 12.009427907s
Jun  8 15:33:06.267: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=true. Elapsed: 14.00863368s
Jun  8 15:33:08.268: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=true. Elapsed: 16.009715069s
Jun  8 15:33:10.267: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=true. Elapsed: 18.008870984s
Jun  8 15:33:12.268: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=true. Elapsed: 20.009775978s
Jun  8 15:33:14.267: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=false. Elapsed: 22.008547838s
Jun  8 15:33:16.267: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00818018s
STEP: Saw pod success 06/08/23 15:33:16.267
Jun  8 15:33:16.267: INFO: Pod "pod-subpath-test-downwardapi-fkz9" satisfied condition "Succeeded or Failed"
Jun  8 15:33:16.271: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-subpath-test-downwardapi-fkz9 container test-container-subpath-downwardapi-fkz9: <nil>
STEP: delete the pod 06/08/23 15:33:16.28
Jun  8 15:33:16.295: INFO: Waiting for pod pod-subpath-test-downwardapi-fkz9 to disappear
Jun  8 15:33:16.298: INFO: Pod pod-subpath-test-downwardapi-fkz9 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-fkz9 06/08/23 15:33:16.298
Jun  8 15:33:16.298: INFO: Deleting pod "pod-subpath-test-downwardapi-fkz9" in namespace "subpath-2198"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jun  8 15:33:16.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2198" for this suite. 06/08/23 15:33:16.307
------------------------------
• [SLOW TEST] [24.098 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:32:52.215
    Jun  8 15:32:52.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename subpath 06/08/23 15:32:52.216
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:32:52.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:32:52.236
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/08/23 15:32:52.24
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-fkz9 06/08/23 15:32:52.25
    STEP: Creating a pod to test atomic-volume-subpath 06/08/23 15:32:52.25
    Jun  8 15:32:52.259: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fkz9" in namespace "subpath-2198" to be "Succeeded or Failed"
    Jun  8 15:32:52.262: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.664851ms
    Jun  8 15:32:54.268: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009249029s
    Jun  8 15:32:56.269: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=true. Elapsed: 4.009875814s
    Jun  8 15:32:58.268: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=true. Elapsed: 6.009508682s
    Jun  8 15:33:00.268: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=true. Elapsed: 8.009067603s
    Jun  8 15:33:02.268: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=true. Elapsed: 10.009574482s
    Jun  8 15:33:04.268: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=true. Elapsed: 12.009427907s
    Jun  8 15:33:06.267: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=true. Elapsed: 14.00863368s
    Jun  8 15:33:08.268: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=true. Elapsed: 16.009715069s
    Jun  8 15:33:10.267: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=true. Elapsed: 18.008870984s
    Jun  8 15:33:12.268: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=true. Elapsed: 20.009775978s
    Jun  8 15:33:14.267: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Running", Reason="", readiness=false. Elapsed: 22.008547838s
    Jun  8 15:33:16.267: INFO: Pod "pod-subpath-test-downwardapi-fkz9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00818018s
    STEP: Saw pod success 06/08/23 15:33:16.267
    Jun  8 15:33:16.267: INFO: Pod "pod-subpath-test-downwardapi-fkz9" satisfied condition "Succeeded or Failed"
    Jun  8 15:33:16.271: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-subpath-test-downwardapi-fkz9 container test-container-subpath-downwardapi-fkz9: <nil>
    STEP: delete the pod 06/08/23 15:33:16.28
    Jun  8 15:33:16.295: INFO: Waiting for pod pod-subpath-test-downwardapi-fkz9 to disappear
    Jun  8 15:33:16.298: INFO: Pod pod-subpath-test-downwardapi-fkz9 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-fkz9 06/08/23 15:33:16.298
    Jun  8 15:33:16.298: INFO: Deleting pod "pod-subpath-test-downwardapi-fkz9" in namespace "subpath-2198"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:33:16.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2198" for this suite. 06/08/23 15:33:16.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:33:16.315
Jun  8 15:33:16.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename disruption 06/08/23 15:33:16.316
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:33:16.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:33:16.337
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:33:16.341
Jun  8 15:33:16.341: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename disruption-2 06/08/23 15:33:16.342
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:33:16.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:33:16.362
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 06/08/23 15:33:16.371
STEP: Waiting for the pdb to be processed 06/08/23 15:33:18.386
STEP: Waiting for the pdb to be processed 06/08/23 15:33:20.402
STEP: listing a collection of PDBs across all namespaces 06/08/23 15:33:22.411
STEP: listing a collection of PDBs in namespace disruption-3657 06/08/23 15:33:22.415
STEP: deleting a collection of PDBs 06/08/23 15:33:22.419
STEP: Waiting for the PDB collection to be deleted 06/08/23 15:33:22.434
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Jun  8 15:33:22.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jun  8 15:33:22.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-9887" for this suite. 06/08/23 15:33:22.449
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-3657" for this suite. 06/08/23 15:33:22.456
------------------------------
• [SLOW TEST] [6.147 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:33:16.315
    Jun  8 15:33:16.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename disruption 06/08/23 15:33:16.316
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:33:16.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:33:16.337
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:33:16.341
    Jun  8 15:33:16.341: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename disruption-2 06/08/23 15:33:16.342
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:33:16.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:33:16.362
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 06/08/23 15:33:16.371
    STEP: Waiting for the pdb to be processed 06/08/23 15:33:18.386
    STEP: Waiting for the pdb to be processed 06/08/23 15:33:20.402
    STEP: listing a collection of PDBs across all namespaces 06/08/23 15:33:22.411
    STEP: listing a collection of PDBs in namespace disruption-3657 06/08/23 15:33:22.415
    STEP: deleting a collection of PDBs 06/08/23 15:33:22.419
    STEP: Waiting for the PDB collection to be deleted 06/08/23 15:33:22.434
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:33:22.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:33:22.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-9887" for this suite. 06/08/23 15:33:22.449
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-3657" for this suite. 06/08/23 15:33:22.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:33:22.463
Jun  8 15:33:22.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename svcaccounts 06/08/23 15:33:22.465
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:33:22.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:33:22.486
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 06/08/23 15:33:22.49
STEP: watching for the ServiceAccount to be added 06/08/23 15:33:22.499
STEP: patching the ServiceAccount 06/08/23 15:33:22.5
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 06/08/23 15:33:22.508
STEP: deleting the ServiceAccount 06/08/23 15:33:22.512
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jun  8 15:33:22.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5946" for this suite. 06/08/23 15:33:22.531
------------------------------
• [0.079 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:33:22.463
    Jun  8 15:33:22.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename svcaccounts 06/08/23 15:33:22.465
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:33:22.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:33:22.486
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 06/08/23 15:33:22.49
    STEP: watching for the ServiceAccount to be added 06/08/23 15:33:22.499
    STEP: patching the ServiceAccount 06/08/23 15:33:22.5
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 06/08/23 15:33:22.508
    STEP: deleting the ServiceAccount 06/08/23 15:33:22.512
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:33:22.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5946" for this suite. 06/08/23 15:33:22.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:33:22.545
Jun  8 15:33:22.545: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename daemonsets 06/08/23 15:33:22.546
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:33:22.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:33:22.57
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 06/08/23 15:33:22.602
STEP: Check that daemon pods launch on every node of the cluster. 06/08/23 15:33:22.609
Jun  8 15:33:22.617: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  8 15:33:22.617: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
Jun  8 15:33:23.629: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  8 15:33:23.629: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
Jun  8 15:33:24.629: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jun  8 15:33:24.629: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 06/08/23 15:33:24.633
Jun  8 15:33:24.658: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jun  8 15:33:24.658: INFO: Node chl8tf-control-plane-003 is running 0 daemon pod, expected 1
Jun  8 15:33:25.670: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jun  8 15:33:25.670: INFO: Node chl8tf-control-plane-003 is running 0 daemon pod, expected 1
Jun  8 15:33:26.670: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jun  8 15:33:26.670: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 06/08/23 15:33:26.67
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 06/08/23 15:33:26.677
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4886, will wait for the garbage collector to delete the pods 06/08/23 15:33:26.677
Jun  8 15:33:26.740: INFO: Deleting DaemonSet.extensions daemon-set took: 8.275037ms
Jun  8 15:33:26.841: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.411709ms
Jun  8 15:33:28.846: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  8 15:33:28.846: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun  8 15:33:28.850: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31075"},"items":null}

Jun  8 15:33:28.853: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31075"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:33:28.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4886" for this suite. 06/08/23 15:33:28.883
------------------------------
• [SLOW TEST] [6.348 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:33:22.545
    Jun  8 15:33:22.545: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename daemonsets 06/08/23 15:33:22.546
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:33:22.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:33:22.57
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 06/08/23 15:33:22.602
    STEP: Check that daemon pods launch on every node of the cluster. 06/08/23 15:33:22.609
    Jun  8 15:33:22.617: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  8 15:33:22.617: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
    Jun  8 15:33:23.629: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  8 15:33:23.629: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
    Jun  8 15:33:24.629: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jun  8 15:33:24.629: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 06/08/23 15:33:24.633
    Jun  8 15:33:24.658: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jun  8 15:33:24.658: INFO: Node chl8tf-control-plane-003 is running 0 daemon pod, expected 1
    Jun  8 15:33:25.670: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jun  8 15:33:25.670: INFO: Node chl8tf-control-plane-003 is running 0 daemon pod, expected 1
    Jun  8 15:33:26.670: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jun  8 15:33:26.670: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 06/08/23 15:33:26.67
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 06/08/23 15:33:26.677
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4886, will wait for the garbage collector to delete the pods 06/08/23 15:33:26.677
    Jun  8 15:33:26.740: INFO: Deleting DaemonSet.extensions daemon-set took: 8.275037ms
    Jun  8 15:33:26.841: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.411709ms
    Jun  8 15:33:28.846: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  8 15:33:28.846: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun  8 15:33:28.850: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31075"},"items":null}

    Jun  8 15:33:28.853: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31075"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:33:28.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4886" for this suite. 06/08/23 15:33:28.883
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:33:28.894
Jun  8 15:33:28.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename container-probe 06/08/23 15:33:28.895
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:33:28.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:33:28.916
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-af70fb33-f0b8-4ade-b78d-79a6a846e75b in namespace container-probe-2526 06/08/23 15:33:28.919
Jun  8 15:33:28.928: INFO: Waiting up to 5m0s for pod "liveness-af70fb33-f0b8-4ade-b78d-79a6a846e75b" in namespace "container-probe-2526" to be "not pending"
Jun  8 15:33:28.931: INFO: Pod "liveness-af70fb33-f0b8-4ade-b78d-79a6a846e75b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.358581ms
Jun  8 15:33:30.937: INFO: Pod "liveness-af70fb33-f0b8-4ade-b78d-79a6a846e75b": Phase="Running", Reason="", readiness=true. Elapsed: 2.008894937s
Jun  8 15:33:30.937: INFO: Pod "liveness-af70fb33-f0b8-4ade-b78d-79a6a846e75b" satisfied condition "not pending"
Jun  8 15:33:30.937: INFO: Started pod liveness-af70fb33-f0b8-4ade-b78d-79a6a846e75b in namespace container-probe-2526
STEP: checking the pod's current state and verifying that restartCount is present 06/08/23 15:33:30.937
Jun  8 15:33:30.941: INFO: Initial restart count of pod liveness-af70fb33-f0b8-4ade-b78d-79a6a846e75b is 0
STEP: deleting the pod 06/08/23 15:37:31.594
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun  8 15:37:31.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2526" for this suite. 06/08/23 15:37:31.615
------------------------------
• [SLOW TEST] [242.728 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:33:28.894
    Jun  8 15:33:28.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename container-probe 06/08/23 15:33:28.895
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:33:28.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:33:28.916
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-af70fb33-f0b8-4ade-b78d-79a6a846e75b in namespace container-probe-2526 06/08/23 15:33:28.919
    Jun  8 15:33:28.928: INFO: Waiting up to 5m0s for pod "liveness-af70fb33-f0b8-4ade-b78d-79a6a846e75b" in namespace "container-probe-2526" to be "not pending"
    Jun  8 15:33:28.931: INFO: Pod "liveness-af70fb33-f0b8-4ade-b78d-79a6a846e75b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.358581ms
    Jun  8 15:33:30.937: INFO: Pod "liveness-af70fb33-f0b8-4ade-b78d-79a6a846e75b": Phase="Running", Reason="", readiness=true. Elapsed: 2.008894937s
    Jun  8 15:33:30.937: INFO: Pod "liveness-af70fb33-f0b8-4ade-b78d-79a6a846e75b" satisfied condition "not pending"
    Jun  8 15:33:30.937: INFO: Started pod liveness-af70fb33-f0b8-4ade-b78d-79a6a846e75b in namespace container-probe-2526
    STEP: checking the pod's current state and verifying that restartCount is present 06/08/23 15:33:30.937
    Jun  8 15:33:30.941: INFO: Initial restart count of pod liveness-af70fb33-f0b8-4ade-b78d-79a6a846e75b is 0
    STEP: deleting the pod 06/08/23 15:37:31.594
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:37:31.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2526" for this suite. 06/08/23 15:37:31.615
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:37:31.622
Jun  8 15:37:31.622: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename pod-network-test 06/08/23 15:37:31.624
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:37:31.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:37:31.646
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-6250 06/08/23 15:37:31.649
STEP: creating a selector 06/08/23 15:37:31.649
STEP: Creating the service pods in kubernetes 06/08/23 15:37:31.649
Jun  8 15:37:31.649: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun  8 15:37:31.708: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6250" to be "running and ready"
Jun  8 15:37:31.715: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.541033ms
Jun  8 15:37:31.715: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:37:33.720: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01161264s
Jun  8 15:37:33.720: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 15:37:35.721: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.012425224s
Jun  8 15:37:35.721: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 15:37:37.720: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011399991s
Jun  8 15:37:37.720: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 15:37:39.719: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010782074s
Jun  8 15:37:39.719: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 15:37:41.721: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01268808s
Jun  8 15:37:41.721: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 15:37:43.722: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.014113188s
Jun  8 15:37:43.723: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun  8 15:37:43.723: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun  8 15:37:43.727: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6250" to be "running and ready"
Jun  8 15:37:43.731: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.846517ms
Jun  8 15:37:43.731: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun  8 15:37:43.731: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun  8 15:37:43.735: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6250" to be "running and ready"
Jun  8 15:37:43.739: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.812225ms
Jun  8 15:37:43.739: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun  8 15:37:43.739: INFO: Pod "netserver-2" satisfied condition "running and ready"
Jun  8 15:37:43.743: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-6250" to be "running and ready"
Jun  8 15:37:43.747: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.146697ms
Jun  8 15:37:43.747: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Jun  8 15:37:43.747: INFO: Pod "netserver-3" satisfied condition "running and ready"
Jun  8 15:37:43.751: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-6250" to be "running and ready"
Jun  8 15:37:43.755: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 4.178131ms
Jun  8 15:37:43.755: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Jun  8 15:37:43.755: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 06/08/23 15:37:43.759
Jun  8 15:37:43.766: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6250" to be "running"
Jun  8 15:37:43.771: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.849245ms
Jun  8 15:37:45.778: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012402724s
Jun  8 15:37:45.778: INFO: Pod "test-container-pod" satisfied condition "running"
Jun  8 15:37:45.784: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Jun  8 15:37:45.784: INFO: Breadth first check of 10.244.0.36 on host 100.100.237.165...
Jun  8 15:37:45.787: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.138:9080/dial?request=hostname&protocol=udp&host=10.244.0.36&port=8081&tries=1'] Namespace:pod-network-test-6250 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:37:45.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:37:45.788: INFO: ExecWithOptions: Clientset creation
Jun  8 15:37:45.788: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6250/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.138%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.0.36%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun  8 15:37:45.892: INFO: Waiting for responses: map[]
Jun  8 15:37:45.892: INFO: reached 10.244.0.36 after 0/1 tries
Jun  8 15:37:45.892: INFO: Breadth first check of 10.244.1.41 on host 100.100.236.41...
Jun  8 15:37:45.897: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.138:9080/dial?request=hostname&protocol=udp&host=10.244.1.41&port=8081&tries=1'] Namespace:pod-network-test-6250 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:37:45.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:37:45.898: INFO: ExecWithOptions: Clientset creation
Jun  8 15:37:45.898: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6250/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.138%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.41%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun  8 15:37:46.000: INFO: Waiting for responses: map[]
Jun  8 15:37:46.000: INFO: reached 10.244.1.41 after 0/1 tries
Jun  8 15:37:46.000: INFO: Breadth first check of 10.244.2.29 on host 100.100.237.235...
Jun  8 15:37:46.005: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.138:9080/dial?request=hostname&protocol=udp&host=10.244.2.29&port=8081&tries=1'] Namespace:pod-network-test-6250 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:37:46.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:37:46.006: INFO: ExecWithOptions: Clientset creation
Jun  8 15:37:46.006: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6250/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.138%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.2.29%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun  8 15:37:46.090: INFO: Waiting for responses: map[]
Jun  8 15:37:46.090: INFO: reached 10.244.2.29 after 0/1 tries
Jun  8 15:37:46.090: INFO: Breadth first check of 10.244.3.137 on host 100.100.236.215...
Jun  8 15:37:46.094: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.138:9080/dial?request=hostname&protocol=udp&host=10.244.3.137&port=8081&tries=1'] Namespace:pod-network-test-6250 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:37:46.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:37:46.095: INFO: ExecWithOptions: Clientset creation
Jun  8 15:37:46.095: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6250/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.138%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.3.137%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun  8 15:37:46.180: INFO: Waiting for responses: map[]
Jun  8 15:37:46.180: INFO: reached 10.244.3.137 after 0/1 tries
Jun  8 15:37:46.180: INFO: Breadth first check of 10.244.4.59 on host 100.100.237.90...
Jun  8 15:37:46.185: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.138:9080/dial?request=hostname&protocol=udp&host=10.244.4.59&port=8081&tries=1'] Namespace:pod-network-test-6250 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:37:46.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:37:46.185: INFO: ExecWithOptions: Clientset creation
Jun  8 15:37:46.185: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6250/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.138%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.4.59%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun  8 15:37:46.281: INFO: Waiting for responses: map[]
Jun  8 15:37:46.281: INFO: reached 10.244.4.59 after 0/1 tries
Jun  8 15:37:46.281: INFO: Going to retry 0 out of 5 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jun  8 15:37:46.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-6250" for this suite. 06/08/23 15:37:46.288
------------------------------
• [SLOW TEST] [14.674 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:37:31.622
    Jun  8 15:37:31.622: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename pod-network-test 06/08/23 15:37:31.624
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:37:31.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:37:31.646
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-6250 06/08/23 15:37:31.649
    STEP: creating a selector 06/08/23 15:37:31.649
    STEP: Creating the service pods in kubernetes 06/08/23 15:37:31.649
    Jun  8 15:37:31.649: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jun  8 15:37:31.708: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6250" to be "running and ready"
    Jun  8 15:37:31.715: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.541033ms
    Jun  8 15:37:31.715: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:37:33.720: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01161264s
    Jun  8 15:37:33.720: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 15:37:35.721: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.012425224s
    Jun  8 15:37:35.721: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 15:37:37.720: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011399991s
    Jun  8 15:37:37.720: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 15:37:39.719: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010782074s
    Jun  8 15:37:39.719: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 15:37:41.721: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01268808s
    Jun  8 15:37:41.721: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 15:37:43.722: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.014113188s
    Jun  8 15:37:43.723: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun  8 15:37:43.723: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun  8 15:37:43.727: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6250" to be "running and ready"
    Jun  8 15:37:43.731: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.846517ms
    Jun  8 15:37:43.731: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun  8 15:37:43.731: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun  8 15:37:43.735: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6250" to be "running and ready"
    Jun  8 15:37:43.739: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.812225ms
    Jun  8 15:37:43.739: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun  8 15:37:43.739: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Jun  8 15:37:43.743: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-6250" to be "running and ready"
    Jun  8 15:37:43.747: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.146697ms
    Jun  8 15:37:43.747: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Jun  8 15:37:43.747: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Jun  8 15:37:43.751: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-6250" to be "running and ready"
    Jun  8 15:37:43.755: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 4.178131ms
    Jun  8 15:37:43.755: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Jun  8 15:37:43.755: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 06/08/23 15:37:43.759
    Jun  8 15:37:43.766: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6250" to be "running"
    Jun  8 15:37:43.771: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.849245ms
    Jun  8 15:37:45.778: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012402724s
    Jun  8 15:37:45.778: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun  8 15:37:45.784: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Jun  8 15:37:45.784: INFO: Breadth first check of 10.244.0.36 on host 100.100.237.165...
    Jun  8 15:37:45.787: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.138:9080/dial?request=hostname&protocol=udp&host=10.244.0.36&port=8081&tries=1'] Namespace:pod-network-test-6250 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:37:45.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:37:45.788: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:37:45.788: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6250/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.138%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.0.36%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun  8 15:37:45.892: INFO: Waiting for responses: map[]
    Jun  8 15:37:45.892: INFO: reached 10.244.0.36 after 0/1 tries
    Jun  8 15:37:45.892: INFO: Breadth first check of 10.244.1.41 on host 100.100.236.41...
    Jun  8 15:37:45.897: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.138:9080/dial?request=hostname&protocol=udp&host=10.244.1.41&port=8081&tries=1'] Namespace:pod-network-test-6250 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:37:45.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:37:45.898: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:37:45.898: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6250/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.138%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.41%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun  8 15:37:46.000: INFO: Waiting for responses: map[]
    Jun  8 15:37:46.000: INFO: reached 10.244.1.41 after 0/1 tries
    Jun  8 15:37:46.000: INFO: Breadth first check of 10.244.2.29 on host 100.100.237.235...
    Jun  8 15:37:46.005: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.138:9080/dial?request=hostname&protocol=udp&host=10.244.2.29&port=8081&tries=1'] Namespace:pod-network-test-6250 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:37:46.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:37:46.006: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:37:46.006: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6250/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.138%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.2.29%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun  8 15:37:46.090: INFO: Waiting for responses: map[]
    Jun  8 15:37:46.090: INFO: reached 10.244.2.29 after 0/1 tries
    Jun  8 15:37:46.090: INFO: Breadth first check of 10.244.3.137 on host 100.100.236.215...
    Jun  8 15:37:46.094: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.138:9080/dial?request=hostname&protocol=udp&host=10.244.3.137&port=8081&tries=1'] Namespace:pod-network-test-6250 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:37:46.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:37:46.095: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:37:46.095: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6250/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.138%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.3.137%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun  8 15:37:46.180: INFO: Waiting for responses: map[]
    Jun  8 15:37:46.180: INFO: reached 10.244.3.137 after 0/1 tries
    Jun  8 15:37:46.180: INFO: Breadth first check of 10.244.4.59 on host 100.100.237.90...
    Jun  8 15:37:46.185: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.138:9080/dial?request=hostname&protocol=udp&host=10.244.4.59&port=8081&tries=1'] Namespace:pod-network-test-6250 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:37:46.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:37:46.185: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:37:46.185: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6250/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.138%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.4.59%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun  8 15:37:46.281: INFO: Waiting for responses: map[]
    Jun  8 15:37:46.281: INFO: reached 10.244.4.59 after 0/1 tries
    Jun  8 15:37:46.281: INFO: Going to retry 0 out of 5 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:37:46.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-6250" for this suite. 06/08/23 15:37:46.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:37:46.298
Jun  8 15:37:46.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename containers 06/08/23 15:37:46.299
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:37:46.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:37:46.322
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 06/08/23 15:37:46.326
Jun  8 15:37:46.337: INFO: Waiting up to 5m0s for pod "client-containers-7611439d-0ade-409d-834e-24fb0821c790" in namespace "containers-1232" to be "Succeeded or Failed"
Jun  8 15:37:46.342: INFO: Pod "client-containers-7611439d-0ade-409d-834e-24fb0821c790": Phase="Pending", Reason="", readiness=false. Elapsed: 5.020497ms
Jun  8 15:37:48.346: INFO: Pod "client-containers-7611439d-0ade-409d-834e-24fb0821c790": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009833603s
Jun  8 15:37:50.347: INFO: Pod "client-containers-7611439d-0ade-409d-834e-24fb0821c790": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010661518s
STEP: Saw pod success 06/08/23 15:37:50.347
Jun  8 15:37:50.347: INFO: Pod "client-containers-7611439d-0ade-409d-834e-24fb0821c790" satisfied condition "Succeeded or Failed"
Jun  8 15:37:50.351: INFO: Trying to get logs from node chl8tf-worker-001 pod client-containers-7611439d-0ade-409d-834e-24fb0821c790 container agnhost-container: <nil>
STEP: delete the pod 06/08/23 15:37:50.366
Jun  8 15:37:50.381: INFO: Waiting for pod client-containers-7611439d-0ade-409d-834e-24fb0821c790 to disappear
Jun  8 15:37:50.386: INFO: Pod client-containers-7611439d-0ade-409d-834e-24fb0821c790 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jun  8 15:37:50.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1232" for this suite. 06/08/23 15:37:50.392
------------------------------
• [4.101 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:37:46.298
    Jun  8 15:37:46.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename containers 06/08/23 15:37:46.299
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:37:46.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:37:46.322
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 06/08/23 15:37:46.326
    Jun  8 15:37:46.337: INFO: Waiting up to 5m0s for pod "client-containers-7611439d-0ade-409d-834e-24fb0821c790" in namespace "containers-1232" to be "Succeeded or Failed"
    Jun  8 15:37:46.342: INFO: Pod "client-containers-7611439d-0ade-409d-834e-24fb0821c790": Phase="Pending", Reason="", readiness=false. Elapsed: 5.020497ms
    Jun  8 15:37:48.346: INFO: Pod "client-containers-7611439d-0ade-409d-834e-24fb0821c790": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009833603s
    Jun  8 15:37:50.347: INFO: Pod "client-containers-7611439d-0ade-409d-834e-24fb0821c790": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010661518s
    STEP: Saw pod success 06/08/23 15:37:50.347
    Jun  8 15:37:50.347: INFO: Pod "client-containers-7611439d-0ade-409d-834e-24fb0821c790" satisfied condition "Succeeded or Failed"
    Jun  8 15:37:50.351: INFO: Trying to get logs from node chl8tf-worker-001 pod client-containers-7611439d-0ade-409d-834e-24fb0821c790 container agnhost-container: <nil>
    STEP: delete the pod 06/08/23 15:37:50.366
    Jun  8 15:37:50.381: INFO: Waiting for pod client-containers-7611439d-0ade-409d-834e-24fb0821c790 to disappear
    Jun  8 15:37:50.386: INFO: Pod client-containers-7611439d-0ade-409d-834e-24fb0821c790 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:37:50.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1232" for this suite. 06/08/23 15:37:50.392
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:37:50.4
Jun  8 15:37:50.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename sched-pred 06/08/23 15:37:50.401
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:37:50.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:37:50.421
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jun  8 15:37:50.424: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  8 15:37:50.435: INFO: Waiting for terminating namespaces to be deleted...
Jun  8 15:37:50.439: INFO: 
Logging pods the apiserver thinks is on node chl8tf-control-plane-001 before test
Jun  8 15:37:50.449: INFO: csi-oci-node-5p7f5 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
Jun  8 15:37:50.449: INFO: 	Container csi-node-registrar ready: true, restart count 0
Jun  8 15:37:50.449: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
Jun  8 15:37:50.449: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
Jun  8 15:37:50.449: INFO: etcd-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:04 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.449: INFO: 	Container etcd ready: true, restart count 0
Jun  8 15:37:50.449: INFO: kube-apiserver-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:21 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.449: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun  8 15:37:50.449: INFO: kube-controller-manager-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:21 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.449: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun  8 15:37:50.449: INFO: kube-flannel-ds-d5bvw from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.450: INFO: 	Container kube-flannel ready: true, restart count 0
Jun  8 15:37:50.450: INFO: kube-proxy-j2p7z from kube-system started at 2023-06-08 13:31:27 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.450: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  8 15:37:50.450: INFO: kube-scheduler-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:04 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.450: INFO: 	Container kube-scheduler ready: true, restart count 1
Jun  8 15:37:50.450: INFO: oci-cloud-controller-manager-9n2zj from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.450: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
Jun  8 15:37:50.450: INFO: netserver-0 from pod-network-test-6250 started at 2023-06-08 15:37:31 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.450: INFO: 	Container webserver ready: true, restart count 0
Jun  8 15:37:50.450: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-wmj9x from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 15:37:50.450: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 15:37:50.450: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  8 15:37:50.450: INFO: 
Logging pods the apiserver thinks is on node chl8tf-control-plane-002 before test
Jun  8 15:37:50.460: INFO: coredns-55b8ccd764-56hlv from kube-system started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.460: INFO: 	Container coredns ready: true, restart count 0
Jun  8 15:37:50.460: INFO: coredns-55b8ccd764-jpqkc from kube-system started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.460: INFO: 	Container coredns ready: true, restart count 0
Jun  8 15:37:50.460: INFO: csi-oci-node-xbn7q from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
Jun  8 15:37:50.460: INFO: 	Container csi-node-registrar ready: true, restart count 0
Jun  8 15:37:50.460: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
Jun  8 15:37:50.460: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
Jun  8 15:37:50.460: INFO: etcd-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.460: INFO: 	Container etcd ready: true, restart count 0
Jun  8 15:37:50.460: INFO: kube-apiserver-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:10 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.460: INFO: 	Container kube-apiserver ready: true, restart count 1
Jun  8 15:37:50.460: INFO: kube-controller-manager-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.460: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun  8 15:37:50.460: INFO: kube-flannel-ds-6fgbp from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.460: INFO: 	Container kube-flannel ready: true, restart count 0
Jun  8 15:37:50.460: INFO: kube-proxy-cgwfj from kube-system started at 2023-06-08 13:32:05 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.460: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  8 15:37:50.460: INFO: kube-scheduler-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.460: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun  8 15:37:50.460: INFO: oci-cloud-controller-manager-dpkmx from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.460: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
Jun  8 15:37:50.460: INFO: kubernetes-dashboard-8c85c4f9-9l4pq from kubernetes-dashboard started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.460: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jun  8 15:37:50.460: INFO: netserver-1 from pod-network-test-6250 started at 2023-06-08 15:37:31 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.460: INFO: 	Container webserver ready: true, restart count 0
Jun  8 15:37:50.460: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-74mph from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 15:37:50.460: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 15:37:50.460: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  8 15:37:50.460: INFO: 
Logging pods the apiserver thinks is on node chl8tf-control-plane-003 before test
Jun  8 15:37:50.471: INFO: csi-oci-node-6zdfs from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
Jun  8 15:37:50.471: INFO: 	Container csi-node-registrar ready: true, restart count 0
Jun  8 15:37:50.471: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
Jun  8 15:37:50.471: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
Jun  8 15:37:50.471: INFO: etcd-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.471: INFO: 	Container etcd ready: true, restart count 0
Jun  8 15:37:50.471: INFO: kube-apiserver-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.471: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun  8 15:37:50.471: INFO: kube-controller-manager-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.471: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun  8 15:37:50.471: INFO: kube-flannel-ds-qf6c6 from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.471: INFO: 	Container kube-flannel ready: true, restart count 0
Jun  8 15:37:50.471: INFO: kube-proxy-9kw5t from kube-system started at 2023-06-08 13:33:04 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.471: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  8 15:37:50.471: INFO: kube-scheduler-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.471: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun  8 15:37:50.471: INFO: oci-cloud-controller-manager-2sdzm from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.471: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
Jun  8 15:37:50.471: INFO: netserver-2 from pod-network-test-6250 started at 2023-06-08 15:37:31 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.471: INFO: 	Container webserver ready: true, restart count 0
Jun  8 15:37:50.471: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-jpq4c from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 15:37:50.471: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 15:37:50.471: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  8 15:37:50.471: INFO: 
Logging pods the apiserver thinks is on node chl8tf-worker-001 before test
Jun  8 15:37:50.480: INFO: csi-oci-node-7ww4x from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
Jun  8 15:37:50.480: INFO: 	Container csi-node-registrar ready: true, restart count 0
Jun  8 15:37:50.480: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
Jun  8 15:37:50.480: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
Jun  8 15:37:50.480: INFO: kube-flannel-ds-s65xh from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.480: INFO: 	Container kube-flannel ready: true, restart count 0
Jun  8 15:37:50.480: INFO: kube-proxy-6kfv2 from kube-system started at 2023-06-08 13:33:27 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.480: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  8 15:37:50.480: INFO: oci-cloud-controller-manager-dttpw from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.480: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 1
Jun  8 15:37:50.480: INFO: netserver-3 from pod-network-test-6250 started at 2023-06-08 15:37:31 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.480: INFO: 	Container webserver ready: true, restart count 0
Jun  8 15:37:50.480: INFO: test-container-pod from pod-network-test-6250 started at 2023-06-08 15:37:43 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.480: INFO: 	Container webserver ready: true, restart count 0
Jun  8 15:37:50.480: INFO: sonobuoy from sonobuoy started at 2023-06-08 15:05:13 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.480: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  8 15:37:50.480: INFO: sonobuoy-e2e-job-e329b7fb80aa4b40 from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 15:37:50.480: INFO: 	Container e2e ready: true, restart count 0
Jun  8 15:37:50.480: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 15:37:50.480: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-rs4qz from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 15:37:50.480: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 15:37:50.480: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  8 15:37:50.480: INFO: 
Logging pods the apiserver thinks is on node chl8tf-worker-002 before test
Jun  8 15:37:50.491: INFO: csi-oci-controller-69f8b488fc-n8th6 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (5 container statuses recorded)
Jun  8 15:37:50.491: INFO: 	Container csi-attacher ready: true, restart count 1
Jun  8 15:37:50.491: INFO: 	Container csi-fss-volume-provisioner ready: true, restart count 1
Jun  8 15:37:50.491: INFO: 	Container csi-resizer ready: true, restart count 0
Jun  8 15:37:50.491: INFO: 	Container csi-volume-provisioner ready: true, restart count 0
Jun  8 15:37:50.491: INFO: 	Container oci-csi-controller-driver ready: true, restart count 0
Jun  8 15:37:50.491: INFO: csi-oci-node-thcvn from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
Jun  8 15:37:50.491: INFO: 	Container csi-node-registrar ready: true, restart count 0
Jun  8 15:37:50.491: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
Jun  8 15:37:50.491: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
Jun  8 15:37:50.491: INFO: kube-flannel-ds-74q2b from kube-system started at 2023-06-08 13:33:47 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.491: INFO: 	Container kube-flannel ready: true, restart count 0
Jun  8 15:37:50.491: INFO: kube-proxy-hjjpt from kube-system started at 2023-06-08 13:33:47 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.491: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  8 15:37:50.491: INFO: oci-cloud-controller-manager-lwnq4 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.491: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
Jun  8 15:37:50.491: INFO: netserver-4 from pod-network-test-6250 started at 2023-06-08 15:37:31 +0000 UTC (1 container statuses recorded)
Jun  8 15:37:50.491: INFO: 	Container webserver ready: true, restart count 0
Jun  8 15:37:50.491: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-6nv2x from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 15:37:50.491: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 15:37:50.491: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 06/08/23 15:37:50.491
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1766b923031d45e8], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] 06/08/23 15:37:50.529
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:37:51.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9649" for this suite. 06/08/23 15:37:51.538
------------------------------
• [1.147 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:37:50.4
    Jun  8 15:37:50.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename sched-pred 06/08/23 15:37:50.401
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:37:50.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:37:50.421
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jun  8 15:37:50.424: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun  8 15:37:50.435: INFO: Waiting for terminating namespaces to be deleted...
    Jun  8 15:37:50.439: INFO: 
    Logging pods the apiserver thinks is on node chl8tf-control-plane-001 before test
    Jun  8 15:37:50.449: INFO: csi-oci-node-5p7f5 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
    Jun  8 15:37:50.449: INFO: 	Container csi-node-registrar ready: true, restart count 0
    Jun  8 15:37:50.449: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
    Jun  8 15:37:50.449: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
    Jun  8 15:37:50.449: INFO: etcd-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:04 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.449: INFO: 	Container etcd ready: true, restart count 0
    Jun  8 15:37:50.449: INFO: kube-apiserver-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:21 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.449: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jun  8 15:37:50.449: INFO: kube-controller-manager-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:21 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.449: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jun  8 15:37:50.449: INFO: kube-flannel-ds-d5bvw from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.450: INFO: 	Container kube-flannel ready: true, restart count 0
    Jun  8 15:37:50.450: INFO: kube-proxy-j2p7z from kube-system started at 2023-06-08 13:31:27 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.450: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  8 15:37:50.450: INFO: kube-scheduler-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:04 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.450: INFO: 	Container kube-scheduler ready: true, restart count 1
    Jun  8 15:37:50.450: INFO: oci-cloud-controller-manager-9n2zj from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.450: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
    Jun  8 15:37:50.450: INFO: netserver-0 from pod-network-test-6250 started at 2023-06-08 15:37:31 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.450: INFO: 	Container webserver ready: true, restart count 0
    Jun  8 15:37:50.450: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-wmj9x from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 15:37:50.450: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 15:37:50.450: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  8 15:37:50.450: INFO: 
    Logging pods the apiserver thinks is on node chl8tf-control-plane-002 before test
    Jun  8 15:37:50.460: INFO: coredns-55b8ccd764-56hlv from kube-system started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.460: INFO: 	Container coredns ready: true, restart count 0
    Jun  8 15:37:50.460: INFO: coredns-55b8ccd764-jpqkc from kube-system started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.460: INFO: 	Container coredns ready: true, restart count 0
    Jun  8 15:37:50.460: INFO: csi-oci-node-xbn7q from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
    Jun  8 15:37:50.460: INFO: 	Container csi-node-registrar ready: true, restart count 0
    Jun  8 15:37:50.460: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
    Jun  8 15:37:50.460: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
    Jun  8 15:37:50.460: INFO: etcd-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.460: INFO: 	Container etcd ready: true, restart count 0
    Jun  8 15:37:50.460: INFO: kube-apiserver-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:10 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.460: INFO: 	Container kube-apiserver ready: true, restart count 1
    Jun  8 15:37:50.460: INFO: kube-controller-manager-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.460: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jun  8 15:37:50.460: INFO: kube-flannel-ds-6fgbp from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.460: INFO: 	Container kube-flannel ready: true, restart count 0
    Jun  8 15:37:50.460: INFO: kube-proxy-cgwfj from kube-system started at 2023-06-08 13:32:05 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.460: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  8 15:37:50.460: INFO: kube-scheduler-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.460: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jun  8 15:37:50.460: INFO: oci-cloud-controller-manager-dpkmx from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.460: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
    Jun  8 15:37:50.460: INFO: kubernetes-dashboard-8c85c4f9-9l4pq from kubernetes-dashboard started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.460: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Jun  8 15:37:50.460: INFO: netserver-1 from pod-network-test-6250 started at 2023-06-08 15:37:31 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.460: INFO: 	Container webserver ready: true, restart count 0
    Jun  8 15:37:50.460: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-74mph from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 15:37:50.460: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 15:37:50.460: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  8 15:37:50.460: INFO: 
    Logging pods the apiserver thinks is on node chl8tf-control-plane-003 before test
    Jun  8 15:37:50.471: INFO: csi-oci-node-6zdfs from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
    Jun  8 15:37:50.471: INFO: 	Container csi-node-registrar ready: true, restart count 0
    Jun  8 15:37:50.471: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
    Jun  8 15:37:50.471: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
    Jun  8 15:37:50.471: INFO: etcd-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.471: INFO: 	Container etcd ready: true, restart count 0
    Jun  8 15:37:50.471: INFO: kube-apiserver-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.471: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jun  8 15:37:50.471: INFO: kube-controller-manager-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.471: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jun  8 15:37:50.471: INFO: kube-flannel-ds-qf6c6 from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.471: INFO: 	Container kube-flannel ready: true, restart count 0
    Jun  8 15:37:50.471: INFO: kube-proxy-9kw5t from kube-system started at 2023-06-08 13:33:04 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.471: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  8 15:37:50.471: INFO: kube-scheduler-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.471: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jun  8 15:37:50.471: INFO: oci-cloud-controller-manager-2sdzm from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.471: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
    Jun  8 15:37:50.471: INFO: netserver-2 from pod-network-test-6250 started at 2023-06-08 15:37:31 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.471: INFO: 	Container webserver ready: true, restart count 0
    Jun  8 15:37:50.471: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-jpq4c from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 15:37:50.471: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 15:37:50.471: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  8 15:37:50.471: INFO: 
    Logging pods the apiserver thinks is on node chl8tf-worker-001 before test
    Jun  8 15:37:50.480: INFO: csi-oci-node-7ww4x from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
    Jun  8 15:37:50.480: INFO: 	Container csi-node-registrar ready: true, restart count 0
    Jun  8 15:37:50.480: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
    Jun  8 15:37:50.480: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
    Jun  8 15:37:50.480: INFO: kube-flannel-ds-s65xh from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.480: INFO: 	Container kube-flannel ready: true, restart count 0
    Jun  8 15:37:50.480: INFO: kube-proxy-6kfv2 from kube-system started at 2023-06-08 13:33:27 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.480: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  8 15:37:50.480: INFO: oci-cloud-controller-manager-dttpw from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.480: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 1
    Jun  8 15:37:50.480: INFO: netserver-3 from pod-network-test-6250 started at 2023-06-08 15:37:31 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.480: INFO: 	Container webserver ready: true, restart count 0
    Jun  8 15:37:50.480: INFO: test-container-pod from pod-network-test-6250 started at 2023-06-08 15:37:43 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.480: INFO: 	Container webserver ready: true, restart count 0
    Jun  8 15:37:50.480: INFO: sonobuoy from sonobuoy started at 2023-06-08 15:05:13 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.480: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun  8 15:37:50.480: INFO: sonobuoy-e2e-job-e329b7fb80aa4b40 from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 15:37:50.480: INFO: 	Container e2e ready: true, restart count 0
    Jun  8 15:37:50.480: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 15:37:50.480: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-rs4qz from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 15:37:50.480: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 15:37:50.480: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  8 15:37:50.480: INFO: 
    Logging pods the apiserver thinks is on node chl8tf-worker-002 before test
    Jun  8 15:37:50.491: INFO: csi-oci-controller-69f8b488fc-n8th6 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (5 container statuses recorded)
    Jun  8 15:37:50.491: INFO: 	Container csi-attacher ready: true, restart count 1
    Jun  8 15:37:50.491: INFO: 	Container csi-fss-volume-provisioner ready: true, restart count 1
    Jun  8 15:37:50.491: INFO: 	Container csi-resizer ready: true, restart count 0
    Jun  8 15:37:50.491: INFO: 	Container csi-volume-provisioner ready: true, restart count 0
    Jun  8 15:37:50.491: INFO: 	Container oci-csi-controller-driver ready: true, restart count 0
    Jun  8 15:37:50.491: INFO: csi-oci-node-thcvn from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
    Jun  8 15:37:50.491: INFO: 	Container csi-node-registrar ready: true, restart count 0
    Jun  8 15:37:50.491: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
    Jun  8 15:37:50.491: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
    Jun  8 15:37:50.491: INFO: kube-flannel-ds-74q2b from kube-system started at 2023-06-08 13:33:47 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.491: INFO: 	Container kube-flannel ready: true, restart count 0
    Jun  8 15:37:50.491: INFO: kube-proxy-hjjpt from kube-system started at 2023-06-08 13:33:47 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.491: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  8 15:37:50.491: INFO: oci-cloud-controller-manager-lwnq4 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.491: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
    Jun  8 15:37:50.491: INFO: netserver-4 from pod-network-test-6250 started at 2023-06-08 15:37:31 +0000 UTC (1 container statuses recorded)
    Jun  8 15:37:50.491: INFO: 	Container webserver ready: true, restart count 0
    Jun  8 15:37:50.491: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-6nv2x from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 15:37:50.491: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 15:37:50.491: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 06/08/23 15:37:50.491
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1766b923031d45e8], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] 06/08/23 15:37:50.529
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:37:51.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9649" for this suite. 06/08/23 15:37:51.538
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:37:51.559
Jun  8 15:37:51.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename podtemplate 06/08/23 15:37:51.561
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:37:51.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:37:51.589
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 06/08/23 15:37:51.593
STEP: Replace a pod template 06/08/23 15:37:51.6
Jun  8 15:37:51.613: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jun  8 15:37:51.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-9288" for this suite. 06/08/23 15:37:51.621
------------------------------
• [0.071 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:37:51.559
    Jun  8 15:37:51.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename podtemplate 06/08/23 15:37:51.561
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:37:51.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:37:51.589
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 06/08/23 15:37:51.593
    STEP: Replace a pod template 06/08/23 15:37:51.6
    Jun  8 15:37:51.613: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:37:51.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-9288" for this suite. 06/08/23 15:37:51.621
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:37:51.638
Jun  8 15:37:51.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename security-context-test 06/08/23 15:37:51.64
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:37:51.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:37:51.671
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Jun  8 15:37:51.687: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-083defe1-c507-4713-996d-37e59132c86d" in namespace "security-context-test-8069" to be "Succeeded or Failed"
Jun  8 15:37:51.694: INFO: Pod "busybox-readonly-false-083defe1-c507-4713-996d-37e59132c86d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.998771ms
Jun  8 15:37:53.700: INFO: Pod "busybox-readonly-false-083defe1-c507-4713-996d-37e59132c86d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012349615s
Jun  8 15:37:55.701: INFO: Pod "busybox-readonly-false-083defe1-c507-4713-996d-37e59132c86d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013695641s
Jun  8 15:37:55.701: INFO: Pod "busybox-readonly-false-083defe1-c507-4713-996d-37e59132c86d" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jun  8 15:37:55.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-8069" for this suite. 06/08/23 15:37:55.711
------------------------------
• [4.104 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:37:51.638
    Jun  8 15:37:51.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename security-context-test 06/08/23 15:37:51.64
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:37:51.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:37:51.671
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Jun  8 15:37:51.687: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-083defe1-c507-4713-996d-37e59132c86d" in namespace "security-context-test-8069" to be "Succeeded or Failed"
    Jun  8 15:37:51.694: INFO: Pod "busybox-readonly-false-083defe1-c507-4713-996d-37e59132c86d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.998771ms
    Jun  8 15:37:53.700: INFO: Pod "busybox-readonly-false-083defe1-c507-4713-996d-37e59132c86d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012349615s
    Jun  8 15:37:55.701: INFO: Pod "busybox-readonly-false-083defe1-c507-4713-996d-37e59132c86d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013695641s
    Jun  8 15:37:55.701: INFO: Pod "busybox-readonly-false-083defe1-c507-4713-996d-37e59132c86d" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:37:55.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-8069" for this suite. 06/08/23 15:37:55.711
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:37:55.745
Jun  8 15:37:55.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename emptydir 06/08/23 15:37:55.746
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:37:55.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:37:55.773
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 06/08/23 15:37:55.776
Jun  8 15:37:55.787: INFO: Waiting up to 5m0s for pod "pod-2226c997-0fb6-4f3a-b4ee-a9dd9ea52b4c" in namespace "emptydir-559" to be "Succeeded or Failed"
Jun  8 15:37:55.792: INFO: Pod "pod-2226c997-0fb6-4f3a-b4ee-a9dd9ea52b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.591591ms
Jun  8 15:37:57.797: INFO: Pod "pod-2226c997-0fb6-4f3a-b4ee-a9dd9ea52b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009578761s
Jun  8 15:37:59.796: INFO: Pod "pod-2226c997-0fb6-4f3a-b4ee-a9dd9ea52b4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00950444s
STEP: Saw pod success 06/08/23 15:37:59.796
Jun  8 15:37:59.797: INFO: Pod "pod-2226c997-0fb6-4f3a-b4ee-a9dd9ea52b4c" satisfied condition "Succeeded or Failed"
Jun  8 15:37:59.801: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-2226c997-0fb6-4f3a-b4ee-a9dd9ea52b4c container test-container: <nil>
STEP: delete the pod 06/08/23 15:37:59.811
Jun  8 15:37:59.825: INFO: Waiting for pod pod-2226c997-0fb6-4f3a-b4ee-a9dd9ea52b4c to disappear
Jun  8 15:37:59.828: INFO: Pod pod-2226c997-0fb6-4f3a-b4ee-a9dd9ea52b4c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  8 15:37:59.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-559" for this suite. 06/08/23 15:37:59.834
------------------------------
• [4.097 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:37:55.745
    Jun  8 15:37:55.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename emptydir 06/08/23 15:37:55.746
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:37:55.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:37:55.773
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 06/08/23 15:37:55.776
    Jun  8 15:37:55.787: INFO: Waiting up to 5m0s for pod "pod-2226c997-0fb6-4f3a-b4ee-a9dd9ea52b4c" in namespace "emptydir-559" to be "Succeeded or Failed"
    Jun  8 15:37:55.792: INFO: Pod "pod-2226c997-0fb6-4f3a-b4ee-a9dd9ea52b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.591591ms
    Jun  8 15:37:57.797: INFO: Pod "pod-2226c997-0fb6-4f3a-b4ee-a9dd9ea52b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009578761s
    Jun  8 15:37:59.796: INFO: Pod "pod-2226c997-0fb6-4f3a-b4ee-a9dd9ea52b4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00950444s
    STEP: Saw pod success 06/08/23 15:37:59.796
    Jun  8 15:37:59.797: INFO: Pod "pod-2226c997-0fb6-4f3a-b4ee-a9dd9ea52b4c" satisfied condition "Succeeded or Failed"
    Jun  8 15:37:59.801: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-2226c997-0fb6-4f3a-b4ee-a9dd9ea52b4c container test-container: <nil>
    STEP: delete the pod 06/08/23 15:37:59.811
    Jun  8 15:37:59.825: INFO: Waiting for pod pod-2226c997-0fb6-4f3a-b4ee-a9dd9ea52b4c to disappear
    Jun  8 15:37:59.828: INFO: Pod pod-2226c997-0fb6-4f3a-b4ee-a9dd9ea52b4c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:37:59.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-559" for this suite. 06/08/23 15:37:59.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:37:59.843
Jun  8 15:37:59.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename downward-api 06/08/23 15:37:59.844
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:37:59.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:37:59.864
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 06/08/23 15:37:59.867
Jun  8 15:37:59.877: INFO: Waiting up to 5m0s for pod "downward-api-8db5cedd-ed67-4723-a73a-9ad7901a4bd0" in namespace "downward-api-8500" to be "Succeeded or Failed"
Jun  8 15:37:59.881: INFO: Pod "downward-api-8db5cedd-ed67-4723-a73a-9ad7901a4bd0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.893004ms
Jun  8 15:38:01.887: INFO: Pod "downward-api-8db5cedd-ed67-4723-a73a-9ad7901a4bd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009856361s
Jun  8 15:38:03.887: INFO: Pod "downward-api-8db5cedd-ed67-4723-a73a-9ad7901a4bd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009687759s
STEP: Saw pod success 06/08/23 15:38:03.887
Jun  8 15:38:03.887: INFO: Pod "downward-api-8db5cedd-ed67-4723-a73a-9ad7901a4bd0" satisfied condition "Succeeded or Failed"
Jun  8 15:38:03.891: INFO: Trying to get logs from node chl8tf-worker-001 pod downward-api-8db5cedd-ed67-4723-a73a-9ad7901a4bd0 container dapi-container: <nil>
STEP: delete the pod 06/08/23 15:38:03.899
Jun  8 15:38:03.914: INFO: Waiting for pod downward-api-8db5cedd-ed67-4723-a73a-9ad7901a4bd0 to disappear
Jun  8 15:38:03.917: INFO: Pod downward-api-8db5cedd-ed67-4723-a73a-9ad7901a4bd0 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jun  8 15:38:03.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8500" for this suite. 06/08/23 15:38:03.923
------------------------------
• [4.086 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:37:59.843
    Jun  8 15:37:59.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename downward-api 06/08/23 15:37:59.844
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:37:59.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:37:59.864
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 06/08/23 15:37:59.867
    Jun  8 15:37:59.877: INFO: Waiting up to 5m0s for pod "downward-api-8db5cedd-ed67-4723-a73a-9ad7901a4bd0" in namespace "downward-api-8500" to be "Succeeded or Failed"
    Jun  8 15:37:59.881: INFO: Pod "downward-api-8db5cedd-ed67-4723-a73a-9ad7901a4bd0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.893004ms
    Jun  8 15:38:01.887: INFO: Pod "downward-api-8db5cedd-ed67-4723-a73a-9ad7901a4bd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009856361s
    Jun  8 15:38:03.887: INFO: Pod "downward-api-8db5cedd-ed67-4723-a73a-9ad7901a4bd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009687759s
    STEP: Saw pod success 06/08/23 15:38:03.887
    Jun  8 15:38:03.887: INFO: Pod "downward-api-8db5cedd-ed67-4723-a73a-9ad7901a4bd0" satisfied condition "Succeeded or Failed"
    Jun  8 15:38:03.891: INFO: Trying to get logs from node chl8tf-worker-001 pod downward-api-8db5cedd-ed67-4723-a73a-9ad7901a4bd0 container dapi-container: <nil>
    STEP: delete the pod 06/08/23 15:38:03.899
    Jun  8 15:38:03.914: INFO: Waiting for pod downward-api-8db5cedd-ed67-4723-a73a-9ad7901a4bd0 to disappear
    Jun  8 15:38:03.917: INFO: Pod downward-api-8db5cedd-ed67-4723-a73a-9ad7901a4bd0 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:38:03.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8500" for this suite. 06/08/23 15:38:03.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:38:03.931
Jun  8 15:38:03.931: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename namespaces 06/08/23 15:38:03.932
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:03.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:03.953
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 06/08/23 15:38:03.956
Jun  8 15:38:03.961: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 06/08/23 15:38:03.961
Jun  8 15:38:03.966: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 06/08/23 15:38:03.966
Jun  8 15:38:03.977: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:38:03.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9529" for this suite. 06/08/23 15:38:03.983
------------------------------
• [0.059 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:38:03.931
    Jun  8 15:38:03.931: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename namespaces 06/08/23 15:38:03.932
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:03.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:03.953
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 06/08/23 15:38:03.956
    Jun  8 15:38:03.961: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 06/08/23 15:38:03.961
    Jun  8 15:38:03.966: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 06/08/23 15:38:03.966
    Jun  8 15:38:03.977: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:38:03.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9529" for this suite. 06/08/23 15:38:03.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:38:03.99
Jun  8 15:38:03.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename configmap 06/08/23 15:38:03.992
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:04.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:04.012
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-b6606b96-7e5e-48ab-a7de-39d2f27d5148 06/08/23 15:38:04.015
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  8 15:38:04.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3024" for this suite. 06/08/23 15:38:04.023
------------------------------
• [0.039 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:38:03.99
    Jun  8 15:38:03.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename configmap 06/08/23 15:38:03.992
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:04.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:04.012
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-b6606b96-7e5e-48ab-a7de-39d2f27d5148 06/08/23 15:38:04.015
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:38:04.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3024" for this suite. 06/08/23 15:38:04.023
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:38:04.03
Jun  8 15:38:04.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename crd-publish-openapi 06/08/23 15:38:04.031
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:04.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:04.051
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Jun  8 15:38:04.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/08/23 15:38:06.116
Jun  8 15:38:06.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-1463 --namespace=crd-publish-openapi-1463 create -f -'
Jun  8 15:38:06.761: INFO: stderr: ""
Jun  8 15:38:06.761: INFO: stdout: "e2e-test-crd-publish-openapi-2068-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun  8 15:38:06.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-1463 --namespace=crd-publish-openapi-1463 delete e2e-test-crd-publish-openapi-2068-crds test-cr'
Jun  8 15:38:06.875: INFO: stderr: ""
Jun  8 15:38:06.875: INFO: stdout: "e2e-test-crd-publish-openapi-2068-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun  8 15:38:06.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-1463 --namespace=crd-publish-openapi-1463 apply -f -'
Jun  8 15:38:07.080: INFO: stderr: ""
Jun  8 15:38:07.080: INFO: stdout: "e2e-test-crd-publish-openapi-2068-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun  8 15:38:07.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-1463 --namespace=crd-publish-openapi-1463 delete e2e-test-crd-publish-openapi-2068-crds test-cr'
Jun  8 15:38:07.169: INFO: stderr: ""
Jun  8 15:38:07.169: INFO: stdout: "e2e-test-crd-publish-openapi-2068-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 06/08/23 15:38:07.169
Jun  8 15:38:07.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-1463 explain e2e-test-crd-publish-openapi-2068-crds'
Jun  8 15:38:07.734: INFO: stderr: ""
Jun  8 15:38:07.734: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2068-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:38:09.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1463" for this suite. 06/08/23 15:38:09.927
------------------------------
• [SLOW TEST] [5.904 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:38:04.03
    Jun  8 15:38:04.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename crd-publish-openapi 06/08/23 15:38:04.031
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:04.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:04.051
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Jun  8 15:38:04.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/08/23 15:38:06.116
    Jun  8 15:38:06.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-1463 --namespace=crd-publish-openapi-1463 create -f -'
    Jun  8 15:38:06.761: INFO: stderr: ""
    Jun  8 15:38:06.761: INFO: stdout: "e2e-test-crd-publish-openapi-2068-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jun  8 15:38:06.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-1463 --namespace=crd-publish-openapi-1463 delete e2e-test-crd-publish-openapi-2068-crds test-cr'
    Jun  8 15:38:06.875: INFO: stderr: ""
    Jun  8 15:38:06.875: INFO: stdout: "e2e-test-crd-publish-openapi-2068-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jun  8 15:38:06.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-1463 --namespace=crd-publish-openapi-1463 apply -f -'
    Jun  8 15:38:07.080: INFO: stderr: ""
    Jun  8 15:38:07.080: INFO: stdout: "e2e-test-crd-publish-openapi-2068-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jun  8 15:38:07.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-1463 --namespace=crd-publish-openapi-1463 delete e2e-test-crd-publish-openapi-2068-crds test-cr'
    Jun  8 15:38:07.169: INFO: stderr: ""
    Jun  8 15:38:07.169: INFO: stdout: "e2e-test-crd-publish-openapi-2068-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 06/08/23 15:38:07.169
    Jun  8 15:38:07.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-1463 explain e2e-test-crd-publish-openapi-2068-crds'
    Jun  8 15:38:07.734: INFO: stderr: ""
    Jun  8 15:38:07.734: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2068-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:38:09.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1463" for this suite. 06/08/23 15:38:09.927
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:38:09.935
Jun  8 15:38:09.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename disruption 06/08/23 15:38:09.937
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:09.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:09.957
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 06/08/23 15:38:09.966
STEP: Waiting for all pods to be running 06/08/23 15:38:11.999
Jun  8 15:38:12.011: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jun  8 15:38:14.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9593" for this suite. 06/08/23 15:38:14.026
------------------------------
• [4.096 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:38:09.935
    Jun  8 15:38:09.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename disruption 06/08/23 15:38:09.937
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:09.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:09.957
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 06/08/23 15:38:09.966
    STEP: Waiting for all pods to be running 06/08/23 15:38:11.999
    Jun  8 15:38:12.011: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:38:14.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9593" for this suite. 06/08/23 15:38:14.026
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:38:14.033
Jun  8 15:38:14.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename configmap 06/08/23 15:38:14.034
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:14.052
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:14.055
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-9073/configmap-test-d3591019-922c-4c1c-95fc-644edcf1dd54 06/08/23 15:38:14.058
STEP: Creating a pod to test consume configMaps 06/08/23 15:38:14.064
Jun  8 15:38:14.074: INFO: Waiting up to 5m0s for pod "pod-configmaps-2f7feff3-e9c7-45eb-bb65-b317f47aa8a4" in namespace "configmap-9073" to be "Succeeded or Failed"
Jun  8 15:38:14.077: INFO: Pod "pod-configmaps-2f7feff3-e9c7-45eb-bb65-b317f47aa8a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.155777ms
Jun  8 15:38:16.083: INFO: Pod "pod-configmaps-2f7feff3-e9c7-45eb-bb65-b317f47aa8a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008400377s
Jun  8 15:38:18.083: INFO: Pod "pod-configmaps-2f7feff3-e9c7-45eb-bb65-b317f47aa8a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00885337s
STEP: Saw pod success 06/08/23 15:38:18.083
Jun  8 15:38:18.083: INFO: Pod "pod-configmaps-2f7feff3-e9c7-45eb-bb65-b317f47aa8a4" satisfied condition "Succeeded or Failed"
Jun  8 15:38:18.087: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-configmaps-2f7feff3-e9c7-45eb-bb65-b317f47aa8a4 container env-test: <nil>
STEP: delete the pod 06/08/23 15:38:18.106
Jun  8 15:38:18.117: INFO: Waiting for pod pod-configmaps-2f7feff3-e9c7-45eb-bb65-b317f47aa8a4 to disappear
Jun  8 15:38:18.122: INFO: Pod pod-configmaps-2f7feff3-e9c7-45eb-bb65-b317f47aa8a4 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  8 15:38:18.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9073" for this suite. 06/08/23 15:38:18.128
------------------------------
• [4.101 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:38:14.033
    Jun  8 15:38:14.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename configmap 06/08/23 15:38:14.034
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:14.052
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:14.055
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-9073/configmap-test-d3591019-922c-4c1c-95fc-644edcf1dd54 06/08/23 15:38:14.058
    STEP: Creating a pod to test consume configMaps 06/08/23 15:38:14.064
    Jun  8 15:38:14.074: INFO: Waiting up to 5m0s for pod "pod-configmaps-2f7feff3-e9c7-45eb-bb65-b317f47aa8a4" in namespace "configmap-9073" to be "Succeeded or Failed"
    Jun  8 15:38:14.077: INFO: Pod "pod-configmaps-2f7feff3-e9c7-45eb-bb65-b317f47aa8a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.155777ms
    Jun  8 15:38:16.083: INFO: Pod "pod-configmaps-2f7feff3-e9c7-45eb-bb65-b317f47aa8a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008400377s
    Jun  8 15:38:18.083: INFO: Pod "pod-configmaps-2f7feff3-e9c7-45eb-bb65-b317f47aa8a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00885337s
    STEP: Saw pod success 06/08/23 15:38:18.083
    Jun  8 15:38:18.083: INFO: Pod "pod-configmaps-2f7feff3-e9c7-45eb-bb65-b317f47aa8a4" satisfied condition "Succeeded or Failed"
    Jun  8 15:38:18.087: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-configmaps-2f7feff3-e9c7-45eb-bb65-b317f47aa8a4 container env-test: <nil>
    STEP: delete the pod 06/08/23 15:38:18.106
    Jun  8 15:38:18.117: INFO: Waiting for pod pod-configmaps-2f7feff3-e9c7-45eb-bb65-b317f47aa8a4 to disappear
    Jun  8 15:38:18.122: INFO: Pod pod-configmaps-2f7feff3-e9c7-45eb-bb65-b317f47aa8a4 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:38:18.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9073" for this suite. 06/08/23 15:38:18.128
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:38:18.135
Jun  8 15:38:18.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename var-expansion 06/08/23 15:38:18.136
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:18.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:18.16
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 06/08/23 15:38:18.163
Jun  8 15:38:18.171: INFO: Waiting up to 5m0s for pod "var-expansion-1630aa96-f1d7-4beb-9b74-56001f1d6fa0" in namespace "var-expansion-457" to be "Succeeded or Failed"
Jun  8 15:38:18.174: INFO: Pod "var-expansion-1630aa96-f1d7-4beb-9b74-56001f1d6fa0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.832021ms
Jun  8 15:38:20.179: INFO: Pod "var-expansion-1630aa96-f1d7-4beb-9b74-56001f1d6fa0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007718879s
Jun  8 15:38:22.178: INFO: Pod "var-expansion-1630aa96-f1d7-4beb-9b74-56001f1d6fa0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006707753s
STEP: Saw pod success 06/08/23 15:38:22.178
Jun  8 15:38:22.178: INFO: Pod "var-expansion-1630aa96-f1d7-4beb-9b74-56001f1d6fa0" satisfied condition "Succeeded or Failed"
Jun  8 15:38:22.181: INFO: Trying to get logs from node chl8tf-worker-001 pod var-expansion-1630aa96-f1d7-4beb-9b74-56001f1d6fa0 container dapi-container: <nil>
STEP: delete the pod 06/08/23 15:38:22.188
Jun  8 15:38:22.200: INFO: Waiting for pod var-expansion-1630aa96-f1d7-4beb-9b74-56001f1d6fa0 to disappear
Jun  8 15:38:22.203: INFO: Pod var-expansion-1630aa96-f1d7-4beb-9b74-56001f1d6fa0 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun  8 15:38:22.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-457" for this suite. 06/08/23 15:38:22.208
------------------------------
• [4.079 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:38:18.135
    Jun  8 15:38:18.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename var-expansion 06/08/23 15:38:18.136
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:18.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:18.16
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 06/08/23 15:38:18.163
    Jun  8 15:38:18.171: INFO: Waiting up to 5m0s for pod "var-expansion-1630aa96-f1d7-4beb-9b74-56001f1d6fa0" in namespace "var-expansion-457" to be "Succeeded or Failed"
    Jun  8 15:38:18.174: INFO: Pod "var-expansion-1630aa96-f1d7-4beb-9b74-56001f1d6fa0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.832021ms
    Jun  8 15:38:20.179: INFO: Pod "var-expansion-1630aa96-f1d7-4beb-9b74-56001f1d6fa0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007718879s
    Jun  8 15:38:22.178: INFO: Pod "var-expansion-1630aa96-f1d7-4beb-9b74-56001f1d6fa0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006707753s
    STEP: Saw pod success 06/08/23 15:38:22.178
    Jun  8 15:38:22.178: INFO: Pod "var-expansion-1630aa96-f1d7-4beb-9b74-56001f1d6fa0" satisfied condition "Succeeded or Failed"
    Jun  8 15:38:22.181: INFO: Trying to get logs from node chl8tf-worker-001 pod var-expansion-1630aa96-f1d7-4beb-9b74-56001f1d6fa0 container dapi-container: <nil>
    STEP: delete the pod 06/08/23 15:38:22.188
    Jun  8 15:38:22.200: INFO: Waiting for pod var-expansion-1630aa96-f1d7-4beb-9b74-56001f1d6fa0 to disappear
    Jun  8 15:38:22.203: INFO: Pod var-expansion-1630aa96-f1d7-4beb-9b74-56001f1d6fa0 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:38:22.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-457" for this suite. 06/08/23 15:38:22.208
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:38:22.215
Jun  8 15:38:22.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename deployment 06/08/23 15:38:22.216
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:22.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:22.235
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jun  8 15:38:22.238: INFO: Creating deployment "test-recreate-deployment"
Jun  8 15:38:22.244: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun  8 15:38:22.250: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Jun  8 15:38:24.259: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun  8 15:38:24.262: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun  8 15:38:24.272: INFO: Updating deployment test-recreate-deployment
Jun  8 15:38:24.272: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun  8 15:38:24.365: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4325  237961a6-f93f-4d75-85bb-7e1399310374 32721 2 2023-06-08 15:38:22 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-08 15:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 15:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032f1c88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-08 15:38:24 +0000 UTC,LastTransitionTime:2023-06-08 15:38:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-06-08 15:38:24 +0000 UTC,LastTransitionTime:2023-06-08 15:38:22 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun  8 15:38:24.369: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-4325  1edda2cb-6cf8-4395-82bc-d5760ecb1f1b 32718 1 2023-06-08 15:38:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 237961a6-f93f-4d75-85bb-7e1399310374 0xc004da41b0 0xc004da41b1}] [] [{kube-controller-manager Update apps/v1 2023-06-08 15:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"237961a6-f93f-4d75-85bb-7e1399310374\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 15:38:24 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004da4248 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  8 15:38:24.369: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun  8 15:38:24.369: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-4325  647a7fbf-33a1-4834-8ebf-df9d6231ab59 32709 2 2023-06-08 15:38:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 237961a6-f93f-4d75-85bb-7e1399310374 0xc004da4087 0xc004da4088}] [] [{kube-controller-manager Update apps/v1 2023-06-08 15:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"237961a6-f93f-4d75-85bb-7e1399310374\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 15:38:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004da4148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  8 15:38:24.373: INFO: Pod "test-recreate-deployment-cff6dc657-dm9gv" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-dm9gv test-recreate-deployment-cff6dc657- deployment-4325  fe8411e0-5d04-44ee-bff4-f301b006acfe 32720 0 2023-06-08 15:38:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 1edda2cb-6cf8-4395-82bc-d5760ecb1f1b 0xc0008b4590 0xc0008b4591}] [] [{kube-controller-manager Update v1 2023-06-08 15:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1edda2cb-6cf8-4395-82bc-d5760ecb1f1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:38:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-smzkb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-smzkb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.215,PodIP:,StartTime:2023-06-08 15:38:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun  8 15:38:24.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4325" for this suite. 06/08/23 15:38:24.378
------------------------------
• [2.171 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:38:22.215
    Jun  8 15:38:22.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename deployment 06/08/23 15:38:22.216
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:22.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:22.235
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jun  8 15:38:22.238: INFO: Creating deployment "test-recreate-deployment"
    Jun  8 15:38:22.244: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jun  8 15:38:22.250: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
    Jun  8 15:38:24.259: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jun  8 15:38:24.262: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jun  8 15:38:24.272: INFO: Updating deployment test-recreate-deployment
    Jun  8 15:38:24.272: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun  8 15:38:24.365: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-4325  237961a6-f93f-4d75-85bb-7e1399310374 32721 2 2023-06-08 15:38:22 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-08 15:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 15:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032f1c88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-08 15:38:24 +0000 UTC,LastTransitionTime:2023-06-08 15:38:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-06-08 15:38:24 +0000 UTC,LastTransitionTime:2023-06-08 15:38:22 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jun  8 15:38:24.369: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-4325  1edda2cb-6cf8-4395-82bc-d5760ecb1f1b 32718 1 2023-06-08 15:38:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 237961a6-f93f-4d75-85bb-7e1399310374 0xc004da41b0 0xc004da41b1}] [] [{kube-controller-manager Update apps/v1 2023-06-08 15:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"237961a6-f93f-4d75-85bb-7e1399310374\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 15:38:24 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004da4248 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun  8 15:38:24.369: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jun  8 15:38:24.369: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-4325  647a7fbf-33a1-4834-8ebf-df9d6231ab59 32709 2 2023-06-08 15:38:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 237961a6-f93f-4d75-85bb-7e1399310374 0xc004da4087 0xc004da4088}] [] [{kube-controller-manager Update apps/v1 2023-06-08 15:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"237961a6-f93f-4d75-85bb-7e1399310374\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 15:38:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004da4148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun  8 15:38:24.373: INFO: Pod "test-recreate-deployment-cff6dc657-dm9gv" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-dm9gv test-recreate-deployment-cff6dc657- deployment-4325  fe8411e0-5d04-44ee-bff4-f301b006acfe 32720 0 2023-06-08 15:38:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 1edda2cb-6cf8-4395-82bc-d5760ecb1f1b 0xc0008b4590 0xc0008b4591}] [] [{kube-controller-manager Update v1 2023-06-08 15:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1edda2cb-6cf8-4395-82bc-d5760ecb1f1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:38:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-smzkb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-smzkb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.215,PodIP:,StartTime:2023-06-08 15:38:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:38:24.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4325" for this suite. 06/08/23 15:38:24.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:38:24.386
Jun  8 15:38:24.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename configmap 06/08/23 15:38:24.388
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:24.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:24.407
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 06/08/23 15:38:24.41
STEP: fetching the ConfigMap 06/08/23 15:38:24.415
STEP: patching the ConfigMap 06/08/23 15:38:24.418
STEP: listing all ConfigMaps in all namespaces with a label selector 06/08/23 15:38:24.426
STEP: deleting the ConfigMap by collection with a label selector 06/08/23 15:38:24.43
STEP: listing all ConfigMaps in test namespace 06/08/23 15:38:24.437
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  8 15:38:24.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9598" for this suite. 06/08/23 15:38:24.446
------------------------------
• [0.066 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:38:24.386
    Jun  8 15:38:24.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename configmap 06/08/23 15:38:24.388
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:24.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:24.407
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 06/08/23 15:38:24.41
    STEP: fetching the ConfigMap 06/08/23 15:38:24.415
    STEP: patching the ConfigMap 06/08/23 15:38:24.418
    STEP: listing all ConfigMaps in all namespaces with a label selector 06/08/23 15:38:24.426
    STEP: deleting the ConfigMap by collection with a label selector 06/08/23 15:38:24.43
    STEP: listing all ConfigMaps in test namespace 06/08/23 15:38:24.437
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:38:24.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9598" for this suite. 06/08/23 15:38:24.446
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:38:24.452
Jun  8 15:38:24.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename emptydir 06/08/23 15:38:24.454
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:24.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:24.473
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 06/08/23 15:38:24.477
Jun  8 15:38:24.484: INFO: Waiting up to 5m0s for pod "pod-398f4693-fcc2-46b1-b02e-ededaa45512a" in namespace "emptydir-3179" to be "Succeeded or Failed"
Jun  8 15:38:24.488: INFO: Pod "pod-398f4693-fcc2-46b1-b02e-ededaa45512a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.377398ms
Jun  8 15:38:26.495: INFO: Pod "pod-398f4693-fcc2-46b1-b02e-ededaa45512a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010771873s
Jun  8 15:38:28.493: INFO: Pod "pod-398f4693-fcc2-46b1-b02e-ededaa45512a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00857673s
STEP: Saw pod success 06/08/23 15:38:28.493
Jun  8 15:38:28.493: INFO: Pod "pod-398f4693-fcc2-46b1-b02e-ededaa45512a" satisfied condition "Succeeded or Failed"
Jun  8 15:38:28.496: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-398f4693-fcc2-46b1-b02e-ededaa45512a container test-container: <nil>
STEP: delete the pod 06/08/23 15:38:28.503
Jun  8 15:38:28.515: INFO: Waiting for pod pod-398f4693-fcc2-46b1-b02e-ededaa45512a to disappear
Jun  8 15:38:28.518: INFO: Pod pod-398f4693-fcc2-46b1-b02e-ededaa45512a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  8 15:38:28.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3179" for this suite. 06/08/23 15:38:28.524
------------------------------
• [4.077 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:38:24.452
    Jun  8 15:38:24.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename emptydir 06/08/23 15:38:24.454
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:24.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:24.473
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 06/08/23 15:38:24.477
    Jun  8 15:38:24.484: INFO: Waiting up to 5m0s for pod "pod-398f4693-fcc2-46b1-b02e-ededaa45512a" in namespace "emptydir-3179" to be "Succeeded or Failed"
    Jun  8 15:38:24.488: INFO: Pod "pod-398f4693-fcc2-46b1-b02e-ededaa45512a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.377398ms
    Jun  8 15:38:26.495: INFO: Pod "pod-398f4693-fcc2-46b1-b02e-ededaa45512a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010771873s
    Jun  8 15:38:28.493: INFO: Pod "pod-398f4693-fcc2-46b1-b02e-ededaa45512a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00857673s
    STEP: Saw pod success 06/08/23 15:38:28.493
    Jun  8 15:38:28.493: INFO: Pod "pod-398f4693-fcc2-46b1-b02e-ededaa45512a" satisfied condition "Succeeded or Failed"
    Jun  8 15:38:28.496: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-398f4693-fcc2-46b1-b02e-ededaa45512a container test-container: <nil>
    STEP: delete the pod 06/08/23 15:38:28.503
    Jun  8 15:38:28.515: INFO: Waiting for pod pod-398f4693-fcc2-46b1-b02e-ededaa45512a to disappear
    Jun  8 15:38:28.518: INFO: Pod pod-398f4693-fcc2-46b1-b02e-ededaa45512a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:38:28.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3179" for this suite. 06/08/23 15:38:28.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:38:28.53
Jun  8 15:38:28.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename deployment 06/08/23 15:38:28.531
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:28.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:28.551
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 06/08/23 15:38:28.557
STEP: waiting for Deployment to be created 06/08/23 15:38:28.563
STEP: waiting for all Replicas to be Ready 06/08/23 15:38:28.564
Jun  8 15:38:28.566: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  8 15:38:28.566: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  8 15:38:28.577: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  8 15:38:28.577: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  8 15:38:28.593: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  8 15:38:28.593: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  8 15:38:28.620: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  8 15:38:28.620: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  8 15:38:29.437: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun  8 15:38:29.437: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun  8 15:38:29.508: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 06/08/23 15:38:29.508
W0608 15:38:29.524295      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jun  8 15:38:29.526: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 06/08/23 15:38:29.526
Jun  8 15:38:29.528: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0
Jun  8 15:38:29.528: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0
Jun  8 15:38:29.528: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0
Jun  8 15:38:29.528: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0
Jun  8 15:38:29.528: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0
Jun  8 15:38:29.528: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0
Jun  8 15:38:29.529: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0
Jun  8 15:38:29.529: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0
Jun  8 15:38:29.529: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
Jun  8 15:38:29.529: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
Jun  8 15:38:29.529: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
Jun  8 15:38:29.529: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
Jun  8 15:38:29.529: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
Jun  8 15:38:29.529: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
Jun  8 15:38:29.539: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
Jun  8 15:38:29.539: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
Jun  8 15:38:29.559: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
Jun  8 15:38:29.559: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
Jun  8 15:38:29.583: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
Jun  8 15:38:29.583: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
Jun  8 15:38:29.596: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
Jun  8 15:38:29.596: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
Jun  8 15:38:30.514: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
Jun  8 15:38:30.514: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
Jun  8 15:38:30.536: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
STEP: listing Deployments 06/08/23 15:38:30.536
Jun  8 15:38:30.543: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 06/08/23 15:38:30.543
Jun  8 15:38:30.556: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 06/08/23 15:38:30.556
Jun  8 15:38:30.569: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  8 15:38:30.570: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  8 15:38:30.590: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  8 15:38:30.607: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  8 15:38:30.616: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  8 15:38:31.452: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun  8 15:38:31.523: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jun  8 15:38:31.557: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun  8 15:38:31.566: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun  8 15:38:32.487: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 06/08/23 15:38:32.512
STEP: fetching the DeploymentStatus 06/08/23 15:38:32.529
Jun  8 15:38:32.544: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
Jun  8 15:38:32.544: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
Jun  8 15:38:32.544: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
Jun  8 15:38:32.545: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
Jun  8 15:38:32.545: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
Jun  8 15:38:32.545: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
Jun  8 15:38:32.545: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 3
Jun  8 15:38:32.545: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
Jun  8 15:38:32.545: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
Jun  8 15:38:32.546: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 3
STEP: deleting the Deployment 06/08/23 15:38:32.546
Jun  8 15:38:32.564: INFO: observed event type MODIFIED
Jun  8 15:38:32.565: INFO: observed event type MODIFIED
Jun  8 15:38:32.565: INFO: observed event type MODIFIED
Jun  8 15:38:32.565: INFO: observed event type MODIFIED
Jun  8 15:38:32.565: INFO: observed event type MODIFIED
Jun  8 15:38:32.565: INFO: observed event type MODIFIED
Jun  8 15:38:32.565: INFO: observed event type MODIFIED
Jun  8 15:38:32.566: INFO: observed event type MODIFIED
Jun  8 15:38:32.566: INFO: observed event type MODIFIED
Jun  8 15:38:32.566: INFO: observed event type MODIFIED
Jun  8 15:38:32.566: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun  8 15:38:32.573: INFO: Log out all the ReplicaSets if there is no deployment created
Jun  8 15:38:32.584: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-866  ac7b22f0-e616-49a5-9139-81bda23dfc18 32940 2 2023-06-08 15:38:30 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment a146a06b-213a-4584-9d6e-dea84f8ce76a 0xc0041c4eb7 0xc0041c4eb8}] [] [{kube-controller-manager Update apps/v1 2023-06-08 15:38:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a146a06b-213a-4584-9d6e-dea84f8ce76a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 15:38:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041c4f50 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jun  8 15:38:32.591: INFO: pod: "test-deployment-7b7876f9d6-zmr96":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-zmr96 test-deployment-7b7876f9d6- deployment-866  c6c023f8-59f3-4a28-821c-01fdea02672b 32909 0 2023-06-08 15:38:30 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 ac7b22f0-e616-49a5-9139-81bda23dfc18 0xc0047802f7 0xc0047802f8}] [] [{kube-controller-manager Update v1 2023-06-08 15:38:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac7b22f0-e616-49a5-9139-81bda23dfc18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:38:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-49wlb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-49wlb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.215,PodIP:10.244.3.151,StartTime:2023-06-08 15:38:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:38:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://e015a045e8c2e6d2217eb63705d5dce48edf6fecc84362be232c4520d0b0d9f1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun  8 15:38:32.591: INFO: pod: "test-deployment-7b7876f9d6-zq5dt":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-zq5dt test-deployment-7b7876f9d6- deployment-866  109f475d-427f-4d1e-b33b-9218560bbca9 32939 0 2023-06-08 15:38:31 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 ac7b22f0-e616-49a5-9139-81bda23dfc18 0xc0047804d7 0xc0047804d8}] [] [{kube-controller-manager Update v1 2023-06-08 15:38:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac7b22f0-e616-49a5-9139-81bda23dfc18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:38:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5chwc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5chwc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.90,PodIP:10.244.4.63,StartTime:2023-06-08 15:38:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:38:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://038197f1f3af09da7801b8ae523a88486aa664994529e51e3127a469789b1192,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun  8 15:38:32.591: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-866  15b7ba3b-e289-4054-9581-353d2d9cfa65 32948 4 2023-06-08 15:38:29 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment a146a06b-213a-4584-9d6e-dea84f8ce76a 0xc0041c4fb7 0xc0041c4fb8}] [] [{kube-controller-manager Update apps/v1 2023-06-08 15:38:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a146a06b-213a-4584-9d6e-dea84f8ce76a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 15:38:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041c5040 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jun  8 15:38:32.596: INFO: pod: "test-deployment-7df74c55ff-wrf2m":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-wrf2m test-deployment-7df74c55ff- deployment-866  21ca3262-bce9-457a-a80c-5c1997f3125d 32944 0 2023-06-08 15:38:30 +0000 UTC 2023-06-08 15:38:33 +0000 UTC 0xc0048fcfe8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 15b7ba3b-e289-4054-9581-353d2d9cfa65 0xc0048fd017 0xc0048fd018}] [] [{kube-controller-manager Update v1 2023-06-08 15:38:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"15b7ba3b-e289-4054-9581-353d2d9cfa65\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:38:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p9s9q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p9s9q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.90,PodIP:10.244.4.62,StartTime:2023-06-08 15:38:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:38:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://ddf28f1bfb50ec94120a1a52fce9bd946d1e2920eb664278023d4b3a4455fe62,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun  8 15:38:32.596: INFO: pod: "test-deployment-7df74c55ff-xgdbb":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-xgdbb test-deployment-7df74c55ff- deployment-866  991438a1-9ec5-482d-bdf8-672873288867 32914 0 2023-06-08 15:38:29 +0000 UTC 2023-06-08 15:38:32 +0000 UTC 0xc0048fd1d0 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 15b7ba3b-e289-4054-9581-353d2d9cfa65 0xc0048fd207 0xc0048fd208}] [] [{kube-controller-manager Update v1 2023-06-08 15:38:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"15b7ba3b-e289-4054-9581-353d2d9cfa65\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:38:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mrzl5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mrzl5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.215,PodIP:10.244.3.150,StartTime:2023-06-08 15:38:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:38:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://6322bfbd4e9a68d4b87cd8d411fae9c7edcfd33055a38433a234150e3db7a134,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun  8 15:38:32.596: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-866  3051b0b6-9ecd-4cf2-b864-567386e96ebd 32868 3 2023-06-08 15:38:28 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment a146a06b-213a-4584-9d6e-dea84f8ce76a 0xc0041c50a7 0xc0041c50a8}] [] [{kube-controller-manager Update apps/v1 2023-06-08 15:38:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a146a06b-213a-4584-9d6e-dea84f8ce76a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 15:38:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041c5130 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun  8 15:38:32.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-866" for this suite. 06/08/23 15:38:32.606
------------------------------
• [4.082 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:38:28.53
    Jun  8 15:38:28.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename deployment 06/08/23 15:38:28.531
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:28.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:28.551
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 06/08/23 15:38:28.557
    STEP: waiting for Deployment to be created 06/08/23 15:38:28.563
    STEP: waiting for all Replicas to be Ready 06/08/23 15:38:28.564
    Jun  8 15:38:28.566: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun  8 15:38:28.566: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun  8 15:38:28.577: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun  8 15:38:28.577: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun  8 15:38:28.593: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun  8 15:38:28.593: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun  8 15:38:28.620: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun  8 15:38:28.620: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun  8 15:38:29.437: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jun  8 15:38:29.437: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jun  8 15:38:29.508: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 06/08/23 15:38:29.508
    W0608 15:38:29.524295      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jun  8 15:38:29.526: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 06/08/23 15:38:29.526
    Jun  8 15:38:29.528: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0
    Jun  8 15:38:29.528: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0
    Jun  8 15:38:29.528: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0
    Jun  8 15:38:29.528: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0
    Jun  8 15:38:29.528: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0
    Jun  8 15:38:29.528: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0
    Jun  8 15:38:29.529: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0
    Jun  8 15:38:29.529: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 0
    Jun  8 15:38:29.529: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
    Jun  8 15:38:29.529: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
    Jun  8 15:38:29.529: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
    Jun  8 15:38:29.529: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
    Jun  8 15:38:29.529: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
    Jun  8 15:38:29.529: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
    Jun  8 15:38:29.539: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
    Jun  8 15:38:29.539: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
    Jun  8 15:38:29.559: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
    Jun  8 15:38:29.559: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
    Jun  8 15:38:29.583: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
    Jun  8 15:38:29.583: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
    Jun  8 15:38:29.596: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
    Jun  8 15:38:29.596: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
    Jun  8 15:38:30.514: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
    Jun  8 15:38:30.514: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
    Jun  8 15:38:30.536: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
    STEP: listing Deployments 06/08/23 15:38:30.536
    Jun  8 15:38:30.543: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 06/08/23 15:38:30.543
    Jun  8 15:38:30.556: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 06/08/23 15:38:30.556
    Jun  8 15:38:30.569: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun  8 15:38:30.570: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun  8 15:38:30.590: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun  8 15:38:30.607: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun  8 15:38:30.616: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun  8 15:38:31.452: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jun  8 15:38:31.523: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Jun  8 15:38:31.557: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jun  8 15:38:31.566: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jun  8 15:38:32.487: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 06/08/23 15:38:32.512
    STEP: fetching the DeploymentStatus 06/08/23 15:38:32.529
    Jun  8 15:38:32.544: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
    Jun  8 15:38:32.544: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
    Jun  8 15:38:32.544: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
    Jun  8 15:38:32.545: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
    Jun  8 15:38:32.545: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 1
    Jun  8 15:38:32.545: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
    Jun  8 15:38:32.545: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 3
    Jun  8 15:38:32.545: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
    Jun  8 15:38:32.545: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 2
    Jun  8 15:38:32.546: INFO: observed Deployment test-deployment in namespace deployment-866 with ReadyReplicas 3
    STEP: deleting the Deployment 06/08/23 15:38:32.546
    Jun  8 15:38:32.564: INFO: observed event type MODIFIED
    Jun  8 15:38:32.565: INFO: observed event type MODIFIED
    Jun  8 15:38:32.565: INFO: observed event type MODIFIED
    Jun  8 15:38:32.565: INFO: observed event type MODIFIED
    Jun  8 15:38:32.565: INFO: observed event type MODIFIED
    Jun  8 15:38:32.565: INFO: observed event type MODIFIED
    Jun  8 15:38:32.565: INFO: observed event type MODIFIED
    Jun  8 15:38:32.566: INFO: observed event type MODIFIED
    Jun  8 15:38:32.566: INFO: observed event type MODIFIED
    Jun  8 15:38:32.566: INFO: observed event type MODIFIED
    Jun  8 15:38:32.566: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun  8 15:38:32.573: INFO: Log out all the ReplicaSets if there is no deployment created
    Jun  8 15:38:32.584: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-866  ac7b22f0-e616-49a5-9139-81bda23dfc18 32940 2 2023-06-08 15:38:30 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment a146a06b-213a-4584-9d6e-dea84f8ce76a 0xc0041c4eb7 0xc0041c4eb8}] [] [{kube-controller-manager Update apps/v1 2023-06-08 15:38:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a146a06b-213a-4584-9d6e-dea84f8ce76a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 15:38:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041c4f50 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jun  8 15:38:32.591: INFO: pod: "test-deployment-7b7876f9d6-zmr96":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-zmr96 test-deployment-7b7876f9d6- deployment-866  c6c023f8-59f3-4a28-821c-01fdea02672b 32909 0 2023-06-08 15:38:30 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 ac7b22f0-e616-49a5-9139-81bda23dfc18 0xc0047802f7 0xc0047802f8}] [] [{kube-controller-manager Update v1 2023-06-08 15:38:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac7b22f0-e616-49a5-9139-81bda23dfc18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:38:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-49wlb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-49wlb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.215,PodIP:10.244.3.151,StartTime:2023-06-08 15:38:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:38:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://e015a045e8c2e6d2217eb63705d5dce48edf6fecc84362be232c4520d0b0d9f1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jun  8 15:38:32.591: INFO: pod: "test-deployment-7b7876f9d6-zq5dt":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-zq5dt test-deployment-7b7876f9d6- deployment-866  109f475d-427f-4d1e-b33b-9218560bbca9 32939 0 2023-06-08 15:38:31 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 ac7b22f0-e616-49a5-9139-81bda23dfc18 0xc0047804d7 0xc0047804d8}] [] [{kube-controller-manager Update v1 2023-06-08 15:38:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac7b22f0-e616-49a5-9139-81bda23dfc18\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:38:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5chwc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5chwc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.90,PodIP:10.244.4.63,StartTime:2023-06-08 15:38:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:38:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://038197f1f3af09da7801b8ae523a88486aa664994529e51e3127a469789b1192,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jun  8 15:38:32.591: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-866  15b7ba3b-e289-4054-9581-353d2d9cfa65 32948 4 2023-06-08 15:38:29 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment a146a06b-213a-4584-9d6e-dea84f8ce76a 0xc0041c4fb7 0xc0041c4fb8}] [] [{kube-controller-manager Update apps/v1 2023-06-08 15:38:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a146a06b-213a-4584-9d6e-dea84f8ce76a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 15:38:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041c5040 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jun  8 15:38:32.596: INFO: pod: "test-deployment-7df74c55ff-wrf2m":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-wrf2m test-deployment-7df74c55ff- deployment-866  21ca3262-bce9-457a-a80c-5c1997f3125d 32944 0 2023-06-08 15:38:30 +0000 UTC 2023-06-08 15:38:33 +0000 UTC 0xc0048fcfe8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 15b7ba3b-e289-4054-9581-353d2d9cfa65 0xc0048fd017 0xc0048fd018}] [] [{kube-controller-manager Update v1 2023-06-08 15:38:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"15b7ba3b-e289-4054-9581-353d2d9cfa65\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:38:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p9s9q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p9s9q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.237.90,PodIP:10.244.4.62,StartTime:2023-06-08 15:38:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:38:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://ddf28f1bfb50ec94120a1a52fce9bd946d1e2920eb664278023d4b3a4455fe62,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jun  8 15:38:32.596: INFO: pod: "test-deployment-7df74c55ff-xgdbb":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-xgdbb test-deployment-7df74c55ff- deployment-866  991438a1-9ec5-482d-bdf8-672873288867 32914 0 2023-06-08 15:38:29 +0000 UTC 2023-06-08 15:38:32 +0000 UTC 0xc0048fd1d0 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 15b7ba3b-e289-4054-9581-353d2d9cfa65 0xc0048fd207 0xc0048fd208}] [] [{kube-controller-manager Update v1 2023-06-08 15:38:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"15b7ba3b-e289-4054-9581-353d2d9cfa65\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:38:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mrzl5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mrzl5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:38:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.215,PodIP:10.244.3.150,StartTime:2023-06-08 15:38:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:38:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://6322bfbd4e9a68d4b87cd8d411fae9c7edcfd33055a38433a234150e3db7a134,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jun  8 15:38:32.596: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-866  3051b0b6-9ecd-4cf2-b864-567386e96ebd 32868 3 2023-06-08 15:38:28 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment a146a06b-213a-4584-9d6e-dea84f8ce76a 0xc0041c50a7 0xc0041c50a8}] [] [{kube-controller-manager Update apps/v1 2023-06-08 15:38:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a146a06b-213a-4584-9d6e-dea84f8ce76a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 15:38:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041c5130 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:38:32.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-866" for this suite. 06/08/23 15:38:32.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:38:32.614
Jun  8 15:38:32.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename var-expansion 06/08/23 15:38:32.616
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:32.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:32.635
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 06/08/23 15:38:32.639
Jun  8 15:38:32.648: INFO: Waiting up to 2m0s for pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8" in namespace "var-expansion-9410" to be "running"
Jun  8 15:38:32.653: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.139068ms
Jun  8 15:38:34.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010060307s
Jun  8 15:38:36.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010018891s
Jun  8 15:38:38.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01032769s
Jun  8 15:38:40.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010026309s
Jun  8 15:38:42.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010052299s
Jun  8 15:38:44.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.010578475s
Jun  8 15:38:46.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009986366s
Jun  8 15:38:48.660: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011419353s
Jun  8 15:38:50.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010288466s
Jun  8 15:38:52.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010645561s
Jun  8 15:38:54.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009998144s
Jun  8 15:38:56.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 24.011272679s
Jun  8 15:38:58.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010363191s
Jun  8 15:39:00.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010727608s
Jun  8 15:39:02.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009685837s
Jun  8 15:39:04.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010041635s
Jun  8 15:39:06.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009742372s
Jun  8 15:39:08.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009577435s
Jun  8 15:39:10.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 38.009833241s
Jun  8 15:39:12.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010743439s
Jun  8 15:39:14.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009409222s
Jun  8 15:39:16.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 44.009662765s
Jun  8 15:39:18.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 46.010686507s
Jun  8 15:39:20.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 48.010416285s
Jun  8 15:39:22.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009483773s
Jun  8 15:39:24.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 52.010860215s
Jun  8 15:39:26.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009778085s
Jun  8 15:39:28.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010012823s
Jun  8 15:39:30.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 58.010363525s
Jun  8 15:39:32.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.010693799s
Jun  8 15:39:34.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.010094371s
Jun  8 15:39:36.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.010738817s
Jun  8 15:39:38.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.00972096s
Jun  8 15:39:40.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010987854s
Jun  8 15:39:42.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.011092466s
Jun  8 15:39:44.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.010104901s
Jun  8 15:39:46.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.010147944s
Jun  8 15:39:48.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.00990171s
Jun  8 15:39:50.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009965633s
Jun  8 15:39:52.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.011331886s
Jun  8 15:39:54.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.010969006s
Jun  8 15:39:56.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.011016181s
Jun  8 15:39:58.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.010784372s
Jun  8 15:40:00.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009607198s
Jun  8 15:40:02.657: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008676655s
Jun  8 15:40:04.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.01033338s
Jun  8 15:40:06.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.011125979s
Jun  8 15:40:08.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011055168s
Jun  8 15:40:10.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.01021751s
Jun  8 15:40:12.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009503748s
Jun  8 15:40:14.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010540658s
Jun  8 15:40:16.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009928143s
Jun  8 15:40:18.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009824671s
Jun  8 15:40:20.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.010408693s
Jun  8 15:40:22.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.00996327s
Jun  8 15:40:24.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009747724s
Jun  8 15:40:26.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.010584776s
Jun  8 15:40:28.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.011359571s
Jun  8 15:40:30.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009863066s
Jun  8 15:40:32.660: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.01198911s
Jun  8 15:40:32.664: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.015679333s
STEP: updating the pod 06/08/23 15:40:32.664
Jun  8 15:40:33.176: INFO: Successfully updated pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8"
STEP: waiting for pod running 06/08/23 15:40:33.176
Jun  8 15:40:33.176: INFO: Waiting up to 2m0s for pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8" in namespace "var-expansion-9410" to be "running"
Jun  8 15:40:33.180: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.272682ms
Jun  8 15:40:35.185: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008444688s
Jun  8 15:40:35.185: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8" satisfied condition "running"
STEP: deleting the pod gracefully 06/08/23 15:40:35.185
Jun  8 15:40:35.185: INFO: Deleting pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8" in namespace "var-expansion-9410"
Jun  8 15:40:35.192: INFO: Wait up to 5m0s for pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun  8 15:41:07.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9410" for this suite. 06/08/23 15:41:07.206
------------------------------
• [SLOW TEST] [154.598 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:38:32.614
    Jun  8 15:38:32.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename var-expansion 06/08/23 15:38:32.616
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:38:32.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:38:32.635
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 06/08/23 15:38:32.639
    Jun  8 15:38:32.648: INFO: Waiting up to 2m0s for pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8" in namespace "var-expansion-9410" to be "running"
    Jun  8 15:38:32.653: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.139068ms
    Jun  8 15:38:34.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010060307s
    Jun  8 15:38:36.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010018891s
    Jun  8 15:38:38.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01032769s
    Jun  8 15:38:40.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010026309s
    Jun  8 15:38:42.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010052299s
    Jun  8 15:38:44.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.010578475s
    Jun  8 15:38:46.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009986366s
    Jun  8 15:38:48.660: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011419353s
    Jun  8 15:38:50.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010288466s
    Jun  8 15:38:52.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010645561s
    Jun  8 15:38:54.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009998144s
    Jun  8 15:38:56.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 24.011272679s
    Jun  8 15:38:58.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010363191s
    Jun  8 15:39:00.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010727608s
    Jun  8 15:39:02.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009685837s
    Jun  8 15:39:04.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010041635s
    Jun  8 15:39:06.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009742372s
    Jun  8 15:39:08.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009577435s
    Jun  8 15:39:10.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 38.009833241s
    Jun  8 15:39:12.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010743439s
    Jun  8 15:39:14.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009409222s
    Jun  8 15:39:16.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 44.009662765s
    Jun  8 15:39:18.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 46.010686507s
    Jun  8 15:39:20.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 48.010416285s
    Jun  8 15:39:22.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009483773s
    Jun  8 15:39:24.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 52.010860215s
    Jun  8 15:39:26.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009778085s
    Jun  8 15:39:28.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010012823s
    Jun  8 15:39:30.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 58.010363525s
    Jun  8 15:39:32.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.010693799s
    Jun  8 15:39:34.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.010094371s
    Jun  8 15:39:36.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.010738817s
    Jun  8 15:39:38.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.00972096s
    Jun  8 15:39:40.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010987854s
    Jun  8 15:39:42.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.011092466s
    Jun  8 15:39:44.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.010104901s
    Jun  8 15:39:46.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.010147944s
    Jun  8 15:39:48.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.00990171s
    Jun  8 15:39:50.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009965633s
    Jun  8 15:39:52.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.011331886s
    Jun  8 15:39:54.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.010969006s
    Jun  8 15:39:56.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.011016181s
    Jun  8 15:39:58.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.010784372s
    Jun  8 15:40:00.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009607198s
    Jun  8 15:40:02.657: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008676655s
    Jun  8 15:40:04.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.01033338s
    Jun  8 15:40:06.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.011125979s
    Jun  8 15:40:08.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011055168s
    Jun  8 15:40:10.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.01021751s
    Jun  8 15:40:12.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009503748s
    Jun  8 15:40:14.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010540658s
    Jun  8 15:40:16.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009928143s
    Jun  8 15:40:18.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009824671s
    Jun  8 15:40:20.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.010408693s
    Jun  8 15:40:22.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.00996327s
    Jun  8 15:40:24.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009747724s
    Jun  8 15:40:26.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.010584776s
    Jun  8 15:40:28.659: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.011359571s
    Jun  8 15:40:30.658: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009863066s
    Jun  8 15:40:32.660: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.01198911s
    Jun  8 15:40:32.664: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.015679333s
    STEP: updating the pod 06/08/23 15:40:32.664
    Jun  8 15:40:33.176: INFO: Successfully updated pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8"
    STEP: waiting for pod running 06/08/23 15:40:33.176
    Jun  8 15:40:33.176: INFO: Waiting up to 2m0s for pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8" in namespace "var-expansion-9410" to be "running"
    Jun  8 15:40:33.180: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.272682ms
    Jun  8 15:40:35.185: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008444688s
    Jun  8 15:40:35.185: INFO: Pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8" satisfied condition "running"
    STEP: deleting the pod gracefully 06/08/23 15:40:35.185
    Jun  8 15:40:35.185: INFO: Deleting pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8" in namespace "var-expansion-9410"
    Jun  8 15:40:35.192: INFO: Wait up to 5m0s for pod "var-expansion-ebf178bd-ddaf-4632-9fde-1d15c3de13b8" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:41:07.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9410" for this suite. 06/08/23 15:41:07.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:41:07.214
Jun  8 15:41:07.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubectl 06/08/23 15:41:07.215
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:41:07.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:41:07.235
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/08/23 15:41:07.238
Jun  8 15:41:07.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6456 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jun  8 15:41:07.332: INFO: stderr: ""
Jun  8 15:41:07.332: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 06/08/23 15:41:07.332
STEP: verifying the pod e2e-test-httpd-pod was created 06/08/23 15:41:12.385
Jun  8 15:41:12.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6456 get pod e2e-test-httpd-pod -o json'
Jun  8 15:41:12.469: INFO: stderr: ""
Jun  8 15:41:12.469: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-06-08T15:41:07Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6456\",\n        \"resourceVersion\": \"33689\",\n        \"uid\": \"bc526389-79aa-4138-808a-d007d383359a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-5nwj6\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"chl8tf-worker-001\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-5nwj6\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-08T15:41:07Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-08T15:41:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-08T15:41:08Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-08T15:41:07Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://e7002b9c4d8423e45390c6478c2be2b58237dae5e0a96bac98a149833f1aa880\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-06-08T15:41:07Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"100.100.236.215\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.3.153\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.3.153\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-06-08T15:41:07Z\"\n    }\n}\n"
STEP: replace the image in the pod 06/08/23 15:41:12.469
Jun  8 15:41:12.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6456 replace -f -'
Jun  8 15:41:13.204: INFO: stderr: ""
Jun  8 15:41:13.204: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 06/08/23 15:41:13.205
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Jun  8 15:41:13.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6456 delete pods e2e-test-httpd-pod'
Jun  8 15:41:14.895: INFO: stderr: ""
Jun  8 15:41:14.895: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  8 15:41:14.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6456" for this suite. 06/08/23 15:41:14.9
------------------------------
• [SLOW TEST] [7.693 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:41:07.214
    Jun  8 15:41:07.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubectl 06/08/23 15:41:07.215
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:41:07.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:41:07.235
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/08/23 15:41:07.238
    Jun  8 15:41:07.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6456 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jun  8 15:41:07.332: INFO: stderr: ""
    Jun  8 15:41:07.332: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 06/08/23 15:41:07.332
    STEP: verifying the pod e2e-test-httpd-pod was created 06/08/23 15:41:12.385
    Jun  8 15:41:12.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6456 get pod e2e-test-httpd-pod -o json'
    Jun  8 15:41:12.469: INFO: stderr: ""
    Jun  8 15:41:12.469: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-06-08T15:41:07Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6456\",\n        \"resourceVersion\": \"33689\",\n        \"uid\": \"bc526389-79aa-4138-808a-d007d383359a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-5nwj6\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"chl8tf-worker-001\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-5nwj6\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-08T15:41:07Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-08T15:41:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-08T15:41:08Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-08T15:41:07Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://e7002b9c4d8423e45390c6478c2be2b58237dae5e0a96bac98a149833f1aa880\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-06-08T15:41:07Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"100.100.236.215\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.3.153\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.3.153\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-06-08T15:41:07Z\"\n    }\n}\n"
    STEP: replace the image in the pod 06/08/23 15:41:12.469
    Jun  8 15:41:12.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6456 replace -f -'
    Jun  8 15:41:13.204: INFO: stderr: ""
    Jun  8 15:41:13.204: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 06/08/23 15:41:13.205
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Jun  8 15:41:13.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6456 delete pods e2e-test-httpd-pod'
    Jun  8 15:41:14.895: INFO: stderr: ""
    Jun  8 15:41:14.895: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:41:14.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6456" for this suite. 06/08/23 15:41:14.9
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:41:14.908
Jun  8 15:41:14.908: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename csiinlinevolumes 06/08/23 15:41:14.909
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:41:14.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:41:14.929
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 06/08/23 15:41:14.932
STEP: getting 06/08/23 15:41:14.952
STEP: listing in namespace 06/08/23 15:41:14.955
STEP: patching 06/08/23 15:41:14.959
STEP: deleting 06/08/23 15:41:14.968
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jun  8 15:41:14.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-4306" for this suite. 06/08/23 15:41:14.982
------------------------------
• [0.080 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:41:14.908
    Jun  8 15:41:14.908: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename csiinlinevolumes 06/08/23 15:41:14.909
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:41:14.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:41:14.929
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 06/08/23 15:41:14.932
    STEP: getting 06/08/23 15:41:14.952
    STEP: listing in namespace 06/08/23 15:41:14.955
    STEP: patching 06/08/23 15:41:14.959
    STEP: deleting 06/08/23 15:41:14.968
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:41:14.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-4306" for this suite. 06/08/23 15:41:14.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:41:14.99
Jun  8 15:41:14.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename cronjob 06/08/23 15:41:14.991
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:41:15.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:41:15.01
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 06/08/23 15:41:15.013
STEP: Ensuring more than one job is running at a time 06/08/23 15:41:15.021
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 06/08/23 15:43:01.026
STEP: Removing cronjob 06/08/23 15:43:01.03
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jun  8 15:43:01.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8643" for this suite. 06/08/23 15:43:01.044
------------------------------
• [SLOW TEST] [106.060 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:41:14.99
    Jun  8 15:41:14.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename cronjob 06/08/23 15:41:14.991
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:41:15.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:41:15.01
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 06/08/23 15:41:15.013
    STEP: Ensuring more than one job is running at a time 06/08/23 15:41:15.021
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 06/08/23 15:43:01.026
    STEP: Removing cronjob 06/08/23 15:43:01.03
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:43:01.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8643" for this suite. 06/08/23 15:43:01.044
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:43:01.051
Jun  8 15:43:01.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename downward-api 06/08/23 15:43:01.053
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:43:01.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:43:01.083
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 06/08/23 15:43:01.087
Jun  8 15:43:01.096: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e2cd0272-0cbc-41ba-9507-0601f794bbfa" in namespace "downward-api-630" to be "Succeeded or Failed"
Jun  8 15:43:01.100: INFO: Pod "downwardapi-volume-e2cd0272-0cbc-41ba-9507-0601f794bbfa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.081109ms
Jun  8 15:43:03.104: INFO: Pod "downwardapi-volume-e2cd0272-0cbc-41ba-9507-0601f794bbfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007751098s
Jun  8 15:43:05.104: INFO: Pod "downwardapi-volume-e2cd0272-0cbc-41ba-9507-0601f794bbfa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007930106s
STEP: Saw pod success 06/08/23 15:43:05.104
Jun  8 15:43:05.105: INFO: Pod "downwardapi-volume-e2cd0272-0cbc-41ba-9507-0601f794bbfa" satisfied condition "Succeeded or Failed"
Jun  8 15:43:05.108: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-e2cd0272-0cbc-41ba-9507-0601f794bbfa container client-container: <nil>
STEP: delete the pod 06/08/23 15:43:05.122
Jun  8 15:43:05.133: INFO: Waiting for pod downwardapi-volume-e2cd0272-0cbc-41ba-9507-0601f794bbfa to disappear
Jun  8 15:43:05.136: INFO: Pod downwardapi-volume-e2cd0272-0cbc-41ba-9507-0601f794bbfa no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  8 15:43:05.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-630" for this suite. 06/08/23 15:43:05.14
------------------------------
• [4.095 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:43:01.051
    Jun  8 15:43:01.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename downward-api 06/08/23 15:43:01.053
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:43:01.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:43:01.083
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 06/08/23 15:43:01.087
    Jun  8 15:43:01.096: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e2cd0272-0cbc-41ba-9507-0601f794bbfa" in namespace "downward-api-630" to be "Succeeded or Failed"
    Jun  8 15:43:01.100: INFO: Pod "downwardapi-volume-e2cd0272-0cbc-41ba-9507-0601f794bbfa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.081109ms
    Jun  8 15:43:03.104: INFO: Pod "downwardapi-volume-e2cd0272-0cbc-41ba-9507-0601f794bbfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007751098s
    Jun  8 15:43:05.104: INFO: Pod "downwardapi-volume-e2cd0272-0cbc-41ba-9507-0601f794bbfa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007930106s
    STEP: Saw pod success 06/08/23 15:43:05.104
    Jun  8 15:43:05.105: INFO: Pod "downwardapi-volume-e2cd0272-0cbc-41ba-9507-0601f794bbfa" satisfied condition "Succeeded or Failed"
    Jun  8 15:43:05.108: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-e2cd0272-0cbc-41ba-9507-0601f794bbfa container client-container: <nil>
    STEP: delete the pod 06/08/23 15:43:05.122
    Jun  8 15:43:05.133: INFO: Waiting for pod downwardapi-volume-e2cd0272-0cbc-41ba-9507-0601f794bbfa to disappear
    Jun  8 15:43:05.136: INFO: Pod downwardapi-volume-e2cd0272-0cbc-41ba-9507-0601f794bbfa no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:43:05.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-630" for this suite. 06/08/23 15:43:05.14
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:43:05.148
Jun  8 15:43:05.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename pods 06/08/23 15:43:05.149
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:43:05.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:43:05.171
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 06/08/23 15:43:05.174
Jun  8 15:43:05.182: INFO: created test-pod-1
Jun  8 15:43:05.189: INFO: created test-pod-2
Jun  8 15:43:05.196: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 06/08/23 15:43:05.196
Jun  8 15:43:05.196: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-202' to be running and ready
Jun  8 15:43:05.208: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun  8 15:43:05.208: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun  8 15:43:05.208: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun  8 15:43:05.208: INFO: 0 / 3 pods in namespace 'pods-202' are running and ready (0 seconds elapsed)
Jun  8 15:43:05.208: INFO: expected 0 pod replicas in namespace 'pods-202', 0 are Running and Ready.
Jun  8 15:43:05.208: INFO: POD         NODE               PHASE    GRACE  CONDITIONS
Jun  8 15:43:05.208: INFO: test-pod-1  chl8tf-worker-001  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:43:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:43:05 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:43:05 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:43:05 +0000 UTC  }]
Jun  8 15:43:05.208: INFO: test-pod-2  chl8tf-worker-002  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:43:05 +0000 UTC  }]
Jun  8 15:43:05.208: INFO: test-pod-3  chl8tf-worker-001  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:43:05 +0000 UTC  }]
Jun  8 15:43:05.208: INFO: 
Jun  8 15:43:07.220: INFO: 3 / 3 pods in namespace 'pods-202' are running and ready (2 seconds elapsed)
Jun  8 15:43:07.220: INFO: expected 0 pod replicas in namespace 'pods-202', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 06/08/23 15:43:07.239
Jun  8 15:43:07.243: INFO: Pod quantity 3 is different from expected quantity 0
Jun  8 15:43:08.250: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun  8 15:43:09.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-202" for this suite. 06/08/23 15:43:09.253
------------------------------
• [4.111 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:43:05.148
    Jun  8 15:43:05.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename pods 06/08/23 15:43:05.149
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:43:05.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:43:05.171
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 06/08/23 15:43:05.174
    Jun  8 15:43:05.182: INFO: created test-pod-1
    Jun  8 15:43:05.189: INFO: created test-pod-2
    Jun  8 15:43:05.196: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 06/08/23 15:43:05.196
    Jun  8 15:43:05.196: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-202' to be running and ready
    Jun  8 15:43:05.208: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun  8 15:43:05.208: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun  8 15:43:05.208: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun  8 15:43:05.208: INFO: 0 / 3 pods in namespace 'pods-202' are running and ready (0 seconds elapsed)
    Jun  8 15:43:05.208: INFO: expected 0 pod replicas in namespace 'pods-202', 0 are Running and Ready.
    Jun  8 15:43:05.208: INFO: POD         NODE               PHASE    GRACE  CONDITIONS
    Jun  8 15:43:05.208: INFO: test-pod-1  chl8tf-worker-001  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:43:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:43:05 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:43:05 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:43:05 +0000 UTC  }]
    Jun  8 15:43:05.208: INFO: test-pod-2  chl8tf-worker-002  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:43:05 +0000 UTC  }]
    Jun  8 15:43:05.208: INFO: test-pod-3  chl8tf-worker-001  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-08 15:43:05 +0000 UTC  }]
    Jun  8 15:43:05.208: INFO: 
    Jun  8 15:43:07.220: INFO: 3 / 3 pods in namespace 'pods-202' are running and ready (2 seconds elapsed)
    Jun  8 15:43:07.220: INFO: expected 0 pod replicas in namespace 'pods-202', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 06/08/23 15:43:07.239
    Jun  8 15:43:07.243: INFO: Pod quantity 3 is different from expected quantity 0
    Jun  8 15:43:08.250: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:43:09.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-202" for this suite. 06/08/23 15:43:09.253
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:43:09.26
Jun  8 15:43:09.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename emptydir 06/08/23 15:43:09.261
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:43:09.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:43:09.28
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 06/08/23 15:43:09.283
Jun  8 15:43:09.292: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-723827ad-235c-4087-9b25-87a677e8a532" in namespace "emptydir-1320" to be "running"
Jun  8 15:43:09.295: INFO: Pod "pod-sharedvolume-723827ad-235c-4087-9b25-87a677e8a532": Phase="Pending", Reason="", readiness=false. Elapsed: 2.992002ms
Jun  8 15:43:11.300: INFO: Pod "pod-sharedvolume-723827ad-235c-4087-9b25-87a677e8a532": Phase="Running", Reason="", readiness=false. Elapsed: 2.008168389s
Jun  8 15:43:11.300: INFO: Pod "pod-sharedvolume-723827ad-235c-4087-9b25-87a677e8a532" satisfied condition "running"
STEP: Reading file content from the nginx-container 06/08/23 15:43:11.3
Jun  8 15:43:11.300: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1320 PodName:pod-sharedvolume-723827ad-235c-4087-9b25-87a677e8a532 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:43:11.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:43:11.301: INFO: ExecWithOptions: Clientset creation
Jun  8 15:43:11.301: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-1320/pods/pod-sharedvolume-723827ad-235c-4087-9b25-87a677e8a532/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jun  8 15:43:11.386: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  8 15:43:11.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1320" for this suite. 06/08/23 15:43:11.392
------------------------------
• [2.139 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:43:09.26
    Jun  8 15:43:09.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename emptydir 06/08/23 15:43:09.261
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:43:09.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:43:09.28
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 06/08/23 15:43:09.283
    Jun  8 15:43:09.292: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-723827ad-235c-4087-9b25-87a677e8a532" in namespace "emptydir-1320" to be "running"
    Jun  8 15:43:09.295: INFO: Pod "pod-sharedvolume-723827ad-235c-4087-9b25-87a677e8a532": Phase="Pending", Reason="", readiness=false. Elapsed: 2.992002ms
    Jun  8 15:43:11.300: INFO: Pod "pod-sharedvolume-723827ad-235c-4087-9b25-87a677e8a532": Phase="Running", Reason="", readiness=false. Elapsed: 2.008168389s
    Jun  8 15:43:11.300: INFO: Pod "pod-sharedvolume-723827ad-235c-4087-9b25-87a677e8a532" satisfied condition "running"
    STEP: Reading file content from the nginx-container 06/08/23 15:43:11.3
    Jun  8 15:43:11.300: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1320 PodName:pod-sharedvolume-723827ad-235c-4087-9b25-87a677e8a532 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:43:11.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:43:11.301: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:43:11.301: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-1320/pods/pod-sharedvolume-723827ad-235c-4087-9b25-87a677e8a532/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jun  8 15:43:11.386: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:43:11.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1320" for this suite. 06/08/23 15:43:11.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:43:11.4
Jun  8 15:43:11.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename proxy 06/08/23 15:43:11.401
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:43:11.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:43:11.422
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 06/08/23 15:43:11.439
STEP: creating replication controller proxy-service-ksp8q in namespace proxy-2947 06/08/23 15:43:11.439
I0608 15:43:11.447102      23 runners.go:193] Created replication controller with name: proxy-service-ksp8q, namespace: proxy-2947, replica count: 1
I0608 15:43:12.498335      23 runners.go:193] proxy-service-ksp8q Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0608 15:43:13.499394      23 runners.go:193] proxy-service-ksp8q Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  8 15:43:13.504: INFO: setup took 2.078620546s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 06/08/23 15:43:13.504
Jun  8 15:43:13.520: INFO: (0) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 15.757177ms)
Jun  8 15:43:13.526: INFO: (0) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 21.984464ms)
Jun  8 15:43:13.526: INFO: (0) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 22.247606ms)
Jun  8 15:43:13.526: INFO: (0) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 22.104526ms)
Jun  8 15:43:13.526: INFO: (0) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 22.159104ms)
Jun  8 15:43:13.526: INFO: (0) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 22.225743ms)
Jun  8 15:43:13.526: INFO: (0) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 22.112134ms)
Jun  8 15:43:13.526: INFO: (0) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 22.094677ms)
Jun  8 15:43:13.526: INFO: (0) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 22.290246ms)
Jun  8 15:43:13.527: INFO: (0) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 22.435342ms)
Jun  8 15:43:13.527: INFO: (0) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 22.777428ms)
Jun  8 15:43:13.527: INFO: (0) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 22.773561ms)
Jun  8 15:43:13.527: INFO: (0) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 23.000789ms)
Jun  8 15:43:13.527: INFO: (0) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 23.232811ms)
Jun  8 15:43:13.527: INFO: (0) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 23.166944ms)
Jun  8 15:43:13.527: INFO: (0) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 23.513569ms)
Jun  8 15:43:13.533: INFO: (1) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 5.524045ms)
Jun  8 15:43:13.536: INFO: (1) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 8.30606ms)
Jun  8 15:43:13.540: INFO: (1) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 12.128112ms)
Jun  8 15:43:13.542: INFO: (1) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 14.18231ms)
Jun  8 15:43:13.542: INFO: (1) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 14.238466ms)
Jun  8 15:43:13.542: INFO: (1) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 14.618116ms)
Jun  8 15:43:13.542: INFO: (1) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 14.517205ms)
Jun  8 15:43:13.542: INFO: (1) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 14.513023ms)
Jun  8 15:43:13.542: INFO: (1) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 14.440222ms)
Jun  8 15:43:13.543: INFO: (1) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 15.053692ms)
Jun  8 15:43:13.543: INFO: (1) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 14.944422ms)
Jun  8 15:43:13.545: INFO: (1) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 17.101608ms)
Jun  8 15:43:13.546: INFO: (1) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 17.618207ms)
Jun  8 15:43:13.547: INFO: (1) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 18.708079ms)
Jun  8 15:43:13.547: INFO: (1) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 19.025749ms)
Jun  8 15:43:13.547: INFO: (1) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 19.077066ms)
Jun  8 15:43:13.556: INFO: (2) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 9.1439ms)
Jun  8 15:43:13.556: INFO: (2) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 9.210847ms)
Jun  8 15:43:13.557: INFO: (2) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 9.684708ms)
Jun  8 15:43:13.557: INFO: (2) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 9.893803ms)
Jun  8 15:43:13.557: INFO: (2) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 9.59766ms)
Jun  8 15:43:13.559: INFO: (2) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 11.664069ms)
Jun  8 15:43:13.561: INFO: (2) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 13.331673ms)
Jun  8 15:43:13.561: INFO: (2) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 13.63285ms)
Jun  8 15:43:13.561: INFO: (2) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 13.96823ms)
Jun  8 15:43:13.562: INFO: (2) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 15.130808ms)
Jun  8 15:43:13.563: INFO: (2) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 15.428246ms)
Jun  8 15:43:13.563: INFO: (2) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 15.317486ms)
Jun  8 15:43:13.563: INFO: (2) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 15.621341ms)
Jun  8 15:43:13.563: INFO: (2) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 15.595069ms)
Jun  8 15:43:13.563: INFO: (2) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 15.577965ms)
Jun  8 15:43:13.563: INFO: (2) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 15.877507ms)
Jun  8 15:43:13.575: INFO: (3) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 11.083417ms)
Jun  8 15:43:13.575: INFO: (3) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 11.842247ms)
Jun  8 15:43:13.575: INFO: (3) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 11.528354ms)
Jun  8 15:43:13.575: INFO: (3) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 11.537454ms)
Jun  8 15:43:13.576: INFO: (3) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 11.974874ms)
Jun  8 15:43:13.576: INFO: (3) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 12.363769ms)
Jun  8 15:43:13.576: INFO: (3) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 12.488426ms)
Jun  8 15:43:13.576: INFO: (3) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 12.526453ms)
Jun  8 15:43:13.576: INFO: (3) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 12.498581ms)
Jun  8 15:43:13.578: INFO: (3) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 14.033598ms)
Jun  8 15:43:13.579: INFO: (3) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 15.772545ms)
Jun  8 15:43:13.580: INFO: (3) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 15.827787ms)
Jun  8 15:43:13.580: INFO: (3) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 16.141861ms)
Jun  8 15:43:13.581: INFO: (3) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 17.235855ms)
Jun  8 15:43:13.581: INFO: (3) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 17.382841ms)
Jun  8 15:43:13.581: INFO: (3) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 17.563863ms)
Jun  8 15:43:13.592: INFO: (4) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 10.724819ms)
Jun  8 15:43:13.593: INFO: (4) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 10.91649ms)
Jun  8 15:43:13.593: INFO: (4) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 11.159579ms)
Jun  8 15:43:13.593: INFO: (4) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 11.625862ms)
Jun  8 15:43:13.593: INFO: (4) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 11.556438ms)
Jun  8 15:43:13.593: INFO: (4) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 11.5911ms)
Jun  8 15:43:13.595: INFO: (4) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 13.288142ms)
Jun  8 15:43:13.595: INFO: (4) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 13.067126ms)
Jun  8 15:43:13.595: INFO: (4) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 13.512815ms)
Jun  8 15:43:13.595: INFO: (4) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 13.153515ms)
Jun  8 15:43:13.598: INFO: (4) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 16.763433ms)
Jun  8 15:43:13.599: INFO: (4) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 17.382542ms)
Jun  8 15:43:13.600: INFO: (4) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 19.140069ms)
Jun  8 15:43:13.600: INFO: (4) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 18.682564ms)
Jun  8 15:43:13.601: INFO: (4) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 19.095509ms)
Jun  8 15:43:13.602: INFO: (4) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 20.459268ms)
Jun  8 15:43:13.615: INFO: (5) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 12.831231ms)
Jun  8 15:43:13.617: INFO: (5) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 14.429088ms)
Jun  8 15:43:13.618: INFO: (5) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 15.927994ms)
Jun  8 15:43:13.618: INFO: (5) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 16.439851ms)
Jun  8 15:43:13.619: INFO: (5) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 16.330926ms)
Jun  8 15:43:13.619: INFO: (5) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 16.605751ms)
Jun  8 15:43:13.619: INFO: (5) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 16.830058ms)
Jun  8 15:43:13.619: INFO: (5) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 17.304177ms)
Jun  8 15:43:13.620: INFO: (5) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 17.173654ms)
Jun  8 15:43:13.620: INFO: (5) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 17.566276ms)
Jun  8 15:43:13.620: INFO: (5) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 17.99501ms)
Jun  8 15:43:13.620: INFO: (5) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 18.084711ms)
Jun  8 15:43:13.621: INFO: (5) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 18.379785ms)
Jun  8 15:43:13.621: INFO: (5) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 18.089781ms)
Jun  8 15:43:13.621: INFO: (5) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 19.216364ms)
Jun  8 15:43:13.623: INFO: (5) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 20.722868ms)
Jun  8 15:43:13.631: INFO: (6) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 7.945764ms)
Jun  8 15:43:13.637: INFO: (6) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 13.066949ms)
Jun  8 15:43:13.637: INFO: (6) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 13.549206ms)
Jun  8 15:43:13.637: INFO: (6) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 13.799694ms)
Jun  8 15:43:13.637: INFO: (6) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 13.877111ms)
Jun  8 15:43:13.638: INFO: (6) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 14.658291ms)
Jun  8 15:43:13.638: INFO: (6) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 14.668427ms)
Jun  8 15:43:13.638: INFO: (6) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 14.938303ms)
Jun  8 15:43:13.638: INFO: (6) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 14.672317ms)
Jun  8 15:43:13.639: INFO: (6) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 15.456643ms)
Jun  8 15:43:13.644: INFO: (6) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 20.52789ms)
Jun  8 15:43:13.644: INFO: (6) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 20.254438ms)
Jun  8 15:43:13.644: INFO: (6) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 20.312078ms)
Jun  8 15:43:13.644: INFO: (6) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 20.832142ms)
Jun  8 15:43:13.644: INFO: (6) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 20.654703ms)
Jun  8 15:43:13.644: INFO: (6) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 20.795623ms)
Jun  8 15:43:13.653: INFO: (7) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 8.422413ms)
Jun  8 15:43:13.653: INFO: (7) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 8.452166ms)
Jun  8 15:43:13.655: INFO: (7) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 10.652629ms)
Jun  8 15:43:13.655: INFO: (7) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 10.559229ms)
Jun  8 15:43:13.656: INFO: (7) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 11.635001ms)
Jun  8 15:43:13.659: INFO: (7) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 13.722434ms)
Jun  8 15:43:13.659: INFO: (7) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 13.779043ms)
Jun  8 15:43:13.659: INFO: (7) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 14.338667ms)
Jun  8 15:43:13.659: INFO: (7) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 14.317432ms)
Jun  8 15:43:13.659: INFO: (7) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 14.481047ms)
Jun  8 15:43:13.659: INFO: (7) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 14.409366ms)
Jun  8 15:43:13.659: INFO: (7) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 14.6217ms)
Jun  8 15:43:13.660: INFO: (7) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 15.091478ms)
Jun  8 15:43:13.660: INFO: (7) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 15.129003ms)
Jun  8 15:43:13.660: INFO: (7) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 15.071079ms)
Jun  8 15:43:13.660: INFO: (7) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 15.024261ms)
Jun  8 15:43:13.666: INFO: (8) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 6.37401ms)
Jun  8 15:43:13.667: INFO: (8) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 6.525ms)
Jun  8 15:43:13.667: INFO: (8) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 7.056074ms)
Jun  8 15:43:13.671: INFO: (8) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 9.978328ms)
Jun  8 15:43:13.672: INFO: (8) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 11.387418ms)
Jun  8 15:43:13.672: INFO: (8) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 11.4089ms)
Jun  8 15:43:13.674: INFO: (8) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 13.834848ms)
Jun  8 15:43:13.674: INFO: (8) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 13.974375ms)
Jun  8 15:43:13.676: INFO: (8) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 15.4098ms)
Jun  8 15:43:13.676: INFO: (8) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 15.99368ms)
Jun  8 15:43:13.677: INFO: (8) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 15.994373ms)
Jun  8 15:43:13.677: INFO: (8) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 16.684372ms)
Jun  8 15:43:13.677: INFO: (8) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 17.40592ms)
Jun  8 15:43:13.677: INFO: (8) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 17.019338ms)
Jun  8 15:43:13.677: INFO: (8) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 17.119992ms)
Jun  8 15:43:13.677: INFO: (8) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 17.510693ms)
Jun  8 15:43:13.684: INFO: (9) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 6.541739ms)
Jun  8 15:43:13.686: INFO: (9) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 8.936754ms)
Jun  8 15:43:13.688: INFO: (9) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 10.043956ms)
Jun  8 15:43:13.693: INFO: (9) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 15.667102ms)
Jun  8 15:43:13.693: INFO: (9) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 15.812906ms)
Jun  8 15:43:13.694: INFO: (9) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 16.004281ms)
Jun  8 15:43:13.694: INFO: (9) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 16.075183ms)
Jun  8 15:43:13.694: INFO: (9) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 16.487206ms)
Jun  8 15:43:13.694: INFO: (9) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 16.68272ms)
Jun  8 15:43:13.694: INFO: (9) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 16.587653ms)
Jun  8 15:43:13.694: INFO: (9) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 16.582694ms)
Jun  8 15:43:13.695: INFO: (9) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 17.234537ms)
Jun  8 15:43:13.695: INFO: (9) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 17.942533ms)
Jun  8 15:43:13.697: INFO: (9) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 19.123501ms)
Jun  8 15:43:13.699: INFO: (9) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 20.975452ms)
Jun  8 15:43:13.699: INFO: (9) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 21.157659ms)
Jun  8 15:43:13.708: INFO: (10) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 9.048302ms)
Jun  8 15:43:13.708: INFO: (10) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 9.514645ms)
Jun  8 15:43:13.709: INFO: (10) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 9.921007ms)
Jun  8 15:43:13.711: INFO: (10) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 11.71053ms)
Jun  8 15:43:13.713: INFO: (10) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 13.549028ms)
Jun  8 15:43:13.713: INFO: (10) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 13.528015ms)
Jun  8 15:43:13.713: INFO: (10) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 13.653897ms)
Jun  8 15:43:13.713: INFO: (10) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 13.652608ms)
Jun  8 15:43:13.713: INFO: (10) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 13.664967ms)
Jun  8 15:43:13.713: INFO: (10) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 13.59484ms)
Jun  8 15:43:13.713: INFO: (10) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 13.618357ms)
Jun  8 15:43:13.717: INFO: (10) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 17.957086ms)
Jun  8 15:43:13.718: INFO: (10) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 18.767213ms)
Jun  8 15:43:13.718: INFO: (10) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 19.116137ms)
Jun  8 15:43:13.718: INFO: (10) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 19.042289ms)
Jun  8 15:43:13.718: INFO: (10) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 19.231625ms)
Jun  8 15:43:13.729: INFO: (11) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 10.952741ms)
Jun  8 15:43:13.730: INFO: (11) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 11.63271ms)
Jun  8 15:43:13.730: INFO: (11) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 11.537291ms)
Jun  8 15:43:13.730: INFO: (11) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 11.53474ms)
Jun  8 15:43:13.731: INFO: (11) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 11.740339ms)
Jun  8 15:43:13.731: INFO: (11) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 11.9731ms)
Jun  8 15:43:13.731: INFO: (11) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 12.086159ms)
Jun  8 15:43:13.731: INFO: (11) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 12.019361ms)
Jun  8 15:43:13.732: INFO: (11) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 13.159419ms)
Jun  8 15:43:13.732: INFO: (11) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 13.572682ms)
Jun  8 15:43:13.737: INFO: (11) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 18.211967ms)
Jun  8 15:43:13.737: INFO: (11) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 19.013592ms)
Jun  8 15:43:13.738: INFO: (11) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 18.996152ms)
Jun  8 15:43:13.738: INFO: (11) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 18.842931ms)
Jun  8 15:43:13.738: INFO: (11) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 18.936509ms)
Jun  8 15:43:13.738: INFO: (11) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 19.300474ms)
Jun  8 15:43:13.743: INFO: (12) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 4.698762ms)
Jun  8 15:43:13.744: INFO: (12) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 5.817667ms)
Jun  8 15:43:13.745: INFO: (12) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 6.974149ms)
Jun  8 15:43:13.748: INFO: (12) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 9.772426ms)
Jun  8 15:43:13.750: INFO: (12) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 12.055818ms)
Jun  8 15:43:13.752: INFO: (12) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 13.130437ms)
Jun  8 15:43:13.752: INFO: (12) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 13.177281ms)
Jun  8 15:43:13.754: INFO: (12) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 15.674456ms)
Jun  8 15:43:13.754: INFO: (12) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 15.89104ms)
Jun  8 15:43:13.754: INFO: (12) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 16.01754ms)
Jun  8 15:43:13.755: INFO: (12) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 16.255603ms)
Jun  8 15:43:13.755: INFO: (12) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 16.440151ms)
Jun  8 15:43:13.755: INFO: (12) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 16.511603ms)
Jun  8 15:43:13.755: INFO: (12) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 16.569055ms)
Jun  8 15:43:13.758: INFO: (12) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 19.467758ms)
Jun  8 15:43:13.759: INFO: (12) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 20.717383ms)
Jun  8 15:43:13.766: INFO: (13) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 6.402246ms)
Jun  8 15:43:13.772: INFO: (13) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 12.372303ms)
Jun  8 15:43:13.772: INFO: (13) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 12.461303ms)
Jun  8 15:43:13.773: INFO: (13) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 13.44748ms)
Jun  8 15:43:13.773: INFO: (13) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 13.857822ms)
Jun  8 15:43:13.773: INFO: (13) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 14.044291ms)
Jun  8 15:43:13.774: INFO: (13) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 14.381037ms)
Jun  8 15:43:13.774: INFO: (13) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 14.803155ms)
Jun  8 15:43:13.775: INFO: (13) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 15.390346ms)
Jun  8 15:43:13.775: INFO: (13) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 15.84793ms)
Jun  8 15:43:13.776: INFO: (13) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 16.355223ms)
Jun  8 15:43:13.776: INFO: (13) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 16.450664ms)
Jun  8 15:43:13.778: INFO: (13) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 18.395325ms)
Jun  8 15:43:13.778: INFO: (13) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 18.701441ms)
Jun  8 15:43:13.778: INFO: (13) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 18.707402ms)
Jun  8 15:43:13.778: INFO: (13) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 18.847383ms)
Jun  8 15:43:13.791: INFO: (14) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 12.527011ms)
Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 14.167702ms)
Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 14.266292ms)
Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 14.293325ms)
Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 14.422933ms)
Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 14.364326ms)
Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 14.432286ms)
Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 14.388327ms)
Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 14.191883ms)
Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 14.545026ms)
Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 14.579579ms)
Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 14.279423ms)
Jun  8 15:43:13.796: INFO: (14) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 16.958423ms)
Jun  8 15:43:13.796: INFO: (14) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 17.503378ms)
Jun  8 15:43:13.797: INFO: (14) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 18.116295ms)
Jun  8 15:43:13.797: INFO: (14) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 18.168829ms)
Jun  8 15:43:13.805: INFO: (15) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 7.921249ms)
Jun  8 15:43:13.807: INFO: (15) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 9.964073ms)
Jun  8 15:43:13.808: INFO: (15) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 10.653763ms)
Jun  8 15:43:13.809: INFO: (15) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 11.616601ms)
Jun  8 15:43:13.809: INFO: (15) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 11.770314ms)
Jun  8 15:43:13.809: INFO: (15) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 11.738946ms)
Jun  8 15:43:13.809: INFO: (15) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 12.276325ms)
Jun  8 15:43:13.809: INFO: (15) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 12.124076ms)
Jun  8 15:43:13.809: INFO: (15) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 12.415581ms)
Jun  8 15:43:13.809: INFO: (15) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 12.285903ms)
Jun  8 15:43:13.809: INFO: (15) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 12.376077ms)
Jun  8 15:43:13.813: INFO: (15) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 15.241211ms)
Jun  8 15:43:13.815: INFO: (15) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 17.518371ms)
Jun  8 15:43:13.815: INFO: (15) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 17.280308ms)
Jun  8 15:43:13.815: INFO: (15) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 17.441047ms)
Jun  8 15:43:13.815: INFO: (15) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 18.283265ms)
Jun  8 15:43:13.828: INFO: (16) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 11.688641ms)
Jun  8 15:43:13.828: INFO: (16) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 11.967117ms)
Jun  8 15:43:13.828: INFO: (16) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 12.33037ms)
Jun  8 15:43:13.829: INFO: (16) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 12.835983ms)
Jun  8 15:43:13.829: INFO: (16) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 12.712203ms)
Jun  8 15:43:13.829: INFO: (16) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 13.176505ms)
Jun  8 15:43:13.829: INFO: (16) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 13.577672ms)
Jun  8 15:43:13.829: INFO: (16) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 13.94432ms)
Jun  8 15:43:13.829: INFO: (16) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 13.841986ms)
Jun  8 15:43:13.830: INFO: (16) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 14.012531ms)
Jun  8 15:43:13.830: INFO: (16) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 13.842579ms)
Jun  8 15:43:13.835: INFO: (16) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 19.117088ms)
Jun  8 15:43:13.835: INFO: (16) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 19.938399ms)
Jun  8 15:43:13.835: INFO: (16) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 19.775631ms)
Jun  8 15:43:13.836: INFO: (16) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 20.012118ms)
Jun  8 15:43:13.836: INFO: (16) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 19.934048ms)
Jun  8 15:43:13.846: INFO: (17) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 9.43466ms)
Jun  8 15:43:13.850: INFO: (17) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 14.566986ms)
Jun  8 15:43:13.851: INFO: (17) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 14.789103ms)
Jun  8 15:43:13.851: INFO: (17) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 15.217624ms)
Jun  8 15:43:13.851: INFO: (17) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 15.130295ms)
Jun  8 15:43:13.851: INFO: (17) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 15.390215ms)
Jun  8 15:43:13.853: INFO: (17) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 17.450284ms)
Jun  8 15:43:13.854: INFO: (17) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 17.588963ms)
Jun  8 15:43:13.855: INFO: (17) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 19.476032ms)
Jun  8 15:43:13.856: INFO: (17) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 19.778953ms)
Jun  8 15:43:13.856: INFO: (17) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 20.022914ms)
Jun  8 15:43:13.858: INFO: (17) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 22.257569ms)
Jun  8 15:43:13.858: INFO: (17) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 22.301685ms)
Jun  8 15:43:13.858: INFO: (17) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 22.324848ms)
Jun  8 15:43:13.858: INFO: (17) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 22.407195ms)
Jun  8 15:43:13.858: INFO: (17) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 22.286706ms)
Jun  8 15:43:13.870: INFO: (18) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 11.738774ms)
Jun  8 15:43:13.871: INFO: (18) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 12.879753ms)
Jun  8 15:43:13.872: INFO: (18) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 12.979599ms)
Jun  8 15:43:13.873: INFO: (18) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 14.475557ms)
Jun  8 15:43:13.873: INFO: (18) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 14.555068ms)
Jun  8 15:43:13.873: INFO: (18) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 14.868881ms)
Jun  8 15:43:13.874: INFO: (18) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 15.151067ms)
Jun  8 15:43:13.876: INFO: (18) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 16.979923ms)
Jun  8 15:43:13.876: INFO: (18) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 17.21006ms)
Jun  8 15:43:13.877: INFO: (18) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 18.291875ms)
Jun  8 15:43:13.877: INFO: (18) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 18.779153ms)
Jun  8 15:43:13.879: INFO: (18) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 20.542392ms)
Jun  8 15:43:13.881: INFO: (18) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 22.943655ms)
Jun  8 15:43:13.882: INFO: (18) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 23.064449ms)
Jun  8 15:43:13.882: INFO: (18) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 23.137989ms)
Jun  8 15:43:13.882: INFO: (18) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 23.134322ms)
Jun  8 15:43:13.891: INFO: (19) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 8.726268ms)
Jun  8 15:43:13.899: INFO: (19) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 16.937159ms)
Jun  8 15:43:13.899: INFO: (19) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 17.247777ms)
Jun  8 15:43:13.899: INFO: (19) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 17.25146ms)
Jun  8 15:43:13.900: INFO: (19) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 17.784561ms)
Jun  8 15:43:13.900: INFO: (19) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 17.66641ms)
Jun  8 15:43:13.900: INFO: (19) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 17.656295ms)
Jun  8 15:43:13.900: INFO: (19) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 17.814198ms)
Jun  8 15:43:13.900: INFO: (19) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 18.096229ms)
Jun  8 15:43:13.900: INFO: (19) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 18.344137ms)
Jun  8 15:43:13.900: INFO: (19) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 18.327084ms)
Jun  8 15:43:13.901: INFO: (19) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 18.610675ms)
Jun  8 15:43:13.901: INFO: (19) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 19.380003ms)
Jun  8 15:43:13.901: INFO: (19) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 19.366776ms)
Jun  8 15:43:13.901: INFO: (19) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 19.631561ms)
Jun  8 15:43:13.902: INFO: (19) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 19.89858ms)
STEP: deleting ReplicationController proxy-service-ksp8q in namespace proxy-2947, will wait for the garbage collector to delete the pods 06/08/23 15:43:13.902
Jun  8 15:43:13.966: INFO: Deleting ReplicationController proxy-service-ksp8q took: 8.289514ms
Jun  8 15:43:14.067: INFO: Terminating ReplicationController proxy-service-ksp8q pods took: 100.927338ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jun  8 15:43:16.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-2947" for this suite. 06/08/23 15:43:16.175
------------------------------
• [4.790 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:43:11.4
    Jun  8 15:43:11.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename proxy 06/08/23 15:43:11.401
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:43:11.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:43:11.422
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 06/08/23 15:43:11.439
    STEP: creating replication controller proxy-service-ksp8q in namespace proxy-2947 06/08/23 15:43:11.439
    I0608 15:43:11.447102      23 runners.go:193] Created replication controller with name: proxy-service-ksp8q, namespace: proxy-2947, replica count: 1
    I0608 15:43:12.498335      23 runners.go:193] proxy-service-ksp8q Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0608 15:43:13.499394      23 runners.go:193] proxy-service-ksp8q Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun  8 15:43:13.504: INFO: setup took 2.078620546s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 06/08/23 15:43:13.504
    Jun  8 15:43:13.520: INFO: (0) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 15.757177ms)
    Jun  8 15:43:13.526: INFO: (0) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 21.984464ms)
    Jun  8 15:43:13.526: INFO: (0) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 22.247606ms)
    Jun  8 15:43:13.526: INFO: (0) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 22.104526ms)
    Jun  8 15:43:13.526: INFO: (0) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 22.159104ms)
    Jun  8 15:43:13.526: INFO: (0) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 22.225743ms)
    Jun  8 15:43:13.526: INFO: (0) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 22.112134ms)
    Jun  8 15:43:13.526: INFO: (0) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 22.094677ms)
    Jun  8 15:43:13.526: INFO: (0) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 22.290246ms)
    Jun  8 15:43:13.527: INFO: (0) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 22.435342ms)
    Jun  8 15:43:13.527: INFO: (0) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 22.777428ms)
    Jun  8 15:43:13.527: INFO: (0) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 22.773561ms)
    Jun  8 15:43:13.527: INFO: (0) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 23.000789ms)
    Jun  8 15:43:13.527: INFO: (0) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 23.232811ms)
    Jun  8 15:43:13.527: INFO: (0) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 23.166944ms)
    Jun  8 15:43:13.527: INFO: (0) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 23.513569ms)
    Jun  8 15:43:13.533: INFO: (1) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 5.524045ms)
    Jun  8 15:43:13.536: INFO: (1) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 8.30606ms)
    Jun  8 15:43:13.540: INFO: (1) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 12.128112ms)
    Jun  8 15:43:13.542: INFO: (1) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 14.18231ms)
    Jun  8 15:43:13.542: INFO: (1) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 14.238466ms)
    Jun  8 15:43:13.542: INFO: (1) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 14.618116ms)
    Jun  8 15:43:13.542: INFO: (1) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 14.517205ms)
    Jun  8 15:43:13.542: INFO: (1) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 14.513023ms)
    Jun  8 15:43:13.542: INFO: (1) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 14.440222ms)
    Jun  8 15:43:13.543: INFO: (1) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 15.053692ms)
    Jun  8 15:43:13.543: INFO: (1) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 14.944422ms)
    Jun  8 15:43:13.545: INFO: (1) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 17.101608ms)
    Jun  8 15:43:13.546: INFO: (1) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 17.618207ms)
    Jun  8 15:43:13.547: INFO: (1) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 18.708079ms)
    Jun  8 15:43:13.547: INFO: (1) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 19.025749ms)
    Jun  8 15:43:13.547: INFO: (1) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 19.077066ms)
    Jun  8 15:43:13.556: INFO: (2) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 9.1439ms)
    Jun  8 15:43:13.556: INFO: (2) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 9.210847ms)
    Jun  8 15:43:13.557: INFO: (2) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 9.684708ms)
    Jun  8 15:43:13.557: INFO: (2) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 9.893803ms)
    Jun  8 15:43:13.557: INFO: (2) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 9.59766ms)
    Jun  8 15:43:13.559: INFO: (2) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 11.664069ms)
    Jun  8 15:43:13.561: INFO: (2) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 13.331673ms)
    Jun  8 15:43:13.561: INFO: (2) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 13.63285ms)
    Jun  8 15:43:13.561: INFO: (2) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 13.96823ms)
    Jun  8 15:43:13.562: INFO: (2) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 15.130808ms)
    Jun  8 15:43:13.563: INFO: (2) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 15.428246ms)
    Jun  8 15:43:13.563: INFO: (2) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 15.317486ms)
    Jun  8 15:43:13.563: INFO: (2) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 15.621341ms)
    Jun  8 15:43:13.563: INFO: (2) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 15.595069ms)
    Jun  8 15:43:13.563: INFO: (2) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 15.577965ms)
    Jun  8 15:43:13.563: INFO: (2) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 15.877507ms)
    Jun  8 15:43:13.575: INFO: (3) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 11.083417ms)
    Jun  8 15:43:13.575: INFO: (3) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 11.842247ms)
    Jun  8 15:43:13.575: INFO: (3) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 11.528354ms)
    Jun  8 15:43:13.575: INFO: (3) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 11.537454ms)
    Jun  8 15:43:13.576: INFO: (3) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 11.974874ms)
    Jun  8 15:43:13.576: INFO: (3) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 12.363769ms)
    Jun  8 15:43:13.576: INFO: (3) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 12.488426ms)
    Jun  8 15:43:13.576: INFO: (3) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 12.526453ms)
    Jun  8 15:43:13.576: INFO: (3) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 12.498581ms)
    Jun  8 15:43:13.578: INFO: (3) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 14.033598ms)
    Jun  8 15:43:13.579: INFO: (3) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 15.772545ms)
    Jun  8 15:43:13.580: INFO: (3) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 15.827787ms)
    Jun  8 15:43:13.580: INFO: (3) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 16.141861ms)
    Jun  8 15:43:13.581: INFO: (3) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 17.235855ms)
    Jun  8 15:43:13.581: INFO: (3) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 17.382841ms)
    Jun  8 15:43:13.581: INFO: (3) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 17.563863ms)
    Jun  8 15:43:13.592: INFO: (4) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 10.724819ms)
    Jun  8 15:43:13.593: INFO: (4) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 10.91649ms)
    Jun  8 15:43:13.593: INFO: (4) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 11.159579ms)
    Jun  8 15:43:13.593: INFO: (4) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 11.625862ms)
    Jun  8 15:43:13.593: INFO: (4) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 11.556438ms)
    Jun  8 15:43:13.593: INFO: (4) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 11.5911ms)
    Jun  8 15:43:13.595: INFO: (4) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 13.288142ms)
    Jun  8 15:43:13.595: INFO: (4) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 13.067126ms)
    Jun  8 15:43:13.595: INFO: (4) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 13.512815ms)
    Jun  8 15:43:13.595: INFO: (4) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 13.153515ms)
    Jun  8 15:43:13.598: INFO: (4) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 16.763433ms)
    Jun  8 15:43:13.599: INFO: (4) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 17.382542ms)
    Jun  8 15:43:13.600: INFO: (4) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 19.140069ms)
    Jun  8 15:43:13.600: INFO: (4) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 18.682564ms)
    Jun  8 15:43:13.601: INFO: (4) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 19.095509ms)
    Jun  8 15:43:13.602: INFO: (4) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 20.459268ms)
    Jun  8 15:43:13.615: INFO: (5) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 12.831231ms)
    Jun  8 15:43:13.617: INFO: (5) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 14.429088ms)
    Jun  8 15:43:13.618: INFO: (5) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 15.927994ms)
    Jun  8 15:43:13.618: INFO: (5) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 16.439851ms)
    Jun  8 15:43:13.619: INFO: (5) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 16.330926ms)
    Jun  8 15:43:13.619: INFO: (5) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 16.605751ms)
    Jun  8 15:43:13.619: INFO: (5) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 16.830058ms)
    Jun  8 15:43:13.619: INFO: (5) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 17.304177ms)
    Jun  8 15:43:13.620: INFO: (5) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 17.173654ms)
    Jun  8 15:43:13.620: INFO: (5) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 17.566276ms)
    Jun  8 15:43:13.620: INFO: (5) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 17.99501ms)
    Jun  8 15:43:13.620: INFO: (5) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 18.084711ms)
    Jun  8 15:43:13.621: INFO: (5) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 18.379785ms)
    Jun  8 15:43:13.621: INFO: (5) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 18.089781ms)
    Jun  8 15:43:13.621: INFO: (5) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 19.216364ms)
    Jun  8 15:43:13.623: INFO: (5) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 20.722868ms)
    Jun  8 15:43:13.631: INFO: (6) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 7.945764ms)
    Jun  8 15:43:13.637: INFO: (6) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 13.066949ms)
    Jun  8 15:43:13.637: INFO: (6) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 13.549206ms)
    Jun  8 15:43:13.637: INFO: (6) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 13.799694ms)
    Jun  8 15:43:13.637: INFO: (6) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 13.877111ms)
    Jun  8 15:43:13.638: INFO: (6) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 14.658291ms)
    Jun  8 15:43:13.638: INFO: (6) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 14.668427ms)
    Jun  8 15:43:13.638: INFO: (6) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 14.938303ms)
    Jun  8 15:43:13.638: INFO: (6) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 14.672317ms)
    Jun  8 15:43:13.639: INFO: (6) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 15.456643ms)
    Jun  8 15:43:13.644: INFO: (6) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 20.52789ms)
    Jun  8 15:43:13.644: INFO: (6) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 20.254438ms)
    Jun  8 15:43:13.644: INFO: (6) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 20.312078ms)
    Jun  8 15:43:13.644: INFO: (6) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 20.832142ms)
    Jun  8 15:43:13.644: INFO: (6) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 20.654703ms)
    Jun  8 15:43:13.644: INFO: (6) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 20.795623ms)
    Jun  8 15:43:13.653: INFO: (7) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 8.422413ms)
    Jun  8 15:43:13.653: INFO: (7) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 8.452166ms)
    Jun  8 15:43:13.655: INFO: (7) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 10.652629ms)
    Jun  8 15:43:13.655: INFO: (7) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 10.559229ms)
    Jun  8 15:43:13.656: INFO: (7) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 11.635001ms)
    Jun  8 15:43:13.659: INFO: (7) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 13.722434ms)
    Jun  8 15:43:13.659: INFO: (7) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 13.779043ms)
    Jun  8 15:43:13.659: INFO: (7) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 14.338667ms)
    Jun  8 15:43:13.659: INFO: (7) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 14.317432ms)
    Jun  8 15:43:13.659: INFO: (7) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 14.481047ms)
    Jun  8 15:43:13.659: INFO: (7) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 14.409366ms)
    Jun  8 15:43:13.659: INFO: (7) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 14.6217ms)
    Jun  8 15:43:13.660: INFO: (7) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 15.091478ms)
    Jun  8 15:43:13.660: INFO: (7) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 15.129003ms)
    Jun  8 15:43:13.660: INFO: (7) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 15.071079ms)
    Jun  8 15:43:13.660: INFO: (7) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 15.024261ms)
    Jun  8 15:43:13.666: INFO: (8) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 6.37401ms)
    Jun  8 15:43:13.667: INFO: (8) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 6.525ms)
    Jun  8 15:43:13.667: INFO: (8) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 7.056074ms)
    Jun  8 15:43:13.671: INFO: (8) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 9.978328ms)
    Jun  8 15:43:13.672: INFO: (8) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 11.387418ms)
    Jun  8 15:43:13.672: INFO: (8) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 11.4089ms)
    Jun  8 15:43:13.674: INFO: (8) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 13.834848ms)
    Jun  8 15:43:13.674: INFO: (8) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 13.974375ms)
    Jun  8 15:43:13.676: INFO: (8) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 15.4098ms)
    Jun  8 15:43:13.676: INFO: (8) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 15.99368ms)
    Jun  8 15:43:13.677: INFO: (8) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 15.994373ms)
    Jun  8 15:43:13.677: INFO: (8) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 16.684372ms)
    Jun  8 15:43:13.677: INFO: (8) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 17.40592ms)
    Jun  8 15:43:13.677: INFO: (8) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 17.019338ms)
    Jun  8 15:43:13.677: INFO: (8) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 17.119992ms)
    Jun  8 15:43:13.677: INFO: (8) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 17.510693ms)
    Jun  8 15:43:13.684: INFO: (9) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 6.541739ms)
    Jun  8 15:43:13.686: INFO: (9) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 8.936754ms)
    Jun  8 15:43:13.688: INFO: (9) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 10.043956ms)
    Jun  8 15:43:13.693: INFO: (9) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 15.667102ms)
    Jun  8 15:43:13.693: INFO: (9) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 15.812906ms)
    Jun  8 15:43:13.694: INFO: (9) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 16.004281ms)
    Jun  8 15:43:13.694: INFO: (9) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 16.075183ms)
    Jun  8 15:43:13.694: INFO: (9) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 16.487206ms)
    Jun  8 15:43:13.694: INFO: (9) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 16.68272ms)
    Jun  8 15:43:13.694: INFO: (9) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 16.587653ms)
    Jun  8 15:43:13.694: INFO: (9) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 16.582694ms)
    Jun  8 15:43:13.695: INFO: (9) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 17.234537ms)
    Jun  8 15:43:13.695: INFO: (9) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 17.942533ms)
    Jun  8 15:43:13.697: INFO: (9) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 19.123501ms)
    Jun  8 15:43:13.699: INFO: (9) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 20.975452ms)
    Jun  8 15:43:13.699: INFO: (9) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 21.157659ms)
    Jun  8 15:43:13.708: INFO: (10) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 9.048302ms)
    Jun  8 15:43:13.708: INFO: (10) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 9.514645ms)
    Jun  8 15:43:13.709: INFO: (10) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 9.921007ms)
    Jun  8 15:43:13.711: INFO: (10) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 11.71053ms)
    Jun  8 15:43:13.713: INFO: (10) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 13.549028ms)
    Jun  8 15:43:13.713: INFO: (10) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 13.528015ms)
    Jun  8 15:43:13.713: INFO: (10) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 13.653897ms)
    Jun  8 15:43:13.713: INFO: (10) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 13.652608ms)
    Jun  8 15:43:13.713: INFO: (10) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 13.664967ms)
    Jun  8 15:43:13.713: INFO: (10) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 13.59484ms)
    Jun  8 15:43:13.713: INFO: (10) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 13.618357ms)
    Jun  8 15:43:13.717: INFO: (10) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 17.957086ms)
    Jun  8 15:43:13.718: INFO: (10) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 18.767213ms)
    Jun  8 15:43:13.718: INFO: (10) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 19.116137ms)
    Jun  8 15:43:13.718: INFO: (10) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 19.042289ms)
    Jun  8 15:43:13.718: INFO: (10) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 19.231625ms)
    Jun  8 15:43:13.729: INFO: (11) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 10.952741ms)
    Jun  8 15:43:13.730: INFO: (11) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 11.63271ms)
    Jun  8 15:43:13.730: INFO: (11) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 11.537291ms)
    Jun  8 15:43:13.730: INFO: (11) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 11.53474ms)
    Jun  8 15:43:13.731: INFO: (11) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 11.740339ms)
    Jun  8 15:43:13.731: INFO: (11) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 11.9731ms)
    Jun  8 15:43:13.731: INFO: (11) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 12.086159ms)
    Jun  8 15:43:13.731: INFO: (11) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 12.019361ms)
    Jun  8 15:43:13.732: INFO: (11) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 13.159419ms)
    Jun  8 15:43:13.732: INFO: (11) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 13.572682ms)
    Jun  8 15:43:13.737: INFO: (11) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 18.211967ms)
    Jun  8 15:43:13.737: INFO: (11) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 19.013592ms)
    Jun  8 15:43:13.738: INFO: (11) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 18.996152ms)
    Jun  8 15:43:13.738: INFO: (11) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 18.842931ms)
    Jun  8 15:43:13.738: INFO: (11) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 18.936509ms)
    Jun  8 15:43:13.738: INFO: (11) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 19.300474ms)
    Jun  8 15:43:13.743: INFO: (12) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 4.698762ms)
    Jun  8 15:43:13.744: INFO: (12) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 5.817667ms)
    Jun  8 15:43:13.745: INFO: (12) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 6.974149ms)
    Jun  8 15:43:13.748: INFO: (12) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 9.772426ms)
    Jun  8 15:43:13.750: INFO: (12) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 12.055818ms)
    Jun  8 15:43:13.752: INFO: (12) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 13.130437ms)
    Jun  8 15:43:13.752: INFO: (12) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 13.177281ms)
    Jun  8 15:43:13.754: INFO: (12) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 15.674456ms)
    Jun  8 15:43:13.754: INFO: (12) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 15.89104ms)
    Jun  8 15:43:13.754: INFO: (12) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 16.01754ms)
    Jun  8 15:43:13.755: INFO: (12) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 16.255603ms)
    Jun  8 15:43:13.755: INFO: (12) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 16.440151ms)
    Jun  8 15:43:13.755: INFO: (12) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 16.511603ms)
    Jun  8 15:43:13.755: INFO: (12) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 16.569055ms)
    Jun  8 15:43:13.758: INFO: (12) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 19.467758ms)
    Jun  8 15:43:13.759: INFO: (12) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 20.717383ms)
    Jun  8 15:43:13.766: INFO: (13) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 6.402246ms)
    Jun  8 15:43:13.772: INFO: (13) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 12.372303ms)
    Jun  8 15:43:13.772: INFO: (13) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 12.461303ms)
    Jun  8 15:43:13.773: INFO: (13) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 13.44748ms)
    Jun  8 15:43:13.773: INFO: (13) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 13.857822ms)
    Jun  8 15:43:13.773: INFO: (13) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 14.044291ms)
    Jun  8 15:43:13.774: INFO: (13) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 14.381037ms)
    Jun  8 15:43:13.774: INFO: (13) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 14.803155ms)
    Jun  8 15:43:13.775: INFO: (13) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 15.390346ms)
    Jun  8 15:43:13.775: INFO: (13) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 15.84793ms)
    Jun  8 15:43:13.776: INFO: (13) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 16.355223ms)
    Jun  8 15:43:13.776: INFO: (13) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 16.450664ms)
    Jun  8 15:43:13.778: INFO: (13) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 18.395325ms)
    Jun  8 15:43:13.778: INFO: (13) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 18.701441ms)
    Jun  8 15:43:13.778: INFO: (13) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 18.707402ms)
    Jun  8 15:43:13.778: INFO: (13) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 18.847383ms)
    Jun  8 15:43:13.791: INFO: (14) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 12.527011ms)
    Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 14.167702ms)
    Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 14.266292ms)
    Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 14.293325ms)
    Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 14.422933ms)
    Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 14.364326ms)
    Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 14.432286ms)
    Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 14.388327ms)
    Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 14.191883ms)
    Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 14.545026ms)
    Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 14.579579ms)
    Jun  8 15:43:13.793: INFO: (14) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 14.279423ms)
    Jun  8 15:43:13.796: INFO: (14) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 16.958423ms)
    Jun  8 15:43:13.796: INFO: (14) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 17.503378ms)
    Jun  8 15:43:13.797: INFO: (14) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 18.116295ms)
    Jun  8 15:43:13.797: INFO: (14) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 18.168829ms)
    Jun  8 15:43:13.805: INFO: (15) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 7.921249ms)
    Jun  8 15:43:13.807: INFO: (15) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 9.964073ms)
    Jun  8 15:43:13.808: INFO: (15) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 10.653763ms)
    Jun  8 15:43:13.809: INFO: (15) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 11.616601ms)
    Jun  8 15:43:13.809: INFO: (15) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 11.770314ms)
    Jun  8 15:43:13.809: INFO: (15) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 11.738946ms)
    Jun  8 15:43:13.809: INFO: (15) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 12.276325ms)
    Jun  8 15:43:13.809: INFO: (15) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 12.124076ms)
    Jun  8 15:43:13.809: INFO: (15) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 12.415581ms)
    Jun  8 15:43:13.809: INFO: (15) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 12.285903ms)
    Jun  8 15:43:13.809: INFO: (15) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 12.376077ms)
    Jun  8 15:43:13.813: INFO: (15) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 15.241211ms)
    Jun  8 15:43:13.815: INFO: (15) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 17.518371ms)
    Jun  8 15:43:13.815: INFO: (15) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 17.280308ms)
    Jun  8 15:43:13.815: INFO: (15) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 17.441047ms)
    Jun  8 15:43:13.815: INFO: (15) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 18.283265ms)
    Jun  8 15:43:13.828: INFO: (16) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 11.688641ms)
    Jun  8 15:43:13.828: INFO: (16) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 11.967117ms)
    Jun  8 15:43:13.828: INFO: (16) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 12.33037ms)
    Jun  8 15:43:13.829: INFO: (16) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 12.835983ms)
    Jun  8 15:43:13.829: INFO: (16) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 12.712203ms)
    Jun  8 15:43:13.829: INFO: (16) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 13.176505ms)
    Jun  8 15:43:13.829: INFO: (16) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 13.577672ms)
    Jun  8 15:43:13.829: INFO: (16) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 13.94432ms)
    Jun  8 15:43:13.829: INFO: (16) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 13.841986ms)
    Jun  8 15:43:13.830: INFO: (16) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 14.012531ms)
    Jun  8 15:43:13.830: INFO: (16) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 13.842579ms)
    Jun  8 15:43:13.835: INFO: (16) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 19.117088ms)
    Jun  8 15:43:13.835: INFO: (16) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 19.938399ms)
    Jun  8 15:43:13.835: INFO: (16) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 19.775631ms)
    Jun  8 15:43:13.836: INFO: (16) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 20.012118ms)
    Jun  8 15:43:13.836: INFO: (16) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 19.934048ms)
    Jun  8 15:43:13.846: INFO: (17) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 9.43466ms)
    Jun  8 15:43:13.850: INFO: (17) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 14.566986ms)
    Jun  8 15:43:13.851: INFO: (17) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 14.789103ms)
    Jun  8 15:43:13.851: INFO: (17) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 15.217624ms)
    Jun  8 15:43:13.851: INFO: (17) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 15.130295ms)
    Jun  8 15:43:13.851: INFO: (17) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 15.390215ms)
    Jun  8 15:43:13.853: INFO: (17) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 17.450284ms)
    Jun  8 15:43:13.854: INFO: (17) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 17.588963ms)
    Jun  8 15:43:13.855: INFO: (17) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 19.476032ms)
    Jun  8 15:43:13.856: INFO: (17) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 19.778953ms)
    Jun  8 15:43:13.856: INFO: (17) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 20.022914ms)
    Jun  8 15:43:13.858: INFO: (17) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 22.257569ms)
    Jun  8 15:43:13.858: INFO: (17) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 22.301685ms)
    Jun  8 15:43:13.858: INFO: (17) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 22.324848ms)
    Jun  8 15:43:13.858: INFO: (17) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 22.407195ms)
    Jun  8 15:43:13.858: INFO: (17) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 22.286706ms)
    Jun  8 15:43:13.870: INFO: (18) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 11.738774ms)
    Jun  8 15:43:13.871: INFO: (18) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 12.879753ms)
    Jun  8 15:43:13.872: INFO: (18) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 12.979599ms)
    Jun  8 15:43:13.873: INFO: (18) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 14.475557ms)
    Jun  8 15:43:13.873: INFO: (18) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 14.555068ms)
    Jun  8 15:43:13.873: INFO: (18) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 14.868881ms)
    Jun  8 15:43:13.874: INFO: (18) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 15.151067ms)
    Jun  8 15:43:13.876: INFO: (18) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 16.979923ms)
    Jun  8 15:43:13.876: INFO: (18) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 17.21006ms)
    Jun  8 15:43:13.877: INFO: (18) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 18.291875ms)
    Jun  8 15:43:13.877: INFO: (18) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 18.779153ms)
    Jun  8 15:43:13.879: INFO: (18) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 20.542392ms)
    Jun  8 15:43:13.881: INFO: (18) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 22.943655ms)
    Jun  8 15:43:13.882: INFO: (18) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 23.064449ms)
    Jun  8 15:43:13.882: INFO: (18) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 23.137989ms)
    Jun  8 15:43:13.882: INFO: (18) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 23.134322ms)
    Jun  8 15:43:13.891: INFO: (19) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:460/proxy/: tls baz (200; 8.726268ms)
    Jun  8 15:43:13.899: INFO: (19) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:462/proxy/: tls qux (200; 16.937159ms)
    Jun  8 15:43:13.899: INFO: (19) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">test<... (200; 17.247777ms)
    Jun  8 15:43:13.899: INFO: (19) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r/proxy/rewriteme">test</a> (200; 17.25146ms)
    Jun  8 15:43:13.900: INFO: (19) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 17.784561ms)
    Jun  8 15:43:13.900: INFO: (19) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:1080/proxy/rewriteme">... (200; 17.66641ms)
    Jun  8 15:43:13.900: INFO: (19) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:160/proxy/: foo (200; 17.656295ms)
    Jun  8 15:43:13.900: INFO: (19) /api/v1/namespaces/proxy-2947/pods/proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 17.814198ms)
    Jun  8 15:43:13.900: INFO: (19) /api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/: <a href="/api/v1/namespaces/proxy-2947/pods/https:proxy-service-ksp8q-xzx6r:443/proxy/tlsrewritem... (200; 18.096229ms)
    Jun  8 15:43:13.900: INFO: (19) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname2/proxy/: tls qux (200; 18.344137ms)
    Jun  8 15:43:13.900: INFO: (19) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname1/proxy/: foo (200; 18.327084ms)
    Jun  8 15:43:13.901: INFO: (19) /api/v1/namespaces/proxy-2947/services/http:proxy-service-ksp8q:portname2/proxy/: bar (200; 18.610675ms)
    Jun  8 15:43:13.901: INFO: (19) /api/v1/namespaces/proxy-2947/services/https:proxy-service-ksp8q:tlsportname1/proxy/: tls baz (200; 19.380003ms)
    Jun  8 15:43:13.901: INFO: (19) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname2/proxy/: bar (200; 19.366776ms)
    Jun  8 15:43:13.901: INFO: (19) /api/v1/namespaces/proxy-2947/pods/http:proxy-service-ksp8q-xzx6r:162/proxy/: bar (200; 19.631561ms)
    Jun  8 15:43:13.902: INFO: (19) /api/v1/namespaces/proxy-2947/services/proxy-service-ksp8q:portname1/proxy/: foo (200; 19.89858ms)
    STEP: deleting ReplicationController proxy-service-ksp8q in namespace proxy-2947, will wait for the garbage collector to delete the pods 06/08/23 15:43:13.902
    Jun  8 15:43:13.966: INFO: Deleting ReplicationController proxy-service-ksp8q took: 8.289514ms
    Jun  8 15:43:14.067: INFO: Terminating ReplicationController proxy-service-ksp8q pods took: 100.927338ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:43:16.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-2947" for this suite. 06/08/23 15:43:16.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:43:16.192
Jun  8 15:43:16.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename statefulset 06/08/23 15:43:16.193
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:43:16.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:43:16.216
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6246 06/08/23 15:43:16.22
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-6246 06/08/23 15:43:16.228
Jun  8 15:43:16.243: INFO: Found 0 stateful pods, waiting for 1
Jun  8 15:43:26.247: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 06/08/23 15:43:26.254
STEP: updating a scale subresource 06/08/23 15:43:26.257
STEP: verifying the statefulset Spec.Replicas was modified 06/08/23 15:43:26.264
STEP: Patch a scale subresource 06/08/23 15:43:26.267
STEP: verifying the statefulset Spec.Replicas was modified 06/08/23 15:43:26.275
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun  8 15:43:26.279: INFO: Deleting all statefulset in ns statefulset-6246
Jun  8 15:43:26.284: INFO: Scaling statefulset ss to 0
Jun  8 15:43:36.302: INFO: Waiting for statefulset status.replicas updated to 0
Jun  8 15:43:36.306: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun  8 15:43:36.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6246" for this suite. 06/08/23 15:43:36.322
------------------------------
• [SLOW TEST] [20.138 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:43:16.192
    Jun  8 15:43:16.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename statefulset 06/08/23 15:43:16.193
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:43:16.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:43:16.216
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6246 06/08/23 15:43:16.22
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-6246 06/08/23 15:43:16.228
    Jun  8 15:43:16.243: INFO: Found 0 stateful pods, waiting for 1
    Jun  8 15:43:26.247: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 06/08/23 15:43:26.254
    STEP: updating a scale subresource 06/08/23 15:43:26.257
    STEP: verifying the statefulset Spec.Replicas was modified 06/08/23 15:43:26.264
    STEP: Patch a scale subresource 06/08/23 15:43:26.267
    STEP: verifying the statefulset Spec.Replicas was modified 06/08/23 15:43:26.275
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun  8 15:43:26.279: INFO: Deleting all statefulset in ns statefulset-6246
    Jun  8 15:43:26.284: INFO: Scaling statefulset ss to 0
    Jun  8 15:43:36.302: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  8 15:43:36.306: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:43:36.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6246" for this suite. 06/08/23 15:43:36.322
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:43:36.331
Jun  8 15:43:36.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename taint-multiple-pods 06/08/23 15:43:36.332
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:43:36.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:43:36.352
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Jun  8 15:43:36.355: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  8 15:44:36.394: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Jun  8 15:44:36.398: INFO: Starting informer...
STEP: Starting pods... 06/08/23 15:44:36.398
Jun  8 15:44:36.616: INFO: Pod1 is running on chl8tf-worker-001. Tainting Node
Jun  8 15:44:36.829: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9748" to be "running"
Jun  8 15:44:36.833: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.91112ms
Jun  8 15:44:38.837: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008218284s
Jun  8 15:44:38.837: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jun  8 15:44:38.837: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9748" to be "running"
Jun  8 15:44:38.840: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 3.499485ms
Jun  8 15:44:38.841: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jun  8 15:44:38.841: INFO: Pod2 is running on chl8tf-worker-001. Tainting Node
STEP: Trying to apply a taint on the Node 06/08/23 15:44:38.841
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/08/23 15:44:38.856
STEP: Waiting for Pod1 and Pod2 to be deleted 06/08/23 15:44:38.86
Jun  8 15:44:44.388: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jun  8 15:45:04.433: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/08/23 15:45:04.447
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:45:04.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-9748" for this suite. 06/08/23 15:45:04.455
------------------------------
• [SLOW TEST] [88.131 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:43:36.331
    Jun  8 15:43:36.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename taint-multiple-pods 06/08/23 15:43:36.332
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:43:36.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:43:36.352
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Jun  8 15:43:36.355: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun  8 15:44:36.394: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Jun  8 15:44:36.398: INFO: Starting informer...
    STEP: Starting pods... 06/08/23 15:44:36.398
    Jun  8 15:44:36.616: INFO: Pod1 is running on chl8tf-worker-001. Tainting Node
    Jun  8 15:44:36.829: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9748" to be "running"
    Jun  8 15:44:36.833: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.91112ms
    Jun  8 15:44:38.837: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008218284s
    Jun  8 15:44:38.837: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jun  8 15:44:38.837: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9748" to be "running"
    Jun  8 15:44:38.840: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 3.499485ms
    Jun  8 15:44:38.841: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jun  8 15:44:38.841: INFO: Pod2 is running on chl8tf-worker-001. Tainting Node
    STEP: Trying to apply a taint on the Node 06/08/23 15:44:38.841
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/08/23 15:44:38.856
    STEP: Waiting for Pod1 and Pod2 to be deleted 06/08/23 15:44:38.86
    Jun  8 15:44:44.388: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jun  8 15:45:04.433: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/08/23 15:45:04.447
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:45:04.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-9748" for this suite. 06/08/23 15:45:04.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:45:04.463
Jun  8 15:45:04.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename crd-publish-openapi 06/08/23 15:45:04.465
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:45:04.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:45:04.503
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 06/08/23 15:45:04.507
Jun  8 15:45:04.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:45:06.611: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:45:14.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6353" for this suite. 06/08/23 15:45:14.374
------------------------------
• [SLOW TEST] [9.918 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:45:04.463
    Jun  8 15:45:04.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename crd-publish-openapi 06/08/23 15:45:04.465
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:45:04.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:45:04.503
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 06/08/23 15:45:04.507
    Jun  8 15:45:04.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:45:06.611: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:45:14.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6353" for this suite. 06/08/23 15:45:14.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:45:14.383
Jun  8 15:45:14.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename webhook 06/08/23 15:45:14.384
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:45:14.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:45:14.406
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/08/23 15:45:14.424
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 15:45:14.799
STEP: Deploying the webhook pod 06/08/23 15:45:14.809
STEP: Wait for the deployment to be ready 06/08/23 15:45:14.822
Jun  8 15:45:14.829: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 06/08/23 15:45:16.844
STEP: Verifying the service has paired with the endpoint 06/08/23 15:45:16.861
Jun  8 15:45:17.861: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 06/08/23 15:45:17.866
STEP: create a pod that should be denied by the webhook 06/08/23 15:45:17.886
STEP: create a pod that causes the webhook to hang 06/08/23 15:45:17.903
STEP: create a configmap that should be denied by the webhook 06/08/23 15:45:27.912
STEP: create a configmap that should be admitted by the webhook 06/08/23 15:45:27.94
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 06/08/23 15:45:27.951
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 06/08/23 15:45:27.96
STEP: create a namespace that bypass the webhook 06/08/23 15:45:27.967
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 06/08/23 15:45:27.976
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:45:28.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-460" for this suite. 06/08/23 15:45:28.057
STEP: Destroying namespace "webhook-460-markers" for this suite. 06/08/23 15:45:28.066
------------------------------
• [SLOW TEST] [13.693 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:45:14.383
    Jun  8 15:45:14.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename webhook 06/08/23 15:45:14.384
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:45:14.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:45:14.406
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/08/23 15:45:14.424
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 15:45:14.799
    STEP: Deploying the webhook pod 06/08/23 15:45:14.809
    STEP: Wait for the deployment to be ready 06/08/23 15:45:14.822
    Jun  8 15:45:14.829: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 06/08/23 15:45:16.844
    STEP: Verifying the service has paired with the endpoint 06/08/23 15:45:16.861
    Jun  8 15:45:17.861: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 06/08/23 15:45:17.866
    STEP: create a pod that should be denied by the webhook 06/08/23 15:45:17.886
    STEP: create a pod that causes the webhook to hang 06/08/23 15:45:17.903
    STEP: create a configmap that should be denied by the webhook 06/08/23 15:45:27.912
    STEP: create a configmap that should be admitted by the webhook 06/08/23 15:45:27.94
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 06/08/23 15:45:27.951
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 06/08/23 15:45:27.96
    STEP: create a namespace that bypass the webhook 06/08/23 15:45:27.967
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 06/08/23 15:45:27.976
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:45:28.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-460" for this suite. 06/08/23 15:45:28.057
    STEP: Destroying namespace "webhook-460-markers" for this suite. 06/08/23 15:45:28.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:45:28.082
Jun  8 15:45:28.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename secrets 06/08/23 15:45:28.084
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:45:28.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:45:28.114
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-6e464c23-a3d1-4f1a-9dfe-5c834efb9d4f 06/08/23 15:45:28.127
STEP: Creating secret with name s-test-opt-upd-b454ccc4-920b-4dbb-8c9e-a79ef8c15205 06/08/23 15:45:28.134
STEP: Creating the pod 06/08/23 15:45:28.141
Jun  8 15:45:28.156: INFO: Waiting up to 5m0s for pod "pod-secrets-81d0da05-4ee6-4739-8540-5840b8356005" in namespace "secrets-577" to be "running and ready"
Jun  8 15:45:28.165: INFO: Pod "pod-secrets-81d0da05-4ee6-4739-8540-5840b8356005": Phase="Pending", Reason="", readiness=false. Elapsed: 9.728434ms
Jun  8 15:45:28.166: INFO: The phase of Pod pod-secrets-81d0da05-4ee6-4739-8540-5840b8356005 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:45:30.171: INFO: Pod "pod-secrets-81d0da05-4ee6-4739-8540-5840b8356005": Phase="Running", Reason="", readiness=true. Elapsed: 2.015507308s
Jun  8 15:45:30.171: INFO: The phase of Pod pod-secrets-81d0da05-4ee6-4739-8540-5840b8356005 is Running (Ready = true)
Jun  8 15:45:30.171: INFO: Pod "pod-secrets-81d0da05-4ee6-4739-8540-5840b8356005" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-6e464c23-a3d1-4f1a-9dfe-5c834efb9d4f 06/08/23 15:45:30.209
STEP: Updating secret s-test-opt-upd-b454ccc4-920b-4dbb-8c9e-a79ef8c15205 06/08/23 15:45:30.216
STEP: Creating secret with name s-test-opt-create-cfa06915-f8b5-48dd-9053-1a540c42f981 06/08/23 15:45:30.222
STEP: waiting to observe update in volume 06/08/23 15:45:30.227
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  8 15:45:32.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-577" for this suite. 06/08/23 15:45:32.264
------------------------------
• [4.189 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:45:28.082
    Jun  8 15:45:28.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename secrets 06/08/23 15:45:28.084
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:45:28.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:45:28.114
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-6e464c23-a3d1-4f1a-9dfe-5c834efb9d4f 06/08/23 15:45:28.127
    STEP: Creating secret with name s-test-opt-upd-b454ccc4-920b-4dbb-8c9e-a79ef8c15205 06/08/23 15:45:28.134
    STEP: Creating the pod 06/08/23 15:45:28.141
    Jun  8 15:45:28.156: INFO: Waiting up to 5m0s for pod "pod-secrets-81d0da05-4ee6-4739-8540-5840b8356005" in namespace "secrets-577" to be "running and ready"
    Jun  8 15:45:28.165: INFO: Pod "pod-secrets-81d0da05-4ee6-4739-8540-5840b8356005": Phase="Pending", Reason="", readiness=false. Elapsed: 9.728434ms
    Jun  8 15:45:28.166: INFO: The phase of Pod pod-secrets-81d0da05-4ee6-4739-8540-5840b8356005 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:45:30.171: INFO: Pod "pod-secrets-81d0da05-4ee6-4739-8540-5840b8356005": Phase="Running", Reason="", readiness=true. Elapsed: 2.015507308s
    Jun  8 15:45:30.171: INFO: The phase of Pod pod-secrets-81d0da05-4ee6-4739-8540-5840b8356005 is Running (Ready = true)
    Jun  8 15:45:30.171: INFO: Pod "pod-secrets-81d0da05-4ee6-4739-8540-5840b8356005" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-6e464c23-a3d1-4f1a-9dfe-5c834efb9d4f 06/08/23 15:45:30.209
    STEP: Updating secret s-test-opt-upd-b454ccc4-920b-4dbb-8c9e-a79ef8c15205 06/08/23 15:45:30.216
    STEP: Creating secret with name s-test-opt-create-cfa06915-f8b5-48dd-9053-1a540c42f981 06/08/23 15:45:30.222
    STEP: waiting to observe update in volume 06/08/23 15:45:30.227
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:45:32.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-577" for this suite. 06/08/23 15:45:32.264
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:45:32.271
Jun  8 15:45:32.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename daemonsets 06/08/23 15:45:32.273
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:45:32.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:45:32.294
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 06/08/23 15:45:32.329
STEP: Check that daemon pods launch on every node of the cluster. 06/08/23 15:45:32.335
Jun  8 15:45:32.343: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  8 15:45:32.343: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
Jun  8 15:45:33.354: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  8 15:45:33.354: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
Jun  8 15:45:34.356: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jun  8 15:45:34.356: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 06/08/23 15:45:34.359
Jun  8 15:45:34.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jun  8 15:45:34.383: INFO: Node chl8tf-worker-002 is running 0 daemon pod, expected 1
Jun  8 15:45:35.397: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jun  8 15:45:35.397: INFO: Node chl8tf-worker-002 is running 0 daemon pod, expected 1
Jun  8 15:45:36.403: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jun  8 15:45:36.403: INFO: Node chl8tf-worker-002 is running 0 daemon pod, expected 1
Jun  8 15:45:37.404: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jun  8 15:45:37.404: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 06/08/23 15:45:37.409
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6256, will wait for the garbage collector to delete the pods 06/08/23 15:45:37.409
Jun  8 15:45:37.472: INFO: Deleting DaemonSet.extensions daemon-set took: 7.980444ms
Jun  8 15:45:37.573: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.15196ms
Jun  8 15:45:40.479: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  8 15:45:40.479: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun  8 15:45:40.483: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35473"},"items":null}

Jun  8 15:45:40.487: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35473"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:45:40.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6256" for this suite. 06/08/23 15:45:40.519
------------------------------
• [SLOW TEST] [8.255 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:45:32.271
    Jun  8 15:45:32.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename daemonsets 06/08/23 15:45:32.273
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:45:32.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:45:32.294
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 06/08/23 15:45:32.329
    STEP: Check that daemon pods launch on every node of the cluster. 06/08/23 15:45:32.335
    Jun  8 15:45:32.343: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  8 15:45:32.343: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
    Jun  8 15:45:33.354: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  8 15:45:33.354: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
    Jun  8 15:45:34.356: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jun  8 15:45:34.356: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 06/08/23 15:45:34.359
    Jun  8 15:45:34.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jun  8 15:45:34.383: INFO: Node chl8tf-worker-002 is running 0 daemon pod, expected 1
    Jun  8 15:45:35.397: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jun  8 15:45:35.397: INFO: Node chl8tf-worker-002 is running 0 daemon pod, expected 1
    Jun  8 15:45:36.403: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jun  8 15:45:36.403: INFO: Node chl8tf-worker-002 is running 0 daemon pod, expected 1
    Jun  8 15:45:37.404: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jun  8 15:45:37.404: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 06/08/23 15:45:37.409
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6256, will wait for the garbage collector to delete the pods 06/08/23 15:45:37.409
    Jun  8 15:45:37.472: INFO: Deleting DaemonSet.extensions daemon-set took: 7.980444ms
    Jun  8 15:45:37.573: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.15196ms
    Jun  8 15:45:40.479: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  8 15:45:40.479: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun  8 15:45:40.483: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35473"},"items":null}

    Jun  8 15:45:40.487: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35473"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:45:40.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6256" for this suite. 06/08/23 15:45:40.519
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:45:40.528
Jun  8 15:45:40.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubectl 06/08/23 15:45:40.529
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:45:40.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:45:40.549
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 06/08/23 15:45:40.553
Jun  8 15:45:40.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 create -f -'
Jun  8 15:45:41.116: INFO: stderr: ""
Jun  8 15:45:41.116: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/08/23 15:45:41.116
Jun  8 15:45:41.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  8 15:45:41.206: INFO: stderr: ""
Jun  8 15:45:41.206: INFO: stdout: "update-demo-nautilus-fd5st update-demo-nautilus-mkz58 "
Jun  8 15:45:41.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 get pods update-demo-nautilus-fd5st -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  8 15:45:41.286: INFO: stderr: ""
Jun  8 15:45:41.286: INFO: stdout: ""
Jun  8 15:45:41.286: INFO: update-demo-nautilus-fd5st is created but not running
Jun  8 15:45:46.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  8 15:45:46.387: INFO: stderr: ""
Jun  8 15:45:46.387: INFO: stdout: "update-demo-nautilus-fd5st update-demo-nautilus-mkz58 "
Jun  8 15:45:46.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 get pods update-demo-nautilus-fd5st -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  8 15:45:46.480: INFO: stderr: ""
Jun  8 15:45:46.480: INFO: stdout: "true"
Jun  8 15:45:46.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 get pods update-demo-nautilus-fd5st -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  8 15:45:46.567: INFO: stderr: ""
Jun  8 15:45:46.567: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun  8 15:45:46.567: INFO: validating pod update-demo-nautilus-fd5st
Jun  8 15:45:46.574: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  8 15:45:46.574: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  8 15:45:46.574: INFO: update-demo-nautilus-fd5st is verified up and running
Jun  8 15:45:46.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 get pods update-demo-nautilus-mkz58 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  8 15:45:46.670: INFO: stderr: ""
Jun  8 15:45:46.670: INFO: stdout: "true"
Jun  8 15:45:46.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 get pods update-demo-nautilus-mkz58 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  8 15:45:46.764: INFO: stderr: ""
Jun  8 15:45:46.764: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun  8 15:45:46.764: INFO: validating pod update-demo-nautilus-mkz58
Jun  8 15:45:46.771: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  8 15:45:46.771: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  8 15:45:46.771: INFO: update-demo-nautilus-mkz58 is verified up and running
STEP: using delete to clean up resources 06/08/23 15:45:46.772
Jun  8 15:45:46.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 delete --grace-period=0 --force -f -'
Jun  8 15:45:46.896: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  8 15:45:46.896: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun  8 15:45:46.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 get rc,svc -l name=update-demo --no-headers'
Jun  8 15:45:47.017: INFO: stderr: "No resources found in kubectl-2875 namespace.\n"
Jun  8 15:45:47.017: INFO: stdout: ""
Jun  8 15:45:47.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  8 15:45:47.129: INFO: stderr: ""
Jun  8 15:45:47.129: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  8 15:45:47.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2875" for this suite. 06/08/23 15:45:47.136
------------------------------
• [SLOW TEST] [6.617 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:45:40.528
    Jun  8 15:45:40.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubectl 06/08/23 15:45:40.529
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:45:40.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:45:40.549
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 06/08/23 15:45:40.553
    Jun  8 15:45:40.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 create -f -'
    Jun  8 15:45:41.116: INFO: stderr: ""
    Jun  8 15:45:41.116: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/08/23 15:45:41.116
    Jun  8 15:45:41.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun  8 15:45:41.206: INFO: stderr: ""
    Jun  8 15:45:41.206: INFO: stdout: "update-demo-nautilus-fd5st update-demo-nautilus-mkz58 "
    Jun  8 15:45:41.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 get pods update-demo-nautilus-fd5st -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  8 15:45:41.286: INFO: stderr: ""
    Jun  8 15:45:41.286: INFO: stdout: ""
    Jun  8 15:45:41.286: INFO: update-demo-nautilus-fd5st is created but not running
    Jun  8 15:45:46.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun  8 15:45:46.387: INFO: stderr: ""
    Jun  8 15:45:46.387: INFO: stdout: "update-demo-nautilus-fd5st update-demo-nautilus-mkz58 "
    Jun  8 15:45:46.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 get pods update-demo-nautilus-fd5st -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  8 15:45:46.480: INFO: stderr: ""
    Jun  8 15:45:46.480: INFO: stdout: "true"
    Jun  8 15:45:46.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 get pods update-demo-nautilus-fd5st -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun  8 15:45:46.567: INFO: stderr: ""
    Jun  8 15:45:46.567: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun  8 15:45:46.567: INFO: validating pod update-demo-nautilus-fd5st
    Jun  8 15:45:46.574: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun  8 15:45:46.574: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun  8 15:45:46.574: INFO: update-demo-nautilus-fd5st is verified up and running
    Jun  8 15:45:46.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 get pods update-demo-nautilus-mkz58 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  8 15:45:46.670: INFO: stderr: ""
    Jun  8 15:45:46.670: INFO: stdout: "true"
    Jun  8 15:45:46.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 get pods update-demo-nautilus-mkz58 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun  8 15:45:46.764: INFO: stderr: ""
    Jun  8 15:45:46.764: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun  8 15:45:46.764: INFO: validating pod update-demo-nautilus-mkz58
    Jun  8 15:45:46.771: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun  8 15:45:46.771: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun  8 15:45:46.771: INFO: update-demo-nautilus-mkz58 is verified up and running
    STEP: using delete to clean up resources 06/08/23 15:45:46.772
    Jun  8 15:45:46.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 delete --grace-period=0 --force -f -'
    Jun  8 15:45:46.896: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun  8 15:45:46.896: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jun  8 15:45:46.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 get rc,svc -l name=update-demo --no-headers'
    Jun  8 15:45:47.017: INFO: stderr: "No resources found in kubectl-2875 namespace.\n"
    Jun  8 15:45:47.017: INFO: stdout: ""
    Jun  8 15:45:47.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-2875 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jun  8 15:45:47.129: INFO: stderr: ""
    Jun  8 15:45:47.129: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:45:47.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2875" for this suite. 06/08/23 15:45:47.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:45:47.147
Jun  8 15:45:47.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename configmap 06/08/23 15:45:47.148
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:45:47.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:45:47.175
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-25a92188-dc11-44f3-92dd-169204bd410c 06/08/23 15:45:47.185
STEP: Creating a pod to test consume configMaps 06/08/23 15:45:47.191
Jun  8 15:45:47.203: INFO: Waiting up to 5m0s for pod "pod-configmaps-a5a8b3f0-9226-4456-ae6e-d0fb4a595507" in namespace "configmap-6826" to be "Succeeded or Failed"
Jun  8 15:45:47.207: INFO: Pod "pod-configmaps-a5a8b3f0-9226-4456-ae6e-d0fb4a595507": Phase="Pending", Reason="", readiness=false. Elapsed: 4.302341ms
Jun  8 15:45:49.212: INFO: Pod "pod-configmaps-a5a8b3f0-9226-4456-ae6e-d0fb4a595507": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009306439s
Jun  8 15:45:51.213: INFO: Pod "pod-configmaps-a5a8b3f0-9226-4456-ae6e-d0fb4a595507": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00962955s
STEP: Saw pod success 06/08/23 15:45:51.213
Jun  8 15:45:51.213: INFO: Pod "pod-configmaps-a5a8b3f0-9226-4456-ae6e-d0fb4a595507" satisfied condition "Succeeded or Failed"
Jun  8 15:45:51.217: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-configmaps-a5a8b3f0-9226-4456-ae6e-d0fb4a595507 container agnhost-container: <nil>
STEP: delete the pod 06/08/23 15:45:51.224
Jun  8 15:45:51.236: INFO: Waiting for pod pod-configmaps-a5a8b3f0-9226-4456-ae6e-d0fb4a595507 to disappear
Jun  8 15:45:51.239: INFO: Pod pod-configmaps-a5a8b3f0-9226-4456-ae6e-d0fb4a595507 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  8 15:45:51.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6826" for this suite. 06/08/23 15:45:51.245
------------------------------
• [4.106 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:45:47.147
    Jun  8 15:45:47.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename configmap 06/08/23 15:45:47.148
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:45:47.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:45:47.175
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-25a92188-dc11-44f3-92dd-169204bd410c 06/08/23 15:45:47.185
    STEP: Creating a pod to test consume configMaps 06/08/23 15:45:47.191
    Jun  8 15:45:47.203: INFO: Waiting up to 5m0s for pod "pod-configmaps-a5a8b3f0-9226-4456-ae6e-d0fb4a595507" in namespace "configmap-6826" to be "Succeeded or Failed"
    Jun  8 15:45:47.207: INFO: Pod "pod-configmaps-a5a8b3f0-9226-4456-ae6e-d0fb4a595507": Phase="Pending", Reason="", readiness=false. Elapsed: 4.302341ms
    Jun  8 15:45:49.212: INFO: Pod "pod-configmaps-a5a8b3f0-9226-4456-ae6e-d0fb4a595507": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009306439s
    Jun  8 15:45:51.213: INFO: Pod "pod-configmaps-a5a8b3f0-9226-4456-ae6e-d0fb4a595507": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00962955s
    STEP: Saw pod success 06/08/23 15:45:51.213
    Jun  8 15:45:51.213: INFO: Pod "pod-configmaps-a5a8b3f0-9226-4456-ae6e-d0fb4a595507" satisfied condition "Succeeded or Failed"
    Jun  8 15:45:51.217: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-configmaps-a5a8b3f0-9226-4456-ae6e-d0fb4a595507 container agnhost-container: <nil>
    STEP: delete the pod 06/08/23 15:45:51.224
    Jun  8 15:45:51.236: INFO: Waiting for pod pod-configmaps-a5a8b3f0-9226-4456-ae6e-d0fb4a595507 to disappear
    Jun  8 15:45:51.239: INFO: Pod pod-configmaps-a5a8b3f0-9226-4456-ae6e-d0fb4a595507 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:45:51.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6826" for this suite. 06/08/23 15:45:51.245
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:45:51.254
Jun  8 15:45:51.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 15:45:51.255
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:45:51.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:45:51.275
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 06/08/23 15:45:51.278
Jun  8 15:45:51.288: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e47c5025-335c-4132-9a8c-ecba78bb54c9" in namespace "projected-6891" to be "Succeeded or Failed"
Jun  8 15:45:51.292: INFO: Pod "downwardapi-volume-e47c5025-335c-4132-9a8c-ecba78bb54c9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.71964ms
Jun  8 15:45:53.298: INFO: Pod "downwardapi-volume-e47c5025-335c-4132-9a8c-ecba78bb54c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010004993s
Jun  8 15:45:55.297: INFO: Pod "downwardapi-volume-e47c5025-335c-4132-9a8c-ecba78bb54c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008728015s
STEP: Saw pod success 06/08/23 15:45:55.297
Jun  8 15:45:55.297: INFO: Pod "downwardapi-volume-e47c5025-335c-4132-9a8c-ecba78bb54c9" satisfied condition "Succeeded or Failed"
Jun  8 15:45:55.301: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-e47c5025-335c-4132-9a8c-ecba78bb54c9 container client-container: <nil>
STEP: delete the pod 06/08/23 15:45:55.309
Jun  8 15:45:55.323: INFO: Waiting for pod downwardapi-volume-e47c5025-335c-4132-9a8c-ecba78bb54c9 to disappear
Jun  8 15:45:55.327: INFO: Pod downwardapi-volume-e47c5025-335c-4132-9a8c-ecba78bb54c9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  8 15:45:55.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6891" for this suite. 06/08/23 15:45:55.332
------------------------------
• [4.085 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:45:51.254
    Jun  8 15:45:51.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 15:45:51.255
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:45:51.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:45:51.275
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 06/08/23 15:45:51.278
    Jun  8 15:45:51.288: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e47c5025-335c-4132-9a8c-ecba78bb54c9" in namespace "projected-6891" to be "Succeeded or Failed"
    Jun  8 15:45:51.292: INFO: Pod "downwardapi-volume-e47c5025-335c-4132-9a8c-ecba78bb54c9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.71964ms
    Jun  8 15:45:53.298: INFO: Pod "downwardapi-volume-e47c5025-335c-4132-9a8c-ecba78bb54c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010004993s
    Jun  8 15:45:55.297: INFO: Pod "downwardapi-volume-e47c5025-335c-4132-9a8c-ecba78bb54c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008728015s
    STEP: Saw pod success 06/08/23 15:45:55.297
    Jun  8 15:45:55.297: INFO: Pod "downwardapi-volume-e47c5025-335c-4132-9a8c-ecba78bb54c9" satisfied condition "Succeeded or Failed"
    Jun  8 15:45:55.301: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-e47c5025-335c-4132-9a8c-ecba78bb54c9 container client-container: <nil>
    STEP: delete the pod 06/08/23 15:45:55.309
    Jun  8 15:45:55.323: INFO: Waiting for pod downwardapi-volume-e47c5025-335c-4132-9a8c-ecba78bb54c9 to disappear
    Jun  8 15:45:55.327: INFO: Pod downwardapi-volume-e47c5025-335c-4132-9a8c-ecba78bb54c9 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:45:55.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6891" for this suite. 06/08/23 15:45:55.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:45:55.341
Jun  8 15:45:55.341: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename webhook 06/08/23 15:45:55.342
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:45:55.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:45:55.364
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/08/23 15:45:55.381
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 15:45:55.743
STEP: Deploying the webhook pod 06/08/23 15:45:55.753
STEP: Wait for the deployment to be ready 06/08/23 15:45:55.767
Jun  8 15:45:55.775: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/08/23 15:45:57.788
STEP: Verifying the service has paired with the endpoint 06/08/23 15:45:57.805
Jun  8 15:45:58.805: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 06/08/23 15:45:58.81
STEP: Updating a mutating webhook configuration's rules to not include the create operation 06/08/23 15:45:58.831
STEP: Creating a configMap that should not be mutated 06/08/23 15:45:58.838
STEP: Patching a mutating webhook configuration's rules to include the create operation 06/08/23 15:45:58.853
STEP: Creating a configMap that should be mutated 06/08/23 15:45:58.862
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:45:58.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3284" for this suite. 06/08/23 15:45:58.965
STEP: Destroying namespace "webhook-3284-markers" for this suite. 06/08/23 15:45:58.98
------------------------------
• [3.649 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:45:55.341
    Jun  8 15:45:55.341: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename webhook 06/08/23 15:45:55.342
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:45:55.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:45:55.364
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/08/23 15:45:55.381
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 15:45:55.743
    STEP: Deploying the webhook pod 06/08/23 15:45:55.753
    STEP: Wait for the deployment to be ready 06/08/23 15:45:55.767
    Jun  8 15:45:55.775: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/08/23 15:45:57.788
    STEP: Verifying the service has paired with the endpoint 06/08/23 15:45:57.805
    Jun  8 15:45:58.805: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 06/08/23 15:45:58.81
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 06/08/23 15:45:58.831
    STEP: Creating a configMap that should not be mutated 06/08/23 15:45:58.838
    STEP: Patching a mutating webhook configuration's rules to include the create operation 06/08/23 15:45:58.853
    STEP: Creating a configMap that should be mutated 06/08/23 15:45:58.862
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:45:58.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3284" for this suite. 06/08/23 15:45:58.965
    STEP: Destroying namespace "webhook-3284-markers" for this suite. 06/08/23 15:45:58.98
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:45:58.992
Jun  8 15:45:58.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename resourcequota 06/08/23 15:45:58.993
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:45:59.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:45:59.022
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 06/08/23 15:45:59.027
STEP: Ensuring ResourceQuota status is calculated 06/08/23 15:45:59.035
STEP: Creating a ResourceQuota with not best effort scope 06/08/23 15:46:01.04
STEP: Ensuring ResourceQuota status is calculated 06/08/23 15:46:01.047
STEP: Creating a best-effort pod 06/08/23 15:46:03.052
STEP: Ensuring resource quota with best effort scope captures the pod usage 06/08/23 15:46:03.068
STEP: Ensuring resource quota with not best effort ignored the pod usage 06/08/23 15:46:05.074
STEP: Deleting the pod 06/08/23 15:46:07.079
STEP: Ensuring resource quota status released the pod usage 06/08/23 15:46:07.094
STEP: Creating a not best-effort pod 06/08/23 15:46:09.1
STEP: Ensuring resource quota with not best effort scope captures the pod usage 06/08/23 15:46:09.113
STEP: Ensuring resource quota with best effort scope ignored the pod usage 06/08/23 15:46:11.118
STEP: Deleting the pod 06/08/23 15:46:13.124
STEP: Ensuring resource quota status released the pod usage 06/08/23 15:46:13.135
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  8 15:46:15.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6879" for this suite. 06/08/23 15:46:15.147
------------------------------
• [SLOW TEST] [16.163 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:45:58.992
    Jun  8 15:45:58.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename resourcequota 06/08/23 15:45:58.993
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:45:59.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:45:59.022
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 06/08/23 15:45:59.027
    STEP: Ensuring ResourceQuota status is calculated 06/08/23 15:45:59.035
    STEP: Creating a ResourceQuota with not best effort scope 06/08/23 15:46:01.04
    STEP: Ensuring ResourceQuota status is calculated 06/08/23 15:46:01.047
    STEP: Creating a best-effort pod 06/08/23 15:46:03.052
    STEP: Ensuring resource quota with best effort scope captures the pod usage 06/08/23 15:46:03.068
    STEP: Ensuring resource quota with not best effort ignored the pod usage 06/08/23 15:46:05.074
    STEP: Deleting the pod 06/08/23 15:46:07.079
    STEP: Ensuring resource quota status released the pod usage 06/08/23 15:46:07.094
    STEP: Creating a not best-effort pod 06/08/23 15:46:09.1
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 06/08/23 15:46:09.113
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 06/08/23 15:46:11.118
    STEP: Deleting the pod 06/08/23 15:46:13.124
    STEP: Ensuring resource quota status released the pod usage 06/08/23 15:46:13.135
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:46:15.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6879" for this suite. 06/08/23 15:46:15.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:46:15.155
Jun  8 15:46:15.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename secrets 06/08/23 15:46:15.156
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:46:15.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:46:15.179
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-83db3d24-26c5-45d3-8b07-8143a6b63d25 06/08/23 15:46:15.182
STEP: Creating a pod to test consume secrets 06/08/23 15:46:15.188
Jun  8 15:46:15.196: INFO: Waiting up to 5m0s for pod "pod-secrets-cb893bf2-d78b-483b-911e-8efa28b2b7e9" in namespace "secrets-4576" to be "Succeeded or Failed"
Jun  8 15:46:15.205: INFO: Pod "pod-secrets-cb893bf2-d78b-483b-911e-8efa28b2b7e9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.176437ms
Jun  8 15:46:17.209: INFO: Pod "pod-secrets-cb893bf2-d78b-483b-911e-8efa28b2b7e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012597551s
Jun  8 15:46:19.210: INFO: Pod "pod-secrets-cb893bf2-d78b-483b-911e-8efa28b2b7e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013339726s
STEP: Saw pod success 06/08/23 15:46:19.21
Jun  8 15:46:19.210: INFO: Pod "pod-secrets-cb893bf2-d78b-483b-911e-8efa28b2b7e9" satisfied condition "Succeeded or Failed"
Jun  8 15:46:19.214: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-secrets-cb893bf2-d78b-483b-911e-8efa28b2b7e9 container secret-volume-test: <nil>
STEP: delete the pod 06/08/23 15:46:19.222
Jun  8 15:46:19.236: INFO: Waiting for pod pod-secrets-cb893bf2-d78b-483b-911e-8efa28b2b7e9 to disappear
Jun  8 15:46:19.239: INFO: Pod pod-secrets-cb893bf2-d78b-483b-911e-8efa28b2b7e9 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  8 15:46:19.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4576" for this suite. 06/08/23 15:46:19.245
------------------------------
• [4.097 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:46:15.155
    Jun  8 15:46:15.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename secrets 06/08/23 15:46:15.156
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:46:15.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:46:15.179
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-83db3d24-26c5-45d3-8b07-8143a6b63d25 06/08/23 15:46:15.182
    STEP: Creating a pod to test consume secrets 06/08/23 15:46:15.188
    Jun  8 15:46:15.196: INFO: Waiting up to 5m0s for pod "pod-secrets-cb893bf2-d78b-483b-911e-8efa28b2b7e9" in namespace "secrets-4576" to be "Succeeded or Failed"
    Jun  8 15:46:15.205: INFO: Pod "pod-secrets-cb893bf2-d78b-483b-911e-8efa28b2b7e9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.176437ms
    Jun  8 15:46:17.209: INFO: Pod "pod-secrets-cb893bf2-d78b-483b-911e-8efa28b2b7e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012597551s
    Jun  8 15:46:19.210: INFO: Pod "pod-secrets-cb893bf2-d78b-483b-911e-8efa28b2b7e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013339726s
    STEP: Saw pod success 06/08/23 15:46:19.21
    Jun  8 15:46:19.210: INFO: Pod "pod-secrets-cb893bf2-d78b-483b-911e-8efa28b2b7e9" satisfied condition "Succeeded or Failed"
    Jun  8 15:46:19.214: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-secrets-cb893bf2-d78b-483b-911e-8efa28b2b7e9 container secret-volume-test: <nil>
    STEP: delete the pod 06/08/23 15:46:19.222
    Jun  8 15:46:19.236: INFO: Waiting for pod pod-secrets-cb893bf2-d78b-483b-911e-8efa28b2b7e9 to disappear
    Jun  8 15:46:19.239: INFO: Pod pod-secrets-cb893bf2-d78b-483b-911e-8efa28b2b7e9 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:46:19.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4576" for this suite. 06/08/23 15:46:19.245
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:46:19.254
Jun  8 15:46:19.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename services 06/08/23 15:46:19.255
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:46:19.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:46:19.276
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  8 15:46:19.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-477" for this suite. 06/08/23 15:46:19.289
------------------------------
• [0.042 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:46:19.254
    Jun  8 15:46:19.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename services 06/08/23 15:46:19.255
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:46:19.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:46:19.276
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:46:19.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-477" for this suite. 06/08/23 15:46:19.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:46:19.298
Jun  8 15:46:19.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename container-probe 06/08/23 15:46:19.299
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:46:19.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:46:19.319
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-1b034b1e-706f-4598-b543-01473611aa24 in namespace container-probe-217 06/08/23 15:46:19.322
Jun  8 15:46:19.332: INFO: Waiting up to 5m0s for pod "liveness-1b034b1e-706f-4598-b543-01473611aa24" in namespace "container-probe-217" to be "not pending"
Jun  8 15:46:19.336: INFO: Pod "liveness-1b034b1e-706f-4598-b543-01473611aa24": Phase="Pending", Reason="", readiness=false. Elapsed: 4.084572ms
Jun  8 15:46:21.342: INFO: Pod "liveness-1b034b1e-706f-4598-b543-01473611aa24": Phase="Running", Reason="", readiness=true. Elapsed: 2.009476564s
Jun  8 15:46:21.342: INFO: Pod "liveness-1b034b1e-706f-4598-b543-01473611aa24" satisfied condition "not pending"
Jun  8 15:46:21.342: INFO: Started pod liveness-1b034b1e-706f-4598-b543-01473611aa24 in namespace container-probe-217
STEP: checking the pod's current state and verifying that restartCount is present 06/08/23 15:46:21.342
Jun  8 15:46:21.346: INFO: Initial restart count of pod liveness-1b034b1e-706f-4598-b543-01473611aa24 is 0
Jun  8 15:46:41.403: INFO: Restart count of pod container-probe-217/liveness-1b034b1e-706f-4598-b543-01473611aa24 is now 1 (20.057679142s elapsed)
Jun  8 15:47:01.463: INFO: Restart count of pod container-probe-217/liveness-1b034b1e-706f-4598-b543-01473611aa24 is now 2 (40.117524794s elapsed)
Jun  8 15:47:21.517: INFO: Restart count of pod container-probe-217/liveness-1b034b1e-706f-4598-b543-01473611aa24 is now 3 (1m0.171319645s elapsed)
Jun  8 15:47:41.573: INFO: Restart count of pod container-probe-217/liveness-1b034b1e-706f-4598-b543-01473611aa24 is now 4 (1m20.227548156s elapsed)
Jun  8 15:48:51.773: INFO: Restart count of pod container-probe-217/liveness-1b034b1e-706f-4598-b543-01473611aa24 is now 5 (2m30.427512615s elapsed)
STEP: deleting the pod 06/08/23 15:48:51.773
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun  8 15:48:51.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-217" for this suite. 06/08/23 15:48:51.796
------------------------------
• [SLOW TEST] [152.507 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:46:19.298
    Jun  8 15:46:19.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename container-probe 06/08/23 15:46:19.299
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:46:19.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:46:19.319
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-1b034b1e-706f-4598-b543-01473611aa24 in namespace container-probe-217 06/08/23 15:46:19.322
    Jun  8 15:46:19.332: INFO: Waiting up to 5m0s for pod "liveness-1b034b1e-706f-4598-b543-01473611aa24" in namespace "container-probe-217" to be "not pending"
    Jun  8 15:46:19.336: INFO: Pod "liveness-1b034b1e-706f-4598-b543-01473611aa24": Phase="Pending", Reason="", readiness=false. Elapsed: 4.084572ms
    Jun  8 15:46:21.342: INFO: Pod "liveness-1b034b1e-706f-4598-b543-01473611aa24": Phase="Running", Reason="", readiness=true. Elapsed: 2.009476564s
    Jun  8 15:46:21.342: INFO: Pod "liveness-1b034b1e-706f-4598-b543-01473611aa24" satisfied condition "not pending"
    Jun  8 15:46:21.342: INFO: Started pod liveness-1b034b1e-706f-4598-b543-01473611aa24 in namespace container-probe-217
    STEP: checking the pod's current state and verifying that restartCount is present 06/08/23 15:46:21.342
    Jun  8 15:46:21.346: INFO: Initial restart count of pod liveness-1b034b1e-706f-4598-b543-01473611aa24 is 0
    Jun  8 15:46:41.403: INFO: Restart count of pod container-probe-217/liveness-1b034b1e-706f-4598-b543-01473611aa24 is now 1 (20.057679142s elapsed)
    Jun  8 15:47:01.463: INFO: Restart count of pod container-probe-217/liveness-1b034b1e-706f-4598-b543-01473611aa24 is now 2 (40.117524794s elapsed)
    Jun  8 15:47:21.517: INFO: Restart count of pod container-probe-217/liveness-1b034b1e-706f-4598-b543-01473611aa24 is now 3 (1m0.171319645s elapsed)
    Jun  8 15:47:41.573: INFO: Restart count of pod container-probe-217/liveness-1b034b1e-706f-4598-b543-01473611aa24 is now 4 (1m20.227548156s elapsed)
    Jun  8 15:48:51.773: INFO: Restart count of pod container-probe-217/liveness-1b034b1e-706f-4598-b543-01473611aa24 is now 5 (2m30.427512615s elapsed)
    STEP: deleting the pod 06/08/23 15:48:51.773
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:48:51.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-217" for this suite. 06/08/23 15:48:51.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:48:51.811
Jun  8 15:48:51.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename container-lifecycle-hook 06/08/23 15:48:51.813
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:48:51.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:48:51.834
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 06/08/23 15:48:51.843
Jun  8 15:48:51.853: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7963" to be "running and ready"
Jun  8 15:48:51.857: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.086289ms
Jun  8 15:48:51.857: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:48:53.869: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.016092276s
Jun  8 15:48:53.869: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun  8 15:48:53.869: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 06/08/23 15:48:53.873
Jun  8 15:48:53.880: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-7963" to be "running and ready"
Jun  8 15:48:53.885: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.159152ms
Jun  8 15:48:53.885: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:48:55.891: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011186496s
Jun  8 15:48:55.891: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:48:57.892: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.011610627s
Jun  8 15:48:57.892: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jun  8 15:48:57.892: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 06/08/23 15:48:57.895
STEP: delete the pod with lifecycle hook 06/08/23 15:48:57.912
Jun  8 15:48:57.920: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  8 15:48:57.924: INFO: Pod pod-with-poststart-http-hook still exists
Jun  8 15:48:59.924: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  8 15:48:59.929: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jun  8 15:48:59.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-7963" for this suite. 06/08/23 15:48:59.936
------------------------------
• [SLOW TEST] [8.133 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:48:51.811
    Jun  8 15:48:51.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/08/23 15:48:51.813
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:48:51.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:48:51.834
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 06/08/23 15:48:51.843
    Jun  8 15:48:51.853: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7963" to be "running and ready"
    Jun  8 15:48:51.857: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.086289ms
    Jun  8 15:48:51.857: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:48:53.869: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.016092276s
    Jun  8 15:48:53.869: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun  8 15:48:53.869: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 06/08/23 15:48:53.873
    Jun  8 15:48:53.880: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-7963" to be "running and ready"
    Jun  8 15:48:53.885: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.159152ms
    Jun  8 15:48:53.885: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:48:55.891: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011186496s
    Jun  8 15:48:55.891: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:48:57.892: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.011610627s
    Jun  8 15:48:57.892: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jun  8 15:48:57.892: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 06/08/23 15:48:57.895
    STEP: delete the pod with lifecycle hook 06/08/23 15:48:57.912
    Jun  8 15:48:57.920: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jun  8 15:48:57.924: INFO: Pod pod-with-poststart-http-hook still exists
    Jun  8 15:48:59.924: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jun  8 15:48:59.929: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:48:59.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-7963" for this suite. 06/08/23 15:48:59.936
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:48:59.945
Jun  8 15:48:59.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename secrets 06/08/23 15:48:59.946
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:48:59.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:48:59.966
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 06/08/23 15:48:59.97
STEP: listing secrets in all namespaces to ensure that there are more than zero 06/08/23 15:48:59.985
STEP: patching the secret 06/08/23 15:48:59.99
STEP: deleting the secret using a LabelSelector 06/08/23 15:49:00.001
STEP: listing secrets in all namespaces, searching for label name and value in patch 06/08/23 15:49:00.01
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  8 15:49:00.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7674" for this suite. 06/08/23 15:49:00.02
------------------------------
• [0.083 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:48:59.945
    Jun  8 15:48:59.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename secrets 06/08/23 15:48:59.946
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:48:59.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:48:59.966
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 06/08/23 15:48:59.97
    STEP: listing secrets in all namespaces to ensure that there are more than zero 06/08/23 15:48:59.985
    STEP: patching the secret 06/08/23 15:48:59.99
    STEP: deleting the secret using a LabelSelector 06/08/23 15:49:00.001
    STEP: listing secrets in all namespaces, searching for label name and value in patch 06/08/23 15:49:00.01
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:49:00.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7674" for this suite. 06/08/23 15:49:00.02
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:49:00.035
Jun  8 15:49:00.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename emptydir 06/08/23 15:49:00.036
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:49:00.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:49:00.058
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 06/08/23 15:49:00.062
Jun  8 15:49:00.071: INFO: Waiting up to 5m0s for pod "pod-448c402e-a1a1-4704-b823-f60b66571185" in namespace "emptydir-9172" to be "Succeeded or Failed"
Jun  8 15:49:00.077: INFO: Pod "pod-448c402e-a1a1-4704-b823-f60b66571185": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075307ms
Jun  8 15:49:02.082: INFO: Pod "pod-448c402e-a1a1-4704-b823-f60b66571185": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010573347s
Jun  8 15:49:04.084: INFO: Pod "pod-448c402e-a1a1-4704-b823-f60b66571185": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012185358s
STEP: Saw pod success 06/08/23 15:49:04.084
Jun  8 15:49:04.084: INFO: Pod "pod-448c402e-a1a1-4704-b823-f60b66571185" satisfied condition "Succeeded or Failed"
Jun  8 15:49:04.087: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-448c402e-a1a1-4704-b823-f60b66571185 container test-container: <nil>
STEP: delete the pod 06/08/23 15:49:04.104
Jun  8 15:49:04.116: INFO: Waiting for pod pod-448c402e-a1a1-4704-b823-f60b66571185 to disappear
Jun  8 15:49:04.119: INFO: Pod pod-448c402e-a1a1-4704-b823-f60b66571185 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  8 15:49:04.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9172" for this suite. 06/08/23 15:49:04.125
------------------------------
• [4.098 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:49:00.035
    Jun  8 15:49:00.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename emptydir 06/08/23 15:49:00.036
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:49:00.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:49:00.058
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 06/08/23 15:49:00.062
    Jun  8 15:49:00.071: INFO: Waiting up to 5m0s for pod "pod-448c402e-a1a1-4704-b823-f60b66571185" in namespace "emptydir-9172" to be "Succeeded or Failed"
    Jun  8 15:49:00.077: INFO: Pod "pod-448c402e-a1a1-4704-b823-f60b66571185": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075307ms
    Jun  8 15:49:02.082: INFO: Pod "pod-448c402e-a1a1-4704-b823-f60b66571185": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010573347s
    Jun  8 15:49:04.084: INFO: Pod "pod-448c402e-a1a1-4704-b823-f60b66571185": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012185358s
    STEP: Saw pod success 06/08/23 15:49:04.084
    Jun  8 15:49:04.084: INFO: Pod "pod-448c402e-a1a1-4704-b823-f60b66571185" satisfied condition "Succeeded or Failed"
    Jun  8 15:49:04.087: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-448c402e-a1a1-4704-b823-f60b66571185 container test-container: <nil>
    STEP: delete the pod 06/08/23 15:49:04.104
    Jun  8 15:49:04.116: INFO: Waiting for pod pod-448c402e-a1a1-4704-b823-f60b66571185 to disappear
    Jun  8 15:49:04.119: INFO: Pod pod-448c402e-a1a1-4704-b823-f60b66571185 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:49:04.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9172" for this suite. 06/08/23 15:49:04.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:49:04.134
Jun  8 15:49:04.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename disruption 06/08/23 15:49:04.135
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:49:04.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:49:04.157
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 06/08/23 15:49:04.165
STEP: Updating PodDisruptionBudget status 06/08/23 15:49:06.174
STEP: Waiting for all pods to be running 06/08/23 15:49:06.184
Jun  8 15:49:06.191: INFO: running pods: 0 < 1
STEP: locating a running pod 06/08/23 15:49:08.197
STEP: Waiting for the pdb to be processed 06/08/23 15:49:08.21
STEP: Patching PodDisruptionBudget status 06/08/23 15:49:08.219
STEP: Waiting for the pdb to be processed 06/08/23 15:49:08.229
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jun  8 15:49:08.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-7278" for this suite. 06/08/23 15:49:08.238
------------------------------
• [4.111 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:49:04.134
    Jun  8 15:49:04.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename disruption 06/08/23 15:49:04.135
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:49:04.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:49:04.157
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 06/08/23 15:49:04.165
    STEP: Updating PodDisruptionBudget status 06/08/23 15:49:06.174
    STEP: Waiting for all pods to be running 06/08/23 15:49:06.184
    Jun  8 15:49:06.191: INFO: running pods: 0 < 1
    STEP: locating a running pod 06/08/23 15:49:08.197
    STEP: Waiting for the pdb to be processed 06/08/23 15:49:08.21
    STEP: Patching PodDisruptionBudget status 06/08/23 15:49:08.219
    STEP: Waiting for the pdb to be processed 06/08/23 15:49:08.229
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:49:08.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-7278" for this suite. 06/08/23 15:49:08.238
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:49:08.245
Jun  8 15:49:08.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename security-context-test 06/08/23 15:49:08.246
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:49:08.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:49:08.266
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Jun  8 15:49:08.278: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-bd7f1973-c9ea-4807-9897-3b66c4968de5" in namespace "security-context-test-5705" to be "Succeeded or Failed"
Jun  8 15:49:08.281: INFO: Pod "alpine-nnp-false-bd7f1973-c9ea-4807-9897-3b66c4968de5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.46422ms
Jun  8 15:49:10.286: INFO: Pod "alpine-nnp-false-bd7f1973-c9ea-4807-9897-3b66c4968de5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008349369s
Jun  8 15:49:12.288: INFO: Pod "alpine-nnp-false-bd7f1973-c9ea-4807-9897-3b66c4968de5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00997897s
Jun  8 15:49:12.288: INFO: Pod "alpine-nnp-false-bd7f1973-c9ea-4807-9897-3b66c4968de5" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jun  8 15:49:12.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-5705" for this suite. 06/08/23 15:49:12.302
------------------------------
• [4.065 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:49:08.245
    Jun  8 15:49:08.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename security-context-test 06/08/23 15:49:08.246
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:49:08.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:49:08.266
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Jun  8 15:49:08.278: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-bd7f1973-c9ea-4807-9897-3b66c4968de5" in namespace "security-context-test-5705" to be "Succeeded or Failed"
    Jun  8 15:49:08.281: INFO: Pod "alpine-nnp-false-bd7f1973-c9ea-4807-9897-3b66c4968de5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.46422ms
    Jun  8 15:49:10.286: INFO: Pod "alpine-nnp-false-bd7f1973-c9ea-4807-9897-3b66c4968de5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008349369s
    Jun  8 15:49:12.288: INFO: Pod "alpine-nnp-false-bd7f1973-c9ea-4807-9897-3b66c4968de5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00997897s
    Jun  8 15:49:12.288: INFO: Pod "alpine-nnp-false-bd7f1973-c9ea-4807-9897-3b66c4968de5" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:49:12.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-5705" for this suite. 06/08/23 15:49:12.302
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:49:12.311
Jun  8 15:49:12.311: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename deployment 06/08/23 15:49:12.312
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:49:12.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:49:12.338
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 06/08/23 15:49:12.345
Jun  8 15:49:12.345: INFO: Creating simple deployment test-deployment-mmqxw
Jun  8 15:49:12.360: INFO: deployment "test-deployment-mmqxw" doesn't have the required revision set
STEP: Getting /status 06/08/23 15:49:14.377
Jun  8 15:49:14.382: INFO: Deployment test-deployment-mmqxw has Conditions: [{Available True 2023-06-08 15:49:13 +0000 UTC 2023-06-08 15:49:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-06-08 15:49:13 +0000 UTC 2023-06-08 15:49:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mmqxw-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 06/08/23 15:49:14.382
Jun  8 15:49:14.394: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 49, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 49, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 49, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 49, 12, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-mmqxw-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 06/08/23 15:49:14.394
Jun  8 15:49:14.396: INFO: Observed &Deployment event: ADDED
Jun  8 15:49:14.396: INFO: Observed Deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-08 15:49:12 +0000 UTC 2023-06-08 15:49:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mmqxw-54bc444df"}
Jun  8 15:49:14.396: INFO: Observed &Deployment event: MODIFIED
Jun  8 15:49:14.396: INFO: Observed Deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-08 15:49:12 +0000 UTC 2023-06-08 15:49:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mmqxw-54bc444df"}
Jun  8 15:49:14.396: INFO: Observed Deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-08 15:49:12 +0000 UTC 2023-06-08 15:49:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun  8 15:49:14.396: INFO: Observed &Deployment event: MODIFIED
Jun  8 15:49:14.396: INFO: Observed Deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-08 15:49:12 +0000 UTC 2023-06-08 15:49:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun  8 15:49:14.396: INFO: Observed Deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-08 15:49:12 +0000 UTC 2023-06-08 15:49:12 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-mmqxw-54bc444df" is progressing.}
Jun  8 15:49:14.397: INFO: Observed &Deployment event: MODIFIED
Jun  8 15:49:14.397: INFO: Observed Deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-08 15:49:13 +0000 UTC 2023-06-08 15:49:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun  8 15:49:14.397: INFO: Observed Deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-08 15:49:13 +0000 UTC 2023-06-08 15:49:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mmqxw-54bc444df" has successfully progressed.}
Jun  8 15:49:14.397: INFO: Observed &Deployment event: MODIFIED
Jun  8 15:49:14.397: INFO: Observed Deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-08 15:49:13 +0000 UTC 2023-06-08 15:49:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun  8 15:49:14.397: INFO: Observed Deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-08 15:49:13 +0000 UTC 2023-06-08 15:49:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mmqxw-54bc444df" has successfully progressed.}
Jun  8 15:49:14.397: INFO: Found Deployment test-deployment-mmqxw in namespace deployment-267 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun  8 15:49:14.397: INFO: Deployment test-deployment-mmqxw has an updated status
STEP: patching the Statefulset Status 06/08/23 15:49:14.397
Jun  8 15:49:14.397: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun  8 15:49:14.406: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 06/08/23 15:49:14.406
Jun  8 15:49:14.408: INFO: Observed &Deployment event: ADDED
Jun  8 15:49:14.408: INFO: Observed deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-08 15:49:12 +0000 UTC 2023-06-08 15:49:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mmqxw-54bc444df"}
Jun  8 15:49:14.408: INFO: Observed &Deployment event: MODIFIED
Jun  8 15:49:14.408: INFO: Observed deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-08 15:49:12 +0000 UTC 2023-06-08 15:49:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mmqxw-54bc444df"}
Jun  8 15:49:14.408: INFO: Observed deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-08 15:49:12 +0000 UTC 2023-06-08 15:49:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun  8 15:49:14.408: INFO: Observed &Deployment event: MODIFIED
Jun  8 15:49:14.408: INFO: Observed deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-08 15:49:12 +0000 UTC 2023-06-08 15:49:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun  8 15:49:14.408: INFO: Observed deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-08 15:49:12 +0000 UTC 2023-06-08 15:49:12 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-mmqxw-54bc444df" is progressing.}
Jun  8 15:49:14.409: INFO: Observed &Deployment event: MODIFIED
Jun  8 15:49:14.409: INFO: Observed deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-08 15:49:13 +0000 UTC 2023-06-08 15:49:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun  8 15:49:14.409: INFO: Observed deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-08 15:49:13 +0000 UTC 2023-06-08 15:49:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mmqxw-54bc444df" has successfully progressed.}
Jun  8 15:49:14.409: INFO: Observed &Deployment event: MODIFIED
Jun  8 15:49:14.409: INFO: Observed deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-08 15:49:13 +0000 UTC 2023-06-08 15:49:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun  8 15:49:14.409: INFO: Observed deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-08 15:49:13 +0000 UTC 2023-06-08 15:49:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mmqxw-54bc444df" has successfully progressed.}
Jun  8 15:49:14.409: INFO: Observed deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun  8 15:49:14.409: INFO: Observed &Deployment event: MODIFIED
Jun  8 15:49:14.409: INFO: Found deployment test-deployment-mmqxw in namespace deployment-267 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jun  8 15:49:14.409: INFO: Deployment test-deployment-mmqxw has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun  8 15:49:14.415: INFO: Deployment "test-deployment-mmqxw":
&Deployment{ObjectMeta:{test-deployment-mmqxw  deployment-267  c047cd3d-2705-4ae2-9ab3-a7c1d018e14b 36803 1 2023-06-08 15:49:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-08 15:49:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-06-08 15:49:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-06-08 15:49:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004172af8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-mmqxw-54bc444df",LastUpdateTime:2023-06-08 15:49:14 +0000 UTC,LastTransitionTime:2023-06-08 15:49:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  8 15:49:14.418: INFO: New ReplicaSet "test-deployment-mmqxw-54bc444df" of Deployment "test-deployment-mmqxw":
&ReplicaSet{ObjectMeta:{test-deployment-mmqxw-54bc444df  deployment-267  fea1b6e6-3d1d-401e-b886-80bff8b5d206 36784 1 2023-06-08 15:49:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-mmqxw c047cd3d-2705-4ae2-9ab3-a7c1d018e14b 0xc004172ef0 0xc004172ef1}] [] [{kube-controller-manager Update apps/v1 2023-06-08 15:49:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c047cd3d-2705-4ae2-9ab3-a7c1d018e14b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 15:49:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004172f98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  8 15:49:14.428: INFO: Pod "test-deployment-mmqxw-54bc444df-qtzkv" is available:
&Pod{ObjectMeta:{test-deployment-mmqxw-54bc444df-qtzkv test-deployment-mmqxw-54bc444df- deployment-267  c264a4f9-294f-46bf-9bf7-05291ef3b590 36783 0 2023-06-08 15:49:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-mmqxw-54bc444df fea1b6e6-3d1d-401e-b886-80bff8b5d206 0xc004173360 0xc004173361}] [] [{kube-controller-manager Update v1 2023-06-08 15:49:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fea1b6e6-3d1d-401e-b886-80bff8b5d206\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:49:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cn5mj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cn5mj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:49:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:49:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:49:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:49:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.215,PodIP:10.244.3.174,StartTime:2023-06-08 15:49:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:49:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://24c8fa0a7b76a4817f954529a0bca2b2819e81f809d34bab1fbbf8a5a850e538,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun  8 15:49:14.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-267" for this suite. 06/08/23 15:49:14.434
------------------------------
• [2.131 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:49:12.311
    Jun  8 15:49:12.311: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename deployment 06/08/23 15:49:12.312
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:49:12.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:49:12.338
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 06/08/23 15:49:12.345
    Jun  8 15:49:12.345: INFO: Creating simple deployment test-deployment-mmqxw
    Jun  8 15:49:12.360: INFO: deployment "test-deployment-mmqxw" doesn't have the required revision set
    STEP: Getting /status 06/08/23 15:49:14.377
    Jun  8 15:49:14.382: INFO: Deployment test-deployment-mmqxw has Conditions: [{Available True 2023-06-08 15:49:13 +0000 UTC 2023-06-08 15:49:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-06-08 15:49:13 +0000 UTC 2023-06-08 15:49:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mmqxw-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 06/08/23 15:49:14.382
    Jun  8 15:49:14.394: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 49, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 49, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 15, 49, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 15, 49, 12, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-mmqxw-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 06/08/23 15:49:14.394
    Jun  8 15:49:14.396: INFO: Observed &Deployment event: ADDED
    Jun  8 15:49:14.396: INFO: Observed Deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-08 15:49:12 +0000 UTC 2023-06-08 15:49:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mmqxw-54bc444df"}
    Jun  8 15:49:14.396: INFO: Observed &Deployment event: MODIFIED
    Jun  8 15:49:14.396: INFO: Observed Deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-08 15:49:12 +0000 UTC 2023-06-08 15:49:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mmqxw-54bc444df"}
    Jun  8 15:49:14.396: INFO: Observed Deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-08 15:49:12 +0000 UTC 2023-06-08 15:49:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun  8 15:49:14.396: INFO: Observed &Deployment event: MODIFIED
    Jun  8 15:49:14.396: INFO: Observed Deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-08 15:49:12 +0000 UTC 2023-06-08 15:49:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun  8 15:49:14.396: INFO: Observed Deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-08 15:49:12 +0000 UTC 2023-06-08 15:49:12 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-mmqxw-54bc444df" is progressing.}
    Jun  8 15:49:14.397: INFO: Observed &Deployment event: MODIFIED
    Jun  8 15:49:14.397: INFO: Observed Deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-08 15:49:13 +0000 UTC 2023-06-08 15:49:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun  8 15:49:14.397: INFO: Observed Deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-08 15:49:13 +0000 UTC 2023-06-08 15:49:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mmqxw-54bc444df" has successfully progressed.}
    Jun  8 15:49:14.397: INFO: Observed &Deployment event: MODIFIED
    Jun  8 15:49:14.397: INFO: Observed Deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-08 15:49:13 +0000 UTC 2023-06-08 15:49:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun  8 15:49:14.397: INFO: Observed Deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-08 15:49:13 +0000 UTC 2023-06-08 15:49:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mmqxw-54bc444df" has successfully progressed.}
    Jun  8 15:49:14.397: INFO: Found Deployment test-deployment-mmqxw in namespace deployment-267 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun  8 15:49:14.397: INFO: Deployment test-deployment-mmqxw has an updated status
    STEP: patching the Statefulset Status 06/08/23 15:49:14.397
    Jun  8 15:49:14.397: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jun  8 15:49:14.406: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 06/08/23 15:49:14.406
    Jun  8 15:49:14.408: INFO: Observed &Deployment event: ADDED
    Jun  8 15:49:14.408: INFO: Observed deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-08 15:49:12 +0000 UTC 2023-06-08 15:49:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mmqxw-54bc444df"}
    Jun  8 15:49:14.408: INFO: Observed &Deployment event: MODIFIED
    Jun  8 15:49:14.408: INFO: Observed deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-08 15:49:12 +0000 UTC 2023-06-08 15:49:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-mmqxw-54bc444df"}
    Jun  8 15:49:14.408: INFO: Observed deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-08 15:49:12 +0000 UTC 2023-06-08 15:49:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun  8 15:49:14.408: INFO: Observed &Deployment event: MODIFIED
    Jun  8 15:49:14.408: INFO: Observed deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-08 15:49:12 +0000 UTC 2023-06-08 15:49:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun  8 15:49:14.408: INFO: Observed deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-08 15:49:12 +0000 UTC 2023-06-08 15:49:12 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-mmqxw-54bc444df" is progressing.}
    Jun  8 15:49:14.409: INFO: Observed &Deployment event: MODIFIED
    Jun  8 15:49:14.409: INFO: Observed deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-08 15:49:13 +0000 UTC 2023-06-08 15:49:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun  8 15:49:14.409: INFO: Observed deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-08 15:49:13 +0000 UTC 2023-06-08 15:49:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mmqxw-54bc444df" has successfully progressed.}
    Jun  8 15:49:14.409: INFO: Observed &Deployment event: MODIFIED
    Jun  8 15:49:14.409: INFO: Observed deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-08 15:49:13 +0000 UTC 2023-06-08 15:49:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun  8 15:49:14.409: INFO: Observed deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-08 15:49:13 +0000 UTC 2023-06-08 15:49:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-mmqxw-54bc444df" has successfully progressed.}
    Jun  8 15:49:14.409: INFO: Observed deployment test-deployment-mmqxw in namespace deployment-267 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun  8 15:49:14.409: INFO: Observed &Deployment event: MODIFIED
    Jun  8 15:49:14.409: INFO: Found deployment test-deployment-mmqxw in namespace deployment-267 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jun  8 15:49:14.409: INFO: Deployment test-deployment-mmqxw has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun  8 15:49:14.415: INFO: Deployment "test-deployment-mmqxw":
    &Deployment{ObjectMeta:{test-deployment-mmqxw  deployment-267  c047cd3d-2705-4ae2-9ab3-a7c1d018e14b 36803 1 2023-06-08 15:49:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-08 15:49:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-06-08 15:49:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-06-08 15:49:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004172af8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-mmqxw-54bc444df",LastUpdateTime:2023-06-08 15:49:14 +0000 UTC,LastTransitionTime:2023-06-08 15:49:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun  8 15:49:14.418: INFO: New ReplicaSet "test-deployment-mmqxw-54bc444df" of Deployment "test-deployment-mmqxw":
    &ReplicaSet{ObjectMeta:{test-deployment-mmqxw-54bc444df  deployment-267  fea1b6e6-3d1d-401e-b886-80bff8b5d206 36784 1 2023-06-08 15:49:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-mmqxw c047cd3d-2705-4ae2-9ab3-a7c1d018e14b 0xc004172ef0 0xc004172ef1}] [] [{kube-controller-manager Update apps/v1 2023-06-08 15:49:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c047cd3d-2705-4ae2-9ab3-a7c1d018e14b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 15:49:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004172f98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun  8 15:49:14.428: INFO: Pod "test-deployment-mmqxw-54bc444df-qtzkv" is available:
    &Pod{ObjectMeta:{test-deployment-mmqxw-54bc444df-qtzkv test-deployment-mmqxw-54bc444df- deployment-267  c264a4f9-294f-46bf-9bf7-05291ef3b590 36783 0 2023-06-08 15:49:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-mmqxw-54bc444df fea1b6e6-3d1d-401e-b886-80bff8b5d206 0xc004173360 0xc004173361}] [] [{kube-controller-manager Update v1 2023-06-08 15:49:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fea1b6e6-3d1d-401e-b886-80bff8b5d206\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:49:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cn5mj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cn5mj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:49:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:49:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:49:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:49:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.215,PodIP:10.244.3.174,StartTime:2023-06-08 15:49:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:49:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://24c8fa0a7b76a4817f954529a0bca2b2819e81f809d34bab1fbbf8a5a850e538,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:49:14.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-267" for this suite. 06/08/23 15:49:14.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:49:14.443
Jun  8 15:49:14.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename secrets 06/08/23 15:49:14.444
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:49:14.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:49:14.466
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-3563a5a9-0b09-42de-a6b6-84b0c81a9ba6 06/08/23 15:49:14.469
STEP: Creating a pod to test consume secrets 06/08/23 15:49:14.474
Jun  8 15:49:14.484: INFO: Waiting up to 5m0s for pod "pod-secrets-ddd7c194-84bc-4bd1-ba34-45b80e43a838" in namespace "secrets-5528" to be "Succeeded or Failed"
Jun  8 15:49:14.488: INFO: Pod "pod-secrets-ddd7c194-84bc-4bd1-ba34-45b80e43a838": Phase="Pending", Reason="", readiness=false. Elapsed: 4.413521ms
Jun  8 15:49:16.493: INFO: Pod "pod-secrets-ddd7c194-84bc-4bd1-ba34-45b80e43a838": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009496109s
Jun  8 15:49:18.494: INFO: Pod "pod-secrets-ddd7c194-84bc-4bd1-ba34-45b80e43a838": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010356771s
STEP: Saw pod success 06/08/23 15:49:18.494
Jun  8 15:49:18.494: INFO: Pod "pod-secrets-ddd7c194-84bc-4bd1-ba34-45b80e43a838" satisfied condition "Succeeded or Failed"
Jun  8 15:49:18.498: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-secrets-ddd7c194-84bc-4bd1-ba34-45b80e43a838 container secret-volume-test: <nil>
STEP: delete the pod 06/08/23 15:49:18.507
Jun  8 15:49:18.522: INFO: Waiting for pod pod-secrets-ddd7c194-84bc-4bd1-ba34-45b80e43a838 to disappear
Jun  8 15:49:18.525: INFO: Pod pod-secrets-ddd7c194-84bc-4bd1-ba34-45b80e43a838 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  8 15:49:18.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5528" for this suite. 06/08/23 15:49:18.531
------------------------------
• [4.095 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:49:14.443
    Jun  8 15:49:14.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename secrets 06/08/23 15:49:14.444
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:49:14.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:49:14.466
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-3563a5a9-0b09-42de-a6b6-84b0c81a9ba6 06/08/23 15:49:14.469
    STEP: Creating a pod to test consume secrets 06/08/23 15:49:14.474
    Jun  8 15:49:14.484: INFO: Waiting up to 5m0s for pod "pod-secrets-ddd7c194-84bc-4bd1-ba34-45b80e43a838" in namespace "secrets-5528" to be "Succeeded or Failed"
    Jun  8 15:49:14.488: INFO: Pod "pod-secrets-ddd7c194-84bc-4bd1-ba34-45b80e43a838": Phase="Pending", Reason="", readiness=false. Elapsed: 4.413521ms
    Jun  8 15:49:16.493: INFO: Pod "pod-secrets-ddd7c194-84bc-4bd1-ba34-45b80e43a838": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009496109s
    Jun  8 15:49:18.494: INFO: Pod "pod-secrets-ddd7c194-84bc-4bd1-ba34-45b80e43a838": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010356771s
    STEP: Saw pod success 06/08/23 15:49:18.494
    Jun  8 15:49:18.494: INFO: Pod "pod-secrets-ddd7c194-84bc-4bd1-ba34-45b80e43a838" satisfied condition "Succeeded or Failed"
    Jun  8 15:49:18.498: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-secrets-ddd7c194-84bc-4bd1-ba34-45b80e43a838 container secret-volume-test: <nil>
    STEP: delete the pod 06/08/23 15:49:18.507
    Jun  8 15:49:18.522: INFO: Waiting for pod pod-secrets-ddd7c194-84bc-4bd1-ba34-45b80e43a838 to disappear
    Jun  8 15:49:18.525: INFO: Pod pod-secrets-ddd7c194-84bc-4bd1-ba34-45b80e43a838 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:49:18.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5528" for this suite. 06/08/23 15:49:18.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:49:18.539
Jun  8 15:49:18.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename webhook 06/08/23 15:49:18.54
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:49:18.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:49:18.563
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/08/23 15:49:18.579
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 15:49:19.083
STEP: Deploying the webhook pod 06/08/23 15:49:19.093
STEP: Wait for the deployment to be ready 06/08/23 15:49:19.107
Jun  8 15:49:19.117: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/08/23 15:49:21.13
STEP: Verifying the service has paired with the endpoint 06/08/23 15:49:21.151
Jun  8 15:49:22.151: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 06/08/23 15:49:22.156
STEP: create a configmap that should be updated by the webhook 06/08/23 15:49:22.176
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:49:22.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1840" for this suite. 06/08/23 15:49:22.256
STEP: Destroying namespace "webhook-1840-markers" for this suite. 06/08/23 15:49:22.267
------------------------------
• [3.740 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:49:18.539
    Jun  8 15:49:18.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename webhook 06/08/23 15:49:18.54
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:49:18.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:49:18.563
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/08/23 15:49:18.579
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 15:49:19.083
    STEP: Deploying the webhook pod 06/08/23 15:49:19.093
    STEP: Wait for the deployment to be ready 06/08/23 15:49:19.107
    Jun  8 15:49:19.117: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/08/23 15:49:21.13
    STEP: Verifying the service has paired with the endpoint 06/08/23 15:49:21.151
    Jun  8 15:49:22.151: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 06/08/23 15:49:22.156
    STEP: create a configmap that should be updated by the webhook 06/08/23 15:49:22.176
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:49:22.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1840" for this suite. 06/08/23 15:49:22.256
    STEP: Destroying namespace "webhook-1840-markers" for this suite. 06/08/23 15:49:22.267
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:49:22.28
Jun  8 15:49:22.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 15:49:22.281
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:49:22.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:49:22.309
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 06/08/23 15:49:22.315
Jun  8 15:49:22.327: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5de69c52-e5b4-4c88-bd1d-91e00a167189" in namespace "projected-6723" to be "Succeeded or Failed"
Jun  8 15:49:22.332: INFO: Pod "downwardapi-volume-5de69c52-e5b4-4c88-bd1d-91e00a167189": Phase="Pending", Reason="", readiness=false. Elapsed: 4.88452ms
Jun  8 15:49:24.339: INFO: Pod "downwardapi-volume-5de69c52-e5b4-4c88-bd1d-91e00a167189": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011324514s
Jun  8 15:49:26.338: INFO: Pod "downwardapi-volume-5de69c52-e5b4-4c88-bd1d-91e00a167189": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010925692s
STEP: Saw pod success 06/08/23 15:49:26.338
Jun  8 15:49:26.338: INFO: Pod "downwardapi-volume-5de69c52-e5b4-4c88-bd1d-91e00a167189" satisfied condition "Succeeded or Failed"
Jun  8 15:49:26.342: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-5de69c52-e5b4-4c88-bd1d-91e00a167189 container client-container: <nil>
STEP: delete the pod 06/08/23 15:49:26.35
Jun  8 15:49:26.365: INFO: Waiting for pod downwardapi-volume-5de69c52-e5b4-4c88-bd1d-91e00a167189 to disappear
Jun  8 15:49:26.369: INFO: Pod downwardapi-volume-5de69c52-e5b4-4c88-bd1d-91e00a167189 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  8 15:49:26.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6723" for this suite. 06/08/23 15:49:26.375
------------------------------
• [4.102 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:49:22.28
    Jun  8 15:49:22.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 15:49:22.281
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:49:22.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:49:22.309
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 06/08/23 15:49:22.315
    Jun  8 15:49:22.327: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5de69c52-e5b4-4c88-bd1d-91e00a167189" in namespace "projected-6723" to be "Succeeded or Failed"
    Jun  8 15:49:22.332: INFO: Pod "downwardapi-volume-5de69c52-e5b4-4c88-bd1d-91e00a167189": Phase="Pending", Reason="", readiness=false. Elapsed: 4.88452ms
    Jun  8 15:49:24.339: INFO: Pod "downwardapi-volume-5de69c52-e5b4-4c88-bd1d-91e00a167189": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011324514s
    Jun  8 15:49:26.338: INFO: Pod "downwardapi-volume-5de69c52-e5b4-4c88-bd1d-91e00a167189": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010925692s
    STEP: Saw pod success 06/08/23 15:49:26.338
    Jun  8 15:49:26.338: INFO: Pod "downwardapi-volume-5de69c52-e5b4-4c88-bd1d-91e00a167189" satisfied condition "Succeeded or Failed"
    Jun  8 15:49:26.342: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-5de69c52-e5b4-4c88-bd1d-91e00a167189 container client-container: <nil>
    STEP: delete the pod 06/08/23 15:49:26.35
    Jun  8 15:49:26.365: INFO: Waiting for pod downwardapi-volume-5de69c52-e5b4-4c88-bd1d-91e00a167189 to disappear
    Jun  8 15:49:26.369: INFO: Pod downwardapi-volume-5de69c52-e5b4-4c88-bd1d-91e00a167189 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:49:26.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6723" for this suite. 06/08/23 15:49:26.375
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:49:26.383
Jun  8 15:49:26.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename crd-webhook 06/08/23 15:49:26.384
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:49:26.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:49:26.404
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 06/08/23 15:49:26.407
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/08/23 15:49:26.928
STEP: Deploying the custom resource conversion webhook pod 06/08/23 15:49:26.934
STEP: Wait for the deployment to be ready 06/08/23 15:49:26.948
Jun  8 15:49:26.957: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/08/23 15:49:28.969
STEP: Verifying the service has paired with the endpoint 06/08/23 15:49:28.985
Jun  8 15:49:29.986: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jun  8 15:49:29.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Creating a v1 custom resource 06/08/23 15:49:32.588
STEP: Create a v2 custom resource 06/08/23 15:49:32.608
STEP: List CRs in v1 06/08/23 15:49:32.664
STEP: List CRs in v2 06/08/23 15:49:32.67
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:49:33.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-7893" for this suite. 06/08/23 15:49:33.261
------------------------------
• [SLOW TEST] [6.891 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:49:26.383
    Jun  8 15:49:26.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename crd-webhook 06/08/23 15:49:26.384
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:49:26.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:49:26.404
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 06/08/23 15:49:26.407
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/08/23 15:49:26.928
    STEP: Deploying the custom resource conversion webhook pod 06/08/23 15:49:26.934
    STEP: Wait for the deployment to be ready 06/08/23 15:49:26.948
    Jun  8 15:49:26.957: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/08/23 15:49:28.969
    STEP: Verifying the service has paired with the endpoint 06/08/23 15:49:28.985
    Jun  8 15:49:29.986: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jun  8 15:49:29.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Creating a v1 custom resource 06/08/23 15:49:32.588
    STEP: Create a v2 custom resource 06/08/23 15:49:32.608
    STEP: List CRs in v1 06/08/23 15:49:32.664
    STEP: List CRs in v2 06/08/23 15:49:32.67
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:49:33.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-7893" for this suite. 06/08/23 15:49:33.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:49:33.276
Jun  8 15:49:33.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename services 06/08/23 15:49:33.278
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:49:33.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:49:33.312
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-2024 06/08/23 15:49:33.318
STEP: creating service affinity-nodeport-transition in namespace services-2024 06/08/23 15:49:33.318
STEP: creating replication controller affinity-nodeport-transition in namespace services-2024 06/08/23 15:49:33.347
I0608 15:49:33.356603      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2024, replica count: 3
I0608 15:49:36.413976      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  8 15:49:36.427: INFO: Creating new exec pod
Jun  8 15:49:36.436: INFO: Waiting up to 5m0s for pod "execpod-affinityc5lz7" in namespace "services-2024" to be "running"
Jun  8 15:49:36.440: INFO: Pod "execpod-affinityc5lz7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.51803ms
Jun  8 15:49:38.447: INFO: Pod "execpod-affinityc5lz7": Phase="Running", Reason="", readiness=true. Elapsed: 2.010483599s
Jun  8 15:49:38.447: INFO: Pod "execpod-affinityc5lz7" satisfied condition "running"
Jun  8 15:49:39.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2024 exec execpod-affinityc5lz7 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Jun  8 15:49:39.628: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jun  8 15:49:39.628: INFO: stdout: ""
Jun  8 15:49:39.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2024 exec execpod-affinityc5lz7 -- /bin/sh -x -c nc -v -z -w 2 10.107.228.30 80'
Jun  8 15:49:39.795: INFO: stderr: "+ nc -v -z -w 2 10.107.228.30 80\nConnection to 10.107.228.30 80 port [tcp/http] succeeded!\n"
Jun  8 15:49:39.795: INFO: stdout: ""
Jun  8 15:49:39.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2024 exec execpod-affinityc5lz7 -- /bin/sh -x -c nc -v -z -w 2 100.100.236.215 30465'
Jun  8 15:49:39.980: INFO: stderr: "+ nc -v -z -w 2 100.100.236.215 30465\nConnection to 100.100.236.215 30465 port [tcp/*] succeeded!\n"
Jun  8 15:49:39.980: INFO: stdout: ""
Jun  8 15:49:39.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2024 exec execpod-affinityc5lz7 -- /bin/sh -x -c nc -v -z -w 2 100.100.237.235 30465'
Jun  8 15:49:40.153: INFO: stderr: "+ nc -v -z -w 2 100.100.237.235 30465\nConnection to 100.100.237.235 30465 port [tcp/*] succeeded!\n"
Jun  8 15:49:40.153: INFO: stdout: ""
Jun  8 15:49:40.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2024 exec execpod-affinityc5lz7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.237.165:30465/ ; done'
Jun  8 15:49:40.563: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n"
Jun  8 15:49:40.563: INFO: stdout: "\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl"
Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:10.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2024 exec execpod-affinityc5lz7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.237.165:30465/ ; done'
Jun  8 15:50:10.847: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n"
Jun  8 15:50:10.848: INFO: stdout: "\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-pghwn\naffinity-nodeport-transition-pghwn\naffinity-nodeport-transition-pghwn\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-pghwn\naffinity-nodeport-transition-5vrhl"
Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-8rn7l
Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-8rn7l
Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-8rn7l
Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-pghwn
Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-pghwn
Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-pghwn
Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-8rn7l
Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-8rn7l
Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-8rn7l
Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-8rn7l
Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-pghwn
Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:10.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2024 exec execpod-affinityc5lz7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.237.165:30465/ ; done'
Jun  8 15:50:11.255: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n"
Jun  8 15:50:11.255: INFO: stdout: "\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-pghwn\naffinity-nodeport-transition-pghwn\naffinity-nodeport-transition-pghwn\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-pghwn\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-pghwn\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-8rn7l"
Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-8rn7l
Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-pghwn
Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-pghwn
Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-pghwn
Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-8rn7l
Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-8rn7l
Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-pghwn
Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-8rn7l
Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-pghwn
Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-8rn7l
Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-8rn7l
Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-8rn7l
Jun  8 15:50:41.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2024 exec execpod-affinityc5lz7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.237.165:30465/ ; done'
Jun  8 15:50:41.538: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n"
Jun  8 15:50:41.538: INFO: stdout: "\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl"
Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
Jun  8 15:50:41.538: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2024, will wait for the garbage collector to delete the pods 06/08/23 15:50:41.553
Jun  8 15:50:41.617: INFO: Deleting ReplicationController affinity-nodeport-transition took: 9.314293ms
Jun  8 15:50:41.718: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.721548ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  8 15:50:43.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2024" for this suite. 06/08/23 15:50:43.577
------------------------------
• [SLOW TEST] [70.312 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:49:33.276
    Jun  8 15:49:33.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename services 06/08/23 15:49:33.278
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:49:33.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:49:33.312
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-2024 06/08/23 15:49:33.318
    STEP: creating service affinity-nodeport-transition in namespace services-2024 06/08/23 15:49:33.318
    STEP: creating replication controller affinity-nodeport-transition in namespace services-2024 06/08/23 15:49:33.347
    I0608 15:49:33.356603      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2024, replica count: 3
    I0608 15:49:36.413976      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun  8 15:49:36.427: INFO: Creating new exec pod
    Jun  8 15:49:36.436: INFO: Waiting up to 5m0s for pod "execpod-affinityc5lz7" in namespace "services-2024" to be "running"
    Jun  8 15:49:36.440: INFO: Pod "execpod-affinityc5lz7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.51803ms
    Jun  8 15:49:38.447: INFO: Pod "execpod-affinityc5lz7": Phase="Running", Reason="", readiness=true. Elapsed: 2.010483599s
    Jun  8 15:49:38.447: INFO: Pod "execpod-affinityc5lz7" satisfied condition "running"
    Jun  8 15:49:39.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2024 exec execpod-affinityc5lz7 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Jun  8 15:49:39.628: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jun  8 15:49:39.628: INFO: stdout: ""
    Jun  8 15:49:39.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2024 exec execpod-affinityc5lz7 -- /bin/sh -x -c nc -v -z -w 2 10.107.228.30 80'
    Jun  8 15:49:39.795: INFO: stderr: "+ nc -v -z -w 2 10.107.228.30 80\nConnection to 10.107.228.30 80 port [tcp/http] succeeded!\n"
    Jun  8 15:49:39.795: INFO: stdout: ""
    Jun  8 15:49:39.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2024 exec execpod-affinityc5lz7 -- /bin/sh -x -c nc -v -z -w 2 100.100.236.215 30465'
    Jun  8 15:49:39.980: INFO: stderr: "+ nc -v -z -w 2 100.100.236.215 30465\nConnection to 100.100.236.215 30465 port [tcp/*] succeeded!\n"
    Jun  8 15:49:39.980: INFO: stdout: ""
    Jun  8 15:49:39.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2024 exec execpod-affinityc5lz7 -- /bin/sh -x -c nc -v -z -w 2 100.100.237.235 30465'
    Jun  8 15:49:40.153: INFO: stderr: "+ nc -v -z -w 2 100.100.237.235 30465\nConnection to 100.100.237.235 30465 port [tcp/*] succeeded!\n"
    Jun  8 15:49:40.153: INFO: stdout: ""
    Jun  8 15:49:40.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2024 exec execpod-affinityc5lz7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.237.165:30465/ ; done'
    Jun  8 15:49:40.563: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n"
    Jun  8 15:49:40.563: INFO: stdout: "\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl"
    Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:49:40.563: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:10.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2024 exec execpod-affinityc5lz7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.237.165:30465/ ; done'
    Jun  8 15:50:10.847: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n"
    Jun  8 15:50:10.848: INFO: stdout: "\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-pghwn\naffinity-nodeport-transition-pghwn\naffinity-nodeport-transition-pghwn\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-pghwn\naffinity-nodeport-transition-5vrhl"
    Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-8rn7l
    Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-8rn7l
    Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-8rn7l
    Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-pghwn
    Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-pghwn
    Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-pghwn
    Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-8rn7l
    Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-8rn7l
    Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-8rn7l
    Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-8rn7l
    Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-pghwn
    Jun  8 15:50:10.848: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:10.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2024 exec execpod-affinityc5lz7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.237.165:30465/ ; done'
    Jun  8 15:50:11.255: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n"
    Jun  8 15:50:11.255: INFO: stdout: "\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-pghwn\naffinity-nodeport-transition-pghwn\naffinity-nodeport-transition-pghwn\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-pghwn\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-pghwn\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-8rn7l\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-8rn7l"
    Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-8rn7l
    Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-pghwn
    Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-pghwn
    Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-pghwn
    Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-8rn7l
    Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-8rn7l
    Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-pghwn
    Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-8rn7l
    Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-pghwn
    Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-8rn7l
    Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-8rn7l
    Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:11.255: INFO: Received response from host: affinity-nodeport-transition-8rn7l
    Jun  8 15:50:41.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2024 exec execpod-affinityc5lz7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.100.237.165:30465/ ; done'
    Jun  8 15:50:41.538: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.100.237.165:30465/\n"
    Jun  8 15:50:41.538: INFO: stdout: "\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl\naffinity-nodeport-transition-5vrhl"
    Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:41.538: INFO: Received response from host: affinity-nodeport-transition-5vrhl
    Jun  8 15:50:41.538: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2024, will wait for the garbage collector to delete the pods 06/08/23 15:50:41.553
    Jun  8 15:50:41.617: INFO: Deleting ReplicationController affinity-nodeport-transition took: 9.314293ms
    Jun  8 15:50:41.718: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.721548ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:50:43.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2024" for this suite. 06/08/23 15:50:43.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:50:43.588
Jun  8 15:50:43.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 15:50:43.59
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:50:43.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:50:43.621
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-8a858745-8487-4ec8-95bd-5aa79a5e6a8c 06/08/23 15:50:43.625
STEP: Creating a pod to test consume configMaps 06/08/23 15:50:43.633
Jun  8 15:50:43.647: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0d05bc7b-0496-48b2-9594-3d7bb3a46c93" in namespace "projected-9225" to be "Succeeded or Failed"
Jun  8 15:50:43.652: INFO: Pod "pod-projected-configmaps-0d05bc7b-0496-48b2-9594-3d7bb3a46c93": Phase="Pending", Reason="", readiness=false. Elapsed: 5.326142ms
Jun  8 15:50:45.657: INFO: Pod "pod-projected-configmaps-0d05bc7b-0496-48b2-9594-3d7bb3a46c93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010534685s
Jun  8 15:50:47.659: INFO: Pod "pod-projected-configmaps-0d05bc7b-0496-48b2-9594-3d7bb3a46c93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012037343s
STEP: Saw pod success 06/08/23 15:50:47.659
Jun  8 15:50:47.659: INFO: Pod "pod-projected-configmaps-0d05bc7b-0496-48b2-9594-3d7bb3a46c93" satisfied condition "Succeeded or Failed"
Jun  8 15:50:47.663: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-configmaps-0d05bc7b-0496-48b2-9594-3d7bb3a46c93 container projected-configmap-volume-test: <nil>
STEP: delete the pod 06/08/23 15:50:47.67
Jun  8 15:50:47.681: INFO: Waiting for pod pod-projected-configmaps-0d05bc7b-0496-48b2-9594-3d7bb3a46c93 to disappear
Jun  8 15:50:47.685: INFO: Pod pod-projected-configmaps-0d05bc7b-0496-48b2-9594-3d7bb3a46c93 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun  8 15:50:47.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9225" for this suite. 06/08/23 15:50:47.69
------------------------------
• [4.109 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:50:43.588
    Jun  8 15:50:43.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 15:50:43.59
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:50:43.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:50:43.621
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-8a858745-8487-4ec8-95bd-5aa79a5e6a8c 06/08/23 15:50:43.625
    STEP: Creating a pod to test consume configMaps 06/08/23 15:50:43.633
    Jun  8 15:50:43.647: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0d05bc7b-0496-48b2-9594-3d7bb3a46c93" in namespace "projected-9225" to be "Succeeded or Failed"
    Jun  8 15:50:43.652: INFO: Pod "pod-projected-configmaps-0d05bc7b-0496-48b2-9594-3d7bb3a46c93": Phase="Pending", Reason="", readiness=false. Elapsed: 5.326142ms
    Jun  8 15:50:45.657: INFO: Pod "pod-projected-configmaps-0d05bc7b-0496-48b2-9594-3d7bb3a46c93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010534685s
    Jun  8 15:50:47.659: INFO: Pod "pod-projected-configmaps-0d05bc7b-0496-48b2-9594-3d7bb3a46c93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012037343s
    STEP: Saw pod success 06/08/23 15:50:47.659
    Jun  8 15:50:47.659: INFO: Pod "pod-projected-configmaps-0d05bc7b-0496-48b2-9594-3d7bb3a46c93" satisfied condition "Succeeded or Failed"
    Jun  8 15:50:47.663: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-configmaps-0d05bc7b-0496-48b2-9594-3d7bb3a46c93 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 06/08/23 15:50:47.67
    Jun  8 15:50:47.681: INFO: Waiting for pod pod-projected-configmaps-0d05bc7b-0496-48b2-9594-3d7bb3a46c93 to disappear
    Jun  8 15:50:47.685: INFO: Pod pod-projected-configmaps-0d05bc7b-0496-48b2-9594-3d7bb3a46c93 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:50:47.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9225" for this suite. 06/08/23 15:50:47.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:50:47.699
Jun  8 15:50:47.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename svcaccounts 06/08/23 15:50:47.7
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:50:47.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:50:47.72
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Jun  8 15:50:47.738: INFO: Waiting up to 5m0s for pod "pod-service-account-ba53e01b-db13-446b-8728-aa74277e71ec" in namespace "svcaccounts-5769" to be "running"
Jun  8 15:50:47.743: INFO: Pod "pod-service-account-ba53e01b-db13-446b-8728-aa74277e71ec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.010847ms
Jun  8 15:50:49.748: INFO: Pod "pod-service-account-ba53e01b-db13-446b-8728-aa74277e71ec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009981848s
Jun  8 15:50:49.748: INFO: Pod "pod-service-account-ba53e01b-db13-446b-8728-aa74277e71ec" satisfied condition "running"
STEP: reading a file in the container 06/08/23 15:50:49.748
Jun  8 15:50:49.748: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5769 pod-service-account-ba53e01b-db13-446b-8728-aa74277e71ec -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 06/08/23 15:50:49.913
Jun  8 15:50:49.913: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5769 pod-service-account-ba53e01b-db13-446b-8728-aa74277e71ec -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 06/08/23 15:50:50.079
Jun  8 15:50:50.079: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5769 pod-service-account-ba53e01b-db13-446b-8728-aa74277e71ec -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jun  8 15:50:50.253: INFO: Got root ca configmap in namespace "svcaccounts-5769"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jun  8 15:50:50.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5769" for this suite. 06/08/23 15:50:50.262
------------------------------
• [2.571 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:50:47.699
    Jun  8 15:50:47.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename svcaccounts 06/08/23 15:50:47.7
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:50:47.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:50:47.72
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Jun  8 15:50:47.738: INFO: Waiting up to 5m0s for pod "pod-service-account-ba53e01b-db13-446b-8728-aa74277e71ec" in namespace "svcaccounts-5769" to be "running"
    Jun  8 15:50:47.743: INFO: Pod "pod-service-account-ba53e01b-db13-446b-8728-aa74277e71ec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.010847ms
    Jun  8 15:50:49.748: INFO: Pod "pod-service-account-ba53e01b-db13-446b-8728-aa74277e71ec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009981848s
    Jun  8 15:50:49.748: INFO: Pod "pod-service-account-ba53e01b-db13-446b-8728-aa74277e71ec" satisfied condition "running"
    STEP: reading a file in the container 06/08/23 15:50:49.748
    Jun  8 15:50:49.748: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5769 pod-service-account-ba53e01b-db13-446b-8728-aa74277e71ec -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 06/08/23 15:50:49.913
    Jun  8 15:50:49.913: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5769 pod-service-account-ba53e01b-db13-446b-8728-aa74277e71ec -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 06/08/23 15:50:50.079
    Jun  8 15:50:50.079: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5769 pod-service-account-ba53e01b-db13-446b-8728-aa74277e71ec -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jun  8 15:50:50.253: INFO: Got root ca configmap in namespace "svcaccounts-5769"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:50:50.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5769" for this suite. 06/08/23 15:50:50.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:50:50.271
Jun  8 15:50:50.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubectl 06/08/23 15:50:50.272
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:50:50.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:50:50.294
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 06/08/23 15:50:50.298
Jun  8 15:50:50.298: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jun  8 15:50:50.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 create -f -'
Jun  8 15:50:51.134: INFO: stderr: ""
Jun  8 15:50:51.134: INFO: stdout: "service/agnhost-replica created\n"
Jun  8 15:50:51.134: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jun  8 15:50:51.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 create -f -'
Jun  8 15:50:52.166: INFO: stderr: ""
Jun  8 15:50:52.166: INFO: stdout: "service/agnhost-primary created\n"
Jun  8 15:50:52.166: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun  8 15:50:52.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 create -f -'
Jun  8 15:50:53.125: INFO: stderr: ""
Jun  8 15:50:53.125: INFO: stdout: "service/frontend created\n"
Jun  8 15:50:53.125: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jun  8 15:50:53.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 create -f -'
Jun  8 15:50:53.455: INFO: stderr: ""
Jun  8 15:50:53.455: INFO: stdout: "deployment.apps/frontend created\n"
Jun  8 15:50:53.455: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun  8 15:50:53.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 create -f -'
Jun  8 15:50:53.784: INFO: stderr: ""
Jun  8 15:50:53.784: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jun  8 15:50:53.784: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun  8 15:50:53.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 create -f -'
Jun  8 15:50:54.102: INFO: stderr: ""
Jun  8 15:50:54.102: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 06/08/23 15:50:54.102
Jun  8 15:50:54.102: INFO: Waiting for all frontend pods to be Running.
Jun  8 15:50:59.154: INFO: Waiting for frontend to serve content.
Jun  8 15:50:59.166: INFO: Trying to add a new entry to the guestbook.
Jun  8 15:50:59.181: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 06/08/23 15:50:59.19
Jun  8 15:50:59.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 delete --grace-period=0 --force -f -'
Jun  8 15:50:59.304: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  8 15:50:59.304: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 06/08/23 15:50:59.304
Jun  8 15:50:59.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 delete --grace-period=0 --force -f -'
Jun  8 15:50:59.440: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  8 15:50:59.440: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 06/08/23 15:50:59.44
Jun  8 15:50:59.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 delete --grace-period=0 --force -f -'
Jun  8 15:50:59.578: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  8 15:50:59.578: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 06/08/23 15:50:59.578
Jun  8 15:50:59.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 delete --grace-period=0 --force -f -'
Jun  8 15:50:59.704: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  8 15:50:59.704: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 06/08/23 15:50:59.704
Jun  8 15:50:59.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 delete --grace-period=0 --force -f -'
Jun  8 15:50:59.860: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  8 15:50:59.860: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 06/08/23 15:50:59.86
Jun  8 15:50:59.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 delete --grace-period=0 --force -f -'
Jun  8 15:51:00.068: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  8 15:51:00.068: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  8 15:51:00.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4377" for this suite. 06/08/23 15:51:00.077
------------------------------
• [SLOW TEST] [9.825 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:50:50.271
    Jun  8 15:50:50.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubectl 06/08/23 15:50:50.272
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:50:50.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:50:50.294
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 06/08/23 15:50:50.298
    Jun  8 15:50:50.298: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jun  8 15:50:50.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 create -f -'
    Jun  8 15:50:51.134: INFO: stderr: ""
    Jun  8 15:50:51.134: INFO: stdout: "service/agnhost-replica created\n"
    Jun  8 15:50:51.134: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jun  8 15:50:51.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 create -f -'
    Jun  8 15:50:52.166: INFO: stderr: ""
    Jun  8 15:50:52.166: INFO: stdout: "service/agnhost-primary created\n"
    Jun  8 15:50:52.166: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jun  8 15:50:52.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 create -f -'
    Jun  8 15:50:53.125: INFO: stderr: ""
    Jun  8 15:50:53.125: INFO: stdout: "service/frontend created\n"
    Jun  8 15:50:53.125: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jun  8 15:50:53.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 create -f -'
    Jun  8 15:50:53.455: INFO: stderr: ""
    Jun  8 15:50:53.455: INFO: stdout: "deployment.apps/frontend created\n"
    Jun  8 15:50:53.455: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jun  8 15:50:53.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 create -f -'
    Jun  8 15:50:53.784: INFO: stderr: ""
    Jun  8 15:50:53.784: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jun  8 15:50:53.784: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jun  8 15:50:53.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 create -f -'
    Jun  8 15:50:54.102: INFO: stderr: ""
    Jun  8 15:50:54.102: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 06/08/23 15:50:54.102
    Jun  8 15:50:54.102: INFO: Waiting for all frontend pods to be Running.
    Jun  8 15:50:59.154: INFO: Waiting for frontend to serve content.
    Jun  8 15:50:59.166: INFO: Trying to add a new entry to the guestbook.
    Jun  8 15:50:59.181: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 06/08/23 15:50:59.19
    Jun  8 15:50:59.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 delete --grace-period=0 --force -f -'
    Jun  8 15:50:59.304: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun  8 15:50:59.304: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 06/08/23 15:50:59.304
    Jun  8 15:50:59.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 delete --grace-period=0 --force -f -'
    Jun  8 15:50:59.440: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun  8 15:50:59.440: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 06/08/23 15:50:59.44
    Jun  8 15:50:59.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 delete --grace-period=0 --force -f -'
    Jun  8 15:50:59.578: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun  8 15:50:59.578: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 06/08/23 15:50:59.578
    Jun  8 15:50:59.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 delete --grace-period=0 --force -f -'
    Jun  8 15:50:59.704: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun  8 15:50:59.704: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 06/08/23 15:50:59.704
    Jun  8 15:50:59.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 delete --grace-period=0 --force -f -'
    Jun  8 15:50:59.860: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun  8 15:50:59.860: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 06/08/23 15:50:59.86
    Jun  8 15:50:59.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4377 delete --grace-period=0 --force -f -'
    Jun  8 15:51:00.068: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun  8 15:51:00.068: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:51:00.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4377" for this suite. 06/08/23 15:51:00.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:51:00.099
Jun  8 15:51:00.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename gc 06/08/23 15:51:00.106
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:00.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:00.138
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 06/08/23 15:51:00.145
STEP: Wait for the Deployment to create new ReplicaSet 06/08/23 15:51:00.154
STEP: delete the deployment 06/08/23 15:51:00.283
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 06/08/23 15:51:00.294
STEP: Gathering metrics 06/08/23 15:51:00.821
Jun  8 15:51:00.853: INFO: Waiting up to 5m0s for pod "kube-controller-manager-chl8tf-control-plane-003" in namespace "kube-system" to be "running and ready"
Jun  8 15:51:00.858: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003": Phase="Running", Reason="", readiness=true. Elapsed: 4.526502ms
Jun  8 15:51:00.858: INFO: The phase of Pod kube-controller-manager-chl8tf-control-plane-003 is Running (Ready = true)
Jun  8 15:51:00.858: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003" satisfied condition "running and ready"
Jun  8 15:51:00.918: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jun  8 15:51:00.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9588" for this suite. 06/08/23 15:51:00.925
------------------------------
• [0.836 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:51:00.099
    Jun  8 15:51:00.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename gc 06/08/23 15:51:00.106
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:00.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:00.138
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 06/08/23 15:51:00.145
    STEP: Wait for the Deployment to create new ReplicaSet 06/08/23 15:51:00.154
    STEP: delete the deployment 06/08/23 15:51:00.283
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 06/08/23 15:51:00.294
    STEP: Gathering metrics 06/08/23 15:51:00.821
    Jun  8 15:51:00.853: INFO: Waiting up to 5m0s for pod "kube-controller-manager-chl8tf-control-plane-003" in namespace "kube-system" to be "running and ready"
    Jun  8 15:51:00.858: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003": Phase="Running", Reason="", readiness=true. Elapsed: 4.526502ms
    Jun  8 15:51:00.858: INFO: The phase of Pod kube-controller-manager-chl8tf-control-plane-003 is Running (Ready = true)
    Jun  8 15:51:00.858: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003" satisfied condition "running and ready"
    Jun  8 15:51:00.918: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:51:00.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9588" for this suite. 06/08/23 15:51:00.925
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:51:00.937
Jun  8 15:51:00.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename cronjob 06/08/23 15:51:00.938
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:00.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:00.96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 06/08/23 15:51:00.964
STEP: creating 06/08/23 15:51:00.964
STEP: getting 06/08/23 15:51:00.971
STEP: listing 06/08/23 15:51:00.976
STEP: watching 06/08/23 15:51:00.98
Jun  8 15:51:00.980: INFO: starting watch
STEP: cluster-wide listing 06/08/23 15:51:00.982
STEP: cluster-wide watching 06/08/23 15:51:00.986
Jun  8 15:51:00.986: INFO: starting watch
STEP: patching 06/08/23 15:51:00.987
STEP: updating 06/08/23 15:51:00.995
Jun  8 15:51:01.004: INFO: waiting for watch events with expected annotations
Jun  8 15:51:01.004: INFO: saw patched and updated annotations
STEP: patching /status 06/08/23 15:51:01.004
STEP: updating /status 06/08/23 15:51:01.012
STEP: get /status 06/08/23 15:51:01.02
STEP: deleting 06/08/23 15:51:01.024
STEP: deleting a collection 06/08/23 15:51:01.04
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jun  8 15:51:01.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-3304" for this suite. 06/08/23 15:51:01.057
------------------------------
• [0.128 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:51:00.937
    Jun  8 15:51:00.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename cronjob 06/08/23 15:51:00.938
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:00.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:00.96
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 06/08/23 15:51:00.964
    STEP: creating 06/08/23 15:51:00.964
    STEP: getting 06/08/23 15:51:00.971
    STEP: listing 06/08/23 15:51:00.976
    STEP: watching 06/08/23 15:51:00.98
    Jun  8 15:51:00.980: INFO: starting watch
    STEP: cluster-wide listing 06/08/23 15:51:00.982
    STEP: cluster-wide watching 06/08/23 15:51:00.986
    Jun  8 15:51:00.986: INFO: starting watch
    STEP: patching 06/08/23 15:51:00.987
    STEP: updating 06/08/23 15:51:00.995
    Jun  8 15:51:01.004: INFO: waiting for watch events with expected annotations
    Jun  8 15:51:01.004: INFO: saw patched and updated annotations
    STEP: patching /status 06/08/23 15:51:01.004
    STEP: updating /status 06/08/23 15:51:01.012
    STEP: get /status 06/08/23 15:51:01.02
    STEP: deleting 06/08/23 15:51:01.024
    STEP: deleting a collection 06/08/23 15:51:01.04
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:51:01.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-3304" for this suite. 06/08/23 15:51:01.057
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:51:01.065
Jun  8 15:51:01.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename secrets 06/08/23 15:51:01.066
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:01.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:01.086
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-9262/secret-test-45821602-9121-4cd3-9d57-558ed29b2919 06/08/23 15:51:01.089
STEP: Creating a pod to test consume secrets 06/08/23 15:51:01.094
Jun  8 15:51:01.104: INFO: Waiting up to 5m0s for pod "pod-configmaps-ba7663a3-882e-486a-8af5-8873e8857ad3" in namespace "secrets-9262" to be "Succeeded or Failed"
Jun  8 15:51:01.108: INFO: Pod "pod-configmaps-ba7663a3-882e-486a-8af5-8873e8857ad3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.592764ms
Jun  8 15:51:03.113: INFO: Pod "pod-configmaps-ba7663a3-882e-486a-8af5-8873e8857ad3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008468669s
Jun  8 15:51:05.114: INFO: Pod "pod-configmaps-ba7663a3-882e-486a-8af5-8873e8857ad3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009546114s
STEP: Saw pod success 06/08/23 15:51:05.114
Jun  8 15:51:05.114: INFO: Pod "pod-configmaps-ba7663a3-882e-486a-8af5-8873e8857ad3" satisfied condition "Succeeded or Failed"
Jun  8 15:51:05.118: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-configmaps-ba7663a3-882e-486a-8af5-8873e8857ad3 container env-test: <nil>
STEP: delete the pod 06/08/23 15:51:05.128
Jun  8 15:51:05.140: INFO: Waiting for pod pod-configmaps-ba7663a3-882e-486a-8af5-8873e8857ad3 to disappear
Jun  8 15:51:05.144: INFO: Pod pod-configmaps-ba7663a3-882e-486a-8af5-8873e8857ad3 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  8 15:51:05.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9262" for this suite. 06/08/23 15:51:05.15
------------------------------
• [4.094 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:51:01.065
    Jun  8 15:51:01.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename secrets 06/08/23 15:51:01.066
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:01.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:01.086
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-9262/secret-test-45821602-9121-4cd3-9d57-558ed29b2919 06/08/23 15:51:01.089
    STEP: Creating a pod to test consume secrets 06/08/23 15:51:01.094
    Jun  8 15:51:01.104: INFO: Waiting up to 5m0s for pod "pod-configmaps-ba7663a3-882e-486a-8af5-8873e8857ad3" in namespace "secrets-9262" to be "Succeeded or Failed"
    Jun  8 15:51:01.108: INFO: Pod "pod-configmaps-ba7663a3-882e-486a-8af5-8873e8857ad3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.592764ms
    Jun  8 15:51:03.113: INFO: Pod "pod-configmaps-ba7663a3-882e-486a-8af5-8873e8857ad3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008468669s
    Jun  8 15:51:05.114: INFO: Pod "pod-configmaps-ba7663a3-882e-486a-8af5-8873e8857ad3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009546114s
    STEP: Saw pod success 06/08/23 15:51:05.114
    Jun  8 15:51:05.114: INFO: Pod "pod-configmaps-ba7663a3-882e-486a-8af5-8873e8857ad3" satisfied condition "Succeeded or Failed"
    Jun  8 15:51:05.118: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-configmaps-ba7663a3-882e-486a-8af5-8873e8857ad3 container env-test: <nil>
    STEP: delete the pod 06/08/23 15:51:05.128
    Jun  8 15:51:05.140: INFO: Waiting for pod pod-configmaps-ba7663a3-882e-486a-8af5-8873e8857ad3 to disappear
    Jun  8 15:51:05.144: INFO: Pod pod-configmaps-ba7663a3-882e-486a-8af5-8873e8857ad3 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:51:05.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9262" for this suite. 06/08/23 15:51:05.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:51:05.16
Jun  8 15:51:05.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename crd-publish-openapi 06/08/23 15:51:05.161
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:05.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:05.183
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 06/08/23 15:51:05.187
Jun  8 15:51:05.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 06/08/23 15:51:12.315
Jun  8 15:51:12.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:51:14.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:51:22.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-12" for this suite. 06/08/23 15:51:22.182
------------------------------
• [SLOW TEST] [17.028 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:51:05.16
    Jun  8 15:51:05.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename crd-publish-openapi 06/08/23 15:51:05.161
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:05.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:05.183
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 06/08/23 15:51:05.187
    Jun  8 15:51:05.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 06/08/23 15:51:12.315
    Jun  8 15:51:12.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:51:14.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:51:22.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-12" for this suite. 06/08/23 15:51:22.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:51:22.189
Jun  8 15:51:22.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename custom-resource-definition 06/08/23 15:51:22.19
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:22.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:22.211
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jun  8 15:51:22.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:51:22.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9425" for this suite. 06/08/23 15:51:22.76
------------------------------
• [0.578 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:51:22.189
    Jun  8 15:51:22.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename custom-resource-definition 06/08/23 15:51:22.19
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:22.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:22.211
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jun  8 15:51:22.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:51:22.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9425" for this suite. 06/08/23 15:51:22.76
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:51:22.768
Jun  8 15:51:22.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubectl 06/08/23 15:51:22.769
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:22.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:22.788
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 06/08/23 15:51:22.791
Jun  8 15:51:22.792: INFO: namespace kubectl-5919
Jun  8 15:51:22.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-5919 create -f -'
Jun  8 15:51:23.320: INFO: stderr: ""
Jun  8 15:51:23.320: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 06/08/23 15:51:23.32
Jun  8 15:51:24.325: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  8 15:51:24.325: INFO: Found 0 / 1
Jun  8 15:51:25.324: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  8 15:51:25.324: INFO: Found 1 / 1
Jun  8 15:51:25.324: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun  8 15:51:25.328: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  8 15:51:25.328: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun  8 15:51:25.328: INFO: wait on agnhost-primary startup in kubectl-5919 
Jun  8 15:51:25.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-5919 logs agnhost-primary-792x6 agnhost-primary'
Jun  8 15:51:25.424: INFO: stderr: ""
Jun  8 15:51:25.424: INFO: stdout: "Paused\n"
STEP: exposing RC 06/08/23 15:51:25.424
Jun  8 15:51:25.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-5919 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jun  8 15:51:25.519: INFO: stderr: ""
Jun  8 15:51:25.519: INFO: stdout: "service/rm2 exposed\n"
Jun  8 15:51:25.523: INFO: Service rm2 in namespace kubectl-5919 found.
STEP: exposing service 06/08/23 15:51:27.531
Jun  8 15:51:27.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-5919 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jun  8 15:51:27.627: INFO: stderr: ""
Jun  8 15:51:27.627: INFO: stdout: "service/rm3 exposed\n"
Jun  8 15:51:27.632: INFO: Service rm3 in namespace kubectl-5919 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  8 15:51:29.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5919" for this suite. 06/08/23 15:51:29.645
------------------------------
• [SLOW TEST] [6.883 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:51:22.768
    Jun  8 15:51:22.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubectl 06/08/23 15:51:22.769
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:22.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:22.788
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 06/08/23 15:51:22.791
    Jun  8 15:51:22.792: INFO: namespace kubectl-5919
    Jun  8 15:51:22.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-5919 create -f -'
    Jun  8 15:51:23.320: INFO: stderr: ""
    Jun  8 15:51:23.320: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 06/08/23 15:51:23.32
    Jun  8 15:51:24.325: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun  8 15:51:24.325: INFO: Found 0 / 1
    Jun  8 15:51:25.324: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun  8 15:51:25.324: INFO: Found 1 / 1
    Jun  8 15:51:25.324: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jun  8 15:51:25.328: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun  8 15:51:25.328: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jun  8 15:51:25.328: INFO: wait on agnhost-primary startup in kubectl-5919 
    Jun  8 15:51:25.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-5919 logs agnhost-primary-792x6 agnhost-primary'
    Jun  8 15:51:25.424: INFO: stderr: ""
    Jun  8 15:51:25.424: INFO: stdout: "Paused\n"
    STEP: exposing RC 06/08/23 15:51:25.424
    Jun  8 15:51:25.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-5919 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jun  8 15:51:25.519: INFO: stderr: ""
    Jun  8 15:51:25.519: INFO: stdout: "service/rm2 exposed\n"
    Jun  8 15:51:25.523: INFO: Service rm2 in namespace kubectl-5919 found.
    STEP: exposing service 06/08/23 15:51:27.531
    Jun  8 15:51:27.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-5919 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jun  8 15:51:27.627: INFO: stderr: ""
    Jun  8 15:51:27.627: INFO: stdout: "service/rm3 exposed\n"
    Jun  8 15:51:27.632: INFO: Service rm3 in namespace kubectl-5919 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:51:29.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5919" for this suite. 06/08/23 15:51:29.645
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:51:29.651
Jun  8 15:51:29.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename deployment 06/08/23 15:51:29.652
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:29.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:29.673
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jun  8 15:51:29.676: INFO: Creating simple deployment test-new-deployment
Jun  8 15:51:29.688: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource 06/08/23 15:51:31.702
STEP: updating a scale subresource 06/08/23 15:51:31.705
STEP: verifying the deployment Spec.Replicas was modified 06/08/23 15:51:31.712
STEP: Patch a scale subresource 06/08/23 15:51:31.714
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun  8 15:51:31.728: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-627  0e42aa75-fc5b-47c6-b7ff-8b9f02084037 38138 3 2023-06-08 15:51:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-06-08 15:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 15:51:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00541a948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-08 15:51:30 +0000 UTC,LastTransitionTime:2023-06-08 15:51:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-06-08 15:51:30 +0000 UTC,LastTransitionTime:2023-06-08 15:51:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  8 15:51:31.731: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-627  ba495f71-3775-457a-977a-00d80394051e 38137 2 2023-06-08 15:51:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 0e42aa75-fc5b-47c6-b7ff-8b9f02084037 0xc004b194a7 0xc004b194a8}] [] [{kube-controller-manager Update apps/v1 2023-06-08 15:51:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-08 15:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e42aa75-fc5b-47c6-b7ff-8b9f02084037\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b19538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  8 15:51:31.738: INFO: Pod "test-new-deployment-7f5969cbc7-m87dc" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-m87dc test-new-deployment-7f5969cbc7- deployment-627  b1233829-3e81-44ad-9ec9-5c20368a7677 38141 0 2023-06-08 15:51:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 ba495f71-3775-457a-977a-00d80394051e 0xc00541adc7 0xc00541adc8}] [] [{kube-controller-manager Update v1 2023-06-08 15:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba495f71-3775-457a-977a-00d80394051e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-26ppz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-26ppz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:51:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  8 15:51:31.738: INFO: Pod "test-new-deployment-7f5969cbc7-xkjmc" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-xkjmc test-new-deployment-7f5969cbc7- deployment-627  9bf2cb30-dad2-4bcb-bade-fa0507e5b091 38128 0 2023-06-08 15:51:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 ba495f71-3775-457a-977a-00d80394051e 0xc00541af20 0xc00541af21}] [] [{kube-controller-manager Update v1 2023-06-08 15:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba495f71-3775-457a-977a-00d80394051e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:51:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x569h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x569h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:51:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:51:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:51:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:51:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.215,PodIP:10.244.3.189,StartTime:2023-06-08 15:51:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:51:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://2516c1d0b4e37ec90db3a33460859ef34431d9d9483f23befa4e938ce59edd51,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun  8 15:51:31.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-627" for this suite. 06/08/23 15:51:31.753
------------------------------
• [2.108 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:51:29.651
    Jun  8 15:51:29.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename deployment 06/08/23 15:51:29.652
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:29.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:29.673
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jun  8 15:51:29.676: INFO: Creating simple deployment test-new-deployment
    Jun  8 15:51:29.688: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    STEP: getting scale subresource 06/08/23 15:51:31.702
    STEP: updating a scale subresource 06/08/23 15:51:31.705
    STEP: verifying the deployment Spec.Replicas was modified 06/08/23 15:51:31.712
    STEP: Patch a scale subresource 06/08/23 15:51:31.714
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun  8 15:51:31.728: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-627  0e42aa75-fc5b-47c6-b7ff-8b9f02084037 38138 3 2023-06-08 15:51:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-06-08 15:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 15:51:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00541a948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-08 15:51:30 +0000 UTC,LastTransitionTime:2023-06-08 15:51:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-06-08 15:51:30 +0000 UTC,LastTransitionTime:2023-06-08 15:51:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun  8 15:51:31.731: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-627  ba495f71-3775-457a-977a-00d80394051e 38137 2 2023-06-08 15:51:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 0e42aa75-fc5b-47c6-b7ff-8b9f02084037 0xc004b194a7 0xc004b194a8}] [] [{kube-controller-manager Update apps/v1 2023-06-08 15:51:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-08 15:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e42aa75-fc5b-47c6-b7ff-8b9f02084037\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b19538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun  8 15:51:31.738: INFO: Pod "test-new-deployment-7f5969cbc7-m87dc" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-m87dc test-new-deployment-7f5969cbc7- deployment-627  b1233829-3e81-44ad-9ec9-5c20368a7677 38141 0 2023-06-08 15:51:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 ba495f71-3775-457a-977a-00d80394051e 0xc00541adc7 0xc00541adc8}] [] [{kube-controller-manager Update v1 2023-06-08 15:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba495f71-3775-457a-977a-00d80394051e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-26ppz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-26ppz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:51:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  8 15:51:31.738: INFO: Pod "test-new-deployment-7f5969cbc7-xkjmc" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-xkjmc test-new-deployment-7f5969cbc7- deployment-627  9bf2cb30-dad2-4bcb-bade-fa0507e5b091 38128 0 2023-06-08 15:51:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 ba495f71-3775-457a-977a-00d80394051e 0xc00541af20 0xc00541af21}] [] [{kube-controller-manager Update v1 2023-06-08 15:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba495f71-3775-457a-977a-00d80394051e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 15:51:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x569h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x569h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:51:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:51:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:51:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 15:51:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.215,PodIP:10.244.3.189,StartTime:2023-06-08 15:51:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 15:51:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://2516c1d0b4e37ec90db3a33460859ef34431d9d9483f23befa4e938ce59edd51,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:51:31.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-627" for this suite. 06/08/23 15:51:31.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:51:31.761
Jun  8 15:51:31.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename emptydir 06/08/23 15:51:31.762
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:31.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:31.786
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 06/08/23 15:51:31.789
Jun  8 15:51:31.797: INFO: Waiting up to 5m0s for pod "pod-69ac19c2-9b3c-4961-a550-a1376ba84f40" in namespace "emptydir-1313" to be "Succeeded or Failed"
Jun  8 15:51:31.800: INFO: Pod "pod-69ac19c2-9b3c-4961-a550-a1376ba84f40": Phase="Pending", Reason="", readiness=false. Elapsed: 3.116849ms
Jun  8 15:51:33.805: INFO: Pod "pod-69ac19c2-9b3c-4961-a550-a1376ba84f40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008250641s
Jun  8 15:51:35.805: INFO: Pod "pod-69ac19c2-9b3c-4961-a550-a1376ba84f40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008002823s
STEP: Saw pod success 06/08/23 15:51:35.805
Jun  8 15:51:35.805: INFO: Pod "pod-69ac19c2-9b3c-4961-a550-a1376ba84f40" satisfied condition "Succeeded or Failed"
Jun  8 15:51:35.808: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-69ac19c2-9b3c-4961-a550-a1376ba84f40 container test-container: <nil>
STEP: delete the pod 06/08/23 15:51:35.823
Jun  8 15:51:35.835: INFO: Waiting for pod pod-69ac19c2-9b3c-4961-a550-a1376ba84f40 to disappear
Jun  8 15:51:35.838: INFO: Pod pod-69ac19c2-9b3c-4961-a550-a1376ba84f40 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  8 15:51:35.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1313" for this suite. 06/08/23 15:51:35.844
------------------------------
• [4.089 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:51:31.761
    Jun  8 15:51:31.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename emptydir 06/08/23 15:51:31.762
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:31.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:31.786
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 06/08/23 15:51:31.789
    Jun  8 15:51:31.797: INFO: Waiting up to 5m0s for pod "pod-69ac19c2-9b3c-4961-a550-a1376ba84f40" in namespace "emptydir-1313" to be "Succeeded or Failed"
    Jun  8 15:51:31.800: INFO: Pod "pod-69ac19c2-9b3c-4961-a550-a1376ba84f40": Phase="Pending", Reason="", readiness=false. Elapsed: 3.116849ms
    Jun  8 15:51:33.805: INFO: Pod "pod-69ac19c2-9b3c-4961-a550-a1376ba84f40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008250641s
    Jun  8 15:51:35.805: INFO: Pod "pod-69ac19c2-9b3c-4961-a550-a1376ba84f40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008002823s
    STEP: Saw pod success 06/08/23 15:51:35.805
    Jun  8 15:51:35.805: INFO: Pod "pod-69ac19c2-9b3c-4961-a550-a1376ba84f40" satisfied condition "Succeeded or Failed"
    Jun  8 15:51:35.808: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-69ac19c2-9b3c-4961-a550-a1376ba84f40 container test-container: <nil>
    STEP: delete the pod 06/08/23 15:51:35.823
    Jun  8 15:51:35.835: INFO: Waiting for pod pod-69ac19c2-9b3c-4961-a550-a1376ba84f40 to disappear
    Jun  8 15:51:35.838: INFO: Pod pod-69ac19c2-9b3c-4961-a550-a1376ba84f40 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:51:35.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1313" for this suite. 06/08/23 15:51:35.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:51:35.852
Jun  8 15:51:35.852: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename configmap 06/08/23 15:51:35.853
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:35.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:35.872
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-7553e79f-377e-48c6-b5ec-bcfdd0173f91 06/08/23 15:51:35.875
STEP: Creating a pod to test consume configMaps 06/08/23 15:51:35.879
Jun  8 15:51:35.887: INFO: Waiting up to 5m0s for pod "pod-configmaps-6eefdacf-dbed-405d-8cd7-3c005141972d" in namespace "configmap-7848" to be "Succeeded or Failed"
Jun  8 15:51:35.890: INFO: Pod "pod-configmaps-6eefdacf-dbed-405d-8cd7-3c005141972d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.330425ms
Jun  8 15:51:37.895: INFO: Pod "pod-configmaps-6eefdacf-dbed-405d-8cd7-3c005141972d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00827425s
Jun  8 15:51:39.895: INFO: Pod "pod-configmaps-6eefdacf-dbed-405d-8cd7-3c005141972d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008149461s
STEP: Saw pod success 06/08/23 15:51:39.895
Jun  8 15:51:39.895: INFO: Pod "pod-configmaps-6eefdacf-dbed-405d-8cd7-3c005141972d" satisfied condition "Succeeded or Failed"
Jun  8 15:51:39.899: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-configmaps-6eefdacf-dbed-405d-8cd7-3c005141972d container agnhost-container: <nil>
STEP: delete the pod 06/08/23 15:51:39.906
Jun  8 15:51:39.917: INFO: Waiting for pod pod-configmaps-6eefdacf-dbed-405d-8cd7-3c005141972d to disappear
Jun  8 15:51:39.921: INFO: Pod pod-configmaps-6eefdacf-dbed-405d-8cd7-3c005141972d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  8 15:51:39.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7848" for this suite. 06/08/23 15:51:39.927
------------------------------
• [4.082 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:51:35.852
    Jun  8 15:51:35.852: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename configmap 06/08/23 15:51:35.853
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:35.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:35.872
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-7553e79f-377e-48c6-b5ec-bcfdd0173f91 06/08/23 15:51:35.875
    STEP: Creating a pod to test consume configMaps 06/08/23 15:51:35.879
    Jun  8 15:51:35.887: INFO: Waiting up to 5m0s for pod "pod-configmaps-6eefdacf-dbed-405d-8cd7-3c005141972d" in namespace "configmap-7848" to be "Succeeded or Failed"
    Jun  8 15:51:35.890: INFO: Pod "pod-configmaps-6eefdacf-dbed-405d-8cd7-3c005141972d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.330425ms
    Jun  8 15:51:37.895: INFO: Pod "pod-configmaps-6eefdacf-dbed-405d-8cd7-3c005141972d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00827425s
    Jun  8 15:51:39.895: INFO: Pod "pod-configmaps-6eefdacf-dbed-405d-8cd7-3c005141972d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008149461s
    STEP: Saw pod success 06/08/23 15:51:39.895
    Jun  8 15:51:39.895: INFO: Pod "pod-configmaps-6eefdacf-dbed-405d-8cd7-3c005141972d" satisfied condition "Succeeded or Failed"
    Jun  8 15:51:39.899: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-configmaps-6eefdacf-dbed-405d-8cd7-3c005141972d container agnhost-container: <nil>
    STEP: delete the pod 06/08/23 15:51:39.906
    Jun  8 15:51:39.917: INFO: Waiting for pod pod-configmaps-6eefdacf-dbed-405d-8cd7-3c005141972d to disappear
    Jun  8 15:51:39.921: INFO: Pod pod-configmaps-6eefdacf-dbed-405d-8cd7-3c005141972d no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:51:39.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7848" for this suite. 06/08/23 15:51:39.927
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:51:39.934
Jun  8 15:51:39.934: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubectl 06/08/23 15:51:39.935
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:39.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:39.955
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 06/08/23 15:51:39.959
Jun  8 15:51:39.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-9645 cluster-info'
Jun  8 15:51:40.039: INFO: stderr: ""
Jun  8 15:51:40.039: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  8 15:51:40.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9645" for this suite. 06/08/23 15:51:40.045
------------------------------
• [0.118 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:51:39.934
    Jun  8 15:51:39.934: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubectl 06/08/23 15:51:39.935
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:39.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:39.955
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 06/08/23 15:51:39.959
    Jun  8 15:51:39.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-9645 cluster-info'
    Jun  8 15:51:40.039: INFO: stderr: ""
    Jun  8 15:51:40.039: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:51:40.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9645" for this suite. 06/08/23 15:51:40.045
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:51:40.055
Jun  8 15:51:40.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename emptydir 06/08/23 15:51:40.056
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:40.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:40.078
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 06/08/23 15:51:40.081
Jun  8 15:51:40.090: INFO: Waiting up to 5m0s for pod "pod-9a17caa5-abff-48ae-a33b-199d5b6a77f8" in namespace "emptydir-6129" to be "Succeeded or Failed"
Jun  8 15:51:40.096: INFO: Pod "pod-9a17caa5-abff-48ae-a33b-199d5b6a77f8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.925129ms
Jun  8 15:51:42.101: INFO: Pod "pod-9a17caa5-abff-48ae-a33b-199d5b6a77f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010704401s
Jun  8 15:51:44.102: INFO: Pod "pod-9a17caa5-abff-48ae-a33b-199d5b6a77f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011734799s
STEP: Saw pod success 06/08/23 15:51:44.102
Jun  8 15:51:44.102: INFO: Pod "pod-9a17caa5-abff-48ae-a33b-199d5b6a77f8" satisfied condition "Succeeded or Failed"
Jun  8 15:51:44.106: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-9a17caa5-abff-48ae-a33b-199d5b6a77f8 container test-container: <nil>
STEP: delete the pod 06/08/23 15:51:44.113
Jun  8 15:51:44.124: INFO: Waiting for pod pod-9a17caa5-abff-48ae-a33b-199d5b6a77f8 to disappear
Jun  8 15:51:44.127: INFO: Pod pod-9a17caa5-abff-48ae-a33b-199d5b6a77f8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  8 15:51:44.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6129" for this suite. 06/08/23 15:51:44.132
------------------------------
• [4.083 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:51:40.055
    Jun  8 15:51:40.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename emptydir 06/08/23 15:51:40.056
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:40.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:40.078
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 06/08/23 15:51:40.081
    Jun  8 15:51:40.090: INFO: Waiting up to 5m0s for pod "pod-9a17caa5-abff-48ae-a33b-199d5b6a77f8" in namespace "emptydir-6129" to be "Succeeded or Failed"
    Jun  8 15:51:40.096: INFO: Pod "pod-9a17caa5-abff-48ae-a33b-199d5b6a77f8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.925129ms
    Jun  8 15:51:42.101: INFO: Pod "pod-9a17caa5-abff-48ae-a33b-199d5b6a77f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010704401s
    Jun  8 15:51:44.102: INFO: Pod "pod-9a17caa5-abff-48ae-a33b-199d5b6a77f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011734799s
    STEP: Saw pod success 06/08/23 15:51:44.102
    Jun  8 15:51:44.102: INFO: Pod "pod-9a17caa5-abff-48ae-a33b-199d5b6a77f8" satisfied condition "Succeeded or Failed"
    Jun  8 15:51:44.106: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-9a17caa5-abff-48ae-a33b-199d5b6a77f8 container test-container: <nil>
    STEP: delete the pod 06/08/23 15:51:44.113
    Jun  8 15:51:44.124: INFO: Waiting for pod pod-9a17caa5-abff-48ae-a33b-199d5b6a77f8 to disappear
    Jun  8 15:51:44.127: INFO: Pod pod-9a17caa5-abff-48ae-a33b-199d5b6a77f8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:51:44.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6129" for this suite. 06/08/23 15:51:44.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:51:44.14
Jun  8 15:51:44.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename pods 06/08/23 15:51:44.142
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:44.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:44.164
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 06/08/23 15:51:44.167
STEP: submitting the pod to kubernetes 06/08/23 15:51:44.167
STEP: verifying QOS class is set on the pod 06/08/23 15:51:44.175
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Jun  8 15:51:44.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7211" for this suite. 06/08/23 15:51:44.187
------------------------------
• [0.061 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:51:44.14
    Jun  8 15:51:44.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename pods 06/08/23 15:51:44.142
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:44.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:44.164
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 06/08/23 15:51:44.167
    STEP: submitting the pod to kubernetes 06/08/23 15:51:44.167
    STEP: verifying QOS class is set on the pod 06/08/23 15:51:44.175
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:51:44.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7211" for this suite. 06/08/23 15:51:44.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:51:44.207
Jun  8 15:51:44.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename container-probe 06/08/23 15:51:44.208
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:44.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:44.256
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-9b31b865-a41a-4f85-bfc7-358df719de40 in namespace container-probe-6505 06/08/23 15:51:44.259
Jun  8 15:51:44.267: INFO: Waiting up to 5m0s for pod "busybox-9b31b865-a41a-4f85-bfc7-358df719de40" in namespace "container-probe-6505" to be "not pending"
Jun  8 15:51:44.272: INFO: Pod "busybox-9b31b865-a41a-4f85-bfc7-358df719de40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.537777ms
Jun  8 15:51:46.276: INFO: Pod "busybox-9b31b865-a41a-4f85-bfc7-358df719de40": Phase="Running", Reason="", readiness=true. Elapsed: 2.008482448s
Jun  8 15:51:46.276: INFO: Pod "busybox-9b31b865-a41a-4f85-bfc7-358df719de40" satisfied condition "not pending"
Jun  8 15:51:46.276: INFO: Started pod busybox-9b31b865-a41a-4f85-bfc7-358df719de40 in namespace container-probe-6505
STEP: checking the pod's current state and verifying that restartCount is present 06/08/23 15:51:46.276
Jun  8 15:51:46.280: INFO: Initial restart count of pod busybox-9b31b865-a41a-4f85-bfc7-358df719de40 is 0
STEP: deleting the pod 06/08/23 15:55:46.863
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun  8 15:55:46.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6505" for this suite. 06/08/23 15:55:46.885
------------------------------
• [SLOW TEST] [242.684 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:51:44.207
    Jun  8 15:51:44.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename container-probe 06/08/23 15:51:44.208
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:51:44.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:51:44.256
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-9b31b865-a41a-4f85-bfc7-358df719de40 in namespace container-probe-6505 06/08/23 15:51:44.259
    Jun  8 15:51:44.267: INFO: Waiting up to 5m0s for pod "busybox-9b31b865-a41a-4f85-bfc7-358df719de40" in namespace "container-probe-6505" to be "not pending"
    Jun  8 15:51:44.272: INFO: Pod "busybox-9b31b865-a41a-4f85-bfc7-358df719de40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.537777ms
    Jun  8 15:51:46.276: INFO: Pod "busybox-9b31b865-a41a-4f85-bfc7-358df719de40": Phase="Running", Reason="", readiness=true. Elapsed: 2.008482448s
    Jun  8 15:51:46.276: INFO: Pod "busybox-9b31b865-a41a-4f85-bfc7-358df719de40" satisfied condition "not pending"
    Jun  8 15:51:46.276: INFO: Started pod busybox-9b31b865-a41a-4f85-bfc7-358df719de40 in namespace container-probe-6505
    STEP: checking the pod's current state and verifying that restartCount is present 06/08/23 15:51:46.276
    Jun  8 15:51:46.280: INFO: Initial restart count of pod busybox-9b31b865-a41a-4f85-bfc7-358df719de40 is 0
    STEP: deleting the pod 06/08/23 15:55:46.863
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:55:46.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6505" for this suite. 06/08/23 15:55:46.885
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:55:46.891
Jun  8 15:55:46.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename configmap 06/08/23 15:55:46.893
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:55:46.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:55:46.924
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-ef1d2aa0-e178-493f-94ee-0da9321367c8 06/08/23 15:55:46.928
STEP: Creating a pod to test consume configMaps 06/08/23 15:55:46.933
Jun  8 15:55:46.940: INFO: Waiting up to 5m0s for pod "pod-configmaps-0befff0a-b7c0-48dd-91f0-9503d4de2b0d" in namespace "configmap-3489" to be "Succeeded or Failed"
Jun  8 15:55:46.952: INFO: Pod "pod-configmaps-0befff0a-b7c0-48dd-91f0-9503d4de2b0d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.216061ms
Jun  8 15:55:48.957: INFO: Pod "pod-configmaps-0befff0a-b7c0-48dd-91f0-9503d4de2b0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017386295s
Jun  8 15:55:50.957: INFO: Pod "pod-configmaps-0befff0a-b7c0-48dd-91f0-9503d4de2b0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017315596s
STEP: Saw pod success 06/08/23 15:55:50.957
Jun  8 15:55:50.958: INFO: Pod "pod-configmaps-0befff0a-b7c0-48dd-91f0-9503d4de2b0d" satisfied condition "Succeeded or Failed"
Jun  8 15:55:50.961: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-configmaps-0befff0a-b7c0-48dd-91f0-9503d4de2b0d container agnhost-container: <nil>
STEP: delete the pod 06/08/23 15:55:50.976
Jun  8 15:55:50.987: INFO: Waiting for pod pod-configmaps-0befff0a-b7c0-48dd-91f0-9503d4de2b0d to disappear
Jun  8 15:55:50.990: INFO: Pod pod-configmaps-0befff0a-b7c0-48dd-91f0-9503d4de2b0d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  8 15:55:50.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3489" for this suite. 06/08/23 15:55:50.996
------------------------------
• [4.110 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:55:46.891
    Jun  8 15:55:46.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename configmap 06/08/23 15:55:46.893
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:55:46.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:55:46.924
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-ef1d2aa0-e178-493f-94ee-0da9321367c8 06/08/23 15:55:46.928
    STEP: Creating a pod to test consume configMaps 06/08/23 15:55:46.933
    Jun  8 15:55:46.940: INFO: Waiting up to 5m0s for pod "pod-configmaps-0befff0a-b7c0-48dd-91f0-9503d4de2b0d" in namespace "configmap-3489" to be "Succeeded or Failed"
    Jun  8 15:55:46.952: INFO: Pod "pod-configmaps-0befff0a-b7c0-48dd-91f0-9503d4de2b0d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.216061ms
    Jun  8 15:55:48.957: INFO: Pod "pod-configmaps-0befff0a-b7c0-48dd-91f0-9503d4de2b0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017386295s
    Jun  8 15:55:50.957: INFO: Pod "pod-configmaps-0befff0a-b7c0-48dd-91f0-9503d4de2b0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017315596s
    STEP: Saw pod success 06/08/23 15:55:50.957
    Jun  8 15:55:50.958: INFO: Pod "pod-configmaps-0befff0a-b7c0-48dd-91f0-9503d4de2b0d" satisfied condition "Succeeded or Failed"
    Jun  8 15:55:50.961: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-configmaps-0befff0a-b7c0-48dd-91f0-9503d4de2b0d container agnhost-container: <nil>
    STEP: delete the pod 06/08/23 15:55:50.976
    Jun  8 15:55:50.987: INFO: Waiting for pod pod-configmaps-0befff0a-b7c0-48dd-91f0-9503d4de2b0d to disappear
    Jun  8 15:55:50.990: INFO: Pod pod-configmaps-0befff0a-b7c0-48dd-91f0-9503d4de2b0d no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:55:50.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3489" for this suite. 06/08/23 15:55:50.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:55:51.003
Jun  8 15:55:51.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename job 06/08/23 15:55:51.005
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:55:51.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:55:51.026
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 06/08/23 15:55:51.029
STEP: Ensuring active pods == parallelism 06/08/23 15:55:51.036
STEP: delete a job 06/08/23 15:55:53.041
STEP: deleting Job.batch foo in namespace job-621, will wait for the garbage collector to delete the pods 06/08/23 15:55:53.041
Jun  8 15:55:53.103: INFO: Deleting Job.batch foo took: 7.706113ms
Jun  8 15:55:53.204: INFO: Terminating Job.batch foo pods took: 100.972014ms
STEP: Ensuring job was deleted 06/08/23 15:56:25.405
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jun  8 15:56:25.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-621" for this suite. 06/08/23 15:56:25.416
------------------------------
• [SLOW TEST] [34.419 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:55:51.003
    Jun  8 15:55:51.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename job 06/08/23 15:55:51.005
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:55:51.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:55:51.026
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 06/08/23 15:55:51.029
    STEP: Ensuring active pods == parallelism 06/08/23 15:55:51.036
    STEP: delete a job 06/08/23 15:55:53.041
    STEP: deleting Job.batch foo in namespace job-621, will wait for the garbage collector to delete the pods 06/08/23 15:55:53.041
    Jun  8 15:55:53.103: INFO: Deleting Job.batch foo took: 7.706113ms
    Jun  8 15:55:53.204: INFO: Terminating Job.batch foo pods took: 100.972014ms
    STEP: Ensuring job was deleted 06/08/23 15:56:25.405
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:56:25.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-621" for this suite. 06/08/23 15:56:25.416
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:56:25.423
Jun  8 15:56:25.423: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename services 06/08/23 15:56:25.424
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:56:25.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:56:25.445
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 06/08/23 15:56:25.448
Jun  8 15:56:25.448: INFO: Creating e2e-svc-a-dthj9
Jun  8 15:56:25.462: INFO: Creating e2e-svc-b-fv4mw
Jun  8 15:56:25.480: INFO: Creating e2e-svc-c-kj2v9
STEP: deleting service collection 06/08/23 15:56:25.504
Jun  8 15:56:25.557: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  8 15:56:25.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9114" for this suite. 06/08/23 15:56:25.564
------------------------------
• [0.152 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:56:25.423
    Jun  8 15:56:25.423: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename services 06/08/23 15:56:25.424
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:56:25.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:56:25.445
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 06/08/23 15:56:25.448
    Jun  8 15:56:25.448: INFO: Creating e2e-svc-a-dthj9
    Jun  8 15:56:25.462: INFO: Creating e2e-svc-b-fv4mw
    Jun  8 15:56:25.480: INFO: Creating e2e-svc-c-kj2v9
    STEP: deleting service collection 06/08/23 15:56:25.504
    Jun  8 15:56:25.557: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:56:25.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9114" for this suite. 06/08/23 15:56:25.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:56:25.576
Jun  8 15:56:25.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename resourcequota 06/08/23 15:56:25.578
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:56:25.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:56:25.609
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 06/08/23 15:56:25.614
STEP: Ensuring ResourceQuota status is calculated 06/08/23 15:56:25.621
STEP: Creating a ResourceQuota with not terminating scope 06/08/23 15:56:27.628
STEP: Ensuring ResourceQuota status is calculated 06/08/23 15:56:27.634
STEP: Creating a long running pod 06/08/23 15:56:29.638
STEP: Ensuring resource quota with not terminating scope captures the pod usage 06/08/23 15:56:29.653
STEP: Ensuring resource quota with terminating scope ignored the pod usage 06/08/23 15:56:31.658
STEP: Deleting the pod 06/08/23 15:56:33.664
STEP: Ensuring resource quota status released the pod usage 06/08/23 15:56:33.675
STEP: Creating a terminating pod 06/08/23 15:56:35.68
STEP: Ensuring resource quota with terminating scope captures the pod usage 06/08/23 15:56:35.691
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 06/08/23 15:56:37.696
STEP: Deleting the pod 06/08/23 15:56:39.701
STEP: Ensuring resource quota status released the pod usage 06/08/23 15:56:39.711
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  8 15:56:41.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9829" for this suite. 06/08/23 15:56:41.722
------------------------------
• [SLOW TEST] [16.153 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:56:25.576
    Jun  8 15:56:25.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename resourcequota 06/08/23 15:56:25.578
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:56:25.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:56:25.609
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 06/08/23 15:56:25.614
    STEP: Ensuring ResourceQuota status is calculated 06/08/23 15:56:25.621
    STEP: Creating a ResourceQuota with not terminating scope 06/08/23 15:56:27.628
    STEP: Ensuring ResourceQuota status is calculated 06/08/23 15:56:27.634
    STEP: Creating a long running pod 06/08/23 15:56:29.638
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 06/08/23 15:56:29.653
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 06/08/23 15:56:31.658
    STEP: Deleting the pod 06/08/23 15:56:33.664
    STEP: Ensuring resource quota status released the pod usage 06/08/23 15:56:33.675
    STEP: Creating a terminating pod 06/08/23 15:56:35.68
    STEP: Ensuring resource quota with terminating scope captures the pod usage 06/08/23 15:56:35.691
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 06/08/23 15:56:37.696
    STEP: Deleting the pod 06/08/23 15:56:39.701
    STEP: Ensuring resource quota status released the pod usage 06/08/23 15:56:39.711
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:56:41.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9829" for this suite. 06/08/23 15:56:41.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:56:41.73
Jun  8 15:56:41.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename pods 06/08/23 15:56:41.732
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:56:41.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:56:41.75
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 06/08/23 15:56:41.754
Jun  8 15:56:41.763: INFO: Waiting up to 5m0s for pod "pod-8pd48" in namespace "pods-4906" to be "running"
Jun  8 15:56:41.766: INFO: Pod "pod-8pd48": Phase="Pending", Reason="", readiness=false. Elapsed: 3.657093ms
Jun  8 15:56:43.771: INFO: Pod "pod-8pd48": Phase="Running", Reason="", readiness=true. Elapsed: 2.008670429s
Jun  8 15:56:43.771: INFO: Pod "pod-8pd48" satisfied condition "running"
STEP: patching /status 06/08/23 15:56:43.771
Jun  8 15:56:43.780: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun  8 15:56:43.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4906" for this suite. 06/08/23 15:56:43.787
------------------------------
• [2.063 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:56:41.73
    Jun  8 15:56:41.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename pods 06/08/23 15:56:41.732
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:56:41.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:56:41.75
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 06/08/23 15:56:41.754
    Jun  8 15:56:41.763: INFO: Waiting up to 5m0s for pod "pod-8pd48" in namespace "pods-4906" to be "running"
    Jun  8 15:56:41.766: INFO: Pod "pod-8pd48": Phase="Pending", Reason="", readiness=false. Elapsed: 3.657093ms
    Jun  8 15:56:43.771: INFO: Pod "pod-8pd48": Phase="Running", Reason="", readiness=true. Elapsed: 2.008670429s
    Jun  8 15:56:43.771: INFO: Pod "pod-8pd48" satisfied condition "running"
    STEP: patching /status 06/08/23 15:56:43.771
    Jun  8 15:56:43.780: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:56:43.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4906" for this suite. 06/08/23 15:56:43.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:56:43.795
Jun  8 15:56:43.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename tables 06/08/23 15:56:43.796
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:56:43.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:56:43.815
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Jun  8 15:56:43.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-4272" for this suite. 06/08/23 15:56:43.829
------------------------------
• [0.041 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:56:43.795
    Jun  8 15:56:43.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename tables 06/08/23 15:56:43.796
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:56:43.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:56:43.815
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:56:43.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-4272" for this suite. 06/08/23 15:56:43.829
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:56:43.838
Jun  8 15:56:43.838: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename emptydir 06/08/23 15:56:43.84
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:56:43.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:56:43.859
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 06/08/23 15:56:43.862
Jun  8 15:56:43.871: INFO: Waiting up to 5m0s for pod "pod-668dc737-faeb-4810-ad0e-24c659f6f37d" in namespace "emptydir-3403" to be "Succeeded or Failed"
Jun  8 15:56:43.875: INFO: Pod "pod-668dc737-faeb-4810-ad0e-24c659f6f37d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.39291ms
Jun  8 15:56:45.881: INFO: Pod "pod-668dc737-faeb-4810-ad0e-24c659f6f37d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010616771s
Jun  8 15:56:47.880: INFO: Pod "pod-668dc737-faeb-4810-ad0e-24c659f6f37d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009124776s
STEP: Saw pod success 06/08/23 15:56:47.88
Jun  8 15:56:47.880: INFO: Pod "pod-668dc737-faeb-4810-ad0e-24c659f6f37d" satisfied condition "Succeeded or Failed"
Jun  8 15:56:47.883: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-668dc737-faeb-4810-ad0e-24c659f6f37d container test-container: <nil>
STEP: delete the pod 06/08/23 15:56:47.89
Jun  8 15:56:47.902: INFO: Waiting for pod pod-668dc737-faeb-4810-ad0e-24c659f6f37d to disappear
Jun  8 15:56:47.906: INFO: Pod pod-668dc737-faeb-4810-ad0e-24c659f6f37d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  8 15:56:47.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3403" for this suite. 06/08/23 15:56:47.912
------------------------------
• [4.081 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:56:43.838
    Jun  8 15:56:43.838: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename emptydir 06/08/23 15:56:43.84
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:56:43.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:56:43.859
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 06/08/23 15:56:43.862
    Jun  8 15:56:43.871: INFO: Waiting up to 5m0s for pod "pod-668dc737-faeb-4810-ad0e-24c659f6f37d" in namespace "emptydir-3403" to be "Succeeded or Failed"
    Jun  8 15:56:43.875: INFO: Pod "pod-668dc737-faeb-4810-ad0e-24c659f6f37d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.39291ms
    Jun  8 15:56:45.881: INFO: Pod "pod-668dc737-faeb-4810-ad0e-24c659f6f37d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010616771s
    Jun  8 15:56:47.880: INFO: Pod "pod-668dc737-faeb-4810-ad0e-24c659f6f37d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009124776s
    STEP: Saw pod success 06/08/23 15:56:47.88
    Jun  8 15:56:47.880: INFO: Pod "pod-668dc737-faeb-4810-ad0e-24c659f6f37d" satisfied condition "Succeeded or Failed"
    Jun  8 15:56:47.883: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-668dc737-faeb-4810-ad0e-24c659f6f37d container test-container: <nil>
    STEP: delete the pod 06/08/23 15:56:47.89
    Jun  8 15:56:47.902: INFO: Waiting for pod pod-668dc737-faeb-4810-ad0e-24c659f6f37d to disappear
    Jun  8 15:56:47.906: INFO: Pod pod-668dc737-faeb-4810-ad0e-24c659f6f37d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:56:47.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3403" for this suite. 06/08/23 15:56:47.912
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:56:47.919
Jun  8 15:56:47.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename container-lifecycle-hook 06/08/23 15:56:47.921
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:56:47.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:56:47.941
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 06/08/23 15:56:47.949
Jun  8 15:56:47.958: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3276" to be "running and ready"
Jun  8 15:56:47.962: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.103774ms
Jun  8 15:56:47.962: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:56:49.966: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.00831315s
Jun  8 15:56:49.966: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun  8 15:56:49.966: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 06/08/23 15:56:49.97
Jun  8 15:56:49.976: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-3276" to be "running and ready"
Jun  8 15:56:49.981: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.207897ms
Jun  8 15:56:49.981: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:56:51.985: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008807569s
Jun  8 15:56:51.985: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jun  8 15:56:51.985: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 06/08/23 15:56:51.989
Jun  8 15:56:51.996: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  8 15:56:52.000: INFO: Pod pod-with-prestop-http-hook still exists
Jun  8 15:56:54.001: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  8 15:56:54.006: INFO: Pod pod-with-prestop-http-hook still exists
Jun  8 15:56:56.002: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  8 15:56:56.007: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 06/08/23 15:56:56.007
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jun  8 15:56:56.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-3276" for this suite. 06/08/23 15:56:56.029
------------------------------
• [SLOW TEST] [8.117 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:56:47.919
    Jun  8 15:56:47.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/08/23 15:56:47.921
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:56:47.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:56:47.941
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 06/08/23 15:56:47.949
    Jun  8 15:56:47.958: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3276" to be "running and ready"
    Jun  8 15:56:47.962: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.103774ms
    Jun  8 15:56:47.962: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:56:49.966: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.00831315s
    Jun  8 15:56:49.966: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun  8 15:56:49.966: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 06/08/23 15:56:49.97
    Jun  8 15:56:49.976: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-3276" to be "running and ready"
    Jun  8 15:56:49.981: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.207897ms
    Jun  8 15:56:49.981: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:56:51.985: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008807569s
    Jun  8 15:56:51.985: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jun  8 15:56:51.985: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 06/08/23 15:56:51.989
    Jun  8 15:56:51.996: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jun  8 15:56:52.000: INFO: Pod pod-with-prestop-http-hook still exists
    Jun  8 15:56:54.001: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jun  8 15:56:54.006: INFO: Pod pod-with-prestop-http-hook still exists
    Jun  8 15:56:56.002: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jun  8 15:56:56.007: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 06/08/23 15:56:56.007
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:56:56.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-3276" for this suite. 06/08/23 15:56:56.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:56:56.04
Jun  8 15:56:56.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename statefulset 06/08/23 15:56:56.041
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:56:56.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:56:56.061
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8394 06/08/23 15:56:56.065
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 06/08/23 15:56:56.072
Jun  8 15:56:56.081: INFO: Found 0 stateful pods, waiting for 3
Jun  8 15:57:06.087: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  8 15:57:06.087: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  8 15:57:06.087: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 06/08/23 15:57:06.099
Jun  8 15:57:06.120: INFO: Updating stateful set ss2
STEP: Creating a new revision 06/08/23 15:57:06.12
STEP: Not applying an update when the partition is greater than the number of replicas 06/08/23 15:57:16.138
STEP: Performing a canary update 06/08/23 15:57:16.138
Jun  8 15:57:16.159: INFO: Updating stateful set ss2
Jun  8 15:57:16.166: INFO: Waiting for Pod statefulset-8394/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 06/08/23 15:57:26.174
Jun  8 15:57:26.210: INFO: Found 1 stateful pods, waiting for 3
Jun  8 15:57:36.216: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  8 15:57:36.216: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  8 15:57:36.216: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 06/08/23 15:57:36.223
Jun  8 15:57:36.243: INFO: Updating stateful set ss2
Jun  8 15:57:36.250: INFO: Waiting for Pod statefulset-8394/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Jun  8 15:57:46.280: INFO: Updating stateful set ss2
Jun  8 15:57:46.288: INFO: Waiting for StatefulSet statefulset-8394/ss2 to complete update
Jun  8 15:57:46.288: INFO: Waiting for Pod statefulset-8394/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun  8 15:57:56.296: INFO: Deleting all statefulset in ns statefulset-8394
Jun  8 15:57:56.299: INFO: Scaling statefulset ss2 to 0
Jun  8 15:58:06.318: INFO: Waiting for statefulset status.replicas updated to 0
Jun  8 15:58:06.321: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun  8 15:58:06.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8394" for this suite. 06/08/23 15:58:06.338
------------------------------
• [SLOW TEST] [70.305 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:56:56.04
    Jun  8 15:56:56.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename statefulset 06/08/23 15:56:56.041
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:56:56.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:56:56.061
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8394 06/08/23 15:56:56.065
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 06/08/23 15:56:56.072
    Jun  8 15:56:56.081: INFO: Found 0 stateful pods, waiting for 3
    Jun  8 15:57:06.087: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun  8 15:57:06.087: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun  8 15:57:06.087: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 06/08/23 15:57:06.099
    Jun  8 15:57:06.120: INFO: Updating stateful set ss2
    STEP: Creating a new revision 06/08/23 15:57:06.12
    STEP: Not applying an update when the partition is greater than the number of replicas 06/08/23 15:57:16.138
    STEP: Performing a canary update 06/08/23 15:57:16.138
    Jun  8 15:57:16.159: INFO: Updating stateful set ss2
    Jun  8 15:57:16.166: INFO: Waiting for Pod statefulset-8394/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 06/08/23 15:57:26.174
    Jun  8 15:57:26.210: INFO: Found 1 stateful pods, waiting for 3
    Jun  8 15:57:36.216: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun  8 15:57:36.216: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun  8 15:57:36.216: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 06/08/23 15:57:36.223
    Jun  8 15:57:36.243: INFO: Updating stateful set ss2
    Jun  8 15:57:36.250: INFO: Waiting for Pod statefulset-8394/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Jun  8 15:57:46.280: INFO: Updating stateful set ss2
    Jun  8 15:57:46.288: INFO: Waiting for StatefulSet statefulset-8394/ss2 to complete update
    Jun  8 15:57:46.288: INFO: Waiting for Pod statefulset-8394/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun  8 15:57:56.296: INFO: Deleting all statefulset in ns statefulset-8394
    Jun  8 15:57:56.299: INFO: Scaling statefulset ss2 to 0
    Jun  8 15:58:06.318: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  8 15:58:06.321: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:58:06.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8394" for this suite. 06/08/23 15:58:06.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:58:06.346
Jun  8 15:58:06.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 15:58:06.347
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:58:06.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:58:06.367
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-5ee1d08f-7a61-4cdf-8e3a-8ff65fdb2607 06/08/23 15:58:06.37
STEP: Creating a pod to test consume configMaps 06/08/23 15:58:06.375
Jun  8 15:58:06.383: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8478383b-d3bf-477e-9f7e-be10d5a8aa50" in namespace "projected-5822" to be "Succeeded or Failed"
Jun  8 15:58:06.386: INFO: Pod "pod-projected-configmaps-8478383b-d3bf-477e-9f7e-be10d5a8aa50": Phase="Pending", Reason="", readiness=false. Elapsed: 3.180017ms
Jun  8 15:58:08.391: INFO: Pod "pod-projected-configmaps-8478383b-d3bf-477e-9f7e-be10d5a8aa50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008005077s
Jun  8 15:58:10.391: INFO: Pod "pod-projected-configmaps-8478383b-d3bf-477e-9f7e-be10d5a8aa50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00820801s
STEP: Saw pod success 06/08/23 15:58:10.391
Jun  8 15:58:10.391: INFO: Pod "pod-projected-configmaps-8478383b-d3bf-477e-9f7e-be10d5a8aa50" satisfied condition "Succeeded or Failed"
Jun  8 15:58:10.395: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-configmaps-8478383b-d3bf-477e-9f7e-be10d5a8aa50 container agnhost-container: <nil>
STEP: delete the pod 06/08/23 15:58:10.401
Jun  8 15:58:10.411: INFO: Waiting for pod pod-projected-configmaps-8478383b-d3bf-477e-9f7e-be10d5a8aa50 to disappear
Jun  8 15:58:10.414: INFO: Pod pod-projected-configmaps-8478383b-d3bf-477e-9f7e-be10d5a8aa50 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun  8 15:58:10.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5822" for this suite. 06/08/23 15:58:10.42
------------------------------
• [4.080 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:58:06.346
    Jun  8 15:58:06.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 15:58:06.347
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:58:06.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:58:06.367
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-5ee1d08f-7a61-4cdf-8e3a-8ff65fdb2607 06/08/23 15:58:06.37
    STEP: Creating a pod to test consume configMaps 06/08/23 15:58:06.375
    Jun  8 15:58:06.383: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8478383b-d3bf-477e-9f7e-be10d5a8aa50" in namespace "projected-5822" to be "Succeeded or Failed"
    Jun  8 15:58:06.386: INFO: Pod "pod-projected-configmaps-8478383b-d3bf-477e-9f7e-be10d5a8aa50": Phase="Pending", Reason="", readiness=false. Elapsed: 3.180017ms
    Jun  8 15:58:08.391: INFO: Pod "pod-projected-configmaps-8478383b-d3bf-477e-9f7e-be10d5a8aa50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008005077s
    Jun  8 15:58:10.391: INFO: Pod "pod-projected-configmaps-8478383b-d3bf-477e-9f7e-be10d5a8aa50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00820801s
    STEP: Saw pod success 06/08/23 15:58:10.391
    Jun  8 15:58:10.391: INFO: Pod "pod-projected-configmaps-8478383b-d3bf-477e-9f7e-be10d5a8aa50" satisfied condition "Succeeded or Failed"
    Jun  8 15:58:10.395: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-configmaps-8478383b-d3bf-477e-9f7e-be10d5a8aa50 container agnhost-container: <nil>
    STEP: delete the pod 06/08/23 15:58:10.401
    Jun  8 15:58:10.411: INFO: Waiting for pod pod-projected-configmaps-8478383b-d3bf-477e-9f7e-be10d5a8aa50 to disappear
    Jun  8 15:58:10.414: INFO: Pod pod-projected-configmaps-8478383b-d3bf-477e-9f7e-be10d5a8aa50 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:58:10.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5822" for this suite. 06/08/23 15:58:10.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:58:10.428
Jun  8 15:58:10.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename resourcequota 06/08/23 15:58:10.429
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:58:10.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:58:10.448
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 06/08/23 15:58:10.451
STEP: Creating a ResourceQuota 06/08/23 15:58:15.457
STEP: Ensuring resource quota status is calculated 06/08/23 15:58:15.465
STEP: Creating a Pod that fits quota 06/08/23 15:58:17.471
STEP: Ensuring ResourceQuota status captures the pod usage 06/08/23 15:58:17.486
STEP: Not allowing a pod to be created that exceeds remaining quota 06/08/23 15:58:19.492
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 06/08/23 15:58:19.495
STEP: Ensuring a pod cannot update its resource requirements 06/08/23 15:58:19.498
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 06/08/23 15:58:19.502
STEP: Deleting the pod 06/08/23 15:58:21.507
STEP: Ensuring resource quota status released the pod usage 06/08/23 15:58:21.518
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  8 15:58:23.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9012" for this suite. 06/08/23 15:58:23.529
------------------------------
• [SLOW TEST] [13.107 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:58:10.428
    Jun  8 15:58:10.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename resourcequota 06/08/23 15:58:10.429
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:58:10.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:58:10.448
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 06/08/23 15:58:10.451
    STEP: Creating a ResourceQuota 06/08/23 15:58:15.457
    STEP: Ensuring resource quota status is calculated 06/08/23 15:58:15.465
    STEP: Creating a Pod that fits quota 06/08/23 15:58:17.471
    STEP: Ensuring ResourceQuota status captures the pod usage 06/08/23 15:58:17.486
    STEP: Not allowing a pod to be created that exceeds remaining quota 06/08/23 15:58:19.492
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 06/08/23 15:58:19.495
    STEP: Ensuring a pod cannot update its resource requirements 06/08/23 15:58:19.498
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 06/08/23 15:58:19.502
    STEP: Deleting the pod 06/08/23 15:58:21.507
    STEP: Ensuring resource quota status released the pod usage 06/08/23 15:58:21.518
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:58:23.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9012" for this suite. 06/08/23 15:58:23.529
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:58:23.535
Jun  8 15:58:23.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename security-context-test 06/08/23 15:58:23.537
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:58:23.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:58:23.557
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Jun  8 15:58:23.568: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-95c48c5d-5776-43be-b948-cf2dbe14d055" in namespace "security-context-test-9043" to be "Succeeded or Failed"
Jun  8 15:58:23.571: INFO: Pod "busybox-privileged-false-95c48c5d-5776-43be-b948-cf2dbe14d055": Phase="Pending", Reason="", readiness=false. Elapsed: 3.341751ms
Jun  8 15:58:25.576: INFO: Pod "busybox-privileged-false-95c48c5d-5776-43be-b948-cf2dbe14d055": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008359149s
Jun  8 15:58:27.582: INFO: Pod "busybox-privileged-false-95c48c5d-5776-43be-b948-cf2dbe14d055": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01387366s
Jun  8 15:58:27.582: INFO: Pod "busybox-privileged-false-95c48c5d-5776-43be-b948-cf2dbe14d055" satisfied condition "Succeeded or Failed"
Jun  8 15:58:27.589: INFO: Got logs for pod "busybox-privileged-false-95c48c5d-5776-43be-b948-cf2dbe14d055": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jun  8 15:58:27.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-9043" for this suite. 06/08/23 15:58:27.594
------------------------------
• [4.065 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:58:23.535
    Jun  8 15:58:23.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename security-context-test 06/08/23 15:58:23.537
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:58:23.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:58:23.557
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Jun  8 15:58:23.568: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-95c48c5d-5776-43be-b948-cf2dbe14d055" in namespace "security-context-test-9043" to be "Succeeded or Failed"
    Jun  8 15:58:23.571: INFO: Pod "busybox-privileged-false-95c48c5d-5776-43be-b948-cf2dbe14d055": Phase="Pending", Reason="", readiness=false. Elapsed: 3.341751ms
    Jun  8 15:58:25.576: INFO: Pod "busybox-privileged-false-95c48c5d-5776-43be-b948-cf2dbe14d055": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008359149s
    Jun  8 15:58:27.582: INFO: Pod "busybox-privileged-false-95c48c5d-5776-43be-b948-cf2dbe14d055": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01387366s
    Jun  8 15:58:27.582: INFO: Pod "busybox-privileged-false-95c48c5d-5776-43be-b948-cf2dbe14d055" satisfied condition "Succeeded or Failed"
    Jun  8 15:58:27.589: INFO: Got logs for pod "busybox-privileged-false-95c48c5d-5776-43be-b948-cf2dbe14d055": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:58:27.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-9043" for this suite. 06/08/23 15:58:27.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:58:27.601
Jun  8 15:58:27.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename ephemeral-containers-test 06/08/23 15:58:27.602
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:58:27.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:58:27.622
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 06/08/23 15:58:27.625
Jun  8 15:58:27.633: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4874" to be "running and ready"
Jun  8 15:58:27.636: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.08315ms
Jun  8 15:58:27.636: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:58:29.641: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007564915s
Jun  8 15:58:29.641: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jun  8 15:58:29.641: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 06/08/23 15:58:29.644
Jun  8 15:58:29.659: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4874" to be "container debugger running"
Jun  8 15:58:29.663: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.323788ms
Jun  8 15:58:31.668: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008599087s
Jun  8 15:58:31.668: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 06/08/23 15:58:31.668
Jun  8 15:58:31.668: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4874 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 15:58:31.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 15:58:31.669: INFO: ExecWithOptions: Clientset creation
Jun  8 15:58:31.669: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-4874/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jun  8 15:58:31.763: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:58:31.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-4874" for this suite. 06/08/23 15:58:31.776
------------------------------
• [4.182 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:58:27.601
    Jun  8 15:58:27.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename ephemeral-containers-test 06/08/23 15:58:27.602
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:58:27.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:58:27.622
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 06/08/23 15:58:27.625
    Jun  8 15:58:27.633: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4874" to be "running and ready"
    Jun  8 15:58:27.636: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.08315ms
    Jun  8 15:58:27.636: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:58:29.641: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007564915s
    Jun  8 15:58:29.641: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jun  8 15:58:29.641: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 06/08/23 15:58:29.644
    Jun  8 15:58:29.659: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4874" to be "container debugger running"
    Jun  8 15:58:29.663: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.323788ms
    Jun  8 15:58:31.668: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008599087s
    Jun  8 15:58:31.668: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 06/08/23 15:58:31.668
    Jun  8 15:58:31.668: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4874 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 15:58:31.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 15:58:31.669: INFO: ExecWithOptions: Clientset creation
    Jun  8 15:58:31.669: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-4874/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jun  8 15:58:31.763: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:58:31.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-4874" for this suite. 06/08/23 15:58:31.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:58:31.787
Jun  8 15:58:31.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename watch 06/08/23 15:58:31.789
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:58:31.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:58:31.808
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 06/08/23 15:58:31.811
STEP: creating a watch on configmaps with label B 06/08/23 15:58:31.813
STEP: creating a watch on configmaps with label A or B 06/08/23 15:58:31.814
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 06/08/23 15:58:31.816
Jun  8 15:58:31.820: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2381  a9c1a9f0-1efb-4004-9fd3-4cde10b71369 40580 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  8 15:58:31.821: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2381  a9c1a9f0-1efb-4004-9fd3-4cde10b71369 40580 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 06/08/23 15:58:31.821
Jun  8 15:58:31.829: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2381  a9c1a9f0-1efb-4004-9fd3-4cde10b71369 40581 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  8 15:58:31.829: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2381  a9c1a9f0-1efb-4004-9fd3-4cde10b71369 40581 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 06/08/23 15:58:31.829
Jun  8 15:58:31.836: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2381  a9c1a9f0-1efb-4004-9fd3-4cde10b71369 40582 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  8 15:58:31.836: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2381  a9c1a9f0-1efb-4004-9fd3-4cde10b71369 40582 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 06/08/23 15:58:31.837
Jun  8 15:58:31.842: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2381  a9c1a9f0-1efb-4004-9fd3-4cde10b71369 40583 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  8 15:58:31.842: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2381  a9c1a9f0-1efb-4004-9fd3-4cde10b71369 40583 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 06/08/23 15:58:31.842
Jun  8 15:58:31.847: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2381  486ef42b-0a1d-48a4-8b72-1b9140fb9440 40584 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  8 15:58:31.847: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2381  486ef42b-0a1d-48a4-8b72-1b9140fb9440 40584 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 06/08/23 15:58:41.848
Jun  8 15:58:41.855: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2381  486ef42b-0a1d-48a4-8b72-1b9140fb9440 40646 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  8 15:58:41.855: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2381  486ef42b-0a1d-48a4-8b72-1b9140fb9440 40646 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jun  8 15:58:51.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2381" for this suite. 06/08/23 15:58:51.862
------------------------------
• [SLOW TEST] [20.082 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:58:31.787
    Jun  8 15:58:31.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename watch 06/08/23 15:58:31.789
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:58:31.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:58:31.808
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 06/08/23 15:58:31.811
    STEP: creating a watch on configmaps with label B 06/08/23 15:58:31.813
    STEP: creating a watch on configmaps with label A or B 06/08/23 15:58:31.814
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 06/08/23 15:58:31.816
    Jun  8 15:58:31.820: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2381  a9c1a9f0-1efb-4004-9fd3-4cde10b71369 40580 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  8 15:58:31.821: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2381  a9c1a9f0-1efb-4004-9fd3-4cde10b71369 40580 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 06/08/23 15:58:31.821
    Jun  8 15:58:31.829: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2381  a9c1a9f0-1efb-4004-9fd3-4cde10b71369 40581 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  8 15:58:31.829: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2381  a9c1a9f0-1efb-4004-9fd3-4cde10b71369 40581 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 06/08/23 15:58:31.829
    Jun  8 15:58:31.836: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2381  a9c1a9f0-1efb-4004-9fd3-4cde10b71369 40582 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  8 15:58:31.836: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2381  a9c1a9f0-1efb-4004-9fd3-4cde10b71369 40582 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 06/08/23 15:58:31.837
    Jun  8 15:58:31.842: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2381  a9c1a9f0-1efb-4004-9fd3-4cde10b71369 40583 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  8 15:58:31.842: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2381  a9c1a9f0-1efb-4004-9fd3-4cde10b71369 40583 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 06/08/23 15:58:31.842
    Jun  8 15:58:31.847: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2381  486ef42b-0a1d-48a4-8b72-1b9140fb9440 40584 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  8 15:58:31.847: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2381  486ef42b-0a1d-48a4-8b72-1b9140fb9440 40584 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 06/08/23 15:58:41.848
    Jun  8 15:58:41.855: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2381  486ef42b-0a1d-48a4-8b72-1b9140fb9440 40646 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  8 15:58:41.855: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2381  486ef42b-0a1d-48a4-8b72-1b9140fb9440 40646 0 2023-06-08 15:58:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-08 15:58:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:58:51.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2381" for this suite. 06/08/23 15:58:51.862
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:58:51.87
Jun  8 15:58:51.871: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename downward-api 06/08/23 15:58:51.872
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:58:51.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:58:51.891
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 06/08/23 15:58:51.894
Jun  8 15:58:51.903: INFO: Waiting up to 5m0s for pod "annotationupdate0ed669ed-0824-44da-9815-1767ad7b43e7" in namespace "downward-api-2777" to be "running and ready"
Jun  8 15:58:51.907: INFO: Pod "annotationupdate0ed669ed-0824-44da-9815-1767ad7b43e7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.391581ms
Jun  8 15:58:51.907: INFO: The phase of Pod annotationupdate0ed669ed-0824-44da-9815-1767ad7b43e7 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 15:58:53.912: INFO: Pod "annotationupdate0ed669ed-0824-44da-9815-1767ad7b43e7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008774253s
Jun  8 15:58:53.912: INFO: The phase of Pod annotationupdate0ed669ed-0824-44da-9815-1767ad7b43e7 is Running (Ready = true)
Jun  8 15:58:53.912: INFO: Pod "annotationupdate0ed669ed-0824-44da-9815-1767ad7b43e7" satisfied condition "running and ready"
Jun  8 15:58:54.436: INFO: Successfully updated pod "annotationupdate0ed669ed-0824-44da-9815-1767ad7b43e7"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  8 15:58:58.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2777" for this suite. 06/08/23 15:58:58.467
------------------------------
• [SLOW TEST] [6.603 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:58:51.87
    Jun  8 15:58:51.871: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename downward-api 06/08/23 15:58:51.872
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:58:51.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:58:51.891
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 06/08/23 15:58:51.894
    Jun  8 15:58:51.903: INFO: Waiting up to 5m0s for pod "annotationupdate0ed669ed-0824-44da-9815-1767ad7b43e7" in namespace "downward-api-2777" to be "running and ready"
    Jun  8 15:58:51.907: INFO: Pod "annotationupdate0ed669ed-0824-44da-9815-1767ad7b43e7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.391581ms
    Jun  8 15:58:51.907: INFO: The phase of Pod annotationupdate0ed669ed-0824-44da-9815-1767ad7b43e7 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 15:58:53.912: INFO: Pod "annotationupdate0ed669ed-0824-44da-9815-1767ad7b43e7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008774253s
    Jun  8 15:58:53.912: INFO: The phase of Pod annotationupdate0ed669ed-0824-44da-9815-1767ad7b43e7 is Running (Ready = true)
    Jun  8 15:58:53.912: INFO: Pod "annotationupdate0ed669ed-0824-44da-9815-1767ad7b43e7" satisfied condition "running and ready"
    Jun  8 15:58:54.436: INFO: Successfully updated pod "annotationupdate0ed669ed-0824-44da-9815-1767ad7b43e7"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:58:58.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2777" for this suite. 06/08/23 15:58:58.467
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:58:58.474
Jun  8 15:58:58.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename replicaset 06/08/23 15:58:58.475
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:58:58.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:58:58.495
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 06/08/23 15:58:58.498
Jun  8 15:58:58.506: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun  8 15:59:03.512: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/08/23 15:59:03.512
STEP: getting scale subresource 06/08/23 15:59:03.512
STEP: updating a scale subresource 06/08/23 15:59:03.515
STEP: verifying the replicaset Spec.Replicas was modified 06/08/23 15:59:03.521
STEP: Patch a scale subresource 06/08/23 15:59:03.524
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jun  8 15:59:03.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1554" for this suite. 06/08/23 15:59:03.542
------------------------------
• [SLOW TEST] [5.091 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:58:58.474
    Jun  8 15:58:58.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename replicaset 06/08/23 15:58:58.475
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:58:58.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:58:58.495
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 06/08/23 15:58:58.498
    Jun  8 15:58:58.506: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun  8 15:59:03.512: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/08/23 15:59:03.512
    STEP: getting scale subresource 06/08/23 15:59:03.512
    STEP: updating a scale subresource 06/08/23 15:59:03.515
    STEP: verifying the replicaset Spec.Replicas was modified 06/08/23 15:59:03.521
    STEP: Patch a scale subresource 06/08/23 15:59:03.524
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:59:03.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1554" for this suite. 06/08/23 15:59:03.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:59:03.565
Jun  8 15:59:03.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename resourcequota 06/08/23 15:59:03.566
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:59:03.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:59:03.591
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 06/08/23 15:59:03.594
STEP: Creating a ResourceQuota 06/08/23 15:59:08.599
STEP: Ensuring resource quota status is calculated 06/08/23 15:59:08.604
STEP: Creating a ReplicaSet 06/08/23 15:59:10.609
STEP: Ensuring resource quota status captures replicaset creation 06/08/23 15:59:10.622
STEP: Deleting a ReplicaSet 06/08/23 15:59:12.626
STEP: Ensuring resource quota status released usage 06/08/23 15:59:12.632
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  8 15:59:14.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9968" for this suite. 06/08/23 15:59:14.642
------------------------------
• [SLOW TEST] [11.083 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:59:03.565
    Jun  8 15:59:03.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename resourcequota 06/08/23 15:59:03.566
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:59:03.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:59:03.591
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 06/08/23 15:59:03.594
    STEP: Creating a ResourceQuota 06/08/23 15:59:08.599
    STEP: Ensuring resource quota status is calculated 06/08/23 15:59:08.604
    STEP: Creating a ReplicaSet 06/08/23 15:59:10.609
    STEP: Ensuring resource quota status captures replicaset creation 06/08/23 15:59:10.622
    STEP: Deleting a ReplicaSet 06/08/23 15:59:12.626
    STEP: Ensuring resource quota status released usage 06/08/23 15:59:12.632
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:59:14.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9968" for this suite. 06/08/23 15:59:14.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:59:14.653
Jun  8 15:59:14.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename endpointslicemirroring 06/08/23 15:59:14.654
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:59:14.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:59:14.677
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 06/08/23 15:59:14.696
Jun  8 15:59:14.707: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 06/08/23 15:59:16.713
Jun  8 15:59:16.722: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 06/08/23 15:59:18.727
Jun  8 15:59:18.737: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Jun  8 15:59:20.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-1077" for this suite. 06/08/23 15:59:20.749
------------------------------
• [SLOW TEST] [6.105 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:59:14.653
    Jun  8 15:59:14.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename endpointslicemirroring 06/08/23 15:59:14.654
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:59:14.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:59:14.677
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 06/08/23 15:59:14.696
    Jun  8 15:59:14.707: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 06/08/23 15:59:16.713
    Jun  8 15:59:16.722: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 06/08/23 15:59:18.727
    Jun  8 15:59:18.737: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:59:20.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-1077" for this suite. 06/08/23 15:59:20.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:59:20.758
Jun  8 15:59:20.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename events 06/08/23 15:59:20.759
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:59:20.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:59:20.777
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 06/08/23 15:59:20.781
STEP: get a list of Events with a label in the current namespace 06/08/23 15:59:20.796
STEP: delete a list of events 06/08/23 15:59:20.8
Jun  8 15:59:20.800: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 06/08/23 15:59:20.819
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jun  8 15:59:20.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-9612" for this suite. 06/08/23 15:59:20.827
------------------------------
• [0.076 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:59:20.758
    Jun  8 15:59:20.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename events 06/08/23 15:59:20.759
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:59:20.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:59:20.777
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 06/08/23 15:59:20.781
    STEP: get a list of Events with a label in the current namespace 06/08/23 15:59:20.796
    STEP: delete a list of events 06/08/23 15:59:20.8
    Jun  8 15:59:20.800: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 06/08/23 15:59:20.819
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:59:20.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-9612" for this suite. 06/08/23 15:59:20.827
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:59:20.835
Jun  8 15:59:20.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename namespaces 06/08/23 15:59:20.836
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:59:20.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:59:20.856
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 06/08/23 15:59:20.859
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:59:20.875
STEP: Creating a service in the namespace 06/08/23 15:59:20.878
STEP: Deleting the namespace 06/08/23 15:59:20.893
STEP: Waiting for the namespace to be removed. 06/08/23 15:59:20.901
STEP: Recreating the namespace 06/08/23 15:59:26.907
STEP: Verifying there is no service in the namespace 06/08/23 15:59:26.929
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 15:59:26.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3618" for this suite. 06/08/23 15:59:26.938
STEP: Destroying namespace "nsdeletetest-1808" for this suite. 06/08/23 15:59:26.943
Jun  8 15:59:26.947: INFO: Namespace nsdeletetest-1808 was already deleted
STEP: Destroying namespace "nsdeletetest-1139" for this suite. 06/08/23 15:59:26.947
------------------------------
• [SLOW TEST] [6.118 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:59:20.835
    Jun  8 15:59:20.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename namespaces 06/08/23 15:59:20.836
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:59:20.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:59:20.856
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 06/08/23 15:59:20.859
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:59:20.875
    STEP: Creating a service in the namespace 06/08/23 15:59:20.878
    STEP: Deleting the namespace 06/08/23 15:59:20.893
    STEP: Waiting for the namespace to be removed. 06/08/23 15:59:20.901
    STEP: Recreating the namespace 06/08/23 15:59:26.907
    STEP: Verifying there is no service in the namespace 06/08/23 15:59:26.929
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:59:26.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3618" for this suite. 06/08/23 15:59:26.938
    STEP: Destroying namespace "nsdeletetest-1808" for this suite. 06/08/23 15:59:26.943
    Jun  8 15:59:26.947: INFO: Namespace nsdeletetest-1808 was already deleted
    STEP: Destroying namespace "nsdeletetest-1139" for this suite. 06/08/23 15:59:26.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:59:26.955
Jun  8 15:59:26.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubectl 06/08/23 15:59:26.956
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:59:26.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:59:26.98
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/08/23 15:59:26.984
Jun  8 15:59:26.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-9185 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jun  8 15:59:27.086: INFO: stderr: ""
Jun  8 15:59:27.086: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 06/08/23 15:59:27.086
Jun  8 15:59:27.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-9185 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Jun  8 15:59:27.294: INFO: stderr: ""
Jun  8 15:59:27.294: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/08/23 15:59:27.294
Jun  8 15:59:27.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-9185 delete pods e2e-test-httpd-pod'
Jun  8 15:59:58.844: INFO: stderr: ""
Jun  8 15:59:58.844: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  8 15:59:58.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9185" for this suite. 06/08/23 15:59:58.85
------------------------------
• [SLOW TEST] [31.901 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:59:26.955
    Jun  8 15:59:26.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubectl 06/08/23 15:59:26.956
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:59:26.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:59:26.98
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/08/23 15:59:26.984
    Jun  8 15:59:26.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-9185 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jun  8 15:59:27.086: INFO: stderr: ""
    Jun  8 15:59:27.086: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 06/08/23 15:59:27.086
    Jun  8 15:59:27.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-9185 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Jun  8 15:59:27.294: INFO: stderr: ""
    Jun  8 15:59:27.294: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/08/23 15:59:27.294
    Jun  8 15:59:27.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-9185 delete pods e2e-test-httpd-pod'
    Jun  8 15:59:58.844: INFO: stderr: ""
    Jun  8 15:59:58.844: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  8 15:59:58.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9185" for this suite. 06/08/23 15:59:58.85
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 15:59:58.867
Jun  8 15:59:58.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename dns 06/08/23 15:59:58.868
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:59:58.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:59:58.888
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 06/08/23 15:59:58.892
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4098.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4098.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4098.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4098.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4098.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4098.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4098.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4098.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4098.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 217.1.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.1.217_udp@PTR;check="$$(dig +tcp +noall +answer +search 217.1.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.1.217_tcp@PTR;sleep 1; done
 06/08/23 15:59:58.912
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4098.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4098.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4098.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4098.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4098.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4098.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4098.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4098.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4098.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4098.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 217.1.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.1.217_udp@PTR;check="$$(dig +tcp +noall +answer +search 217.1.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.1.217_tcp@PTR;sleep 1; done
 06/08/23 15:59:58.913
STEP: creating a pod to probe DNS 06/08/23 15:59:58.913
STEP: submitting the pod to kubernetes 06/08/23 15:59:58.913
Jun  8 15:59:58.925: INFO: Waiting up to 15m0s for pod "dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68" in namespace "dns-4098" to be "running"
Jun  8 15:59:58.929: INFO: Pod "dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68": Phase="Pending", Reason="", readiness=false. Elapsed: 3.819132ms
Jun  8 16:00:00.934: INFO: Pod "dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68": Phase="Running", Reason="", readiness=true. Elapsed: 2.0090573s
Jun  8 16:00:00.935: INFO: Pod "dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68" satisfied condition "running"
STEP: retrieving the pod 06/08/23 16:00:00.935
STEP: looking for the results for each expected name from probers 06/08/23 16:00:00.939
Jun  8 16:00:00.946: INFO: Unable to read wheezy_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:00.951: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:00.956: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:00.962: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:00.988: INFO: Unable to read jessie_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:00.993: INFO: Unable to read jessie_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:00.998: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:01.003: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:01.023: INFO: Lookups using dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68 failed for: [wheezy_udp@dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_udp@dns-test-service.dns-4098.svc.cluster.local jessie_tcp@dns-test-service.dns-4098.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local]

Jun  8 16:00:06.030: INFO: Unable to read wheezy_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:06.034: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:06.038: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:06.042: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:06.060: INFO: Unable to read jessie_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:06.063: INFO: Unable to read jessie_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:06.067: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:06.071: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:06.085: INFO: Lookups using dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68 failed for: [wheezy_udp@dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_udp@dns-test-service.dns-4098.svc.cluster.local jessie_tcp@dns-test-service.dns-4098.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local]

Jun  8 16:00:11.029: INFO: Unable to read wheezy_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:11.033: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:11.037: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:11.041: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:11.059: INFO: Unable to read jessie_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:11.062: INFO: Unable to read jessie_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:11.066: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:11.069: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:11.083: INFO: Lookups using dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68 failed for: [wheezy_udp@dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_udp@dns-test-service.dns-4098.svc.cluster.local jessie_tcp@dns-test-service.dns-4098.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local]

Jun  8 16:00:16.031: INFO: Unable to read wheezy_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:16.037: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:16.042: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:16.046: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:16.069: INFO: Unable to read jessie_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:16.073: INFO: Unable to read jessie_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:16.078: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:16.082: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:16.099: INFO: Lookups using dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68 failed for: [wheezy_udp@dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_udp@dns-test-service.dns-4098.svc.cluster.local jessie_tcp@dns-test-service.dns-4098.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local]

Jun  8 16:00:21.029: INFO: Unable to read wheezy_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:21.034: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:21.038: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:21.042: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:21.060: INFO: Unable to read jessie_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:21.063: INFO: Unable to read jessie_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:21.067: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:21.070: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:21.085: INFO: Lookups using dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68 failed for: [wheezy_udp@dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_udp@dns-test-service.dns-4098.svc.cluster.local jessie_tcp@dns-test-service.dns-4098.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local]

Jun  8 16:00:26.030: INFO: Unable to read wheezy_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:26.034: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:26.038: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:26.041: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:26.059: INFO: Unable to read jessie_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:26.063: INFO: Unable to read jessie_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:26.067: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:26.070: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
Jun  8 16:00:26.085: INFO: Lookups using dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68 failed for: [wheezy_udp@dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_udp@dns-test-service.dns-4098.svc.cluster.local jessie_tcp@dns-test-service.dns-4098.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local]

Jun  8 16:00:31.083: INFO: DNS probes using dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68 succeeded

STEP: deleting the pod 06/08/23 16:00:31.083
STEP: deleting the test service 06/08/23 16:00:31.096
STEP: deleting the test headless service 06/08/23 16:00:31.132
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun  8 16:00:31.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4098" for this suite. 06/08/23 16:00:31.157
------------------------------
• [SLOW TEST] [32.302 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 15:59:58.867
    Jun  8 15:59:58.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename dns 06/08/23 15:59:58.868
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 15:59:58.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 15:59:58.888
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 06/08/23 15:59:58.892
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4098.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4098.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4098.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4098.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4098.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4098.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4098.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4098.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4098.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 217.1.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.1.217_udp@PTR;check="$$(dig +tcp +noall +answer +search 217.1.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.1.217_tcp@PTR;sleep 1; done
     06/08/23 15:59:58.912
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4098.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4098.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4098.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4098.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4098.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4098.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4098.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4098.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4098.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4098.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 217.1.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.1.217_udp@PTR;check="$$(dig +tcp +noall +answer +search 217.1.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.1.217_tcp@PTR;sleep 1; done
     06/08/23 15:59:58.913
    STEP: creating a pod to probe DNS 06/08/23 15:59:58.913
    STEP: submitting the pod to kubernetes 06/08/23 15:59:58.913
    Jun  8 15:59:58.925: INFO: Waiting up to 15m0s for pod "dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68" in namespace "dns-4098" to be "running"
    Jun  8 15:59:58.929: INFO: Pod "dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68": Phase="Pending", Reason="", readiness=false. Elapsed: 3.819132ms
    Jun  8 16:00:00.934: INFO: Pod "dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68": Phase="Running", Reason="", readiness=true. Elapsed: 2.0090573s
    Jun  8 16:00:00.935: INFO: Pod "dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68" satisfied condition "running"
    STEP: retrieving the pod 06/08/23 16:00:00.935
    STEP: looking for the results for each expected name from probers 06/08/23 16:00:00.939
    Jun  8 16:00:00.946: INFO: Unable to read wheezy_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:00.951: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:00.956: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:00.962: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:00.988: INFO: Unable to read jessie_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:00.993: INFO: Unable to read jessie_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:00.998: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:01.003: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:01.023: INFO: Lookups using dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68 failed for: [wheezy_udp@dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_udp@dns-test-service.dns-4098.svc.cluster.local jessie_tcp@dns-test-service.dns-4098.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local]

    Jun  8 16:00:06.030: INFO: Unable to read wheezy_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:06.034: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:06.038: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:06.042: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:06.060: INFO: Unable to read jessie_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:06.063: INFO: Unable to read jessie_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:06.067: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:06.071: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:06.085: INFO: Lookups using dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68 failed for: [wheezy_udp@dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_udp@dns-test-service.dns-4098.svc.cluster.local jessie_tcp@dns-test-service.dns-4098.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local]

    Jun  8 16:00:11.029: INFO: Unable to read wheezy_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:11.033: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:11.037: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:11.041: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:11.059: INFO: Unable to read jessie_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:11.062: INFO: Unable to read jessie_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:11.066: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:11.069: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:11.083: INFO: Lookups using dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68 failed for: [wheezy_udp@dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_udp@dns-test-service.dns-4098.svc.cluster.local jessie_tcp@dns-test-service.dns-4098.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local]

    Jun  8 16:00:16.031: INFO: Unable to read wheezy_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:16.037: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:16.042: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:16.046: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:16.069: INFO: Unable to read jessie_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:16.073: INFO: Unable to read jessie_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:16.078: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:16.082: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:16.099: INFO: Lookups using dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68 failed for: [wheezy_udp@dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_udp@dns-test-service.dns-4098.svc.cluster.local jessie_tcp@dns-test-service.dns-4098.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local]

    Jun  8 16:00:21.029: INFO: Unable to read wheezy_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:21.034: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:21.038: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:21.042: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:21.060: INFO: Unable to read jessie_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:21.063: INFO: Unable to read jessie_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:21.067: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:21.070: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:21.085: INFO: Lookups using dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68 failed for: [wheezy_udp@dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_udp@dns-test-service.dns-4098.svc.cluster.local jessie_tcp@dns-test-service.dns-4098.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local]

    Jun  8 16:00:26.030: INFO: Unable to read wheezy_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:26.034: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:26.038: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:26.041: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:26.059: INFO: Unable to read jessie_udp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:26.063: INFO: Unable to read jessie_tcp@dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:26.067: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:26.070: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local from pod dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68: the server could not find the requested resource (get pods dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68)
    Jun  8 16:00:26.085: INFO: Lookups using dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68 failed for: [wheezy_udp@dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@dns-test-service.dns-4098.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_udp@dns-test-service.dns-4098.svc.cluster.local jessie_tcp@dns-test-service.dns-4098.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4098.svc.cluster.local]

    Jun  8 16:00:31.083: INFO: DNS probes using dns-4098/dns-test-0946bedf-1461-48a1-b668-65a7c1deaf68 succeeded

    STEP: deleting the pod 06/08/23 16:00:31.083
    STEP: deleting the test service 06/08/23 16:00:31.096
    STEP: deleting the test headless service 06/08/23 16:00:31.132
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:00:31.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4098" for this suite. 06/08/23 16:00:31.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:00:31.17
Jun  8 16:00:31.171: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 16:00:31.173
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:00:31.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:00:31.199
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-34e0ec60-824a-496d-aa58-fa2be20e36d7 06/08/23 16:00:31.204
STEP: Creating a pod to test consume secrets 06/08/23 16:00:31.21
Jun  8 16:00:31.222: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-219e8a50-0e86-415c-88ff-abce9aedb307" in namespace "projected-4458" to be "Succeeded or Failed"
Jun  8 16:00:31.229: INFO: Pod "pod-projected-secrets-219e8a50-0e86-415c-88ff-abce9aedb307": Phase="Pending", Reason="", readiness=false. Elapsed: 6.816014ms
Jun  8 16:00:33.234: INFO: Pod "pod-projected-secrets-219e8a50-0e86-415c-88ff-abce9aedb307": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012343749s
Jun  8 16:00:35.233: INFO: Pod "pod-projected-secrets-219e8a50-0e86-415c-88ff-abce9aedb307": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010963891s
STEP: Saw pod success 06/08/23 16:00:35.233
Jun  8 16:00:35.233: INFO: Pod "pod-projected-secrets-219e8a50-0e86-415c-88ff-abce9aedb307" satisfied condition "Succeeded or Failed"
Jun  8 16:00:35.236: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-secrets-219e8a50-0e86-415c-88ff-abce9aedb307 container projected-secret-volume-test: <nil>
STEP: delete the pod 06/08/23 16:00:35.252
Jun  8 16:00:35.263: INFO: Waiting for pod pod-projected-secrets-219e8a50-0e86-415c-88ff-abce9aedb307 to disappear
Jun  8 16:00:35.266: INFO: Pod pod-projected-secrets-219e8a50-0e86-415c-88ff-abce9aedb307 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jun  8 16:00:35.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4458" for this suite. 06/08/23 16:00:35.271
------------------------------
• [4.106 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:00:31.17
    Jun  8 16:00:31.171: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 16:00:31.173
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:00:31.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:00:31.199
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-34e0ec60-824a-496d-aa58-fa2be20e36d7 06/08/23 16:00:31.204
    STEP: Creating a pod to test consume secrets 06/08/23 16:00:31.21
    Jun  8 16:00:31.222: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-219e8a50-0e86-415c-88ff-abce9aedb307" in namespace "projected-4458" to be "Succeeded or Failed"
    Jun  8 16:00:31.229: INFO: Pod "pod-projected-secrets-219e8a50-0e86-415c-88ff-abce9aedb307": Phase="Pending", Reason="", readiness=false. Elapsed: 6.816014ms
    Jun  8 16:00:33.234: INFO: Pod "pod-projected-secrets-219e8a50-0e86-415c-88ff-abce9aedb307": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012343749s
    Jun  8 16:00:35.233: INFO: Pod "pod-projected-secrets-219e8a50-0e86-415c-88ff-abce9aedb307": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010963891s
    STEP: Saw pod success 06/08/23 16:00:35.233
    Jun  8 16:00:35.233: INFO: Pod "pod-projected-secrets-219e8a50-0e86-415c-88ff-abce9aedb307" satisfied condition "Succeeded or Failed"
    Jun  8 16:00:35.236: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-secrets-219e8a50-0e86-415c-88ff-abce9aedb307 container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/08/23 16:00:35.252
    Jun  8 16:00:35.263: INFO: Waiting for pod pod-projected-secrets-219e8a50-0e86-415c-88ff-abce9aedb307 to disappear
    Jun  8 16:00:35.266: INFO: Pod pod-projected-secrets-219e8a50-0e86-415c-88ff-abce9aedb307 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:00:35.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4458" for this suite. 06/08/23 16:00:35.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:00:35.278
Jun  8 16:00:35.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename taint-single-pod 06/08/23 16:00:35.279
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:00:35.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:00:35.298
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Jun  8 16:00:35.301: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  8 16:01:35.342: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Jun  8 16:01:35.346: INFO: Starting informer...
STEP: Starting pod... 06/08/23 16:01:35.346
Jun  8 16:01:35.562: INFO: Pod is running on chl8tf-worker-001. Tainting Node
STEP: Trying to apply a taint on the Node 06/08/23 16:01:35.562
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/08/23 16:01:35.577
STEP: Waiting short time to make sure Pod is queued for deletion 06/08/23 16:01:35.581
Jun  8 16:01:35.581: INFO: Pod wasn't evicted. Proceeding
Jun  8 16:01:35.581: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/08/23 16:01:35.599
STEP: Waiting some time to make sure that toleration time passed. 06/08/23 16:01:35.606
Jun  8 16:02:50.607: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:02:50.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-4442" for this suite. 06/08/23 16:02:50.614
------------------------------
• [SLOW TEST] [135.342 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:00:35.278
    Jun  8 16:00:35.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename taint-single-pod 06/08/23 16:00:35.279
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:00:35.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:00:35.298
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Jun  8 16:00:35.301: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun  8 16:01:35.342: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Jun  8 16:01:35.346: INFO: Starting informer...
    STEP: Starting pod... 06/08/23 16:01:35.346
    Jun  8 16:01:35.562: INFO: Pod is running on chl8tf-worker-001. Tainting Node
    STEP: Trying to apply a taint on the Node 06/08/23 16:01:35.562
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/08/23 16:01:35.577
    STEP: Waiting short time to make sure Pod is queued for deletion 06/08/23 16:01:35.581
    Jun  8 16:01:35.581: INFO: Pod wasn't evicted. Proceeding
    Jun  8 16:01:35.581: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/08/23 16:01:35.599
    STEP: Waiting some time to make sure that toleration time passed. 06/08/23 16:01:35.606
    Jun  8 16:02:50.607: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:02:50.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-4442" for this suite. 06/08/23 16:02:50.614
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:02:50.621
Jun  8 16:02:50.621: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 16:02:50.622
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:02:50.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:02:50.641
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-6845a95d-e938-4c7c-8934-ed9ff42cc830 06/08/23 16:02:50.645
STEP: Creating a pod to test consume configMaps 06/08/23 16:02:50.649
Jun  8 16:02:50.658: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-65f9235a-3ff7-4973-be2a-6f5cf43d9499" in namespace "projected-4703" to be "Succeeded or Failed"
Jun  8 16:02:50.662: INFO: Pod "pod-projected-configmaps-65f9235a-3ff7-4973-be2a-6f5cf43d9499": Phase="Pending", Reason="", readiness=false. Elapsed: 4.196682ms
Jun  8 16:02:52.667: INFO: Pod "pod-projected-configmaps-65f9235a-3ff7-4973-be2a-6f5cf43d9499": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008433302s
Jun  8 16:02:54.667: INFO: Pod "pod-projected-configmaps-65f9235a-3ff7-4973-be2a-6f5cf43d9499": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008588647s
STEP: Saw pod success 06/08/23 16:02:54.667
Jun  8 16:02:54.667: INFO: Pod "pod-projected-configmaps-65f9235a-3ff7-4973-be2a-6f5cf43d9499" satisfied condition "Succeeded or Failed"
Jun  8 16:02:54.670: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-configmaps-65f9235a-3ff7-4973-be2a-6f5cf43d9499 container agnhost-container: <nil>
STEP: delete the pod 06/08/23 16:02:54.684
Jun  8 16:02:54.713: INFO: Waiting for pod pod-projected-configmaps-65f9235a-3ff7-4973-be2a-6f5cf43d9499 to disappear
Jun  8 16:02:54.718: INFO: Pod pod-projected-configmaps-65f9235a-3ff7-4973-be2a-6f5cf43d9499 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun  8 16:02:54.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4703" for this suite. 06/08/23 16:02:54.726
------------------------------
• [4.129 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:02:50.621
    Jun  8 16:02:50.621: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 16:02:50.622
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:02:50.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:02:50.641
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-6845a95d-e938-4c7c-8934-ed9ff42cc830 06/08/23 16:02:50.645
    STEP: Creating a pod to test consume configMaps 06/08/23 16:02:50.649
    Jun  8 16:02:50.658: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-65f9235a-3ff7-4973-be2a-6f5cf43d9499" in namespace "projected-4703" to be "Succeeded or Failed"
    Jun  8 16:02:50.662: INFO: Pod "pod-projected-configmaps-65f9235a-3ff7-4973-be2a-6f5cf43d9499": Phase="Pending", Reason="", readiness=false. Elapsed: 4.196682ms
    Jun  8 16:02:52.667: INFO: Pod "pod-projected-configmaps-65f9235a-3ff7-4973-be2a-6f5cf43d9499": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008433302s
    Jun  8 16:02:54.667: INFO: Pod "pod-projected-configmaps-65f9235a-3ff7-4973-be2a-6f5cf43d9499": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008588647s
    STEP: Saw pod success 06/08/23 16:02:54.667
    Jun  8 16:02:54.667: INFO: Pod "pod-projected-configmaps-65f9235a-3ff7-4973-be2a-6f5cf43d9499" satisfied condition "Succeeded or Failed"
    Jun  8 16:02:54.670: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-configmaps-65f9235a-3ff7-4973-be2a-6f5cf43d9499 container agnhost-container: <nil>
    STEP: delete the pod 06/08/23 16:02:54.684
    Jun  8 16:02:54.713: INFO: Waiting for pod pod-projected-configmaps-65f9235a-3ff7-4973-be2a-6f5cf43d9499 to disappear
    Jun  8 16:02:54.718: INFO: Pod pod-projected-configmaps-65f9235a-3ff7-4973-be2a-6f5cf43d9499 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:02:54.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4703" for this suite. 06/08/23 16:02:54.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:02:54.75
Jun  8 16:02:54.751: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename replicaset 06/08/23 16:02:54.752
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:02:54.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:02:54.771
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 06/08/23 16:02:54.778
STEP: Verify that the required pods have come up. 06/08/23 16:02:54.783
Jun  8 16:02:54.786: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun  8 16:02:59.791: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/08/23 16:02:59.791
STEP: Getting /status 06/08/23 16:02:59.791
Jun  8 16:02:59.796: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 06/08/23 16:02:59.796
Jun  8 16:02:59.807: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 06/08/23 16:02:59.807
Jun  8 16:02:59.809: INFO: Observed &ReplicaSet event: ADDED
Jun  8 16:02:59.809: INFO: Observed &ReplicaSet event: MODIFIED
Jun  8 16:02:59.809: INFO: Observed &ReplicaSet event: MODIFIED
Jun  8 16:02:59.809: INFO: Observed &ReplicaSet event: MODIFIED
Jun  8 16:02:59.809: INFO: Found replicaset test-rs in namespace replicaset-3515 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun  8 16:02:59.809: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 06/08/23 16:02:59.809
Jun  8 16:02:59.809: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun  8 16:02:59.816: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 06/08/23 16:02:59.816
Jun  8 16:02:59.818: INFO: Observed &ReplicaSet event: ADDED
Jun  8 16:02:59.818: INFO: Observed &ReplicaSet event: MODIFIED
Jun  8 16:02:59.819: INFO: Observed &ReplicaSet event: MODIFIED
Jun  8 16:02:59.819: INFO: Observed &ReplicaSet event: MODIFIED
Jun  8 16:02:59.819: INFO: Observed replicaset test-rs in namespace replicaset-3515 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun  8 16:02:59.819: INFO: Observed &ReplicaSet event: MODIFIED
Jun  8 16:02:59.819: INFO: Found replicaset test-rs in namespace replicaset-3515 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jun  8 16:02:59.819: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jun  8 16:02:59.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3515" for this suite. 06/08/23 16:02:59.824
------------------------------
• [SLOW TEST] [5.081 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:02:54.75
    Jun  8 16:02:54.751: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename replicaset 06/08/23 16:02:54.752
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:02:54.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:02:54.771
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 06/08/23 16:02:54.778
    STEP: Verify that the required pods have come up. 06/08/23 16:02:54.783
    Jun  8 16:02:54.786: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun  8 16:02:59.791: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/08/23 16:02:59.791
    STEP: Getting /status 06/08/23 16:02:59.791
    Jun  8 16:02:59.796: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 06/08/23 16:02:59.796
    Jun  8 16:02:59.807: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 06/08/23 16:02:59.807
    Jun  8 16:02:59.809: INFO: Observed &ReplicaSet event: ADDED
    Jun  8 16:02:59.809: INFO: Observed &ReplicaSet event: MODIFIED
    Jun  8 16:02:59.809: INFO: Observed &ReplicaSet event: MODIFIED
    Jun  8 16:02:59.809: INFO: Observed &ReplicaSet event: MODIFIED
    Jun  8 16:02:59.809: INFO: Found replicaset test-rs in namespace replicaset-3515 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun  8 16:02:59.809: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 06/08/23 16:02:59.809
    Jun  8 16:02:59.809: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jun  8 16:02:59.816: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 06/08/23 16:02:59.816
    Jun  8 16:02:59.818: INFO: Observed &ReplicaSet event: ADDED
    Jun  8 16:02:59.818: INFO: Observed &ReplicaSet event: MODIFIED
    Jun  8 16:02:59.819: INFO: Observed &ReplicaSet event: MODIFIED
    Jun  8 16:02:59.819: INFO: Observed &ReplicaSet event: MODIFIED
    Jun  8 16:02:59.819: INFO: Observed replicaset test-rs in namespace replicaset-3515 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun  8 16:02:59.819: INFO: Observed &ReplicaSet event: MODIFIED
    Jun  8 16:02:59.819: INFO: Found replicaset test-rs in namespace replicaset-3515 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jun  8 16:02:59.819: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:02:59.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3515" for this suite. 06/08/23 16:02:59.824
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:02:59.832
Jun  8 16:02:59.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubectl 06/08/23 16:02:59.834
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:02:59.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:02:59.853
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 06/08/23 16:02:59.856
Jun  8 16:02:59.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 create -f -'
Jun  8 16:03:00.711: INFO: stderr: ""
Jun  8 16:03:00.711: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/08/23 16:03:00.711
Jun  8 16:03:00.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  8 16:03:00.803: INFO: stderr: ""
Jun  8 16:03:00.803: INFO: stdout: "update-demo-nautilus-ht455 update-demo-nautilus-sh7lj "
Jun  8 16:03:00.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-ht455 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  8 16:03:00.880: INFO: stderr: ""
Jun  8 16:03:00.880: INFO: stdout: ""
Jun  8 16:03:00.880: INFO: update-demo-nautilus-ht455 is created but not running
Jun  8 16:03:05.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  8 16:03:05.963: INFO: stderr: ""
Jun  8 16:03:05.963: INFO: stdout: "update-demo-nautilus-ht455 update-demo-nautilus-sh7lj "
Jun  8 16:03:05.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-ht455 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  8 16:03:06.046: INFO: stderr: ""
Jun  8 16:03:06.046: INFO: stdout: "true"
Jun  8 16:03:06.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-ht455 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  8 16:03:06.126: INFO: stderr: ""
Jun  8 16:03:06.126: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun  8 16:03:06.126: INFO: validating pod update-demo-nautilus-ht455
Jun  8 16:03:06.132: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  8 16:03:06.132: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  8 16:03:06.132: INFO: update-demo-nautilus-ht455 is verified up and running
Jun  8 16:03:06.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-sh7lj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  8 16:03:06.209: INFO: stderr: ""
Jun  8 16:03:06.209: INFO: stdout: "true"
Jun  8 16:03:06.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-sh7lj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  8 16:03:06.293: INFO: stderr: ""
Jun  8 16:03:06.293: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun  8 16:03:06.293: INFO: validating pod update-demo-nautilus-sh7lj
Jun  8 16:03:06.299: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  8 16:03:06.299: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  8 16:03:06.299: INFO: update-demo-nautilus-sh7lj is verified up and running
STEP: scaling down the replication controller 06/08/23 16:03:06.299
Jun  8 16:03:06.302: INFO: scanned /root for discovery docs: <nil>
Jun  8 16:03:06.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jun  8 16:03:07.402: INFO: stderr: ""
Jun  8 16:03:07.402: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/08/23 16:03:07.402
Jun  8 16:03:07.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  8 16:03:07.479: INFO: stderr: ""
Jun  8 16:03:07.479: INFO: stdout: "update-demo-nautilus-ht455 "
Jun  8 16:03:07.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-ht455 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  8 16:03:07.555: INFO: stderr: ""
Jun  8 16:03:07.555: INFO: stdout: "true"
Jun  8 16:03:07.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-ht455 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  8 16:03:07.630: INFO: stderr: ""
Jun  8 16:03:07.630: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun  8 16:03:07.630: INFO: validating pod update-demo-nautilus-ht455
Jun  8 16:03:07.635: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  8 16:03:07.635: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  8 16:03:07.635: INFO: update-demo-nautilus-ht455 is verified up and running
STEP: scaling up the replication controller 06/08/23 16:03:07.635
Jun  8 16:03:07.637: INFO: scanned /root for discovery docs: <nil>
Jun  8 16:03:07.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jun  8 16:03:08.740: INFO: stderr: ""
Jun  8 16:03:08.740: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/08/23 16:03:08.74
Jun  8 16:03:08.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  8 16:03:08.822: INFO: stderr: ""
Jun  8 16:03:08.822: INFO: stdout: "update-demo-nautilus-ht455 update-demo-nautilus-l7k6b "
Jun  8 16:03:08.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-ht455 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  8 16:03:08.901: INFO: stderr: ""
Jun  8 16:03:08.901: INFO: stdout: "true"
Jun  8 16:03:08.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-ht455 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  8 16:03:08.979: INFO: stderr: ""
Jun  8 16:03:08.979: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun  8 16:03:08.979: INFO: validating pod update-demo-nautilus-ht455
Jun  8 16:03:08.984: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  8 16:03:08.984: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  8 16:03:08.984: INFO: update-demo-nautilus-ht455 is verified up and running
Jun  8 16:03:08.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-l7k6b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  8 16:03:09.064: INFO: stderr: ""
Jun  8 16:03:09.064: INFO: stdout: ""
Jun  8 16:03:09.064: INFO: update-demo-nautilus-l7k6b is created but not running
Jun  8 16:03:14.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  8 16:03:14.150: INFO: stderr: ""
Jun  8 16:03:14.150: INFO: stdout: "update-demo-nautilus-ht455 update-demo-nautilus-l7k6b "
Jun  8 16:03:14.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-ht455 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  8 16:03:14.232: INFO: stderr: ""
Jun  8 16:03:14.232: INFO: stdout: "true"
Jun  8 16:03:14.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-ht455 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  8 16:03:14.309: INFO: stderr: ""
Jun  8 16:03:14.309: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun  8 16:03:14.309: INFO: validating pod update-demo-nautilus-ht455
Jun  8 16:03:14.314: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  8 16:03:14.314: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  8 16:03:14.314: INFO: update-demo-nautilus-ht455 is verified up and running
Jun  8 16:03:14.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-l7k6b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  8 16:03:14.393: INFO: stderr: ""
Jun  8 16:03:14.393: INFO: stdout: "true"
Jun  8 16:03:14.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-l7k6b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  8 16:03:14.473: INFO: stderr: ""
Jun  8 16:03:14.473: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun  8 16:03:14.473: INFO: validating pod update-demo-nautilus-l7k6b
Jun  8 16:03:14.479: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  8 16:03:14.479: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  8 16:03:14.479: INFO: update-demo-nautilus-l7k6b is verified up and running
STEP: using delete to clean up resources 06/08/23 16:03:14.479
Jun  8 16:03:14.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 delete --grace-period=0 --force -f -'
Jun  8 16:03:14.560: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  8 16:03:14.560: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun  8 16:03:14.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get rc,svc -l name=update-demo --no-headers'
Jun  8 16:03:14.656: INFO: stderr: "No resources found in kubectl-4438 namespace.\n"
Jun  8 16:03:14.656: INFO: stdout: ""
Jun  8 16:03:14.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  8 16:03:14.763: INFO: stderr: ""
Jun  8 16:03:14.763: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  8 16:03:14.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4438" for this suite. 06/08/23 16:03:14.769
------------------------------
• [SLOW TEST] [14.943 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:02:59.832
    Jun  8 16:02:59.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubectl 06/08/23 16:02:59.834
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:02:59.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:02:59.853
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 06/08/23 16:02:59.856
    Jun  8 16:02:59.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 create -f -'
    Jun  8 16:03:00.711: INFO: stderr: ""
    Jun  8 16:03:00.711: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/08/23 16:03:00.711
    Jun  8 16:03:00.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun  8 16:03:00.803: INFO: stderr: ""
    Jun  8 16:03:00.803: INFO: stdout: "update-demo-nautilus-ht455 update-demo-nautilus-sh7lj "
    Jun  8 16:03:00.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-ht455 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  8 16:03:00.880: INFO: stderr: ""
    Jun  8 16:03:00.880: INFO: stdout: ""
    Jun  8 16:03:00.880: INFO: update-demo-nautilus-ht455 is created but not running
    Jun  8 16:03:05.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun  8 16:03:05.963: INFO: stderr: ""
    Jun  8 16:03:05.963: INFO: stdout: "update-demo-nautilus-ht455 update-demo-nautilus-sh7lj "
    Jun  8 16:03:05.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-ht455 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  8 16:03:06.046: INFO: stderr: ""
    Jun  8 16:03:06.046: INFO: stdout: "true"
    Jun  8 16:03:06.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-ht455 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun  8 16:03:06.126: INFO: stderr: ""
    Jun  8 16:03:06.126: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun  8 16:03:06.126: INFO: validating pod update-demo-nautilus-ht455
    Jun  8 16:03:06.132: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun  8 16:03:06.132: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun  8 16:03:06.132: INFO: update-demo-nautilus-ht455 is verified up and running
    Jun  8 16:03:06.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-sh7lj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  8 16:03:06.209: INFO: stderr: ""
    Jun  8 16:03:06.209: INFO: stdout: "true"
    Jun  8 16:03:06.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-sh7lj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun  8 16:03:06.293: INFO: stderr: ""
    Jun  8 16:03:06.293: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun  8 16:03:06.293: INFO: validating pod update-demo-nautilus-sh7lj
    Jun  8 16:03:06.299: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun  8 16:03:06.299: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun  8 16:03:06.299: INFO: update-demo-nautilus-sh7lj is verified up and running
    STEP: scaling down the replication controller 06/08/23 16:03:06.299
    Jun  8 16:03:06.302: INFO: scanned /root for discovery docs: <nil>
    Jun  8 16:03:06.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jun  8 16:03:07.402: INFO: stderr: ""
    Jun  8 16:03:07.402: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/08/23 16:03:07.402
    Jun  8 16:03:07.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun  8 16:03:07.479: INFO: stderr: ""
    Jun  8 16:03:07.479: INFO: stdout: "update-demo-nautilus-ht455 "
    Jun  8 16:03:07.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-ht455 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  8 16:03:07.555: INFO: stderr: ""
    Jun  8 16:03:07.555: INFO: stdout: "true"
    Jun  8 16:03:07.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-ht455 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun  8 16:03:07.630: INFO: stderr: ""
    Jun  8 16:03:07.630: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun  8 16:03:07.630: INFO: validating pod update-demo-nautilus-ht455
    Jun  8 16:03:07.635: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun  8 16:03:07.635: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun  8 16:03:07.635: INFO: update-demo-nautilus-ht455 is verified up and running
    STEP: scaling up the replication controller 06/08/23 16:03:07.635
    Jun  8 16:03:07.637: INFO: scanned /root for discovery docs: <nil>
    Jun  8 16:03:07.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jun  8 16:03:08.740: INFO: stderr: ""
    Jun  8 16:03:08.740: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/08/23 16:03:08.74
    Jun  8 16:03:08.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun  8 16:03:08.822: INFO: stderr: ""
    Jun  8 16:03:08.822: INFO: stdout: "update-demo-nautilus-ht455 update-demo-nautilus-l7k6b "
    Jun  8 16:03:08.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-ht455 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  8 16:03:08.901: INFO: stderr: ""
    Jun  8 16:03:08.901: INFO: stdout: "true"
    Jun  8 16:03:08.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-ht455 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun  8 16:03:08.979: INFO: stderr: ""
    Jun  8 16:03:08.979: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun  8 16:03:08.979: INFO: validating pod update-demo-nautilus-ht455
    Jun  8 16:03:08.984: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun  8 16:03:08.984: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun  8 16:03:08.984: INFO: update-demo-nautilus-ht455 is verified up and running
    Jun  8 16:03:08.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-l7k6b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  8 16:03:09.064: INFO: stderr: ""
    Jun  8 16:03:09.064: INFO: stdout: ""
    Jun  8 16:03:09.064: INFO: update-demo-nautilus-l7k6b is created but not running
    Jun  8 16:03:14.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun  8 16:03:14.150: INFO: stderr: ""
    Jun  8 16:03:14.150: INFO: stdout: "update-demo-nautilus-ht455 update-demo-nautilus-l7k6b "
    Jun  8 16:03:14.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-ht455 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  8 16:03:14.232: INFO: stderr: ""
    Jun  8 16:03:14.232: INFO: stdout: "true"
    Jun  8 16:03:14.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-ht455 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun  8 16:03:14.309: INFO: stderr: ""
    Jun  8 16:03:14.309: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun  8 16:03:14.309: INFO: validating pod update-demo-nautilus-ht455
    Jun  8 16:03:14.314: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun  8 16:03:14.314: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun  8 16:03:14.314: INFO: update-demo-nautilus-ht455 is verified up and running
    Jun  8 16:03:14.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-l7k6b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  8 16:03:14.393: INFO: stderr: ""
    Jun  8 16:03:14.393: INFO: stdout: "true"
    Jun  8 16:03:14.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods update-demo-nautilus-l7k6b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun  8 16:03:14.473: INFO: stderr: ""
    Jun  8 16:03:14.473: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun  8 16:03:14.473: INFO: validating pod update-demo-nautilus-l7k6b
    Jun  8 16:03:14.479: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun  8 16:03:14.479: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun  8 16:03:14.479: INFO: update-demo-nautilus-l7k6b is verified up and running
    STEP: using delete to clean up resources 06/08/23 16:03:14.479
    Jun  8 16:03:14.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 delete --grace-period=0 --force -f -'
    Jun  8 16:03:14.560: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun  8 16:03:14.560: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jun  8 16:03:14.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get rc,svc -l name=update-demo --no-headers'
    Jun  8 16:03:14.656: INFO: stderr: "No resources found in kubectl-4438 namespace.\n"
    Jun  8 16:03:14.656: INFO: stdout: ""
    Jun  8 16:03:14.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-4438 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jun  8 16:03:14.763: INFO: stderr: ""
    Jun  8 16:03:14.763: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:03:14.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4438" for this suite. 06/08/23 16:03:14.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:03:14.776
Jun  8 16:03:14.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename deployment 06/08/23 16:03:14.777
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:14.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:14.801
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jun  8 16:03:14.805: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun  8 16:03:14.814: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun  8 16:03:19.818: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/08/23 16:03:19.818
Jun  8 16:03:19.818: INFO: Creating deployment "test-rolling-update-deployment"
Jun  8 16:03:19.825: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun  8 16:03:19.832: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun  8 16:03:21.841: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun  8 16:03:21.844: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun  8 16:03:21.854: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1895  1cca94d6-7f07-4573-a70a-db626fcb5a0e 42294 1 2023-06-08 16:03:19 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-06-08 16:03:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 16:03:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d79f18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-08 16:03:19 +0000 UTC,LastTransitionTime:2023-06-08 16:03:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-06-08 16:03:21 +0000 UTC,LastTransitionTime:2023-06-08 16:03:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  8 16:03:21.857: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1895  c1956ab8-1ab3-4f71-bd81-f4df738eb0a4 42284 1 2023-06-08 16:03:19 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 1cca94d6-7f07-4573-a70a-db626fcb5a0e 0xc0036df607 0xc0036df608}] [] [{kube-controller-manager Update apps/v1 2023-06-08 16:03:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1cca94d6-7f07-4573-a70a-db626fcb5a0e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 16:03:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0036df6b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  8 16:03:21.857: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun  8 16:03:21.858: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1895  10524950-2c67-452e-be10-76c159dea57b 42293 2 2023-06-08 16:03:14 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 1cca94d6-7f07-4573-a70a-db626fcb5a0e 0xc0036df4d7 0xc0036df4d8}] [] [{e2e.test Update apps/v1 2023-06-08 16:03:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 16:03:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1cca94d6-7f07-4573-a70a-db626fcb5a0e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-08 16:03:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0036df598 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  8 16:03:21.861: INFO: Pod "test-rolling-update-deployment-7549d9f46d-xllx7" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-xllx7 test-rolling-update-deployment-7549d9f46d- deployment-1895  8829a2b5-e2b8-4285-9a13-35c6e04d2308 42283 0 2023-06-08 16:03:19 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d c1956ab8-1ab3-4f71-bd81-f4df738eb0a4 0xc003711767 0xc003711768}] [] [{kube-controller-manager Update v1 2023-06-08 16:03:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1956ab8-1ab3-4f71-bd81-f4df738eb0a4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 16:03:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.215\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h7ccs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h7ccs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 16:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 16:03:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 16:03:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 16:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.215,PodIP:10.244.3.215,StartTime:2023-06-08 16:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 16:03:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://4c245679a0c40757e30a258e8cdd48b148571585fb68645715a759ccbb013824,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun  8 16:03:21.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1895" for this suite. 06/08/23 16:03:21.866
------------------------------
• [SLOW TEST] [7.097 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:03:14.776
    Jun  8 16:03:14.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename deployment 06/08/23 16:03:14.777
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:14.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:14.801
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jun  8 16:03:14.805: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jun  8 16:03:14.814: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun  8 16:03:19.818: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/08/23 16:03:19.818
    Jun  8 16:03:19.818: INFO: Creating deployment "test-rolling-update-deployment"
    Jun  8 16:03:19.825: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jun  8 16:03:19.832: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jun  8 16:03:21.841: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jun  8 16:03:21.844: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun  8 16:03:21.854: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1895  1cca94d6-7f07-4573-a70a-db626fcb5a0e 42294 1 2023-06-08 16:03:19 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-06-08 16:03:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 16:03:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d79f18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-08 16:03:19 +0000 UTC,LastTransitionTime:2023-06-08 16:03:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-06-08 16:03:21 +0000 UTC,LastTransitionTime:2023-06-08 16:03:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun  8 16:03:21.857: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1895  c1956ab8-1ab3-4f71-bd81-f4df738eb0a4 42284 1 2023-06-08 16:03:19 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 1cca94d6-7f07-4573-a70a-db626fcb5a0e 0xc0036df607 0xc0036df608}] [] [{kube-controller-manager Update apps/v1 2023-06-08 16:03:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1cca94d6-7f07-4573-a70a-db626fcb5a0e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 16:03:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0036df6b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun  8 16:03:21.857: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jun  8 16:03:21.858: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1895  10524950-2c67-452e-be10-76c159dea57b 42293 2 2023-06-08 16:03:14 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 1cca94d6-7f07-4573-a70a-db626fcb5a0e 0xc0036df4d7 0xc0036df4d8}] [] [{e2e.test Update apps/v1 2023-06-08 16:03:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 16:03:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1cca94d6-7f07-4573-a70a-db626fcb5a0e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-08 16:03:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0036df598 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun  8 16:03:21.861: INFO: Pod "test-rolling-update-deployment-7549d9f46d-xllx7" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-xllx7 test-rolling-update-deployment-7549d9f46d- deployment-1895  8829a2b5-e2b8-4285-9a13-35c6e04d2308 42283 0 2023-06-08 16:03:19 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d c1956ab8-1ab3-4f71-bd81-f4df738eb0a4 0xc003711767 0xc003711768}] [] [{kube-controller-manager Update v1 2023-06-08 16:03:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1956ab8-1ab3-4f71-bd81-f4df738eb0a4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 16:03:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.215\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h7ccs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h7ccs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 16:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 16:03:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 16:03:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 16:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.215,PodIP:10.244.3.215,StartTime:2023-06-08 16:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 16:03:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://4c245679a0c40757e30a258e8cdd48b148571585fb68645715a759ccbb013824,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:03:21.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1895" for this suite. 06/08/23 16:03:21.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:03:21.874
Jun  8 16:03:21.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename container-runtime 06/08/23 16:03:21.875
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:21.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:21.897
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 06/08/23 16:03:21.9
STEP: wait for the container to reach Succeeded 06/08/23 16:03:21.909
STEP: get the container status 06/08/23 16:03:25.931
STEP: the container should be terminated 06/08/23 16:03:25.934
STEP: the termination message should be set 06/08/23 16:03:25.935
Jun  8 16:03:25.935: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 06/08/23 16:03:25.935
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jun  8 16:03:25.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5399" for this suite. 06/08/23 16:03:25.956
------------------------------
• [4.088 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:03:21.874
    Jun  8 16:03:21.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename container-runtime 06/08/23 16:03:21.875
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:21.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:21.897
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 06/08/23 16:03:21.9
    STEP: wait for the container to reach Succeeded 06/08/23 16:03:21.909
    STEP: get the container status 06/08/23 16:03:25.931
    STEP: the container should be terminated 06/08/23 16:03:25.934
    STEP: the termination message should be set 06/08/23 16:03:25.935
    Jun  8 16:03:25.935: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 06/08/23 16:03:25.935
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:03:25.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5399" for this suite. 06/08/23 16:03:25.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:03:25.963
Jun  8 16:03:25.963: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename downward-api 06/08/23 16:03:25.964
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:25.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:25.984
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 06/08/23 16:03:25.987
Jun  8 16:03:25.995: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ff25ae55-c67d-41a6-afe5-d239064a4094" in namespace "downward-api-9807" to be "Succeeded or Failed"
Jun  8 16:03:25.998: INFO: Pod "downwardapi-volume-ff25ae55-c67d-41a6-afe5-d239064a4094": Phase="Pending", Reason="", readiness=false. Elapsed: 2.967396ms
Jun  8 16:03:28.001: INFO: Pod "downwardapi-volume-ff25ae55-c67d-41a6-afe5-d239064a4094": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006803646s
Jun  8 16:03:30.002: INFO: Pod "downwardapi-volume-ff25ae55-c67d-41a6-afe5-d239064a4094": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007728601s
STEP: Saw pod success 06/08/23 16:03:30.002
Jun  8 16:03:30.003: INFO: Pod "downwardapi-volume-ff25ae55-c67d-41a6-afe5-d239064a4094" satisfied condition "Succeeded or Failed"
Jun  8 16:03:30.006: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-ff25ae55-c67d-41a6-afe5-d239064a4094 container client-container: <nil>
STEP: delete the pod 06/08/23 16:03:30.013
Jun  8 16:03:30.024: INFO: Waiting for pod downwardapi-volume-ff25ae55-c67d-41a6-afe5-d239064a4094 to disappear
Jun  8 16:03:30.027: INFO: Pod downwardapi-volume-ff25ae55-c67d-41a6-afe5-d239064a4094 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  8 16:03:30.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9807" for this suite. 06/08/23 16:03:30.032
------------------------------
• [4.075 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:03:25.963
    Jun  8 16:03:25.963: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename downward-api 06/08/23 16:03:25.964
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:25.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:25.984
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 06/08/23 16:03:25.987
    Jun  8 16:03:25.995: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ff25ae55-c67d-41a6-afe5-d239064a4094" in namespace "downward-api-9807" to be "Succeeded or Failed"
    Jun  8 16:03:25.998: INFO: Pod "downwardapi-volume-ff25ae55-c67d-41a6-afe5-d239064a4094": Phase="Pending", Reason="", readiness=false. Elapsed: 2.967396ms
    Jun  8 16:03:28.001: INFO: Pod "downwardapi-volume-ff25ae55-c67d-41a6-afe5-d239064a4094": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006803646s
    Jun  8 16:03:30.002: INFO: Pod "downwardapi-volume-ff25ae55-c67d-41a6-afe5-d239064a4094": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007728601s
    STEP: Saw pod success 06/08/23 16:03:30.002
    Jun  8 16:03:30.003: INFO: Pod "downwardapi-volume-ff25ae55-c67d-41a6-afe5-d239064a4094" satisfied condition "Succeeded or Failed"
    Jun  8 16:03:30.006: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-ff25ae55-c67d-41a6-afe5-d239064a4094 container client-container: <nil>
    STEP: delete the pod 06/08/23 16:03:30.013
    Jun  8 16:03:30.024: INFO: Waiting for pod downwardapi-volume-ff25ae55-c67d-41a6-afe5-d239064a4094 to disappear
    Jun  8 16:03:30.027: INFO: Pod downwardapi-volume-ff25ae55-c67d-41a6-afe5-d239064a4094 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:03:30.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9807" for this suite. 06/08/23 16:03:30.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:03:30.038
Jun  8 16:03:30.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename containers 06/08/23 16:03:30.04
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:30.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:30.059
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 06/08/23 16:03:30.062
Jun  8 16:03:30.070: INFO: Waiting up to 5m0s for pod "client-containers-47d4bd69-11b0-4d1d-80c9-3e93e0308daf" in namespace "containers-6686" to be "Succeeded or Failed"
Jun  8 16:03:30.073: INFO: Pod "client-containers-47d4bd69-11b0-4d1d-80c9-3e93e0308daf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.283057ms
Jun  8 16:03:32.077: INFO: Pod "client-containers-47d4bd69-11b0-4d1d-80c9-3e93e0308daf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007252394s
Jun  8 16:03:34.078: INFO: Pod "client-containers-47d4bd69-11b0-4d1d-80c9-3e93e0308daf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007994252s
STEP: Saw pod success 06/08/23 16:03:34.078
Jun  8 16:03:34.078: INFO: Pod "client-containers-47d4bd69-11b0-4d1d-80c9-3e93e0308daf" satisfied condition "Succeeded or Failed"
Jun  8 16:03:34.081: INFO: Trying to get logs from node chl8tf-worker-001 pod client-containers-47d4bd69-11b0-4d1d-80c9-3e93e0308daf container agnhost-container: <nil>
STEP: delete the pod 06/08/23 16:03:34.088
Jun  8 16:03:34.100: INFO: Waiting for pod client-containers-47d4bd69-11b0-4d1d-80c9-3e93e0308daf to disappear
Jun  8 16:03:34.103: INFO: Pod client-containers-47d4bd69-11b0-4d1d-80c9-3e93e0308daf no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jun  8 16:03:34.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6686" for this suite. 06/08/23 16:03:34.108
------------------------------
• [4.076 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:03:30.038
    Jun  8 16:03:30.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename containers 06/08/23 16:03:30.04
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:30.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:30.059
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 06/08/23 16:03:30.062
    Jun  8 16:03:30.070: INFO: Waiting up to 5m0s for pod "client-containers-47d4bd69-11b0-4d1d-80c9-3e93e0308daf" in namespace "containers-6686" to be "Succeeded or Failed"
    Jun  8 16:03:30.073: INFO: Pod "client-containers-47d4bd69-11b0-4d1d-80c9-3e93e0308daf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.283057ms
    Jun  8 16:03:32.077: INFO: Pod "client-containers-47d4bd69-11b0-4d1d-80c9-3e93e0308daf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007252394s
    Jun  8 16:03:34.078: INFO: Pod "client-containers-47d4bd69-11b0-4d1d-80c9-3e93e0308daf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007994252s
    STEP: Saw pod success 06/08/23 16:03:34.078
    Jun  8 16:03:34.078: INFO: Pod "client-containers-47d4bd69-11b0-4d1d-80c9-3e93e0308daf" satisfied condition "Succeeded or Failed"
    Jun  8 16:03:34.081: INFO: Trying to get logs from node chl8tf-worker-001 pod client-containers-47d4bd69-11b0-4d1d-80c9-3e93e0308daf container agnhost-container: <nil>
    STEP: delete the pod 06/08/23 16:03:34.088
    Jun  8 16:03:34.100: INFO: Waiting for pod client-containers-47d4bd69-11b0-4d1d-80c9-3e93e0308daf to disappear
    Jun  8 16:03:34.103: INFO: Pod client-containers-47d4bd69-11b0-4d1d-80c9-3e93e0308daf no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:03:34.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6686" for this suite. 06/08/23 16:03:34.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:03:34.115
Jun  8 16:03:34.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename certificates 06/08/23 16:03:34.116
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:34.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:34.135
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 06/08/23 16:03:34.727
STEP: getting /apis/certificates.k8s.io 06/08/23 16:03:34.73
STEP: getting /apis/certificates.k8s.io/v1 06/08/23 16:03:34.731
STEP: creating 06/08/23 16:03:34.733
STEP: getting 06/08/23 16:03:34.749
STEP: listing 06/08/23 16:03:34.752
STEP: watching 06/08/23 16:03:34.755
Jun  8 16:03:34.755: INFO: starting watch
STEP: patching 06/08/23 16:03:34.756
STEP: updating 06/08/23 16:03:34.762
Jun  8 16:03:34.768: INFO: waiting for watch events with expected annotations
Jun  8 16:03:34.768: INFO: saw patched and updated annotations
STEP: getting /approval 06/08/23 16:03:34.768
STEP: patching /approval 06/08/23 16:03:34.772
STEP: updating /approval 06/08/23 16:03:34.778
STEP: getting /status 06/08/23 16:03:34.784
STEP: patching /status 06/08/23 16:03:34.787
STEP: updating /status 06/08/23 16:03:34.795
STEP: deleting 06/08/23 16:03:34.802
STEP: deleting a collection 06/08/23 16:03:34.814
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:03:34.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-6666" for this suite. 06/08/23 16:03:34.833
------------------------------
• [0.724 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:03:34.115
    Jun  8 16:03:34.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename certificates 06/08/23 16:03:34.116
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:34.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:34.135
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 06/08/23 16:03:34.727
    STEP: getting /apis/certificates.k8s.io 06/08/23 16:03:34.73
    STEP: getting /apis/certificates.k8s.io/v1 06/08/23 16:03:34.731
    STEP: creating 06/08/23 16:03:34.733
    STEP: getting 06/08/23 16:03:34.749
    STEP: listing 06/08/23 16:03:34.752
    STEP: watching 06/08/23 16:03:34.755
    Jun  8 16:03:34.755: INFO: starting watch
    STEP: patching 06/08/23 16:03:34.756
    STEP: updating 06/08/23 16:03:34.762
    Jun  8 16:03:34.768: INFO: waiting for watch events with expected annotations
    Jun  8 16:03:34.768: INFO: saw patched and updated annotations
    STEP: getting /approval 06/08/23 16:03:34.768
    STEP: patching /approval 06/08/23 16:03:34.772
    STEP: updating /approval 06/08/23 16:03:34.778
    STEP: getting /status 06/08/23 16:03:34.784
    STEP: patching /status 06/08/23 16:03:34.787
    STEP: updating /status 06/08/23 16:03:34.795
    STEP: deleting 06/08/23 16:03:34.802
    STEP: deleting a collection 06/08/23 16:03:34.814
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:03:34.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-6666" for this suite. 06/08/23 16:03:34.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:03:34.84
Jun  8 16:03:34.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename endpointslice 06/08/23 16:03:34.841
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:34.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:34.861
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Jun  8 16:03:34.878: INFO: Endpoints addresses: [100.100.236.41 100.100.237.165 100.100.237.235] , ports: [6444]
Jun  8 16:03:34.878: INFO: EndpointSlices addresses: [100.100.236.41 100.100.237.165 100.100.237.235] , ports: [6444]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jun  8 16:03:34.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-6826" for this suite. 06/08/23 16:03:34.883
------------------------------
• [0.050 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:03:34.84
    Jun  8 16:03:34.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename endpointslice 06/08/23 16:03:34.841
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:34.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:34.861
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Jun  8 16:03:34.878: INFO: Endpoints addresses: [100.100.236.41 100.100.237.165 100.100.237.235] , ports: [6444]
    Jun  8 16:03:34.878: INFO: EndpointSlices addresses: [100.100.236.41 100.100.237.165 100.100.237.235] , ports: [6444]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:03:34.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-6826" for this suite. 06/08/23 16:03:34.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:03:34.89
Jun  8 16:03:34.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename custom-resource-definition 06/08/23 16:03:34.892
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:34.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:34.911
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 06/08/23 16:03:34.914
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 06/08/23 16:03:34.916
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 06/08/23 16:03:34.916
STEP: fetching the /apis/apiextensions.k8s.io discovery document 06/08/23 16:03:34.916
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 06/08/23 16:03:34.917
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 06/08/23 16:03:34.917
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 06/08/23 16:03:34.919
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:03:34.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9668" for this suite. 06/08/23 16:03:34.923
------------------------------
• [0.038 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:03:34.89
    Jun  8 16:03:34.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename custom-resource-definition 06/08/23 16:03:34.892
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:34.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:34.911
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 06/08/23 16:03:34.914
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 06/08/23 16:03:34.916
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 06/08/23 16:03:34.916
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 06/08/23 16:03:34.916
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 06/08/23 16:03:34.917
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 06/08/23 16:03:34.917
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 06/08/23 16:03:34.919
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:03:34.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9668" for this suite. 06/08/23 16:03:34.923
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:03:34.929
Jun  8 16:03:34.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename server-version 06/08/23 16:03:34.93
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:34.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:34.95
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 06/08/23 16:03:34.953
STEP: Confirm major version 06/08/23 16:03:34.954
Jun  8 16:03:34.954: INFO: Major version: 1
STEP: Confirm minor version 06/08/23 16:03:34.954
Jun  8 16:03:34.954: INFO: cleanMinorVersion: 26
Jun  8 16:03:34.954: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Jun  8 16:03:34.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-4564" for this suite. 06/08/23 16:03:34.959
------------------------------
• [0.036 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:03:34.929
    Jun  8 16:03:34.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename server-version 06/08/23 16:03:34.93
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:34.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:34.95
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 06/08/23 16:03:34.953
    STEP: Confirm major version 06/08/23 16:03:34.954
    Jun  8 16:03:34.954: INFO: Major version: 1
    STEP: Confirm minor version 06/08/23 16:03:34.954
    Jun  8 16:03:34.954: INFO: cleanMinorVersion: 26
    Jun  8 16:03:34.954: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:03:34.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-4564" for this suite. 06/08/23 16:03:34.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:03:34.967
Jun  8 16:03:34.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename deployment 06/08/23 16:03:34.968
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:34.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:34.986
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jun  8 16:03:34.998: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun  8 16:03:40.002: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/08/23 16:03:40.002
Jun  8 16:03:40.002: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 06/08/23 16:03:40.014
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun  8 16:03:40.027: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7894  dee9585f-dbbb-4f45-b160-4efae87d7f64 42515 1 2023-06-08 16:03:40 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-06-08 16:03:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003dc3d98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jun  8 16:03:40.031: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun  8 16:03:40.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7894" for this suite. 06/08/23 16:03:40.043
------------------------------
• [SLOW TEST] [5.084 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:03:34.967
    Jun  8 16:03:34.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename deployment 06/08/23 16:03:34.968
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:34.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:34.986
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jun  8 16:03:34.998: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jun  8 16:03:40.002: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/08/23 16:03:40.002
    Jun  8 16:03:40.002: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 06/08/23 16:03:40.014
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun  8 16:03:40.027: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7894  dee9585f-dbbb-4f45-b160-4efae87d7f64 42515 1 2023-06-08 16:03:40 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-06-08 16:03:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003dc3d98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jun  8 16:03:40.031: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:03:40.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7894" for this suite. 06/08/23 16:03:40.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:03:40.052
Jun  8 16:03:40.052: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename secrets 06/08/23 16:03:40.053
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:40.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:40.099
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-aca539d2-474a-4ca3-9cfe-4e1b190b5b0c 06/08/23 16:03:40.103
STEP: Creating a pod to test consume secrets 06/08/23 16:03:40.11
Jun  8 16:03:40.119: INFO: Waiting up to 5m0s for pod "pod-secrets-c2b3c802-ea4a-4969-b466-8a6e49deae28" in namespace "secrets-7831" to be "Succeeded or Failed"
Jun  8 16:03:40.123: INFO: Pod "pod-secrets-c2b3c802-ea4a-4969-b466-8a6e49deae28": Phase="Pending", Reason="", readiness=false. Elapsed: 4.308003ms
Jun  8 16:03:42.129: INFO: Pod "pod-secrets-c2b3c802-ea4a-4969-b466-8a6e49deae28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009844044s
Jun  8 16:03:44.129: INFO: Pod "pod-secrets-c2b3c802-ea4a-4969-b466-8a6e49deae28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009992629s
STEP: Saw pod success 06/08/23 16:03:44.129
Jun  8 16:03:44.129: INFO: Pod "pod-secrets-c2b3c802-ea4a-4969-b466-8a6e49deae28" satisfied condition "Succeeded or Failed"
Jun  8 16:03:44.133: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-secrets-c2b3c802-ea4a-4969-b466-8a6e49deae28 container secret-volume-test: <nil>
STEP: delete the pod 06/08/23 16:03:44.141
Jun  8 16:03:44.159: INFO: Waiting for pod pod-secrets-c2b3c802-ea4a-4969-b466-8a6e49deae28 to disappear
Jun  8 16:03:44.162: INFO: Pod pod-secrets-c2b3c802-ea4a-4969-b466-8a6e49deae28 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  8 16:03:44.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7831" for this suite. 06/08/23 16:03:44.168
------------------------------
• [4.123 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:03:40.052
    Jun  8 16:03:40.052: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename secrets 06/08/23 16:03:40.053
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:40.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:40.099
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-aca539d2-474a-4ca3-9cfe-4e1b190b5b0c 06/08/23 16:03:40.103
    STEP: Creating a pod to test consume secrets 06/08/23 16:03:40.11
    Jun  8 16:03:40.119: INFO: Waiting up to 5m0s for pod "pod-secrets-c2b3c802-ea4a-4969-b466-8a6e49deae28" in namespace "secrets-7831" to be "Succeeded or Failed"
    Jun  8 16:03:40.123: INFO: Pod "pod-secrets-c2b3c802-ea4a-4969-b466-8a6e49deae28": Phase="Pending", Reason="", readiness=false. Elapsed: 4.308003ms
    Jun  8 16:03:42.129: INFO: Pod "pod-secrets-c2b3c802-ea4a-4969-b466-8a6e49deae28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009844044s
    Jun  8 16:03:44.129: INFO: Pod "pod-secrets-c2b3c802-ea4a-4969-b466-8a6e49deae28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009992629s
    STEP: Saw pod success 06/08/23 16:03:44.129
    Jun  8 16:03:44.129: INFO: Pod "pod-secrets-c2b3c802-ea4a-4969-b466-8a6e49deae28" satisfied condition "Succeeded or Failed"
    Jun  8 16:03:44.133: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-secrets-c2b3c802-ea4a-4969-b466-8a6e49deae28 container secret-volume-test: <nil>
    STEP: delete the pod 06/08/23 16:03:44.141
    Jun  8 16:03:44.159: INFO: Waiting for pod pod-secrets-c2b3c802-ea4a-4969-b466-8a6e49deae28 to disappear
    Jun  8 16:03:44.162: INFO: Pod pod-secrets-c2b3c802-ea4a-4969-b466-8a6e49deae28 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:03:44.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7831" for this suite. 06/08/23 16:03:44.168
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:03:44.176
Jun  8 16:03:44.176: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename container-probe 06/08/23 16:03:44.178
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:44.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:44.203
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-df04dd2a-d1cb-40c5-82ee-d3e83bc7171a in namespace container-probe-1226 06/08/23 16:03:44.207
Jun  8 16:03:44.216: INFO: Waiting up to 5m0s for pod "liveness-df04dd2a-d1cb-40c5-82ee-d3e83bc7171a" in namespace "container-probe-1226" to be "not pending"
Jun  8 16:03:44.219: INFO: Pod "liveness-df04dd2a-d1cb-40c5-82ee-d3e83bc7171a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.502774ms
Jun  8 16:03:46.224: INFO: Pod "liveness-df04dd2a-d1cb-40c5-82ee-d3e83bc7171a": Phase="Running", Reason="", readiness=true. Elapsed: 2.008572171s
Jun  8 16:03:46.224: INFO: Pod "liveness-df04dd2a-d1cb-40c5-82ee-d3e83bc7171a" satisfied condition "not pending"
Jun  8 16:03:46.224: INFO: Started pod liveness-df04dd2a-d1cb-40c5-82ee-d3e83bc7171a in namespace container-probe-1226
STEP: checking the pod's current state and verifying that restartCount is present 06/08/23 16:03:46.224
Jun  8 16:03:46.228: INFO: Initial restart count of pod liveness-df04dd2a-d1cb-40c5-82ee-d3e83bc7171a is 0
Jun  8 16:04:06.280: INFO: Restart count of pod container-probe-1226/liveness-df04dd2a-d1cb-40c5-82ee-d3e83bc7171a is now 1 (20.052629286s elapsed)
STEP: deleting the pod 06/08/23 16:04:06.28
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun  8 16:04:06.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1226" for this suite. 06/08/23 16:04:06.3
------------------------------
• [SLOW TEST] [22.133 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:03:44.176
    Jun  8 16:03:44.176: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename container-probe 06/08/23 16:03:44.178
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:03:44.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:03:44.203
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-df04dd2a-d1cb-40c5-82ee-d3e83bc7171a in namespace container-probe-1226 06/08/23 16:03:44.207
    Jun  8 16:03:44.216: INFO: Waiting up to 5m0s for pod "liveness-df04dd2a-d1cb-40c5-82ee-d3e83bc7171a" in namespace "container-probe-1226" to be "not pending"
    Jun  8 16:03:44.219: INFO: Pod "liveness-df04dd2a-d1cb-40c5-82ee-d3e83bc7171a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.502774ms
    Jun  8 16:03:46.224: INFO: Pod "liveness-df04dd2a-d1cb-40c5-82ee-d3e83bc7171a": Phase="Running", Reason="", readiness=true. Elapsed: 2.008572171s
    Jun  8 16:03:46.224: INFO: Pod "liveness-df04dd2a-d1cb-40c5-82ee-d3e83bc7171a" satisfied condition "not pending"
    Jun  8 16:03:46.224: INFO: Started pod liveness-df04dd2a-d1cb-40c5-82ee-d3e83bc7171a in namespace container-probe-1226
    STEP: checking the pod's current state and verifying that restartCount is present 06/08/23 16:03:46.224
    Jun  8 16:03:46.228: INFO: Initial restart count of pod liveness-df04dd2a-d1cb-40c5-82ee-d3e83bc7171a is 0
    Jun  8 16:04:06.280: INFO: Restart count of pod container-probe-1226/liveness-df04dd2a-d1cb-40c5-82ee-d3e83bc7171a is now 1 (20.052629286s elapsed)
    STEP: deleting the pod 06/08/23 16:04:06.28
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:04:06.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1226" for this suite. 06/08/23 16:04:06.3
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:04:06.31
Jun  8 16:04:06.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename replicaset 06/08/23 16:04:06.311
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:04:06.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:04:06.335
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 06/08/23 16:04:06.338
STEP: Verify that the required pods have come up 06/08/23 16:04:06.343
Jun  8 16:04:06.346: INFO: Pod name sample-pod: Found 0 pods out of 3
Jun  8 16:04:11.352: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 06/08/23 16:04:11.352
Jun  8 16:04:11.355: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 06/08/23 16:04:11.355
STEP: DeleteCollection of the ReplicaSets 06/08/23 16:04:11.359
STEP: After DeleteCollection verify that ReplicaSets have been deleted 06/08/23 16:04:11.367
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jun  8 16:04:11.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-776" for this suite. 06/08/23 16:04:11.376
------------------------------
• [SLOW TEST] [5.073 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:04:06.31
    Jun  8 16:04:06.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename replicaset 06/08/23 16:04:06.311
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:04:06.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:04:06.335
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 06/08/23 16:04:06.338
    STEP: Verify that the required pods have come up 06/08/23 16:04:06.343
    Jun  8 16:04:06.346: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jun  8 16:04:11.352: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 06/08/23 16:04:11.352
    Jun  8 16:04:11.355: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 06/08/23 16:04:11.355
    STEP: DeleteCollection of the ReplicaSets 06/08/23 16:04:11.359
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 06/08/23 16:04:11.367
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:04:11.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-776" for this suite. 06/08/23 16:04:11.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:04:11.383
Jun  8 16:04:11.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename pods 06/08/23 16:04:11.384
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:04:11.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:04:11.419
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Jun  8 16:04:11.432: INFO: Waiting up to 5m0s for pod "server-envvars-f1e239c3-04f9-49a6-a40d-ed21f9f6ffd1" in namespace "pods-7124" to be "running and ready"
Jun  8 16:04:11.435: INFO: Pod "server-envvars-f1e239c3-04f9-49a6-a40d-ed21f9f6ffd1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.510091ms
Jun  8 16:04:11.435: INFO: The phase of Pod server-envvars-f1e239c3-04f9-49a6-a40d-ed21f9f6ffd1 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 16:04:13.440: INFO: Pod "server-envvars-f1e239c3-04f9-49a6-a40d-ed21f9f6ffd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007959837s
Jun  8 16:04:13.440: INFO: The phase of Pod server-envvars-f1e239c3-04f9-49a6-a40d-ed21f9f6ffd1 is Running (Ready = true)
Jun  8 16:04:13.440: INFO: Pod "server-envvars-f1e239c3-04f9-49a6-a40d-ed21f9f6ffd1" satisfied condition "running and ready"
Jun  8 16:04:13.465: INFO: Waiting up to 5m0s for pod "client-envvars-d75abc1a-c952-4d5e-ae3a-9ac99e558be8" in namespace "pods-7124" to be "Succeeded or Failed"
Jun  8 16:04:13.469: INFO: Pod "client-envvars-d75abc1a-c952-4d5e-ae3a-9ac99e558be8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.924357ms
Jun  8 16:04:15.474: INFO: Pod "client-envvars-d75abc1a-c952-4d5e-ae3a-9ac99e558be8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008291208s
Jun  8 16:04:17.474: INFO: Pod "client-envvars-d75abc1a-c952-4d5e-ae3a-9ac99e558be8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008103717s
STEP: Saw pod success 06/08/23 16:04:17.474
Jun  8 16:04:17.474: INFO: Pod "client-envvars-d75abc1a-c952-4d5e-ae3a-9ac99e558be8" satisfied condition "Succeeded or Failed"
Jun  8 16:04:17.478: INFO: Trying to get logs from node chl8tf-worker-001 pod client-envvars-d75abc1a-c952-4d5e-ae3a-9ac99e558be8 container env3cont: <nil>
STEP: delete the pod 06/08/23 16:04:17.485
Jun  8 16:04:17.496: INFO: Waiting for pod client-envvars-d75abc1a-c952-4d5e-ae3a-9ac99e558be8 to disappear
Jun  8 16:04:17.499: INFO: Pod client-envvars-d75abc1a-c952-4d5e-ae3a-9ac99e558be8 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun  8 16:04:17.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7124" for this suite. 06/08/23 16:04:17.504
------------------------------
• [SLOW TEST] [6.126 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:04:11.383
    Jun  8 16:04:11.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename pods 06/08/23 16:04:11.384
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:04:11.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:04:11.419
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Jun  8 16:04:11.432: INFO: Waiting up to 5m0s for pod "server-envvars-f1e239c3-04f9-49a6-a40d-ed21f9f6ffd1" in namespace "pods-7124" to be "running and ready"
    Jun  8 16:04:11.435: INFO: Pod "server-envvars-f1e239c3-04f9-49a6-a40d-ed21f9f6ffd1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.510091ms
    Jun  8 16:04:11.435: INFO: The phase of Pod server-envvars-f1e239c3-04f9-49a6-a40d-ed21f9f6ffd1 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 16:04:13.440: INFO: Pod "server-envvars-f1e239c3-04f9-49a6-a40d-ed21f9f6ffd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007959837s
    Jun  8 16:04:13.440: INFO: The phase of Pod server-envvars-f1e239c3-04f9-49a6-a40d-ed21f9f6ffd1 is Running (Ready = true)
    Jun  8 16:04:13.440: INFO: Pod "server-envvars-f1e239c3-04f9-49a6-a40d-ed21f9f6ffd1" satisfied condition "running and ready"
    Jun  8 16:04:13.465: INFO: Waiting up to 5m0s for pod "client-envvars-d75abc1a-c952-4d5e-ae3a-9ac99e558be8" in namespace "pods-7124" to be "Succeeded or Failed"
    Jun  8 16:04:13.469: INFO: Pod "client-envvars-d75abc1a-c952-4d5e-ae3a-9ac99e558be8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.924357ms
    Jun  8 16:04:15.474: INFO: Pod "client-envvars-d75abc1a-c952-4d5e-ae3a-9ac99e558be8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008291208s
    Jun  8 16:04:17.474: INFO: Pod "client-envvars-d75abc1a-c952-4d5e-ae3a-9ac99e558be8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008103717s
    STEP: Saw pod success 06/08/23 16:04:17.474
    Jun  8 16:04:17.474: INFO: Pod "client-envvars-d75abc1a-c952-4d5e-ae3a-9ac99e558be8" satisfied condition "Succeeded or Failed"
    Jun  8 16:04:17.478: INFO: Trying to get logs from node chl8tf-worker-001 pod client-envvars-d75abc1a-c952-4d5e-ae3a-9ac99e558be8 container env3cont: <nil>
    STEP: delete the pod 06/08/23 16:04:17.485
    Jun  8 16:04:17.496: INFO: Waiting for pod client-envvars-d75abc1a-c952-4d5e-ae3a-9ac99e558be8 to disappear
    Jun  8 16:04:17.499: INFO: Pod client-envvars-d75abc1a-c952-4d5e-ae3a-9ac99e558be8 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:04:17.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7124" for this suite. 06/08/23 16:04:17.504
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:04:17.51
Jun  8 16:04:17.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename webhook 06/08/23 16:04:17.511
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:04:17.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:04:17.532
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/08/23 16:04:17.547
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 16:04:18.07
STEP: Deploying the webhook pod 06/08/23 16:04:18.079
STEP: Wait for the deployment to be ready 06/08/23 16:04:18.091
Jun  8 16:04:18.098: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 06/08/23 16:04:20.109
STEP: Verifying the service has paired with the endpoint 06/08/23 16:04:20.124
Jun  8 16:04:21.125: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 06/08/23 16:04:21.129
STEP: Creating a configMap that does not comply to the validation webhook rules 06/08/23 16:04:21.149
STEP: Updating a validating webhook configuration's rules to not include the create operation 06/08/23 16:04:21.157
STEP: Creating a configMap that does not comply to the validation webhook rules 06/08/23 16:04:21.168
STEP: Patching a validating webhook configuration's rules to include the create operation 06/08/23 16:04:21.179
STEP: Creating a configMap that does not comply to the validation webhook rules 06/08/23 16:04:21.188
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:04:21.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8640" for this suite. 06/08/23 16:04:21.246
STEP: Destroying namespace "webhook-8640-markers" for this suite. 06/08/23 16:04:21.252
------------------------------
• [3.752 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:04:17.51
    Jun  8 16:04:17.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename webhook 06/08/23 16:04:17.511
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:04:17.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:04:17.532
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/08/23 16:04:17.547
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 16:04:18.07
    STEP: Deploying the webhook pod 06/08/23 16:04:18.079
    STEP: Wait for the deployment to be ready 06/08/23 16:04:18.091
    Jun  8 16:04:18.098: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 06/08/23 16:04:20.109
    STEP: Verifying the service has paired with the endpoint 06/08/23 16:04:20.124
    Jun  8 16:04:21.125: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 06/08/23 16:04:21.129
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/08/23 16:04:21.149
    STEP: Updating a validating webhook configuration's rules to not include the create operation 06/08/23 16:04:21.157
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/08/23 16:04:21.168
    STEP: Patching a validating webhook configuration's rules to include the create operation 06/08/23 16:04:21.179
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/08/23 16:04:21.188
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:04:21.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8640" for this suite. 06/08/23 16:04:21.246
    STEP: Destroying namespace "webhook-8640-markers" for this suite. 06/08/23 16:04:21.252
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:04:21.265
Jun  8 16:04:21.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename services 06/08/23 16:04:21.266
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:04:21.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:04:21.289
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-5460 06/08/23 16:04:21.294
STEP: creating service affinity-clusterip-transition in namespace services-5460 06/08/23 16:04:21.294
STEP: creating replication controller affinity-clusterip-transition in namespace services-5460 06/08/23 16:04:21.311
I0608 16:04:21.318183      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5460, replica count: 3
I0608 16:04:24.369848      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  8 16:04:24.378: INFO: Creating new exec pod
Jun  8 16:04:24.388: INFO: Waiting up to 5m0s for pod "execpod-affinitytzwx9" in namespace "services-5460" to be "running"
Jun  8 16:04:24.392: INFO: Pod "execpod-affinitytzwx9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.119796ms
Jun  8 16:04:26.396: INFO: Pod "execpod-affinitytzwx9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008118325s
Jun  8 16:04:26.397: INFO: Pod "execpod-affinitytzwx9" satisfied condition "running"
Jun  8 16:04:27.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-5460 exec execpod-affinitytzwx9 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Jun  8 16:04:27.572: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jun  8 16:04:27.572: INFO: stdout: ""
Jun  8 16:04:27.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-5460 exec execpod-affinitytzwx9 -- /bin/sh -x -c nc -v -z -w 2 10.98.190.203 80'
Jun  8 16:04:27.743: INFO: stderr: "+ nc -v -z -w 2 10.98.190.203 80\nConnection to 10.98.190.203 80 port [tcp/http] succeeded!\n"
Jun  8 16:04:27.743: INFO: stdout: ""
Jun  8 16:04:27.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-5460 exec execpod-affinitytzwx9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.98.190.203:80/ ; done'
Jun  8 16:04:28.144: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n"
Jun  8 16:04:28.144: INFO: stdout: "\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck"
Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:58.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-5460 exec execpod-affinitytzwx9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.98.190.203:80/ ; done'
Jun  8 16:04:58.429: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n"
Jun  8 16:04:58.429: INFO: stdout: "\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-xk8xw\naffinity-clusterip-transition-xk8xw\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-xk8xw\naffinity-clusterip-transition-xk8xw\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-xk8xw\naffinity-clusterip-transition-xk8xw\naffinity-clusterip-transition-xk8xw\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-xk8xw"
Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-xk8xw
Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-xk8xw
Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-xk8xw
Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-xk8xw
Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-xk8xw
Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-xk8xw
Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-xk8xw
Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:04:58.430: INFO: Received response from host: affinity-clusterip-transition-xk8xw
Jun  8 16:04:58.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-5460 exec execpod-affinitytzwx9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.98.190.203:80/ ; done'
Jun  8 16:04:58.815: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n"
Jun  8 16:04:58.815: INFO: stdout: "\naffinity-clusterip-transition-xk8xw\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-xk8xw\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-xk8xw\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck"
Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-xk8xw
Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-xk8xw
Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-xk8xw
Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-ccwck
Jun  8 16:05:28.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-5460 exec execpod-affinitytzwx9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.98.190.203:80/ ; done'
Jun  8 16:05:29.073: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n"
Jun  8 16:05:29.073: INFO: stdout: "\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k"
Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
Jun  8 16:05:29.074: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5460, will wait for the garbage collector to delete the pods 06/08/23 16:05:29.088
Jun  8 16:05:29.151: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.711633ms
Jun  8 16:05:29.255: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 103.866215ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  8 16:05:31.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5460" for this suite. 06/08/23 16:05:31.289
------------------------------
• [SLOW TEST] [70.032 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:04:21.265
    Jun  8 16:04:21.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename services 06/08/23 16:04:21.266
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:04:21.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:04:21.289
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-5460 06/08/23 16:04:21.294
    STEP: creating service affinity-clusterip-transition in namespace services-5460 06/08/23 16:04:21.294
    STEP: creating replication controller affinity-clusterip-transition in namespace services-5460 06/08/23 16:04:21.311
    I0608 16:04:21.318183      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5460, replica count: 3
    I0608 16:04:24.369848      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun  8 16:04:24.378: INFO: Creating new exec pod
    Jun  8 16:04:24.388: INFO: Waiting up to 5m0s for pod "execpod-affinitytzwx9" in namespace "services-5460" to be "running"
    Jun  8 16:04:24.392: INFO: Pod "execpod-affinitytzwx9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.119796ms
    Jun  8 16:04:26.396: INFO: Pod "execpod-affinitytzwx9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008118325s
    Jun  8 16:04:26.397: INFO: Pod "execpod-affinitytzwx9" satisfied condition "running"
    Jun  8 16:04:27.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-5460 exec execpod-affinitytzwx9 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Jun  8 16:04:27.572: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jun  8 16:04:27.572: INFO: stdout: ""
    Jun  8 16:04:27.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-5460 exec execpod-affinitytzwx9 -- /bin/sh -x -c nc -v -z -w 2 10.98.190.203 80'
    Jun  8 16:04:27.743: INFO: stderr: "+ nc -v -z -w 2 10.98.190.203 80\nConnection to 10.98.190.203 80 port [tcp/http] succeeded!\n"
    Jun  8 16:04:27.743: INFO: stdout: ""
    Jun  8 16:04:27.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-5460 exec execpod-affinitytzwx9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.98.190.203:80/ ; done'
    Jun  8 16:04:28.144: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n"
    Jun  8 16:04:28.144: INFO: stdout: "\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck"
    Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:28.144: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:58.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-5460 exec execpod-affinitytzwx9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.98.190.203:80/ ; done'
    Jun  8 16:04:58.429: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n"
    Jun  8 16:04:58.429: INFO: stdout: "\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-xk8xw\naffinity-clusterip-transition-xk8xw\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-xk8xw\naffinity-clusterip-transition-xk8xw\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-xk8xw\naffinity-clusterip-transition-xk8xw\naffinity-clusterip-transition-xk8xw\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-xk8xw"
    Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-xk8xw
    Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-xk8xw
    Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-xk8xw
    Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-xk8xw
    Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-xk8xw
    Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-xk8xw
    Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-xk8xw
    Jun  8 16:04:58.429: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:04:58.430: INFO: Received response from host: affinity-clusterip-transition-xk8xw
    Jun  8 16:04:58.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-5460 exec execpod-affinitytzwx9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.98.190.203:80/ ; done'
    Jun  8 16:04:58.815: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n"
    Jun  8 16:04:58.815: INFO: stdout: "\naffinity-clusterip-transition-xk8xw\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-xk8xw\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-xk8xw\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck\naffinity-clusterip-transition-ccwck"
    Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-xk8xw
    Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-xk8xw
    Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-xk8xw
    Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:04:58.815: INFO: Received response from host: affinity-clusterip-transition-ccwck
    Jun  8 16:05:28.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-5460 exec execpod-affinitytzwx9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.98.190.203:80/ ; done'
    Jun  8 16:05:29.073: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.190.203:80/\n"
    Jun  8 16:05:29.073: INFO: stdout: "\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k\naffinity-clusterip-transition-j9m9k"
    Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:05:29.073: INFO: Received response from host: affinity-clusterip-transition-j9m9k
    Jun  8 16:05:29.074: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5460, will wait for the garbage collector to delete the pods 06/08/23 16:05:29.088
    Jun  8 16:05:29.151: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.711633ms
    Jun  8 16:05:29.255: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 103.866215ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:05:31.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5460" for this suite. 06/08/23 16:05:31.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:05:31.299
Jun  8 16:05:31.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename downward-api 06/08/23 16:05:31.301
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:05:31.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:05:31.329
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 06/08/23 16:05:31.334
Jun  8 16:05:31.345: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1dc1d00a-95e7-4d7e-a867-cfecaa415817" in namespace "downward-api-8476" to be "Succeeded or Failed"
Jun  8 16:05:31.350: INFO: Pod "downwardapi-volume-1dc1d00a-95e7-4d7e-a867-cfecaa415817": Phase="Pending", Reason="", readiness=false. Elapsed: 4.631684ms
Jun  8 16:05:33.355: INFO: Pod "downwardapi-volume-1dc1d00a-95e7-4d7e-a867-cfecaa415817": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009425765s
Jun  8 16:05:35.355: INFO: Pod "downwardapi-volume-1dc1d00a-95e7-4d7e-a867-cfecaa415817": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009353314s
STEP: Saw pod success 06/08/23 16:05:35.355
Jun  8 16:05:35.355: INFO: Pod "downwardapi-volume-1dc1d00a-95e7-4d7e-a867-cfecaa415817" satisfied condition "Succeeded or Failed"
Jun  8 16:05:35.358: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-1dc1d00a-95e7-4d7e-a867-cfecaa415817 container client-container: <nil>
STEP: delete the pod 06/08/23 16:05:35.365
Jun  8 16:05:35.378: INFO: Waiting for pod downwardapi-volume-1dc1d00a-95e7-4d7e-a867-cfecaa415817 to disappear
Jun  8 16:05:35.381: INFO: Pod downwardapi-volume-1dc1d00a-95e7-4d7e-a867-cfecaa415817 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  8 16:05:35.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8476" for this suite. 06/08/23 16:05:35.386
------------------------------
• [4.094 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:05:31.299
    Jun  8 16:05:31.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename downward-api 06/08/23 16:05:31.301
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:05:31.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:05:31.329
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 06/08/23 16:05:31.334
    Jun  8 16:05:31.345: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1dc1d00a-95e7-4d7e-a867-cfecaa415817" in namespace "downward-api-8476" to be "Succeeded or Failed"
    Jun  8 16:05:31.350: INFO: Pod "downwardapi-volume-1dc1d00a-95e7-4d7e-a867-cfecaa415817": Phase="Pending", Reason="", readiness=false. Elapsed: 4.631684ms
    Jun  8 16:05:33.355: INFO: Pod "downwardapi-volume-1dc1d00a-95e7-4d7e-a867-cfecaa415817": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009425765s
    Jun  8 16:05:35.355: INFO: Pod "downwardapi-volume-1dc1d00a-95e7-4d7e-a867-cfecaa415817": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009353314s
    STEP: Saw pod success 06/08/23 16:05:35.355
    Jun  8 16:05:35.355: INFO: Pod "downwardapi-volume-1dc1d00a-95e7-4d7e-a867-cfecaa415817" satisfied condition "Succeeded or Failed"
    Jun  8 16:05:35.358: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-1dc1d00a-95e7-4d7e-a867-cfecaa415817 container client-container: <nil>
    STEP: delete the pod 06/08/23 16:05:35.365
    Jun  8 16:05:35.378: INFO: Waiting for pod downwardapi-volume-1dc1d00a-95e7-4d7e-a867-cfecaa415817 to disappear
    Jun  8 16:05:35.381: INFO: Pod downwardapi-volume-1dc1d00a-95e7-4d7e-a867-cfecaa415817 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:05:35.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8476" for this suite. 06/08/23 16:05:35.386
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:05:35.395
Jun  8 16:05:35.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename sched-pred 06/08/23 16:05:35.396
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:05:35.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:05:35.416
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jun  8 16:05:35.419: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  8 16:05:35.429: INFO: Waiting for terminating namespaces to be deleted...
Jun  8 16:05:35.432: INFO: 
Logging pods the apiserver thinks is on node chl8tf-control-plane-001 before test
Jun  8 16:05:35.441: INFO: csi-oci-node-5p7f5 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
Jun  8 16:05:35.441: INFO: 	Container csi-node-registrar ready: true, restart count 0
Jun  8 16:05:35.441: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
Jun  8 16:05:35.441: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
Jun  8 16:05:35.441: INFO: etcd-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:04 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.442: INFO: 	Container etcd ready: true, restart count 0
Jun  8 16:05:35.442: INFO: kube-apiserver-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:21 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.442: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun  8 16:05:35.442: INFO: kube-controller-manager-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:21 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.442: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun  8 16:05:35.442: INFO: kube-flannel-ds-d5bvw from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.442: INFO: 	Container kube-flannel ready: true, restart count 0
Jun  8 16:05:35.442: INFO: kube-proxy-j2p7z from kube-system started at 2023-06-08 13:31:27 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.442: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  8 16:05:35.442: INFO: kube-scheduler-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:04 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.442: INFO: 	Container kube-scheduler ready: true, restart count 1
Jun  8 16:05:35.442: INFO: oci-cloud-controller-manager-9n2zj from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.442: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
Jun  8 16:05:35.442: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-wmj9x from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 16:05:35.442: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 16:05:35.442: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  8 16:05:35.442: INFO: 
Logging pods the apiserver thinks is on node chl8tf-control-plane-002 before test
Jun  8 16:05:35.450: INFO: coredns-55b8ccd764-56hlv from kube-system started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.450: INFO: 	Container coredns ready: true, restart count 0
Jun  8 16:05:35.450: INFO: coredns-55b8ccd764-jpqkc from kube-system started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.450: INFO: 	Container coredns ready: true, restart count 0
Jun  8 16:05:35.450: INFO: csi-oci-node-xbn7q from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
Jun  8 16:05:35.450: INFO: 	Container csi-node-registrar ready: true, restart count 0
Jun  8 16:05:35.450: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
Jun  8 16:05:35.450: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
Jun  8 16:05:35.450: INFO: etcd-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.450: INFO: 	Container etcd ready: true, restart count 0
Jun  8 16:05:35.450: INFO: kube-apiserver-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:10 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.450: INFO: 	Container kube-apiserver ready: true, restart count 1
Jun  8 16:05:35.450: INFO: kube-controller-manager-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.450: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun  8 16:05:35.450: INFO: kube-flannel-ds-6fgbp from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.450: INFO: 	Container kube-flannel ready: true, restart count 0
Jun  8 16:05:35.450: INFO: kube-proxy-cgwfj from kube-system started at 2023-06-08 13:32:05 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.450: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  8 16:05:35.450: INFO: kube-scheduler-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.450: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun  8 16:05:35.450: INFO: oci-cloud-controller-manager-dpkmx from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.450: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
Jun  8 16:05:35.450: INFO: kubernetes-dashboard-8c85c4f9-9l4pq from kubernetes-dashboard started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.450: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jun  8 16:05:35.450: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-74mph from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 16:05:35.450: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 16:05:35.450: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  8 16:05:35.450: INFO: 
Logging pods the apiserver thinks is on node chl8tf-control-plane-003 before test
Jun  8 16:05:35.459: INFO: csi-oci-node-6zdfs from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
Jun  8 16:05:35.459: INFO: 	Container csi-node-registrar ready: true, restart count 0
Jun  8 16:05:35.459: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
Jun  8 16:05:35.459: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
Jun  8 16:05:35.459: INFO: etcd-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.459: INFO: 	Container etcd ready: true, restart count 0
Jun  8 16:05:35.459: INFO: kube-apiserver-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.459: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun  8 16:05:35.459: INFO: kube-controller-manager-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.459: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun  8 16:05:35.459: INFO: kube-flannel-ds-qf6c6 from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.459: INFO: 	Container kube-flannel ready: true, restart count 0
Jun  8 16:05:35.459: INFO: kube-proxy-9kw5t from kube-system started at 2023-06-08 13:33:04 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.459: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  8 16:05:35.459: INFO: kube-scheduler-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.459: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun  8 16:05:35.459: INFO: oci-cloud-controller-manager-2sdzm from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.459: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
Jun  8 16:05:35.459: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-jpq4c from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 16:05:35.459: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 16:05:35.459: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  8 16:05:35.459: INFO: 
Logging pods the apiserver thinks is on node chl8tf-worker-001 before test
Jun  8 16:05:35.467: INFO: csi-oci-node-7ww4x from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
Jun  8 16:05:35.467: INFO: 	Container csi-node-registrar ready: true, restart count 0
Jun  8 16:05:35.467: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
Jun  8 16:05:35.467: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
Jun  8 16:05:35.467: INFO: kube-flannel-ds-vg6nz from kube-system started at 2023-06-08 16:01:36 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.467: INFO: 	Container kube-flannel ready: true, restart count 0
Jun  8 16:05:35.467: INFO: kube-proxy-6kfv2 from kube-system started at 2023-06-08 13:33:27 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.467: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  8 16:05:35.467: INFO: oci-cloud-controller-manager-fchj2 from kube-system started at 2023-06-08 16:01:36 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.467: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
Jun  8 16:05:35.467: INFO: sonobuoy from sonobuoy started at 2023-06-08 15:05:13 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.467: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  8 16:05:35.467: INFO: sonobuoy-e2e-job-e329b7fb80aa4b40 from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 16:05:35.467: INFO: 	Container e2e ready: true, restart count 0
Jun  8 16:05:35.467: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 16:05:35.467: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-rs4qz from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 16:05:35.467: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 16:05:35.467: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  8 16:05:35.467: INFO: 
Logging pods the apiserver thinks is on node chl8tf-worker-002 before test
Jun  8 16:05:35.476: INFO: csi-oci-controller-69f8b488fc-n8th6 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (5 container statuses recorded)
Jun  8 16:05:35.476: INFO: 	Container csi-attacher ready: true, restart count 1
Jun  8 16:05:35.476: INFO: 	Container csi-fss-volume-provisioner ready: true, restart count 1
Jun  8 16:05:35.476: INFO: 	Container csi-resizer ready: true, restart count 0
Jun  8 16:05:35.476: INFO: 	Container csi-volume-provisioner ready: true, restart count 0
Jun  8 16:05:35.476: INFO: 	Container oci-csi-controller-driver ready: true, restart count 0
Jun  8 16:05:35.476: INFO: csi-oci-node-thcvn from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
Jun  8 16:05:35.477: INFO: 	Container csi-node-registrar ready: true, restart count 0
Jun  8 16:05:35.477: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
Jun  8 16:05:35.477: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
Jun  8 16:05:35.477: INFO: kube-flannel-ds-74q2b from kube-system started at 2023-06-08 13:33:47 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.477: INFO: 	Container kube-flannel ready: true, restart count 0
Jun  8 16:05:35.477: INFO: kube-proxy-hjjpt from kube-system started at 2023-06-08 13:33:47 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.477: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  8 16:05:35.477: INFO: oci-cloud-controller-manager-lwnq4 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
Jun  8 16:05:35.477: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
Jun  8 16:05:35.477: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-6nv2x from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 16:05:35.477: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 16:05:35.477: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 06/08/23 16:05:35.477
Jun  8 16:05:35.485: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8412" to be "running"
Jun  8 16:05:35.488: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.166204ms
Jun  8 16:05:37.494: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008769966s
Jun  8 16:05:37.494: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 06/08/23 16:05:37.497
STEP: Trying to apply a random label on the found node. 06/08/23 16:05:37.511
STEP: verifying the node has the label kubernetes.io/e2e-0e39c55a-e925-4522-b37e-fa13b32809c9 42 06/08/23 16:05:37.525
STEP: Trying to relaunch the pod, now with labels. 06/08/23 16:05:37.529
Jun  8 16:05:37.535: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-8412" to be "not pending"
Jun  8 16:05:37.539: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 3.349373ms
Jun  8 16:05:39.544: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.008381066s
Jun  8 16:05:39.544: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-0e39c55a-e925-4522-b37e-fa13b32809c9 off the node chl8tf-worker-001 06/08/23 16:05:39.547
STEP: verifying the node doesn't have the label kubernetes.io/e2e-0e39c55a-e925-4522-b37e-fa13b32809c9 06/08/23 16:05:39.562
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:05:39.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-8412" for this suite. 06/08/23 16:05:39.574
------------------------------
• [4.187 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:05:35.395
    Jun  8 16:05:35.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename sched-pred 06/08/23 16:05:35.396
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:05:35.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:05:35.416
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jun  8 16:05:35.419: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun  8 16:05:35.429: INFO: Waiting for terminating namespaces to be deleted...
    Jun  8 16:05:35.432: INFO: 
    Logging pods the apiserver thinks is on node chl8tf-control-plane-001 before test
    Jun  8 16:05:35.441: INFO: csi-oci-node-5p7f5 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
    Jun  8 16:05:35.441: INFO: 	Container csi-node-registrar ready: true, restart count 0
    Jun  8 16:05:35.441: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
    Jun  8 16:05:35.441: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
    Jun  8 16:05:35.441: INFO: etcd-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:04 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.442: INFO: 	Container etcd ready: true, restart count 0
    Jun  8 16:05:35.442: INFO: kube-apiserver-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:21 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.442: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jun  8 16:05:35.442: INFO: kube-controller-manager-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:21 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.442: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jun  8 16:05:35.442: INFO: kube-flannel-ds-d5bvw from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.442: INFO: 	Container kube-flannel ready: true, restart count 0
    Jun  8 16:05:35.442: INFO: kube-proxy-j2p7z from kube-system started at 2023-06-08 13:31:27 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.442: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  8 16:05:35.442: INFO: kube-scheduler-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:04 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.442: INFO: 	Container kube-scheduler ready: true, restart count 1
    Jun  8 16:05:35.442: INFO: oci-cloud-controller-manager-9n2zj from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.442: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
    Jun  8 16:05:35.442: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-wmj9x from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 16:05:35.442: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 16:05:35.442: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  8 16:05:35.442: INFO: 
    Logging pods the apiserver thinks is on node chl8tf-control-plane-002 before test
    Jun  8 16:05:35.450: INFO: coredns-55b8ccd764-56hlv from kube-system started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.450: INFO: 	Container coredns ready: true, restart count 0
    Jun  8 16:05:35.450: INFO: coredns-55b8ccd764-jpqkc from kube-system started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.450: INFO: 	Container coredns ready: true, restart count 0
    Jun  8 16:05:35.450: INFO: csi-oci-node-xbn7q from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
    Jun  8 16:05:35.450: INFO: 	Container csi-node-registrar ready: true, restart count 0
    Jun  8 16:05:35.450: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
    Jun  8 16:05:35.450: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
    Jun  8 16:05:35.450: INFO: etcd-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.450: INFO: 	Container etcd ready: true, restart count 0
    Jun  8 16:05:35.450: INFO: kube-apiserver-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:10 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.450: INFO: 	Container kube-apiserver ready: true, restart count 1
    Jun  8 16:05:35.450: INFO: kube-controller-manager-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.450: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jun  8 16:05:35.450: INFO: kube-flannel-ds-6fgbp from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.450: INFO: 	Container kube-flannel ready: true, restart count 0
    Jun  8 16:05:35.450: INFO: kube-proxy-cgwfj from kube-system started at 2023-06-08 13:32:05 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.450: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  8 16:05:35.450: INFO: kube-scheduler-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.450: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jun  8 16:05:35.450: INFO: oci-cloud-controller-manager-dpkmx from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.450: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
    Jun  8 16:05:35.450: INFO: kubernetes-dashboard-8c85c4f9-9l4pq from kubernetes-dashboard started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.450: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Jun  8 16:05:35.450: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-74mph from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 16:05:35.450: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 16:05:35.450: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  8 16:05:35.450: INFO: 
    Logging pods the apiserver thinks is on node chl8tf-control-plane-003 before test
    Jun  8 16:05:35.459: INFO: csi-oci-node-6zdfs from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
    Jun  8 16:05:35.459: INFO: 	Container csi-node-registrar ready: true, restart count 0
    Jun  8 16:05:35.459: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
    Jun  8 16:05:35.459: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
    Jun  8 16:05:35.459: INFO: etcd-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.459: INFO: 	Container etcd ready: true, restart count 0
    Jun  8 16:05:35.459: INFO: kube-apiserver-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.459: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jun  8 16:05:35.459: INFO: kube-controller-manager-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.459: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jun  8 16:05:35.459: INFO: kube-flannel-ds-qf6c6 from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.459: INFO: 	Container kube-flannel ready: true, restart count 0
    Jun  8 16:05:35.459: INFO: kube-proxy-9kw5t from kube-system started at 2023-06-08 13:33:04 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.459: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  8 16:05:35.459: INFO: kube-scheduler-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.459: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jun  8 16:05:35.459: INFO: oci-cloud-controller-manager-2sdzm from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.459: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
    Jun  8 16:05:35.459: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-jpq4c from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 16:05:35.459: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 16:05:35.459: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  8 16:05:35.459: INFO: 
    Logging pods the apiserver thinks is on node chl8tf-worker-001 before test
    Jun  8 16:05:35.467: INFO: csi-oci-node-7ww4x from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
    Jun  8 16:05:35.467: INFO: 	Container csi-node-registrar ready: true, restart count 0
    Jun  8 16:05:35.467: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
    Jun  8 16:05:35.467: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
    Jun  8 16:05:35.467: INFO: kube-flannel-ds-vg6nz from kube-system started at 2023-06-08 16:01:36 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.467: INFO: 	Container kube-flannel ready: true, restart count 0
    Jun  8 16:05:35.467: INFO: kube-proxy-6kfv2 from kube-system started at 2023-06-08 13:33:27 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.467: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  8 16:05:35.467: INFO: oci-cloud-controller-manager-fchj2 from kube-system started at 2023-06-08 16:01:36 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.467: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
    Jun  8 16:05:35.467: INFO: sonobuoy from sonobuoy started at 2023-06-08 15:05:13 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.467: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun  8 16:05:35.467: INFO: sonobuoy-e2e-job-e329b7fb80aa4b40 from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 16:05:35.467: INFO: 	Container e2e ready: true, restart count 0
    Jun  8 16:05:35.467: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 16:05:35.467: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-rs4qz from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 16:05:35.467: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 16:05:35.467: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  8 16:05:35.467: INFO: 
    Logging pods the apiserver thinks is on node chl8tf-worker-002 before test
    Jun  8 16:05:35.476: INFO: csi-oci-controller-69f8b488fc-n8th6 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (5 container statuses recorded)
    Jun  8 16:05:35.476: INFO: 	Container csi-attacher ready: true, restart count 1
    Jun  8 16:05:35.476: INFO: 	Container csi-fss-volume-provisioner ready: true, restart count 1
    Jun  8 16:05:35.476: INFO: 	Container csi-resizer ready: true, restart count 0
    Jun  8 16:05:35.476: INFO: 	Container csi-volume-provisioner ready: true, restart count 0
    Jun  8 16:05:35.476: INFO: 	Container oci-csi-controller-driver ready: true, restart count 0
    Jun  8 16:05:35.476: INFO: csi-oci-node-thcvn from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
    Jun  8 16:05:35.477: INFO: 	Container csi-node-registrar ready: true, restart count 0
    Jun  8 16:05:35.477: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
    Jun  8 16:05:35.477: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
    Jun  8 16:05:35.477: INFO: kube-flannel-ds-74q2b from kube-system started at 2023-06-08 13:33:47 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.477: INFO: 	Container kube-flannel ready: true, restart count 0
    Jun  8 16:05:35.477: INFO: kube-proxy-hjjpt from kube-system started at 2023-06-08 13:33:47 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.477: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  8 16:05:35.477: INFO: oci-cloud-controller-manager-lwnq4 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
    Jun  8 16:05:35.477: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
    Jun  8 16:05:35.477: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-6nv2x from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 16:05:35.477: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 16:05:35.477: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 06/08/23 16:05:35.477
    Jun  8 16:05:35.485: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8412" to be "running"
    Jun  8 16:05:35.488: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.166204ms
    Jun  8 16:05:37.494: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008769966s
    Jun  8 16:05:37.494: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 06/08/23 16:05:37.497
    STEP: Trying to apply a random label on the found node. 06/08/23 16:05:37.511
    STEP: verifying the node has the label kubernetes.io/e2e-0e39c55a-e925-4522-b37e-fa13b32809c9 42 06/08/23 16:05:37.525
    STEP: Trying to relaunch the pod, now with labels. 06/08/23 16:05:37.529
    Jun  8 16:05:37.535: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-8412" to be "not pending"
    Jun  8 16:05:37.539: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 3.349373ms
    Jun  8 16:05:39.544: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.008381066s
    Jun  8 16:05:39.544: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-0e39c55a-e925-4522-b37e-fa13b32809c9 off the node chl8tf-worker-001 06/08/23 16:05:39.547
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-0e39c55a-e925-4522-b37e-fa13b32809c9 06/08/23 16:05:39.562
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:05:39.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-8412" for this suite. 06/08/23 16:05:39.574
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:05:39.585
Jun  8 16:05:39.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 16:05:39.587
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:05:39.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:05:39.612
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 06/08/23 16:05:39.617
Jun  8 16:05:39.627: INFO: Waiting up to 5m0s for pod "downwardapi-volume-42c05b52-2701-4c8a-8595-b18713a4a7a4" in namespace "projected-5021" to be "Succeeded or Failed"
Jun  8 16:05:39.631: INFO: Pod "downwardapi-volume-42c05b52-2701-4c8a-8595-b18713a4a7a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.896533ms
Jun  8 16:05:41.636: INFO: Pod "downwardapi-volume-42c05b52-2701-4c8a-8595-b18713a4a7a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00846357s
Jun  8 16:05:43.637: INFO: Pod "downwardapi-volume-42c05b52-2701-4c8a-8595-b18713a4a7a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009531382s
STEP: Saw pod success 06/08/23 16:05:43.637
Jun  8 16:05:43.637: INFO: Pod "downwardapi-volume-42c05b52-2701-4c8a-8595-b18713a4a7a4" satisfied condition "Succeeded or Failed"
Jun  8 16:05:43.641: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-42c05b52-2701-4c8a-8595-b18713a4a7a4 container client-container: <nil>
STEP: delete the pod 06/08/23 16:05:43.647
Jun  8 16:05:43.659: INFO: Waiting for pod downwardapi-volume-42c05b52-2701-4c8a-8595-b18713a4a7a4 to disappear
Jun  8 16:05:43.662: INFO: Pod downwardapi-volume-42c05b52-2701-4c8a-8595-b18713a4a7a4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  8 16:05:43.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5021" for this suite. 06/08/23 16:05:43.668
------------------------------
• [4.089 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:05:39.585
    Jun  8 16:05:39.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 16:05:39.587
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:05:39.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:05:39.612
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 06/08/23 16:05:39.617
    Jun  8 16:05:39.627: INFO: Waiting up to 5m0s for pod "downwardapi-volume-42c05b52-2701-4c8a-8595-b18713a4a7a4" in namespace "projected-5021" to be "Succeeded or Failed"
    Jun  8 16:05:39.631: INFO: Pod "downwardapi-volume-42c05b52-2701-4c8a-8595-b18713a4a7a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.896533ms
    Jun  8 16:05:41.636: INFO: Pod "downwardapi-volume-42c05b52-2701-4c8a-8595-b18713a4a7a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00846357s
    Jun  8 16:05:43.637: INFO: Pod "downwardapi-volume-42c05b52-2701-4c8a-8595-b18713a4a7a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009531382s
    STEP: Saw pod success 06/08/23 16:05:43.637
    Jun  8 16:05:43.637: INFO: Pod "downwardapi-volume-42c05b52-2701-4c8a-8595-b18713a4a7a4" satisfied condition "Succeeded or Failed"
    Jun  8 16:05:43.641: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-42c05b52-2701-4c8a-8595-b18713a4a7a4 container client-container: <nil>
    STEP: delete the pod 06/08/23 16:05:43.647
    Jun  8 16:05:43.659: INFO: Waiting for pod downwardapi-volume-42c05b52-2701-4c8a-8595-b18713a4a7a4 to disappear
    Jun  8 16:05:43.662: INFO: Pod downwardapi-volume-42c05b52-2701-4c8a-8595-b18713a4a7a4 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:05:43.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5021" for this suite. 06/08/23 16:05:43.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:05:43.676
Jun  8 16:05:43.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename webhook 06/08/23 16:05:43.677
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:05:43.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:05:43.696
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/08/23 16:05:43.712
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 16:05:44.98
STEP: Deploying the webhook pod 06/08/23 16:05:44.989
STEP: Wait for the deployment to be ready 06/08/23 16:05:45
Jun  8 16:05:45.008: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 06/08/23 16:05:47.021
STEP: Verifying the service has paired with the endpoint 06/08/23 16:05:47.035
Jun  8 16:05:48.035: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 06/08/23 16:05:48.101
STEP: Creating a configMap that should be mutated 06/08/23 16:05:48.119
STEP: Deleting the collection of validation webhooks 06/08/23 16:05:48.15
STEP: Creating a configMap that should not be mutated 06/08/23 16:05:48.197
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:05:48.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8201" for this suite. 06/08/23 16:05:48.261
STEP: Destroying namespace "webhook-8201-markers" for this suite. 06/08/23 16:05:48.27
------------------------------
• [4.604 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:05:43.676
    Jun  8 16:05:43.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename webhook 06/08/23 16:05:43.677
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:05:43.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:05:43.696
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/08/23 16:05:43.712
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 16:05:44.98
    STEP: Deploying the webhook pod 06/08/23 16:05:44.989
    STEP: Wait for the deployment to be ready 06/08/23 16:05:45
    Jun  8 16:05:45.008: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 06/08/23 16:05:47.021
    STEP: Verifying the service has paired with the endpoint 06/08/23 16:05:47.035
    Jun  8 16:05:48.035: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 06/08/23 16:05:48.101
    STEP: Creating a configMap that should be mutated 06/08/23 16:05:48.119
    STEP: Deleting the collection of validation webhooks 06/08/23 16:05:48.15
    STEP: Creating a configMap that should not be mutated 06/08/23 16:05:48.197
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:05:48.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8201" for this suite. 06/08/23 16:05:48.261
    STEP: Destroying namespace "webhook-8201-markers" for this suite. 06/08/23 16:05:48.27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:05:48.283
Jun  8 16:05:48.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename crd-publish-openapi 06/08/23 16:05:48.284
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:05:48.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:05:48.309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Jun  8 16:05:48.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/08/23 16:05:50.543
Jun  8 16:05:50.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-2524 --namespace=crd-publish-openapi-2524 create -f -'
Jun  8 16:05:51.215: INFO: stderr: ""
Jun  8 16:05:51.215: INFO: stdout: "e2e-test-crd-publish-openapi-6505-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun  8 16:05:51.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-2524 --namespace=crd-publish-openapi-2524 delete e2e-test-crd-publish-openapi-6505-crds test-cr'
Jun  8 16:05:51.330: INFO: stderr: ""
Jun  8 16:05:51.330: INFO: stdout: "e2e-test-crd-publish-openapi-6505-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun  8 16:05:51.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-2524 --namespace=crd-publish-openapi-2524 apply -f -'
Jun  8 16:05:51.888: INFO: stderr: ""
Jun  8 16:05:51.888: INFO: stdout: "e2e-test-crd-publish-openapi-6505-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun  8 16:05:51.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-2524 --namespace=crd-publish-openapi-2524 delete e2e-test-crd-publish-openapi-6505-crds test-cr'
Jun  8 16:05:51.976: INFO: stderr: ""
Jun  8 16:05:51.976: INFO: stdout: "e2e-test-crd-publish-openapi-6505-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 06/08/23 16:05:51.976
Jun  8 16:05:51.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-2524 explain e2e-test-crd-publish-openapi-6505-crds'
Jun  8 16:05:52.160: INFO: stderr: ""
Jun  8 16:05:52.160: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6505-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:05:54.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2524" for this suite. 06/08/23 16:05:54.331
------------------------------
• [SLOW TEST] [6.056 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:05:48.283
    Jun  8 16:05:48.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename crd-publish-openapi 06/08/23 16:05:48.284
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:05:48.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:05:48.309
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Jun  8 16:05:48.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/08/23 16:05:50.543
    Jun  8 16:05:50.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-2524 --namespace=crd-publish-openapi-2524 create -f -'
    Jun  8 16:05:51.215: INFO: stderr: ""
    Jun  8 16:05:51.215: INFO: stdout: "e2e-test-crd-publish-openapi-6505-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jun  8 16:05:51.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-2524 --namespace=crd-publish-openapi-2524 delete e2e-test-crd-publish-openapi-6505-crds test-cr'
    Jun  8 16:05:51.330: INFO: stderr: ""
    Jun  8 16:05:51.330: INFO: stdout: "e2e-test-crd-publish-openapi-6505-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jun  8 16:05:51.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-2524 --namespace=crd-publish-openapi-2524 apply -f -'
    Jun  8 16:05:51.888: INFO: stderr: ""
    Jun  8 16:05:51.888: INFO: stdout: "e2e-test-crd-publish-openapi-6505-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jun  8 16:05:51.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-2524 --namespace=crd-publish-openapi-2524 delete e2e-test-crd-publish-openapi-6505-crds test-cr'
    Jun  8 16:05:51.976: INFO: stderr: ""
    Jun  8 16:05:51.976: INFO: stdout: "e2e-test-crd-publish-openapi-6505-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 06/08/23 16:05:51.976
    Jun  8 16:05:51.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-2524 explain e2e-test-crd-publish-openapi-6505-crds'
    Jun  8 16:05:52.160: INFO: stderr: ""
    Jun  8 16:05:52.160: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6505-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:05:54.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2524" for this suite. 06/08/23 16:05:54.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:05:54.339
Jun  8 16:05:54.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename watch 06/08/23 16:05:54.341
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:05:54.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:05:54.364
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 06/08/23 16:05:54.367
STEP: starting a background goroutine to produce watch events 06/08/23 16:05:54.371
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 06/08/23 16:05:54.371
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jun  8 16:05:57.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5664" for this suite. 06/08/23 16:05:57.197
------------------------------
• [2.917 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:05:54.339
    Jun  8 16:05:54.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename watch 06/08/23 16:05:54.341
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:05:54.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:05:54.364
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 06/08/23 16:05:54.367
    STEP: starting a background goroutine to produce watch events 06/08/23 16:05:54.371
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 06/08/23 16:05:54.371
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:05:57.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5664" for this suite. 06/08/23 16:05:57.197
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:05:57.256
Jun  8 16:05:57.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename services 06/08/23 16:05:57.258
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:05:57.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:05:57.279
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 06/08/23 16:05:57.283
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  8 16:05:57.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7688" for this suite. 06/08/23 16:05:57.293
------------------------------
• [0.044 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:05:57.256
    Jun  8 16:05:57.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename services 06/08/23 16:05:57.258
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:05:57.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:05:57.279
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 06/08/23 16:05:57.283
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:05:57.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7688" for this suite. 06/08/23 16:05:57.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:05:57.303
Jun  8 16:05:57.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename deployment 06/08/23 16:05:57.304
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:05:57.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:05:57.328
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jun  8 16:05:57.341: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun  8 16:06:02.346: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/08/23 16:06:02.346
Jun  8 16:06:02.346: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun  8 16:06:04.351: INFO: Creating deployment "test-rollover-deployment"
Jun  8 16:06:04.361: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun  8 16:06:06.369: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun  8 16:06:06.377: INFO: Ensure that both replica sets have 1 created replica
Jun  8 16:06:06.384: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun  8 16:06:06.395: INFO: Updating deployment test-rollover-deployment
Jun  8 16:06:06.395: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun  8 16:06:08.404: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun  8 16:06:08.414: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun  8 16:06:08.421: INFO: all replica sets need to contain the pod-template-hash label
Jun  8 16:06:08.421: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 6, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  8 16:06:10.432: INFO: all replica sets need to contain the pod-template-hash label
Jun  8 16:06:10.432: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 6, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  8 16:06:12.432: INFO: all replica sets need to contain the pod-template-hash label
Jun  8 16:06:12.432: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 6, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  8 16:06:14.430: INFO: all replica sets need to contain the pod-template-hash label
Jun  8 16:06:14.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 6, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  8 16:06:16.431: INFO: all replica sets need to contain the pod-template-hash label
Jun  8 16:06:16.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 6, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  8 16:06:18.432: INFO: 
Jun  8 16:06:18.432: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun  8 16:06:18.443: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4827  22d37c33-af41-4131-b126-c9a03ce6501e 43912 2 2023-06-08 16:06:04 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-08 16:06:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 16:06:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051c9fe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-08 16:06:04 +0000 UTC,LastTransitionTime:2023-06-08 16:06:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-06-08 16:06:17 +0000 UTC,LastTransitionTime:2023-06-08 16:06:04 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  8 16:06:18.447: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-4827  edbe550b-c5d4-4e47-ac03-5f24547dcb48 43902 2 2023-06-08 16:06:06 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 22d37c33-af41-4131-b126-c9a03ce6501e 0xc0053a3b57 0xc0053a3b58}] [] [{kube-controller-manager Update apps/v1 2023-06-08 16:06:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d37c33-af41-4131-b126-c9a03ce6501e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 16:06:17 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053a3c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  8 16:06:18.447: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun  8 16:06:18.447: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4827  b30683fa-57ef-4345-95eb-f7260af3a3bf 43911 2 2023-06-08 16:05:57 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 22d37c33-af41-4131-b126-c9a03ce6501e 0xc0053a3a27 0xc0053a3a28}] [] [{e2e.test Update apps/v1 2023-06-08 16:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 16:06:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d37c33-af41-4131-b126-c9a03ce6501e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-08 16:06:17 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0053a3ae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  8 16:06:18.447: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-4827  17764e26-d26d-4186-9a8f-c33cb2db99e2 43841 2 2023-06-08 16:06:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 22d37c33-af41-4131-b126-c9a03ce6501e 0xc0053a3c77 0xc0053a3c78}] [] [{kube-controller-manager Update apps/v1 2023-06-08 16:06:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d37c33-af41-4131-b126-c9a03ce6501e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 16:06:06 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053a3d28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  8 16:06:18.452: INFO: Pod "test-rollover-deployment-6c6df9974f-sb6h9" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-sb6h9 test-rollover-deployment-6c6df9974f- deployment-4827  03deb624-0f1f-462d-95b3-5f9aa51edade 43856 0 2023-06-08 16:06:06 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f edbe550b-c5d4-4e47-ac03-5f24547dcb48 0xc0055c6257 0xc0055c6258}] [] [{kube-controller-manager Update v1 2023-06-08 16:06:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"edbe550b-c5d4-4e47-ac03-5f24547dcb48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 16:06:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-77cbq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-77cbq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 16:06:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 16:06:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 16:06:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 16:06:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.215,PodIP:10.244.3.235,StartTime:2023-06-08 16:06:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 16:06:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://e73a068faad833f84aff5b805277d8a4bb638f42a45d349f256bfdd4cec91f84,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.235,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun  8 16:06:18.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4827" for this suite. 06/08/23 16:06:18.457
------------------------------
• [SLOW TEST] [21.162 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:05:57.303
    Jun  8 16:05:57.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename deployment 06/08/23 16:05:57.304
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:05:57.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:05:57.328
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jun  8 16:05:57.341: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jun  8 16:06:02.346: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/08/23 16:06:02.346
    Jun  8 16:06:02.346: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jun  8 16:06:04.351: INFO: Creating deployment "test-rollover-deployment"
    Jun  8 16:06:04.361: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jun  8 16:06:06.369: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jun  8 16:06:06.377: INFO: Ensure that both replica sets have 1 created replica
    Jun  8 16:06:06.384: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jun  8 16:06:06.395: INFO: Updating deployment test-rollover-deployment
    Jun  8 16:06:06.395: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jun  8 16:06:08.404: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jun  8 16:06:08.414: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jun  8 16:06:08.421: INFO: all replica sets need to contain the pod-template-hash label
    Jun  8 16:06:08.421: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 6, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  8 16:06:10.432: INFO: all replica sets need to contain the pod-template-hash label
    Jun  8 16:06:10.432: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 6, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  8 16:06:12.432: INFO: all replica sets need to contain the pod-template-hash label
    Jun  8 16:06:12.432: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 6, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  8 16:06:14.430: INFO: all replica sets need to contain the pod-template-hash label
    Jun  8 16:06:14.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 6, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  8 16:06:16.431: INFO: all replica sets need to contain the pod-template-hash label
    Jun  8 16:06:16.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 6, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 6, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  8 16:06:18.432: INFO: 
    Jun  8 16:06:18.432: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun  8 16:06:18.443: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-4827  22d37c33-af41-4131-b126-c9a03ce6501e 43912 2 2023-06-08 16:06:04 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-08 16:06:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 16:06:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051c9fe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-08 16:06:04 +0000 UTC,LastTransitionTime:2023-06-08 16:06:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-06-08 16:06:17 +0000 UTC,LastTransitionTime:2023-06-08 16:06:04 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun  8 16:06:18.447: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-4827  edbe550b-c5d4-4e47-ac03-5f24547dcb48 43902 2 2023-06-08 16:06:06 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 22d37c33-af41-4131-b126-c9a03ce6501e 0xc0053a3b57 0xc0053a3b58}] [] [{kube-controller-manager Update apps/v1 2023-06-08 16:06:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d37c33-af41-4131-b126-c9a03ce6501e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 16:06:17 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053a3c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun  8 16:06:18.447: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jun  8 16:06:18.447: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4827  b30683fa-57ef-4345-95eb-f7260af3a3bf 43911 2 2023-06-08 16:05:57 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 22d37c33-af41-4131-b126-c9a03ce6501e 0xc0053a3a27 0xc0053a3a28}] [] [{e2e.test Update apps/v1 2023-06-08 16:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 16:06:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d37c33-af41-4131-b126-c9a03ce6501e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-08 16:06:17 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0053a3ae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun  8 16:06:18.447: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-4827  17764e26-d26d-4186-9a8f-c33cb2db99e2 43841 2 2023-06-08 16:06:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 22d37c33-af41-4131-b126-c9a03ce6501e 0xc0053a3c77 0xc0053a3c78}] [] [{kube-controller-manager Update apps/v1 2023-06-08 16:06:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d37c33-af41-4131-b126-c9a03ce6501e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-08 16:06:06 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053a3d28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun  8 16:06:18.452: INFO: Pod "test-rollover-deployment-6c6df9974f-sb6h9" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-sb6h9 test-rollover-deployment-6c6df9974f- deployment-4827  03deb624-0f1f-462d-95b3-5f9aa51edade 43856 0 2023-06-08 16:06:06 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f edbe550b-c5d4-4e47-ac03-5f24547dcb48 0xc0055c6257 0xc0055c6258}] [] [{kube-controller-manager Update v1 2023-06-08 16:06:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"edbe550b-c5d4-4e47-ac03-5f24547dcb48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-08 16:06:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-77cbq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-77cbq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:chl8tf-worker-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 16:06:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 16:06:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 16:06:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-08 16:06:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:100.100.236.215,PodIP:10.244.3.235,StartTime:2023-06-08 16:06:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-08 16:06:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://e73a068faad833f84aff5b805277d8a4bb638f42a45d349f256bfdd4cec91f84,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.235,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:06:18.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4827" for this suite. 06/08/23 16:06:18.457
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:06:18.465
Jun  8 16:06:18.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename gc 06/08/23 16:06:18.466
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:06:18.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:06:18.487
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 06/08/23 16:06:18.495
STEP: delete the rc 06/08/23 16:06:23.511
STEP: wait for the rc to be deleted 06/08/23 16:06:23.535
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 06/08/23 16:06:28.543
STEP: Gathering metrics 06/08/23 16:06:58.559
Jun  8 16:06:58.589: INFO: Waiting up to 5m0s for pod "kube-controller-manager-chl8tf-control-plane-003" in namespace "kube-system" to be "running and ready"
Jun  8 16:06:58.592: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003": Phase="Running", Reason="", readiness=true. Elapsed: 3.854039ms
Jun  8 16:06:58.593: INFO: The phase of Pod kube-controller-manager-chl8tf-control-plane-003 is Running (Ready = true)
Jun  8 16:06:58.593: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003" satisfied condition "running and ready"
Jun  8 16:06:58.656: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun  8 16:06:58.656: INFO: Deleting pod "simpletest.rc-2fj8w" in namespace "gc-1293"
Jun  8 16:06:58.672: INFO: Deleting pod "simpletest.rc-45rs9" in namespace "gc-1293"
Jun  8 16:06:58.683: INFO: Deleting pod "simpletest.rc-486vw" in namespace "gc-1293"
Jun  8 16:06:58.704: INFO: Deleting pod "simpletest.rc-4c9t7" in namespace "gc-1293"
Jun  8 16:06:58.716: INFO: Deleting pod "simpletest.rc-4d87t" in namespace "gc-1293"
Jun  8 16:06:58.728: INFO: Deleting pod "simpletest.rc-4dz6c" in namespace "gc-1293"
Jun  8 16:06:58.740: INFO: Deleting pod "simpletest.rc-4hg5b" in namespace "gc-1293"
Jun  8 16:06:58.754: INFO: Deleting pod "simpletest.rc-56gn7" in namespace "gc-1293"
Jun  8 16:06:58.770: INFO: Deleting pod "simpletest.rc-5mmdc" in namespace "gc-1293"
Jun  8 16:06:58.783: INFO: Deleting pod "simpletest.rc-6jsc9" in namespace "gc-1293"
Jun  8 16:06:58.812: INFO: Deleting pod "simpletest.rc-6ldd8" in namespace "gc-1293"
Jun  8 16:06:58.825: INFO: Deleting pod "simpletest.rc-75z5j" in namespace "gc-1293"
Jun  8 16:06:58.841: INFO: Deleting pod "simpletest.rc-7r5pz" in namespace "gc-1293"
Jun  8 16:06:58.856: INFO: Deleting pod "simpletest.rc-7vqpj" in namespace "gc-1293"
Jun  8 16:06:58.872: INFO: Deleting pod "simpletest.rc-7wm24" in namespace "gc-1293"
Jun  8 16:06:58.885: INFO: Deleting pod "simpletest.rc-8z22g" in namespace "gc-1293"
Jun  8 16:06:58.922: INFO: Deleting pod "simpletest.rc-9clws" in namespace "gc-1293"
Jun  8 16:06:58.941: INFO: Deleting pod "simpletest.rc-9j9kz" in namespace "gc-1293"
Jun  8 16:06:58.963: INFO: Deleting pod "simpletest.rc-9kwvt" in namespace "gc-1293"
Jun  8 16:06:58.979: INFO: Deleting pod "simpletest.rc-9m9sf" in namespace "gc-1293"
Jun  8 16:06:58.992: INFO: Deleting pod "simpletest.rc-9pngk" in namespace "gc-1293"
Jun  8 16:06:59.015: INFO: Deleting pod "simpletest.rc-b7tjx" in namespace "gc-1293"
Jun  8 16:06:59.040: INFO: Deleting pod "simpletest.rc-bqw2p" in namespace "gc-1293"
Jun  8 16:06:59.055: INFO: Deleting pod "simpletest.rc-brqhp" in namespace "gc-1293"
Jun  8 16:06:59.072: INFO: Deleting pod "simpletest.rc-bt289" in namespace "gc-1293"
Jun  8 16:06:59.096: INFO: Deleting pod "simpletest.rc-c7zrh" in namespace "gc-1293"
Jun  8 16:06:59.113: INFO: Deleting pod "simpletest.rc-c88ws" in namespace "gc-1293"
Jun  8 16:06:59.133: INFO: Deleting pod "simpletest.rc-cb9wk" in namespace "gc-1293"
Jun  8 16:06:59.150: INFO: Deleting pod "simpletest.rc-cf7k5" in namespace "gc-1293"
Jun  8 16:06:59.168: INFO: Deleting pod "simpletest.rc-cltvb" in namespace "gc-1293"
Jun  8 16:06:59.186: INFO: Deleting pod "simpletest.rc-cmw95" in namespace "gc-1293"
Jun  8 16:06:59.209: INFO: Deleting pod "simpletest.rc-dbdw7" in namespace "gc-1293"
Jun  8 16:06:59.247: INFO: Deleting pod "simpletest.rc-dh599" in namespace "gc-1293"
Jun  8 16:06:59.270: INFO: Deleting pod "simpletest.rc-dz6cl" in namespace "gc-1293"
Jun  8 16:06:59.286: INFO: Deleting pod "simpletest.rc-ff7rn" in namespace "gc-1293"
Jun  8 16:06:59.311: INFO: Deleting pod "simpletest.rc-ff8s6" in namespace "gc-1293"
Jun  8 16:06:59.333: INFO: Deleting pod "simpletest.rc-fqzqm" in namespace "gc-1293"
Jun  8 16:06:59.357: INFO: Deleting pod "simpletest.rc-g8kv5" in namespace "gc-1293"
Jun  8 16:06:59.375: INFO: Deleting pod "simpletest.rc-g9kzt" in namespace "gc-1293"
Jun  8 16:06:59.400: INFO: Deleting pod "simpletest.rc-ghkqn" in namespace "gc-1293"
Jun  8 16:06:59.429: INFO: Deleting pod "simpletest.rc-gqhpx" in namespace "gc-1293"
Jun  8 16:06:59.452: INFO: Deleting pod "simpletest.rc-h26wp" in namespace "gc-1293"
Jun  8 16:06:59.478: INFO: Deleting pod "simpletest.rc-h7hjc" in namespace "gc-1293"
Jun  8 16:06:59.499: INFO: Deleting pod "simpletest.rc-hbdl7" in namespace "gc-1293"
Jun  8 16:06:59.519: INFO: Deleting pod "simpletest.rc-hd8bq" in namespace "gc-1293"
Jun  8 16:06:59.538: INFO: Deleting pod "simpletest.rc-hfm6g" in namespace "gc-1293"
Jun  8 16:06:59.573: INFO: Deleting pod "simpletest.rc-hllzw" in namespace "gc-1293"
Jun  8 16:06:59.595: INFO: Deleting pod "simpletest.rc-hmcvt" in namespace "gc-1293"
Jun  8 16:06:59.618: INFO: Deleting pod "simpletest.rc-j4pzb" in namespace "gc-1293"
Jun  8 16:06:59.651: INFO: Deleting pod "simpletest.rc-k6lrc" in namespace "gc-1293"
Jun  8 16:06:59.693: INFO: Deleting pod "simpletest.rc-kqz4k" in namespace "gc-1293"
Jun  8 16:06:59.720: INFO: Deleting pod "simpletest.rc-krdfx" in namespace "gc-1293"
Jun  8 16:06:59.737: INFO: Deleting pod "simpletest.rc-l4xlh" in namespace "gc-1293"
Jun  8 16:06:59.757: INFO: Deleting pod "simpletest.rc-lcnfm" in namespace "gc-1293"
Jun  8 16:06:59.774: INFO: Deleting pod "simpletest.rc-lhnqd" in namespace "gc-1293"
Jun  8 16:06:59.791: INFO: Deleting pod "simpletest.rc-lprjg" in namespace "gc-1293"
Jun  8 16:06:59.809: INFO: Deleting pod "simpletest.rc-mqvfw" in namespace "gc-1293"
Jun  8 16:06:59.844: INFO: Deleting pod "simpletest.rc-ns2hx" in namespace "gc-1293"
Jun  8 16:06:59.863: INFO: Deleting pod "simpletest.rc-q2jjw" in namespace "gc-1293"
Jun  8 16:06:59.882: INFO: Deleting pod "simpletest.rc-q7d8j" in namespace "gc-1293"
Jun  8 16:06:59.910: INFO: Deleting pod "simpletest.rc-qnlbk" in namespace "gc-1293"
Jun  8 16:06:59.968: INFO: Deleting pod "simpletest.rc-qpf9n" in namespace "gc-1293"
Jun  8 16:06:59.987: INFO: Deleting pod "simpletest.rc-qpw8c" in namespace "gc-1293"
Jun  8 16:07:00.015: INFO: Deleting pod "simpletest.rc-qr492" in namespace "gc-1293"
Jun  8 16:07:00.036: INFO: Deleting pod "simpletest.rc-rd9tp" in namespace "gc-1293"
Jun  8 16:07:00.059: INFO: Deleting pod "simpletest.rc-rfxhl" in namespace "gc-1293"
Jun  8 16:07:00.088: INFO: Deleting pod "simpletest.rc-rpgst" in namespace "gc-1293"
Jun  8 16:07:00.108: INFO: Deleting pod "simpletest.rc-rqsj9" in namespace "gc-1293"
Jun  8 16:07:00.134: INFO: Deleting pod "simpletest.rc-rr7zv" in namespace "gc-1293"
Jun  8 16:07:00.169: INFO: Deleting pod "simpletest.rc-rxxlc" in namespace "gc-1293"
Jun  8 16:07:00.199: INFO: Deleting pod "simpletest.rc-skldc" in namespace "gc-1293"
Jun  8 16:07:00.221: INFO: Deleting pod "simpletest.rc-spq8d" in namespace "gc-1293"
Jun  8 16:07:00.244: INFO: Deleting pod "simpletest.rc-sww4x" in namespace "gc-1293"
Jun  8 16:07:00.264: INFO: Deleting pod "simpletest.rc-t4v8f" in namespace "gc-1293"
Jun  8 16:07:00.295: INFO: Deleting pod "simpletest.rc-t5t9g" in namespace "gc-1293"
Jun  8 16:07:00.314: INFO: Deleting pod "simpletest.rc-t64pg" in namespace "gc-1293"
Jun  8 16:07:00.338: INFO: Deleting pod "simpletest.rc-tlzqz" in namespace "gc-1293"
Jun  8 16:07:00.356: INFO: Deleting pod "simpletest.rc-tr9k2" in namespace "gc-1293"
Jun  8 16:07:00.378: INFO: Deleting pod "simpletest.rc-tszmx" in namespace "gc-1293"
Jun  8 16:07:00.401: INFO: Deleting pod "simpletest.rc-v7vj4" in namespace "gc-1293"
Jun  8 16:07:00.433: INFO: Deleting pod "simpletest.rc-v9kn8" in namespace "gc-1293"
Jun  8 16:07:00.460: INFO: Deleting pod "simpletest.rc-vbnfp" in namespace "gc-1293"
Jun  8 16:07:00.479: INFO: Deleting pod "simpletest.rc-vnfqm" in namespace "gc-1293"
Jun  8 16:07:00.497: INFO: Deleting pod "simpletest.rc-vsnch" in namespace "gc-1293"
Jun  8 16:07:00.532: INFO: Deleting pod "simpletest.rc-vsvvt" in namespace "gc-1293"
Jun  8 16:07:00.572: INFO: Deleting pod "simpletest.rc-w5klv" in namespace "gc-1293"
Jun  8 16:07:00.644: INFO: Deleting pod "simpletest.rc-wsrm4" in namespace "gc-1293"
Jun  8 16:07:00.669: INFO: Deleting pod "simpletest.rc-x5nr7" in namespace "gc-1293"
Jun  8 16:07:00.695: INFO: Deleting pod "simpletest.rc-x75h9" in namespace "gc-1293"
Jun  8 16:07:00.719: INFO: Deleting pod "simpletest.rc-xc6c2" in namespace "gc-1293"
Jun  8 16:07:00.783: INFO: Deleting pod "simpletest.rc-xgnrn" in namespace "gc-1293"
Jun  8 16:07:00.833: INFO: Deleting pod "simpletest.rc-xrjv9" in namespace "gc-1293"
Jun  8 16:07:00.871: INFO: Deleting pod "simpletest.rc-xt4xd" in namespace "gc-1293"
Jun  8 16:07:00.915: INFO: Deleting pod "simpletest.rc-xzcx7" in namespace "gc-1293"
Jun  8 16:07:00.982: INFO: Deleting pod "simpletest.rc-z4c2c" in namespace "gc-1293"
Jun  8 16:07:01.017: INFO: Deleting pod "simpletest.rc-z9bv9" in namespace "gc-1293"
Jun  8 16:07:01.068: INFO: Deleting pod "simpletest.rc-zp2b8" in namespace "gc-1293"
Jun  8 16:07:01.136: INFO: Deleting pod "simpletest.rc-zwwzn" in namespace "gc-1293"
Jun  8 16:07:01.169: INFO: Deleting pod "simpletest.rc-zx77w" in namespace "gc-1293"
Jun  8 16:07:01.228: INFO: Deleting pod "simpletest.rc-zzsz5" in namespace "gc-1293"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jun  8 16:07:01.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1293" for this suite. 06/08/23 16:07:01.325
------------------------------
• [SLOW TEST] [42.894 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:06:18.465
    Jun  8 16:06:18.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename gc 06/08/23 16:06:18.466
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:06:18.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:06:18.487
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 06/08/23 16:06:18.495
    STEP: delete the rc 06/08/23 16:06:23.511
    STEP: wait for the rc to be deleted 06/08/23 16:06:23.535
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 06/08/23 16:06:28.543
    STEP: Gathering metrics 06/08/23 16:06:58.559
    Jun  8 16:06:58.589: INFO: Waiting up to 5m0s for pod "kube-controller-manager-chl8tf-control-plane-003" in namespace "kube-system" to be "running and ready"
    Jun  8 16:06:58.592: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003": Phase="Running", Reason="", readiness=true. Elapsed: 3.854039ms
    Jun  8 16:06:58.593: INFO: The phase of Pod kube-controller-manager-chl8tf-control-plane-003 is Running (Ready = true)
    Jun  8 16:06:58.593: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003" satisfied condition "running and ready"
    Jun  8 16:06:58.656: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jun  8 16:06:58.656: INFO: Deleting pod "simpletest.rc-2fj8w" in namespace "gc-1293"
    Jun  8 16:06:58.672: INFO: Deleting pod "simpletest.rc-45rs9" in namespace "gc-1293"
    Jun  8 16:06:58.683: INFO: Deleting pod "simpletest.rc-486vw" in namespace "gc-1293"
    Jun  8 16:06:58.704: INFO: Deleting pod "simpletest.rc-4c9t7" in namespace "gc-1293"
    Jun  8 16:06:58.716: INFO: Deleting pod "simpletest.rc-4d87t" in namespace "gc-1293"
    Jun  8 16:06:58.728: INFO: Deleting pod "simpletest.rc-4dz6c" in namespace "gc-1293"
    Jun  8 16:06:58.740: INFO: Deleting pod "simpletest.rc-4hg5b" in namespace "gc-1293"
    Jun  8 16:06:58.754: INFO: Deleting pod "simpletest.rc-56gn7" in namespace "gc-1293"
    Jun  8 16:06:58.770: INFO: Deleting pod "simpletest.rc-5mmdc" in namespace "gc-1293"
    Jun  8 16:06:58.783: INFO: Deleting pod "simpletest.rc-6jsc9" in namespace "gc-1293"
    Jun  8 16:06:58.812: INFO: Deleting pod "simpletest.rc-6ldd8" in namespace "gc-1293"
    Jun  8 16:06:58.825: INFO: Deleting pod "simpletest.rc-75z5j" in namespace "gc-1293"
    Jun  8 16:06:58.841: INFO: Deleting pod "simpletest.rc-7r5pz" in namespace "gc-1293"
    Jun  8 16:06:58.856: INFO: Deleting pod "simpletest.rc-7vqpj" in namespace "gc-1293"
    Jun  8 16:06:58.872: INFO: Deleting pod "simpletest.rc-7wm24" in namespace "gc-1293"
    Jun  8 16:06:58.885: INFO: Deleting pod "simpletest.rc-8z22g" in namespace "gc-1293"
    Jun  8 16:06:58.922: INFO: Deleting pod "simpletest.rc-9clws" in namespace "gc-1293"
    Jun  8 16:06:58.941: INFO: Deleting pod "simpletest.rc-9j9kz" in namespace "gc-1293"
    Jun  8 16:06:58.963: INFO: Deleting pod "simpletest.rc-9kwvt" in namespace "gc-1293"
    Jun  8 16:06:58.979: INFO: Deleting pod "simpletest.rc-9m9sf" in namespace "gc-1293"
    Jun  8 16:06:58.992: INFO: Deleting pod "simpletest.rc-9pngk" in namespace "gc-1293"
    Jun  8 16:06:59.015: INFO: Deleting pod "simpletest.rc-b7tjx" in namespace "gc-1293"
    Jun  8 16:06:59.040: INFO: Deleting pod "simpletest.rc-bqw2p" in namespace "gc-1293"
    Jun  8 16:06:59.055: INFO: Deleting pod "simpletest.rc-brqhp" in namespace "gc-1293"
    Jun  8 16:06:59.072: INFO: Deleting pod "simpletest.rc-bt289" in namespace "gc-1293"
    Jun  8 16:06:59.096: INFO: Deleting pod "simpletest.rc-c7zrh" in namespace "gc-1293"
    Jun  8 16:06:59.113: INFO: Deleting pod "simpletest.rc-c88ws" in namespace "gc-1293"
    Jun  8 16:06:59.133: INFO: Deleting pod "simpletest.rc-cb9wk" in namespace "gc-1293"
    Jun  8 16:06:59.150: INFO: Deleting pod "simpletest.rc-cf7k5" in namespace "gc-1293"
    Jun  8 16:06:59.168: INFO: Deleting pod "simpletest.rc-cltvb" in namespace "gc-1293"
    Jun  8 16:06:59.186: INFO: Deleting pod "simpletest.rc-cmw95" in namespace "gc-1293"
    Jun  8 16:06:59.209: INFO: Deleting pod "simpletest.rc-dbdw7" in namespace "gc-1293"
    Jun  8 16:06:59.247: INFO: Deleting pod "simpletest.rc-dh599" in namespace "gc-1293"
    Jun  8 16:06:59.270: INFO: Deleting pod "simpletest.rc-dz6cl" in namespace "gc-1293"
    Jun  8 16:06:59.286: INFO: Deleting pod "simpletest.rc-ff7rn" in namespace "gc-1293"
    Jun  8 16:06:59.311: INFO: Deleting pod "simpletest.rc-ff8s6" in namespace "gc-1293"
    Jun  8 16:06:59.333: INFO: Deleting pod "simpletest.rc-fqzqm" in namespace "gc-1293"
    Jun  8 16:06:59.357: INFO: Deleting pod "simpletest.rc-g8kv5" in namespace "gc-1293"
    Jun  8 16:06:59.375: INFO: Deleting pod "simpletest.rc-g9kzt" in namespace "gc-1293"
    Jun  8 16:06:59.400: INFO: Deleting pod "simpletest.rc-ghkqn" in namespace "gc-1293"
    Jun  8 16:06:59.429: INFO: Deleting pod "simpletest.rc-gqhpx" in namespace "gc-1293"
    Jun  8 16:06:59.452: INFO: Deleting pod "simpletest.rc-h26wp" in namespace "gc-1293"
    Jun  8 16:06:59.478: INFO: Deleting pod "simpletest.rc-h7hjc" in namespace "gc-1293"
    Jun  8 16:06:59.499: INFO: Deleting pod "simpletest.rc-hbdl7" in namespace "gc-1293"
    Jun  8 16:06:59.519: INFO: Deleting pod "simpletest.rc-hd8bq" in namespace "gc-1293"
    Jun  8 16:06:59.538: INFO: Deleting pod "simpletest.rc-hfm6g" in namespace "gc-1293"
    Jun  8 16:06:59.573: INFO: Deleting pod "simpletest.rc-hllzw" in namespace "gc-1293"
    Jun  8 16:06:59.595: INFO: Deleting pod "simpletest.rc-hmcvt" in namespace "gc-1293"
    Jun  8 16:06:59.618: INFO: Deleting pod "simpletest.rc-j4pzb" in namespace "gc-1293"
    Jun  8 16:06:59.651: INFO: Deleting pod "simpletest.rc-k6lrc" in namespace "gc-1293"
    Jun  8 16:06:59.693: INFO: Deleting pod "simpletest.rc-kqz4k" in namespace "gc-1293"
    Jun  8 16:06:59.720: INFO: Deleting pod "simpletest.rc-krdfx" in namespace "gc-1293"
    Jun  8 16:06:59.737: INFO: Deleting pod "simpletest.rc-l4xlh" in namespace "gc-1293"
    Jun  8 16:06:59.757: INFO: Deleting pod "simpletest.rc-lcnfm" in namespace "gc-1293"
    Jun  8 16:06:59.774: INFO: Deleting pod "simpletest.rc-lhnqd" in namespace "gc-1293"
    Jun  8 16:06:59.791: INFO: Deleting pod "simpletest.rc-lprjg" in namespace "gc-1293"
    Jun  8 16:06:59.809: INFO: Deleting pod "simpletest.rc-mqvfw" in namespace "gc-1293"
    Jun  8 16:06:59.844: INFO: Deleting pod "simpletest.rc-ns2hx" in namespace "gc-1293"
    Jun  8 16:06:59.863: INFO: Deleting pod "simpletest.rc-q2jjw" in namespace "gc-1293"
    Jun  8 16:06:59.882: INFO: Deleting pod "simpletest.rc-q7d8j" in namespace "gc-1293"
    Jun  8 16:06:59.910: INFO: Deleting pod "simpletest.rc-qnlbk" in namespace "gc-1293"
    Jun  8 16:06:59.968: INFO: Deleting pod "simpletest.rc-qpf9n" in namespace "gc-1293"
    Jun  8 16:06:59.987: INFO: Deleting pod "simpletest.rc-qpw8c" in namespace "gc-1293"
    Jun  8 16:07:00.015: INFO: Deleting pod "simpletest.rc-qr492" in namespace "gc-1293"
    Jun  8 16:07:00.036: INFO: Deleting pod "simpletest.rc-rd9tp" in namespace "gc-1293"
    Jun  8 16:07:00.059: INFO: Deleting pod "simpletest.rc-rfxhl" in namespace "gc-1293"
    Jun  8 16:07:00.088: INFO: Deleting pod "simpletest.rc-rpgst" in namespace "gc-1293"
    Jun  8 16:07:00.108: INFO: Deleting pod "simpletest.rc-rqsj9" in namespace "gc-1293"
    Jun  8 16:07:00.134: INFO: Deleting pod "simpletest.rc-rr7zv" in namespace "gc-1293"
    Jun  8 16:07:00.169: INFO: Deleting pod "simpletest.rc-rxxlc" in namespace "gc-1293"
    Jun  8 16:07:00.199: INFO: Deleting pod "simpletest.rc-skldc" in namespace "gc-1293"
    Jun  8 16:07:00.221: INFO: Deleting pod "simpletest.rc-spq8d" in namespace "gc-1293"
    Jun  8 16:07:00.244: INFO: Deleting pod "simpletest.rc-sww4x" in namespace "gc-1293"
    Jun  8 16:07:00.264: INFO: Deleting pod "simpletest.rc-t4v8f" in namespace "gc-1293"
    Jun  8 16:07:00.295: INFO: Deleting pod "simpletest.rc-t5t9g" in namespace "gc-1293"
    Jun  8 16:07:00.314: INFO: Deleting pod "simpletest.rc-t64pg" in namespace "gc-1293"
    Jun  8 16:07:00.338: INFO: Deleting pod "simpletest.rc-tlzqz" in namespace "gc-1293"
    Jun  8 16:07:00.356: INFO: Deleting pod "simpletest.rc-tr9k2" in namespace "gc-1293"
    Jun  8 16:07:00.378: INFO: Deleting pod "simpletest.rc-tszmx" in namespace "gc-1293"
    Jun  8 16:07:00.401: INFO: Deleting pod "simpletest.rc-v7vj4" in namespace "gc-1293"
    Jun  8 16:07:00.433: INFO: Deleting pod "simpletest.rc-v9kn8" in namespace "gc-1293"
    Jun  8 16:07:00.460: INFO: Deleting pod "simpletest.rc-vbnfp" in namespace "gc-1293"
    Jun  8 16:07:00.479: INFO: Deleting pod "simpletest.rc-vnfqm" in namespace "gc-1293"
    Jun  8 16:07:00.497: INFO: Deleting pod "simpletest.rc-vsnch" in namespace "gc-1293"
    Jun  8 16:07:00.532: INFO: Deleting pod "simpletest.rc-vsvvt" in namespace "gc-1293"
    Jun  8 16:07:00.572: INFO: Deleting pod "simpletest.rc-w5klv" in namespace "gc-1293"
    Jun  8 16:07:00.644: INFO: Deleting pod "simpletest.rc-wsrm4" in namespace "gc-1293"
    Jun  8 16:07:00.669: INFO: Deleting pod "simpletest.rc-x5nr7" in namespace "gc-1293"
    Jun  8 16:07:00.695: INFO: Deleting pod "simpletest.rc-x75h9" in namespace "gc-1293"
    Jun  8 16:07:00.719: INFO: Deleting pod "simpletest.rc-xc6c2" in namespace "gc-1293"
    Jun  8 16:07:00.783: INFO: Deleting pod "simpletest.rc-xgnrn" in namespace "gc-1293"
    Jun  8 16:07:00.833: INFO: Deleting pod "simpletest.rc-xrjv9" in namespace "gc-1293"
    Jun  8 16:07:00.871: INFO: Deleting pod "simpletest.rc-xt4xd" in namespace "gc-1293"
    Jun  8 16:07:00.915: INFO: Deleting pod "simpletest.rc-xzcx7" in namespace "gc-1293"
    Jun  8 16:07:00.982: INFO: Deleting pod "simpletest.rc-z4c2c" in namespace "gc-1293"
    Jun  8 16:07:01.017: INFO: Deleting pod "simpletest.rc-z9bv9" in namespace "gc-1293"
    Jun  8 16:07:01.068: INFO: Deleting pod "simpletest.rc-zp2b8" in namespace "gc-1293"
    Jun  8 16:07:01.136: INFO: Deleting pod "simpletest.rc-zwwzn" in namespace "gc-1293"
    Jun  8 16:07:01.169: INFO: Deleting pod "simpletest.rc-zx77w" in namespace "gc-1293"
    Jun  8 16:07:01.228: INFO: Deleting pod "simpletest.rc-zzsz5" in namespace "gc-1293"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:07:01.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1293" for this suite. 06/08/23 16:07:01.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:07:01.365
Jun  8 16:07:01.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename daemonsets 06/08/23 16:07:01.37
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:07:01.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:07:01.432
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Jun  8 16:07:01.512: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 06/08/23 16:07:01.526
Jun  8 16:07:01.547: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  8 16:07:01.547: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
Jun  8 16:07:02.565: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  8 16:07:02.565: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
Jun  8 16:07:03.557: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  8 16:07:03.557: INFO: Node chl8tf-control-plane-002 is running 0 daemon pod, expected 1
Jun  8 16:07:04.560: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jun  8 16:07:04.560: INFO: Node chl8tf-control-plane-003 is running 0 daemon pod, expected 1
Jun  8 16:07:05.558: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jun  8 16:07:05.558: INFO: Node chl8tf-control-plane-003 is running 0 daemon pod, expected 1
Jun  8 16:07:06.559: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jun  8 16:07:06.559: INFO: Node chl8tf-control-plane-003 is running 0 daemon pod, expected 1
Jun  8 16:07:07.559: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jun  8 16:07:07.559: INFO: Node chl8tf-control-plane-003 is running 0 daemon pod, expected 1
Jun  8 16:07:08.560: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jun  8 16:07:08.560: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Update daemon pods image. 06/08/23 16:07:08.577
STEP: Check that daemon pods images are updated. 06/08/23 16:07:08.594
Jun  8 16:07:08.598: INFO: Wrong image for pod: daemon-set-dztd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:08.598: INFO: Wrong image for pod: daemon-set-m6c4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:08.598: INFO: Wrong image for pod: daemon-set-qtfzk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:08.598: INFO: Wrong image for pod: daemon-set-t5c47. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:08.598: INFO: Wrong image for pod: daemon-set-z7dn7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:09.612: INFO: Wrong image for pod: daemon-set-m6c4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:09.612: INFO: Pod daemon-set-nd9mf is not available
Jun  8 16:07:09.612: INFO: Wrong image for pod: daemon-set-qtfzk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:09.612: INFO: Wrong image for pod: daemon-set-t5c47. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:09.612: INFO: Wrong image for pod: daemon-set-z7dn7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:10.611: INFO: Wrong image for pod: daemon-set-m6c4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:10.611: INFO: Pod daemon-set-nd9mf is not available
Jun  8 16:07:10.611: INFO: Wrong image for pod: daemon-set-qtfzk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:10.611: INFO: Wrong image for pod: daemon-set-t5c47. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:10.611: INFO: Wrong image for pod: daemon-set-z7dn7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:11.610: INFO: Wrong image for pod: daemon-set-m6c4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:11.611: INFO: Wrong image for pod: daemon-set-qtfzk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:11.611: INFO: Wrong image for pod: daemon-set-z7dn7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:12.611: INFO: Wrong image for pod: daemon-set-m6c4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:12.611: INFO: Wrong image for pod: daemon-set-qtfzk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:12.611: INFO: Pod daemon-set-z7bfp is not available
Jun  8 16:07:12.611: INFO: Wrong image for pod: daemon-set-z7dn7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:13.611: INFO: Wrong image for pod: daemon-set-m6c4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:13.611: INFO: Wrong image for pod: daemon-set-qtfzk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:13.611: INFO: Pod daemon-set-v8blf is not available
Jun  8 16:07:14.612: INFO: Wrong image for pod: daemon-set-m6c4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:15.611: INFO: Wrong image for pod: daemon-set-m6c4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  8 16:07:15.611: INFO: Pod daemon-set-n7brw is not available
Jun  8 16:07:17.611: INFO: Pod daemon-set-qzb9m is not available
STEP: Check that daemon pods are still running on every node of the cluster. 06/08/23 16:07:17.617
Jun  8 16:07:17.626: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jun  8 16:07:17.626: INFO: Node chl8tf-worker-001 is running 0 daemon pod, expected 1
Jun  8 16:07:18.638: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jun  8 16:07:18.638: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 06/08/23 16:07:18.657
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-638, will wait for the garbage collector to delete the pods 06/08/23 16:07:18.657
Jun  8 16:07:18.721: INFO: Deleting DaemonSet.extensions daemon-set took: 8.749829ms
Jun  8 16:07:18.821: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.709795ms
Jun  8 16:07:20.827: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  8 16:07:20.827: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun  8 16:07:20.830: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46156"},"items":null}

Jun  8 16:07:20.835: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46156"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:07:20.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-638" for this suite. 06/08/23 16:07:20.865
------------------------------
• [SLOW TEST] [19.507 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:07:01.365
    Jun  8 16:07:01.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename daemonsets 06/08/23 16:07:01.37
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:07:01.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:07:01.432
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Jun  8 16:07:01.512: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 06/08/23 16:07:01.526
    Jun  8 16:07:01.547: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  8 16:07:01.547: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
    Jun  8 16:07:02.565: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  8 16:07:02.565: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
    Jun  8 16:07:03.557: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun  8 16:07:03.557: INFO: Node chl8tf-control-plane-002 is running 0 daemon pod, expected 1
    Jun  8 16:07:04.560: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jun  8 16:07:04.560: INFO: Node chl8tf-control-plane-003 is running 0 daemon pod, expected 1
    Jun  8 16:07:05.558: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jun  8 16:07:05.558: INFO: Node chl8tf-control-plane-003 is running 0 daemon pod, expected 1
    Jun  8 16:07:06.559: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jun  8 16:07:06.559: INFO: Node chl8tf-control-plane-003 is running 0 daemon pod, expected 1
    Jun  8 16:07:07.559: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jun  8 16:07:07.559: INFO: Node chl8tf-control-plane-003 is running 0 daemon pod, expected 1
    Jun  8 16:07:08.560: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jun  8 16:07:08.560: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Update daemon pods image. 06/08/23 16:07:08.577
    STEP: Check that daemon pods images are updated. 06/08/23 16:07:08.594
    Jun  8 16:07:08.598: INFO: Wrong image for pod: daemon-set-dztd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:08.598: INFO: Wrong image for pod: daemon-set-m6c4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:08.598: INFO: Wrong image for pod: daemon-set-qtfzk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:08.598: INFO: Wrong image for pod: daemon-set-t5c47. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:08.598: INFO: Wrong image for pod: daemon-set-z7dn7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:09.612: INFO: Wrong image for pod: daemon-set-m6c4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:09.612: INFO: Pod daemon-set-nd9mf is not available
    Jun  8 16:07:09.612: INFO: Wrong image for pod: daemon-set-qtfzk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:09.612: INFO: Wrong image for pod: daemon-set-t5c47. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:09.612: INFO: Wrong image for pod: daemon-set-z7dn7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:10.611: INFO: Wrong image for pod: daemon-set-m6c4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:10.611: INFO: Pod daemon-set-nd9mf is not available
    Jun  8 16:07:10.611: INFO: Wrong image for pod: daemon-set-qtfzk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:10.611: INFO: Wrong image for pod: daemon-set-t5c47. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:10.611: INFO: Wrong image for pod: daemon-set-z7dn7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:11.610: INFO: Wrong image for pod: daemon-set-m6c4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:11.611: INFO: Wrong image for pod: daemon-set-qtfzk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:11.611: INFO: Wrong image for pod: daemon-set-z7dn7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:12.611: INFO: Wrong image for pod: daemon-set-m6c4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:12.611: INFO: Wrong image for pod: daemon-set-qtfzk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:12.611: INFO: Pod daemon-set-z7bfp is not available
    Jun  8 16:07:12.611: INFO: Wrong image for pod: daemon-set-z7dn7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:13.611: INFO: Wrong image for pod: daemon-set-m6c4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:13.611: INFO: Wrong image for pod: daemon-set-qtfzk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:13.611: INFO: Pod daemon-set-v8blf is not available
    Jun  8 16:07:14.612: INFO: Wrong image for pod: daemon-set-m6c4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:15.611: INFO: Wrong image for pod: daemon-set-m6c4f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  8 16:07:15.611: INFO: Pod daemon-set-n7brw is not available
    Jun  8 16:07:17.611: INFO: Pod daemon-set-qzb9m is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 06/08/23 16:07:17.617
    Jun  8 16:07:17.626: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jun  8 16:07:17.626: INFO: Node chl8tf-worker-001 is running 0 daemon pod, expected 1
    Jun  8 16:07:18.638: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jun  8 16:07:18.638: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 06/08/23 16:07:18.657
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-638, will wait for the garbage collector to delete the pods 06/08/23 16:07:18.657
    Jun  8 16:07:18.721: INFO: Deleting DaemonSet.extensions daemon-set took: 8.749829ms
    Jun  8 16:07:18.821: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.709795ms
    Jun  8 16:07:20.827: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  8 16:07:20.827: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun  8 16:07:20.830: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46156"},"items":null}

    Jun  8 16:07:20.835: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46156"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:07:20.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-638" for this suite. 06/08/23 16:07:20.865
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:07:20.872
Jun  8 16:07:20.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename daemonsets 06/08/23 16:07:20.874
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:07:20.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:07:20.896
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Jun  8 16:07:20.928: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 06/08/23 16:07:20.934
Jun  8 16:07:20.937: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  8 16:07:20.937: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 06/08/23 16:07:20.937
Jun  8 16:07:20.960: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  8 16:07:20.961: INFO: Node chl8tf-worker-001 is running 0 daemon pod, expected 1
Jun  8 16:07:21.965: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  8 16:07:21.965: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 06/08/23 16:07:21.969
Jun  8 16:07:21.988: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  8 16:07:21.988: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jun  8 16:07:22.994: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  8 16:07:22.994: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 06/08/23 16:07:22.994
Jun  8 16:07:23.005: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  8 16:07:23.005: INFO: Node chl8tf-worker-001 is running 0 daemon pod, expected 1
Jun  8 16:07:24.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  8 16:07:24.010: INFO: Node chl8tf-worker-001 is running 0 daemon pod, expected 1
Jun  8 16:07:25.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  8 16:07:25.011: INFO: Node chl8tf-worker-001 is running 0 daemon pod, expected 1
Jun  8 16:07:26.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  8 16:07:26.011: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 06/08/23 16:07:26.019
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5778, will wait for the garbage collector to delete the pods 06/08/23 16:07:26.019
Jun  8 16:07:26.081: INFO: Deleting DaemonSet.extensions daemon-set took: 7.431751ms
Jun  8 16:07:26.182: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.377588ms
Jun  8 16:07:28.787: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  8 16:07:28.787: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun  8 16:07:28.790: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46306"},"items":null}

Jun  8 16:07:28.794: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46306"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:07:28.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5778" for this suite. 06/08/23 16:07:28.836
------------------------------
• [SLOW TEST] [7.970 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:07:20.872
    Jun  8 16:07:20.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename daemonsets 06/08/23 16:07:20.874
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:07:20.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:07:20.896
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Jun  8 16:07:20.928: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 06/08/23 16:07:20.934
    Jun  8 16:07:20.937: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  8 16:07:20.937: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 06/08/23 16:07:20.937
    Jun  8 16:07:20.960: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  8 16:07:20.961: INFO: Node chl8tf-worker-001 is running 0 daemon pod, expected 1
    Jun  8 16:07:21.965: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  8 16:07:21.965: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 06/08/23 16:07:21.969
    Jun  8 16:07:21.988: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  8 16:07:21.988: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jun  8 16:07:22.994: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  8 16:07:22.994: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 06/08/23 16:07:22.994
    Jun  8 16:07:23.005: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  8 16:07:23.005: INFO: Node chl8tf-worker-001 is running 0 daemon pod, expected 1
    Jun  8 16:07:24.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  8 16:07:24.010: INFO: Node chl8tf-worker-001 is running 0 daemon pod, expected 1
    Jun  8 16:07:25.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  8 16:07:25.011: INFO: Node chl8tf-worker-001 is running 0 daemon pod, expected 1
    Jun  8 16:07:26.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  8 16:07:26.011: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 06/08/23 16:07:26.019
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5778, will wait for the garbage collector to delete the pods 06/08/23 16:07:26.019
    Jun  8 16:07:26.081: INFO: Deleting DaemonSet.extensions daemon-set took: 7.431751ms
    Jun  8 16:07:26.182: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.377588ms
    Jun  8 16:07:28.787: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  8 16:07:28.787: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun  8 16:07:28.790: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46306"},"items":null}

    Jun  8 16:07:28.794: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46306"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:07:28.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5778" for this suite. 06/08/23 16:07:28.836
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:07:28.843
Jun  8 16:07:28.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename container-runtime 06/08/23 16:07:28.845
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:07:28.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:07:28.87
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 06/08/23 16:07:28.874
STEP: wait for the container to reach Succeeded 06/08/23 16:07:28.884
STEP: get the container status 06/08/23 16:07:31.906
STEP: the container should be terminated 06/08/23 16:07:31.91
STEP: the termination message should be set 06/08/23 16:07:31.91
Jun  8 16:07:31.910: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 06/08/23 16:07:31.91
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jun  8 16:07:31.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6088" for this suite. 06/08/23 16:07:31.933
------------------------------
• [3.097 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:07:28.843
    Jun  8 16:07:28.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename container-runtime 06/08/23 16:07:28.845
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:07:28.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:07:28.87
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 06/08/23 16:07:28.874
    STEP: wait for the container to reach Succeeded 06/08/23 16:07:28.884
    STEP: get the container status 06/08/23 16:07:31.906
    STEP: the container should be terminated 06/08/23 16:07:31.91
    STEP: the termination message should be set 06/08/23 16:07:31.91
    Jun  8 16:07:31.910: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 06/08/23 16:07:31.91
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:07:31.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6088" for this suite. 06/08/23 16:07:31.933
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:07:31.942
Jun  8 16:07:31.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename var-expansion 06/08/23 16:07:31.943
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:07:31.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:07:31.964
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 06/08/23 16:07:31.967
Jun  8 16:07:31.977: INFO: Waiting up to 5m0s for pod "var-expansion-f7267df0-0b8d-4c30-a5d7-a34020b6d970" in namespace "var-expansion-4389" to be "Succeeded or Failed"
Jun  8 16:07:31.980: INFO: Pod "var-expansion-f7267df0-0b8d-4c30-a5d7-a34020b6d970": Phase="Pending", Reason="", readiness=false. Elapsed: 3.581832ms
Jun  8 16:07:33.986: INFO: Pod "var-expansion-f7267df0-0b8d-4c30-a5d7-a34020b6d970": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009197288s
Jun  8 16:07:35.985: INFO: Pod "var-expansion-f7267df0-0b8d-4c30-a5d7-a34020b6d970": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008780218s
STEP: Saw pod success 06/08/23 16:07:35.985
Jun  8 16:07:35.986: INFO: Pod "var-expansion-f7267df0-0b8d-4c30-a5d7-a34020b6d970" satisfied condition "Succeeded or Failed"
Jun  8 16:07:35.990: INFO: Trying to get logs from node chl8tf-worker-001 pod var-expansion-f7267df0-0b8d-4c30-a5d7-a34020b6d970 container dapi-container: <nil>
STEP: delete the pod 06/08/23 16:07:36.008
Jun  8 16:07:36.020: INFO: Waiting for pod var-expansion-f7267df0-0b8d-4c30-a5d7-a34020b6d970 to disappear
Jun  8 16:07:36.024: INFO: Pod var-expansion-f7267df0-0b8d-4c30-a5d7-a34020b6d970 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun  8 16:07:36.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4389" for this suite. 06/08/23 16:07:36.03
------------------------------
• [4.096 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:07:31.942
    Jun  8 16:07:31.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename var-expansion 06/08/23 16:07:31.943
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:07:31.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:07:31.964
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 06/08/23 16:07:31.967
    Jun  8 16:07:31.977: INFO: Waiting up to 5m0s for pod "var-expansion-f7267df0-0b8d-4c30-a5d7-a34020b6d970" in namespace "var-expansion-4389" to be "Succeeded or Failed"
    Jun  8 16:07:31.980: INFO: Pod "var-expansion-f7267df0-0b8d-4c30-a5d7-a34020b6d970": Phase="Pending", Reason="", readiness=false. Elapsed: 3.581832ms
    Jun  8 16:07:33.986: INFO: Pod "var-expansion-f7267df0-0b8d-4c30-a5d7-a34020b6d970": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009197288s
    Jun  8 16:07:35.985: INFO: Pod "var-expansion-f7267df0-0b8d-4c30-a5d7-a34020b6d970": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008780218s
    STEP: Saw pod success 06/08/23 16:07:35.985
    Jun  8 16:07:35.986: INFO: Pod "var-expansion-f7267df0-0b8d-4c30-a5d7-a34020b6d970" satisfied condition "Succeeded or Failed"
    Jun  8 16:07:35.990: INFO: Trying to get logs from node chl8tf-worker-001 pod var-expansion-f7267df0-0b8d-4c30-a5d7-a34020b6d970 container dapi-container: <nil>
    STEP: delete the pod 06/08/23 16:07:36.008
    Jun  8 16:07:36.020: INFO: Waiting for pod var-expansion-f7267df0-0b8d-4c30-a5d7-a34020b6d970 to disappear
    Jun  8 16:07:36.024: INFO: Pod var-expansion-f7267df0-0b8d-4c30-a5d7-a34020b6d970 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:07:36.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4389" for this suite. 06/08/23 16:07:36.03
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:07:36.039
Jun  8 16:07:36.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename downward-api 06/08/23 16:07:36.04
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:07:36.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:07:36.06
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 06/08/23 16:07:36.063
Jun  8 16:07:36.073: INFO: Waiting up to 5m0s for pod "downward-api-abc51aca-14f9-4e5e-82b7-ba01fb8eb7c0" in namespace "downward-api-2254" to be "Succeeded or Failed"
Jun  8 16:07:36.077: INFO: Pod "downward-api-abc51aca-14f9-4e5e-82b7-ba01fb8eb7c0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.750432ms
Jun  8 16:07:38.082: INFO: Pod "downward-api-abc51aca-14f9-4e5e-82b7-ba01fb8eb7c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009304469s
Jun  8 16:07:40.083: INFO: Pod "downward-api-abc51aca-14f9-4e5e-82b7-ba01fb8eb7c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009721074s
STEP: Saw pod success 06/08/23 16:07:40.083
Jun  8 16:07:40.083: INFO: Pod "downward-api-abc51aca-14f9-4e5e-82b7-ba01fb8eb7c0" satisfied condition "Succeeded or Failed"
Jun  8 16:07:40.087: INFO: Trying to get logs from node chl8tf-worker-001 pod downward-api-abc51aca-14f9-4e5e-82b7-ba01fb8eb7c0 container dapi-container: <nil>
STEP: delete the pod 06/08/23 16:07:40.096
Jun  8 16:07:40.112: INFO: Waiting for pod downward-api-abc51aca-14f9-4e5e-82b7-ba01fb8eb7c0 to disappear
Jun  8 16:07:40.115: INFO: Pod downward-api-abc51aca-14f9-4e5e-82b7-ba01fb8eb7c0 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jun  8 16:07:40.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2254" for this suite. 06/08/23 16:07:40.121
------------------------------
• [4.090 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:07:36.039
    Jun  8 16:07:36.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename downward-api 06/08/23 16:07:36.04
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:07:36.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:07:36.06
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 06/08/23 16:07:36.063
    Jun  8 16:07:36.073: INFO: Waiting up to 5m0s for pod "downward-api-abc51aca-14f9-4e5e-82b7-ba01fb8eb7c0" in namespace "downward-api-2254" to be "Succeeded or Failed"
    Jun  8 16:07:36.077: INFO: Pod "downward-api-abc51aca-14f9-4e5e-82b7-ba01fb8eb7c0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.750432ms
    Jun  8 16:07:38.082: INFO: Pod "downward-api-abc51aca-14f9-4e5e-82b7-ba01fb8eb7c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009304469s
    Jun  8 16:07:40.083: INFO: Pod "downward-api-abc51aca-14f9-4e5e-82b7-ba01fb8eb7c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009721074s
    STEP: Saw pod success 06/08/23 16:07:40.083
    Jun  8 16:07:40.083: INFO: Pod "downward-api-abc51aca-14f9-4e5e-82b7-ba01fb8eb7c0" satisfied condition "Succeeded or Failed"
    Jun  8 16:07:40.087: INFO: Trying to get logs from node chl8tf-worker-001 pod downward-api-abc51aca-14f9-4e5e-82b7-ba01fb8eb7c0 container dapi-container: <nil>
    STEP: delete the pod 06/08/23 16:07:40.096
    Jun  8 16:07:40.112: INFO: Waiting for pod downward-api-abc51aca-14f9-4e5e-82b7-ba01fb8eb7c0 to disappear
    Jun  8 16:07:40.115: INFO: Pod downward-api-abc51aca-14f9-4e5e-82b7-ba01fb8eb7c0 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:07:40.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2254" for this suite. 06/08/23 16:07:40.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:07:40.13
Jun  8 16:07:40.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename configmap 06/08/23 16:07:40.131
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:07:40.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:07:40.151
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-76ffd59b-9630-4a24-aa66-924bdd55695f 06/08/23 16:07:40.161
STEP: Creating the pod 06/08/23 16:07:40.166
Jun  8 16:07:40.179: INFO: Waiting up to 5m0s for pod "pod-configmaps-94156f7c-7dbf-4ded-bcec-264453392b0b" in namespace "configmap-5488" to be "running and ready"
Jun  8 16:07:40.184: INFO: Pod "pod-configmaps-94156f7c-7dbf-4ded-bcec-264453392b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.812692ms
Jun  8 16:07:40.184: INFO: The phase of Pod pod-configmaps-94156f7c-7dbf-4ded-bcec-264453392b0b is Pending, waiting for it to be Running (with Ready = true)
Jun  8 16:07:42.190: INFO: Pod "pod-configmaps-94156f7c-7dbf-4ded-bcec-264453392b0b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010282682s
Jun  8 16:07:42.190: INFO: The phase of Pod pod-configmaps-94156f7c-7dbf-4ded-bcec-264453392b0b is Running (Ready = true)
Jun  8 16:07:42.190: INFO: Pod "pod-configmaps-94156f7c-7dbf-4ded-bcec-264453392b0b" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-76ffd59b-9630-4a24-aa66-924bdd55695f 06/08/23 16:07:42.203
STEP: waiting to observe update in volume 06/08/23 16:07:42.21
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  8 16:08:48.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5488" for this suite. 06/08/23 16:08:48.58
------------------------------
• [SLOW TEST] [68.459 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:07:40.13
    Jun  8 16:07:40.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename configmap 06/08/23 16:07:40.131
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:07:40.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:07:40.151
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-76ffd59b-9630-4a24-aa66-924bdd55695f 06/08/23 16:07:40.161
    STEP: Creating the pod 06/08/23 16:07:40.166
    Jun  8 16:07:40.179: INFO: Waiting up to 5m0s for pod "pod-configmaps-94156f7c-7dbf-4ded-bcec-264453392b0b" in namespace "configmap-5488" to be "running and ready"
    Jun  8 16:07:40.184: INFO: Pod "pod-configmaps-94156f7c-7dbf-4ded-bcec-264453392b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.812692ms
    Jun  8 16:07:40.184: INFO: The phase of Pod pod-configmaps-94156f7c-7dbf-4ded-bcec-264453392b0b is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 16:07:42.190: INFO: Pod "pod-configmaps-94156f7c-7dbf-4ded-bcec-264453392b0b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010282682s
    Jun  8 16:07:42.190: INFO: The phase of Pod pod-configmaps-94156f7c-7dbf-4ded-bcec-264453392b0b is Running (Ready = true)
    Jun  8 16:07:42.190: INFO: Pod "pod-configmaps-94156f7c-7dbf-4ded-bcec-264453392b0b" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-76ffd59b-9630-4a24-aa66-924bdd55695f 06/08/23 16:07:42.203
    STEP: waiting to observe update in volume 06/08/23 16:07:42.21
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:08:48.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5488" for this suite. 06/08/23 16:08:48.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:08:48.589
Jun  8 16:08:48.589: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename downward-api 06/08/23 16:08:48.59
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:08:48.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:08:48.612
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 06/08/23 16:08:48.616
Jun  8 16:08:48.625: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2a33baa3-4090-4ea4-98ce-af2daea5c4b1" in namespace "downward-api-5421" to be "Succeeded or Failed"
Jun  8 16:08:48.629: INFO: Pod "downwardapi-volume-2a33baa3-4090-4ea4-98ce-af2daea5c4b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.78315ms
Jun  8 16:08:50.634: INFO: Pod "downwardapi-volume-2a33baa3-4090-4ea4-98ce-af2daea5c4b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009110153s
Jun  8 16:08:52.635: INFO: Pod "downwardapi-volume-2a33baa3-4090-4ea4-98ce-af2daea5c4b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009459256s
STEP: Saw pod success 06/08/23 16:08:52.635
Jun  8 16:08:52.635: INFO: Pod "downwardapi-volume-2a33baa3-4090-4ea4-98ce-af2daea5c4b1" satisfied condition "Succeeded or Failed"
Jun  8 16:08:52.639: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-2a33baa3-4090-4ea4-98ce-af2daea5c4b1 container client-container: <nil>
STEP: delete the pod 06/08/23 16:08:52.646
Jun  8 16:08:52.658: INFO: Waiting for pod downwardapi-volume-2a33baa3-4090-4ea4-98ce-af2daea5c4b1 to disappear
Jun  8 16:08:52.663: INFO: Pod downwardapi-volume-2a33baa3-4090-4ea4-98ce-af2daea5c4b1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  8 16:08:52.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5421" for this suite. 06/08/23 16:08:52.669
------------------------------
• [4.087 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:08:48.589
    Jun  8 16:08:48.589: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename downward-api 06/08/23 16:08:48.59
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:08:48.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:08:48.612
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 06/08/23 16:08:48.616
    Jun  8 16:08:48.625: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2a33baa3-4090-4ea4-98ce-af2daea5c4b1" in namespace "downward-api-5421" to be "Succeeded or Failed"
    Jun  8 16:08:48.629: INFO: Pod "downwardapi-volume-2a33baa3-4090-4ea4-98ce-af2daea5c4b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.78315ms
    Jun  8 16:08:50.634: INFO: Pod "downwardapi-volume-2a33baa3-4090-4ea4-98ce-af2daea5c4b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009110153s
    Jun  8 16:08:52.635: INFO: Pod "downwardapi-volume-2a33baa3-4090-4ea4-98ce-af2daea5c4b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009459256s
    STEP: Saw pod success 06/08/23 16:08:52.635
    Jun  8 16:08:52.635: INFO: Pod "downwardapi-volume-2a33baa3-4090-4ea4-98ce-af2daea5c4b1" satisfied condition "Succeeded or Failed"
    Jun  8 16:08:52.639: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-2a33baa3-4090-4ea4-98ce-af2daea5c4b1 container client-container: <nil>
    STEP: delete the pod 06/08/23 16:08:52.646
    Jun  8 16:08:52.658: INFO: Waiting for pod downwardapi-volume-2a33baa3-4090-4ea4-98ce-af2daea5c4b1 to disappear
    Jun  8 16:08:52.663: INFO: Pod downwardapi-volume-2a33baa3-4090-4ea4-98ce-af2daea5c4b1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:08:52.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5421" for this suite. 06/08/23 16:08:52.669
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:08:52.677
Jun  8 16:08:52.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename statefulset 06/08/23 16:08:52.678
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:08:52.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:08:52.699
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-227 06/08/23 16:08:52.703
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 06/08/23 16:08:52.709
STEP: Creating pod with conflicting port in namespace statefulset-227 06/08/23 16:08:52.716
STEP: Waiting until pod test-pod will start running in namespace statefulset-227 06/08/23 16:08:52.725
Jun  8 16:08:52.725: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-227" to be "running"
Jun  8 16:08:52.729: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.548579ms
Jun  8 16:08:54.733: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008048767s
Jun  8 16:08:54.733: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-227 06/08/23 16:08:54.733
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-227 06/08/23 16:08:54.74
Jun  8 16:08:54.757: INFO: Observed stateful pod in namespace: statefulset-227, name: ss-0, uid: e7c610bc-109b-4127-9e19-06d264df390d, status phase: Pending. Waiting for statefulset controller to delete.
Jun  8 16:08:54.770: INFO: Observed stateful pod in namespace: statefulset-227, name: ss-0, uid: e7c610bc-109b-4127-9e19-06d264df390d, status phase: Failed. Waiting for statefulset controller to delete.
Jun  8 16:08:54.781: INFO: Observed stateful pod in namespace: statefulset-227, name: ss-0, uid: e7c610bc-109b-4127-9e19-06d264df390d, status phase: Failed. Waiting for statefulset controller to delete.
Jun  8 16:08:54.785: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-227
STEP: Removing pod with conflicting port in namespace statefulset-227 06/08/23 16:08:54.785
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-227 and will be in running state 06/08/23 16:08:54.8
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun  8 16:09:08.844: INFO: Deleting all statefulset in ns statefulset-227
Jun  8 16:09:08.848: INFO: Scaling statefulset ss to 0
Jun  8 16:09:18.868: INFO: Waiting for statefulset status.replicas updated to 0
Jun  8 16:09:18.872: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun  8 16:09:18.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-227" for this suite. 06/08/23 16:09:18.892
------------------------------
• [SLOW TEST] [26.223 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:08:52.677
    Jun  8 16:08:52.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename statefulset 06/08/23 16:08:52.678
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:08:52.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:08:52.699
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-227 06/08/23 16:08:52.703
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 06/08/23 16:08:52.709
    STEP: Creating pod with conflicting port in namespace statefulset-227 06/08/23 16:08:52.716
    STEP: Waiting until pod test-pod will start running in namespace statefulset-227 06/08/23 16:08:52.725
    Jun  8 16:08:52.725: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-227" to be "running"
    Jun  8 16:08:52.729: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.548579ms
    Jun  8 16:08:54.733: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008048767s
    Jun  8 16:08:54.733: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-227 06/08/23 16:08:54.733
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-227 06/08/23 16:08:54.74
    Jun  8 16:08:54.757: INFO: Observed stateful pod in namespace: statefulset-227, name: ss-0, uid: e7c610bc-109b-4127-9e19-06d264df390d, status phase: Pending. Waiting for statefulset controller to delete.
    Jun  8 16:08:54.770: INFO: Observed stateful pod in namespace: statefulset-227, name: ss-0, uid: e7c610bc-109b-4127-9e19-06d264df390d, status phase: Failed. Waiting for statefulset controller to delete.
    Jun  8 16:08:54.781: INFO: Observed stateful pod in namespace: statefulset-227, name: ss-0, uid: e7c610bc-109b-4127-9e19-06d264df390d, status phase: Failed. Waiting for statefulset controller to delete.
    Jun  8 16:08:54.785: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-227
    STEP: Removing pod with conflicting port in namespace statefulset-227 06/08/23 16:08:54.785
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-227 and will be in running state 06/08/23 16:08:54.8
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun  8 16:09:08.844: INFO: Deleting all statefulset in ns statefulset-227
    Jun  8 16:09:08.848: INFO: Scaling statefulset ss to 0
    Jun  8 16:09:18.868: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  8 16:09:18.872: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:09:18.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-227" for this suite. 06/08/23 16:09:18.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:09:18.9
Jun  8 16:09:18.901: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename replication-controller 06/08/23 16:09:18.902
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:09:18.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:09:18.923
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-z98j2" 06/08/23 16:09:18.927
Jun  8 16:09:18.932: INFO: Get Replication Controller "e2e-rc-z98j2" to confirm replicas
Jun  8 16:09:19.936: INFO: Get Replication Controller "e2e-rc-z98j2" to confirm replicas
Jun  8 16:09:19.940: INFO: Found 1 replicas for "e2e-rc-z98j2" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-z98j2" 06/08/23 16:09:19.94
STEP: Updating a scale subresource 06/08/23 16:09:19.945
STEP: Verifying replicas where modified for replication controller "e2e-rc-z98j2" 06/08/23 16:09:19.952
Jun  8 16:09:19.953: INFO: Get Replication Controller "e2e-rc-z98j2" to confirm replicas
Jun  8 16:09:20.957: INFO: Get Replication Controller "e2e-rc-z98j2" to confirm replicas
Jun  8 16:09:20.962: INFO: Found 2 replicas for "e2e-rc-z98j2" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jun  8 16:09:20.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2048" for this suite. 06/08/23 16:09:20.968
------------------------------
• [2.075 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:09:18.9
    Jun  8 16:09:18.901: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename replication-controller 06/08/23 16:09:18.902
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:09:18.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:09:18.923
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-z98j2" 06/08/23 16:09:18.927
    Jun  8 16:09:18.932: INFO: Get Replication Controller "e2e-rc-z98j2" to confirm replicas
    Jun  8 16:09:19.936: INFO: Get Replication Controller "e2e-rc-z98j2" to confirm replicas
    Jun  8 16:09:19.940: INFO: Found 1 replicas for "e2e-rc-z98j2" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-z98j2" 06/08/23 16:09:19.94
    STEP: Updating a scale subresource 06/08/23 16:09:19.945
    STEP: Verifying replicas where modified for replication controller "e2e-rc-z98j2" 06/08/23 16:09:19.952
    Jun  8 16:09:19.953: INFO: Get Replication Controller "e2e-rc-z98j2" to confirm replicas
    Jun  8 16:09:20.957: INFO: Get Replication Controller "e2e-rc-z98j2" to confirm replicas
    Jun  8 16:09:20.962: INFO: Found 2 replicas for "e2e-rc-z98j2" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:09:20.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2048" for this suite. 06/08/23 16:09:20.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:09:20.978
Jun  8 16:09:20.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename gc 06/08/23 16:09:20.979
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:09:20.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:09:21.001
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 06/08/23 16:09:21.01
STEP: delete the rc 06/08/23 16:09:26.025
STEP: wait for the rc to be deleted 06/08/23 16:09:26.038
Jun  8 16:09:27.119: INFO: 80 pods remaining
Jun  8 16:09:27.119: INFO: 80 pods has nil DeletionTimestamp
Jun  8 16:09:27.119: INFO: 
Jun  8 16:09:28.062: INFO: 74 pods remaining
Jun  8 16:09:28.062: INFO: 72 pods has nil DeletionTimestamp
Jun  8 16:09:28.062: INFO: 
Jun  8 16:09:29.056: INFO: 59 pods remaining
Jun  8 16:09:29.056: INFO: 59 pods has nil DeletionTimestamp
Jun  8 16:09:29.056: INFO: 
Jun  8 16:09:30.055: INFO: 40 pods remaining
Jun  8 16:09:30.055: INFO: 40 pods has nil DeletionTimestamp
Jun  8 16:09:30.055: INFO: 
Jun  8 16:09:31.057: INFO: 32 pods remaining
Jun  8 16:09:31.057: INFO: 32 pods has nil DeletionTimestamp
Jun  8 16:09:31.057: INFO: 
Jun  8 16:09:32.050: INFO: 19 pods remaining
Jun  8 16:09:32.050: INFO: 19 pods has nil DeletionTimestamp
Jun  8 16:09:32.050: INFO: 
STEP: Gathering metrics 06/08/23 16:09:33.061
Jun  8 16:09:33.096: INFO: Waiting up to 5m0s for pod "kube-controller-manager-chl8tf-control-plane-003" in namespace "kube-system" to be "running and ready"
Jun  8 16:09:33.101: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003": Phase="Running", Reason="", readiness=true. Elapsed: 4.892835ms
Jun  8 16:09:33.102: INFO: The phase of Pod kube-controller-manager-chl8tf-control-plane-003 is Running (Ready = true)
Jun  8 16:09:33.102: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003" satisfied condition "running and ready"
Jun  8 16:09:33.171: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jun  8 16:09:33.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5240" for this suite. 06/08/23 16:09:33.178
------------------------------
• [SLOW TEST] [12.209 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:09:20.978
    Jun  8 16:09:20.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename gc 06/08/23 16:09:20.979
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:09:20.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:09:21.001
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 06/08/23 16:09:21.01
    STEP: delete the rc 06/08/23 16:09:26.025
    STEP: wait for the rc to be deleted 06/08/23 16:09:26.038
    Jun  8 16:09:27.119: INFO: 80 pods remaining
    Jun  8 16:09:27.119: INFO: 80 pods has nil DeletionTimestamp
    Jun  8 16:09:27.119: INFO: 
    Jun  8 16:09:28.062: INFO: 74 pods remaining
    Jun  8 16:09:28.062: INFO: 72 pods has nil DeletionTimestamp
    Jun  8 16:09:28.062: INFO: 
    Jun  8 16:09:29.056: INFO: 59 pods remaining
    Jun  8 16:09:29.056: INFO: 59 pods has nil DeletionTimestamp
    Jun  8 16:09:29.056: INFO: 
    Jun  8 16:09:30.055: INFO: 40 pods remaining
    Jun  8 16:09:30.055: INFO: 40 pods has nil DeletionTimestamp
    Jun  8 16:09:30.055: INFO: 
    Jun  8 16:09:31.057: INFO: 32 pods remaining
    Jun  8 16:09:31.057: INFO: 32 pods has nil DeletionTimestamp
    Jun  8 16:09:31.057: INFO: 
    Jun  8 16:09:32.050: INFO: 19 pods remaining
    Jun  8 16:09:32.050: INFO: 19 pods has nil DeletionTimestamp
    Jun  8 16:09:32.050: INFO: 
    STEP: Gathering metrics 06/08/23 16:09:33.061
    Jun  8 16:09:33.096: INFO: Waiting up to 5m0s for pod "kube-controller-manager-chl8tf-control-plane-003" in namespace "kube-system" to be "running and ready"
    Jun  8 16:09:33.101: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003": Phase="Running", Reason="", readiness=true. Elapsed: 4.892835ms
    Jun  8 16:09:33.102: INFO: The phase of Pod kube-controller-manager-chl8tf-control-plane-003 is Running (Ready = true)
    Jun  8 16:09:33.102: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003" satisfied condition "running and ready"
    Jun  8 16:09:33.171: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:09:33.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5240" for this suite. 06/08/23 16:09:33.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:09:33.188
Jun  8 16:09:33.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename downward-api 06/08/23 16:09:33.189
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:09:33.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:09:33.22
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 06/08/23 16:09:33.224
Jun  8 16:09:33.237: INFO: Waiting up to 5m0s for pod "downward-api-fb665eb8-1409-433e-9316-b9a139add795" in namespace "downward-api-4457" to be "Succeeded or Failed"
Jun  8 16:09:33.242: INFO: Pod "downward-api-fb665eb8-1409-433e-9316-b9a139add795": Phase="Pending", Reason="", readiness=false. Elapsed: 4.732123ms
Jun  8 16:09:35.247: INFO: Pod "downward-api-fb665eb8-1409-433e-9316-b9a139add795": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010549936s
Jun  8 16:09:37.247: INFO: Pod "downward-api-fb665eb8-1409-433e-9316-b9a139add795": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010144884s
Jun  8 16:09:39.248: INFO: Pod "downward-api-fb665eb8-1409-433e-9316-b9a139add795": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010564177s
STEP: Saw pod success 06/08/23 16:09:39.248
Jun  8 16:09:39.248: INFO: Pod "downward-api-fb665eb8-1409-433e-9316-b9a139add795" satisfied condition "Succeeded or Failed"
Jun  8 16:09:39.252: INFO: Trying to get logs from node chl8tf-worker-001 pod downward-api-fb665eb8-1409-433e-9316-b9a139add795 container dapi-container: <nil>
STEP: delete the pod 06/08/23 16:09:39.261
Jun  8 16:09:39.272: INFO: Waiting for pod downward-api-fb665eb8-1409-433e-9316-b9a139add795 to disappear
Jun  8 16:09:39.276: INFO: Pod downward-api-fb665eb8-1409-433e-9316-b9a139add795 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jun  8 16:09:39.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4457" for this suite. 06/08/23 16:09:39.281
------------------------------
• [SLOW TEST] [6.102 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:09:33.188
    Jun  8 16:09:33.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename downward-api 06/08/23 16:09:33.189
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:09:33.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:09:33.22
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 06/08/23 16:09:33.224
    Jun  8 16:09:33.237: INFO: Waiting up to 5m0s for pod "downward-api-fb665eb8-1409-433e-9316-b9a139add795" in namespace "downward-api-4457" to be "Succeeded or Failed"
    Jun  8 16:09:33.242: INFO: Pod "downward-api-fb665eb8-1409-433e-9316-b9a139add795": Phase="Pending", Reason="", readiness=false. Elapsed: 4.732123ms
    Jun  8 16:09:35.247: INFO: Pod "downward-api-fb665eb8-1409-433e-9316-b9a139add795": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010549936s
    Jun  8 16:09:37.247: INFO: Pod "downward-api-fb665eb8-1409-433e-9316-b9a139add795": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010144884s
    Jun  8 16:09:39.248: INFO: Pod "downward-api-fb665eb8-1409-433e-9316-b9a139add795": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010564177s
    STEP: Saw pod success 06/08/23 16:09:39.248
    Jun  8 16:09:39.248: INFO: Pod "downward-api-fb665eb8-1409-433e-9316-b9a139add795" satisfied condition "Succeeded or Failed"
    Jun  8 16:09:39.252: INFO: Trying to get logs from node chl8tf-worker-001 pod downward-api-fb665eb8-1409-433e-9316-b9a139add795 container dapi-container: <nil>
    STEP: delete the pod 06/08/23 16:09:39.261
    Jun  8 16:09:39.272: INFO: Waiting for pod downward-api-fb665eb8-1409-433e-9316-b9a139add795 to disappear
    Jun  8 16:09:39.276: INFO: Pod downward-api-fb665eb8-1409-433e-9316-b9a139add795 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:09:39.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4457" for this suite. 06/08/23 16:09:39.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:09:39.291
Jun  8 16:09:39.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename emptydir 06/08/23 16:09:39.293
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:09:39.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:09:39.313
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 06/08/23 16:09:39.317
Jun  8 16:09:39.328: INFO: Waiting up to 5m0s for pod "pod-2d8d9aa7-cdd9-48d2-bd0f-8378921fc0d3" in namespace "emptydir-4018" to be "Succeeded or Failed"
Jun  8 16:09:39.334: INFO: Pod "pod-2d8d9aa7-cdd9-48d2-bd0f-8378921fc0d3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.36822ms
Jun  8 16:09:41.340: INFO: Pod "pod-2d8d9aa7-cdd9-48d2-bd0f-8378921fc0d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011879312s
Jun  8 16:09:43.340: INFO: Pod "pod-2d8d9aa7-cdd9-48d2-bd0f-8378921fc0d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012491241s
STEP: Saw pod success 06/08/23 16:09:43.34
Jun  8 16:09:43.340: INFO: Pod "pod-2d8d9aa7-cdd9-48d2-bd0f-8378921fc0d3" satisfied condition "Succeeded or Failed"
Jun  8 16:09:43.345: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-2d8d9aa7-cdd9-48d2-bd0f-8378921fc0d3 container test-container: <nil>
STEP: delete the pod 06/08/23 16:09:43.352
Jun  8 16:09:43.365: INFO: Waiting for pod pod-2d8d9aa7-cdd9-48d2-bd0f-8378921fc0d3 to disappear
Jun  8 16:09:43.368: INFO: Pod pod-2d8d9aa7-cdd9-48d2-bd0f-8378921fc0d3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  8 16:09:43.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4018" for this suite. 06/08/23 16:09:43.375
------------------------------
• [4.091 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:09:39.291
    Jun  8 16:09:39.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename emptydir 06/08/23 16:09:39.293
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:09:39.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:09:39.313
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 06/08/23 16:09:39.317
    Jun  8 16:09:39.328: INFO: Waiting up to 5m0s for pod "pod-2d8d9aa7-cdd9-48d2-bd0f-8378921fc0d3" in namespace "emptydir-4018" to be "Succeeded or Failed"
    Jun  8 16:09:39.334: INFO: Pod "pod-2d8d9aa7-cdd9-48d2-bd0f-8378921fc0d3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.36822ms
    Jun  8 16:09:41.340: INFO: Pod "pod-2d8d9aa7-cdd9-48d2-bd0f-8378921fc0d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011879312s
    Jun  8 16:09:43.340: INFO: Pod "pod-2d8d9aa7-cdd9-48d2-bd0f-8378921fc0d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012491241s
    STEP: Saw pod success 06/08/23 16:09:43.34
    Jun  8 16:09:43.340: INFO: Pod "pod-2d8d9aa7-cdd9-48d2-bd0f-8378921fc0d3" satisfied condition "Succeeded or Failed"
    Jun  8 16:09:43.345: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-2d8d9aa7-cdd9-48d2-bd0f-8378921fc0d3 container test-container: <nil>
    STEP: delete the pod 06/08/23 16:09:43.352
    Jun  8 16:09:43.365: INFO: Waiting for pod pod-2d8d9aa7-cdd9-48d2-bd0f-8378921fc0d3 to disappear
    Jun  8 16:09:43.368: INFO: Pod pod-2d8d9aa7-cdd9-48d2-bd0f-8378921fc0d3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:09:43.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4018" for this suite. 06/08/23 16:09:43.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:09:43.384
Jun  8 16:09:43.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename emptydir-wrapper 06/08/23 16:09:43.385
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:09:43.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:09:43.412
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jun  8 16:09:43.439: INFO: Waiting up to 5m0s for pod "pod-secrets-9b89f9f5-0bc6-4111-ace8-e162d1f4fa70" in namespace "emptydir-wrapper-5933" to be "running and ready"
Jun  8 16:09:43.445: INFO: Pod "pod-secrets-9b89f9f5-0bc6-4111-ace8-e162d1f4fa70": Phase="Pending", Reason="", readiness=false. Elapsed: 5.909492ms
Jun  8 16:09:43.445: INFO: The phase of Pod pod-secrets-9b89f9f5-0bc6-4111-ace8-e162d1f4fa70 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 16:09:45.451: INFO: Pod "pod-secrets-9b89f9f5-0bc6-4111-ace8-e162d1f4fa70": Phase="Running", Reason="", readiness=true. Elapsed: 2.01192881s
Jun  8 16:09:45.451: INFO: The phase of Pod pod-secrets-9b89f9f5-0bc6-4111-ace8-e162d1f4fa70 is Running (Ready = true)
Jun  8 16:09:45.451: INFO: Pod "pod-secrets-9b89f9f5-0bc6-4111-ace8-e162d1f4fa70" satisfied condition "running and ready"
STEP: Cleaning up the secret 06/08/23 16:09:45.455
STEP: Cleaning up the configmap 06/08/23 16:09:45.462
STEP: Cleaning up the pod 06/08/23 16:09:45.469
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jun  8 16:09:45.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-5933" for this suite. 06/08/23 16:09:45.488
------------------------------
• [2.111 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:09:43.384
    Jun  8 16:09:43.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename emptydir-wrapper 06/08/23 16:09:43.385
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:09:43.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:09:43.412
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jun  8 16:09:43.439: INFO: Waiting up to 5m0s for pod "pod-secrets-9b89f9f5-0bc6-4111-ace8-e162d1f4fa70" in namespace "emptydir-wrapper-5933" to be "running and ready"
    Jun  8 16:09:43.445: INFO: Pod "pod-secrets-9b89f9f5-0bc6-4111-ace8-e162d1f4fa70": Phase="Pending", Reason="", readiness=false. Elapsed: 5.909492ms
    Jun  8 16:09:43.445: INFO: The phase of Pod pod-secrets-9b89f9f5-0bc6-4111-ace8-e162d1f4fa70 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 16:09:45.451: INFO: Pod "pod-secrets-9b89f9f5-0bc6-4111-ace8-e162d1f4fa70": Phase="Running", Reason="", readiness=true. Elapsed: 2.01192881s
    Jun  8 16:09:45.451: INFO: The phase of Pod pod-secrets-9b89f9f5-0bc6-4111-ace8-e162d1f4fa70 is Running (Ready = true)
    Jun  8 16:09:45.451: INFO: Pod "pod-secrets-9b89f9f5-0bc6-4111-ace8-e162d1f4fa70" satisfied condition "running and ready"
    STEP: Cleaning up the secret 06/08/23 16:09:45.455
    STEP: Cleaning up the configmap 06/08/23 16:09:45.462
    STEP: Cleaning up the pod 06/08/23 16:09:45.469
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:09:45.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-5933" for this suite. 06/08/23 16:09:45.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:09:45.496
Jun  8 16:09:45.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 16:09:45.497
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:09:45.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:09:45.519
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-d3fea1c0-0948-484d-b2c8-bf205fddd9cb 06/08/23 16:09:45.523
STEP: Creating a pod to test consume configMaps 06/08/23 16:09:45.53
Jun  8 16:09:45.544: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-033201cb-4c80-43a8-bd3b-cb3a59694312" in namespace "projected-8399" to be "Succeeded or Failed"
Jun  8 16:09:45.548: INFO: Pod "pod-projected-configmaps-033201cb-4c80-43a8-bd3b-cb3a59694312": Phase="Pending", Reason="", readiness=false. Elapsed: 4.436477ms
Jun  8 16:09:47.554: INFO: Pod "pod-projected-configmaps-033201cb-4c80-43a8-bd3b-cb3a59694312": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010580908s
Jun  8 16:09:49.553: INFO: Pod "pod-projected-configmaps-033201cb-4c80-43a8-bd3b-cb3a59694312": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009487056s
STEP: Saw pod success 06/08/23 16:09:49.553
Jun  8 16:09:49.554: INFO: Pod "pod-projected-configmaps-033201cb-4c80-43a8-bd3b-cb3a59694312" satisfied condition "Succeeded or Failed"
Jun  8 16:09:49.558: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-configmaps-033201cb-4c80-43a8-bd3b-cb3a59694312 container agnhost-container: <nil>
STEP: delete the pod 06/08/23 16:09:49.565
Jun  8 16:09:49.579: INFO: Waiting for pod pod-projected-configmaps-033201cb-4c80-43a8-bd3b-cb3a59694312 to disappear
Jun  8 16:09:49.582: INFO: Pod pod-projected-configmaps-033201cb-4c80-43a8-bd3b-cb3a59694312 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun  8 16:09:49.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8399" for this suite. 06/08/23 16:09:49.589
------------------------------
• [4.100 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:09:45.496
    Jun  8 16:09:45.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 16:09:45.497
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:09:45.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:09:45.519
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-d3fea1c0-0948-484d-b2c8-bf205fddd9cb 06/08/23 16:09:45.523
    STEP: Creating a pod to test consume configMaps 06/08/23 16:09:45.53
    Jun  8 16:09:45.544: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-033201cb-4c80-43a8-bd3b-cb3a59694312" in namespace "projected-8399" to be "Succeeded or Failed"
    Jun  8 16:09:45.548: INFO: Pod "pod-projected-configmaps-033201cb-4c80-43a8-bd3b-cb3a59694312": Phase="Pending", Reason="", readiness=false. Elapsed: 4.436477ms
    Jun  8 16:09:47.554: INFO: Pod "pod-projected-configmaps-033201cb-4c80-43a8-bd3b-cb3a59694312": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010580908s
    Jun  8 16:09:49.553: INFO: Pod "pod-projected-configmaps-033201cb-4c80-43a8-bd3b-cb3a59694312": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009487056s
    STEP: Saw pod success 06/08/23 16:09:49.553
    Jun  8 16:09:49.554: INFO: Pod "pod-projected-configmaps-033201cb-4c80-43a8-bd3b-cb3a59694312" satisfied condition "Succeeded or Failed"
    Jun  8 16:09:49.558: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-configmaps-033201cb-4c80-43a8-bd3b-cb3a59694312 container agnhost-container: <nil>
    STEP: delete the pod 06/08/23 16:09:49.565
    Jun  8 16:09:49.579: INFO: Waiting for pod pod-projected-configmaps-033201cb-4c80-43a8-bd3b-cb3a59694312 to disappear
    Jun  8 16:09:49.582: INFO: Pod pod-projected-configmaps-033201cb-4c80-43a8-bd3b-cb3a59694312 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:09:49.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8399" for this suite. 06/08/23 16:09:49.589
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:09:49.597
Jun  8 16:09:49.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 16:09:49.598
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:09:49.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:09:49.619
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 06/08/23 16:09:49.622
Jun  8 16:09:49.632: INFO: Waiting up to 5m0s for pod "downwardapi-volume-946a6744-2e63-4d30-93c0-8740fdd1c65e" in namespace "projected-7614" to be "Succeeded or Failed"
Jun  8 16:09:49.635: INFO: Pod "downwardapi-volume-946a6744-2e63-4d30-93c0-8740fdd1c65e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.649679ms
Jun  8 16:09:51.641: INFO: Pod "downwardapi-volume-946a6744-2e63-4d30-93c0-8740fdd1c65e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008986128s
Jun  8 16:09:53.641: INFO: Pod "downwardapi-volume-946a6744-2e63-4d30-93c0-8740fdd1c65e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009137684s
STEP: Saw pod success 06/08/23 16:09:53.641
Jun  8 16:09:53.641: INFO: Pod "downwardapi-volume-946a6744-2e63-4d30-93c0-8740fdd1c65e" satisfied condition "Succeeded or Failed"
Jun  8 16:09:53.645: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-946a6744-2e63-4d30-93c0-8740fdd1c65e container client-container: <nil>
STEP: delete the pod 06/08/23 16:09:53.653
Jun  8 16:09:53.664: INFO: Waiting for pod downwardapi-volume-946a6744-2e63-4d30-93c0-8740fdd1c65e to disappear
Jun  8 16:09:53.667: INFO: Pod downwardapi-volume-946a6744-2e63-4d30-93c0-8740fdd1c65e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  8 16:09:53.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7614" for this suite. 06/08/23 16:09:53.673
------------------------------
• [4.084 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:09:49.597
    Jun  8 16:09:49.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 16:09:49.598
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:09:49.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:09:49.619
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 06/08/23 16:09:49.622
    Jun  8 16:09:49.632: INFO: Waiting up to 5m0s for pod "downwardapi-volume-946a6744-2e63-4d30-93c0-8740fdd1c65e" in namespace "projected-7614" to be "Succeeded or Failed"
    Jun  8 16:09:49.635: INFO: Pod "downwardapi-volume-946a6744-2e63-4d30-93c0-8740fdd1c65e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.649679ms
    Jun  8 16:09:51.641: INFO: Pod "downwardapi-volume-946a6744-2e63-4d30-93c0-8740fdd1c65e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008986128s
    Jun  8 16:09:53.641: INFO: Pod "downwardapi-volume-946a6744-2e63-4d30-93c0-8740fdd1c65e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009137684s
    STEP: Saw pod success 06/08/23 16:09:53.641
    Jun  8 16:09:53.641: INFO: Pod "downwardapi-volume-946a6744-2e63-4d30-93c0-8740fdd1c65e" satisfied condition "Succeeded or Failed"
    Jun  8 16:09:53.645: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-946a6744-2e63-4d30-93c0-8740fdd1c65e container client-container: <nil>
    STEP: delete the pod 06/08/23 16:09:53.653
    Jun  8 16:09:53.664: INFO: Waiting for pod downwardapi-volume-946a6744-2e63-4d30-93c0-8740fdd1c65e to disappear
    Jun  8 16:09:53.667: INFO: Pod downwardapi-volume-946a6744-2e63-4d30-93c0-8740fdd1c65e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:09:53.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7614" for this suite. 06/08/23 16:09:53.673
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:09:53.681
Jun  8 16:09:53.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename resourcequota 06/08/23 16:09:53.683
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:09:53.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:09:53.703
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 06/08/23 16:09:53.706
STEP: Getting a ResourceQuota 06/08/23 16:09:53.712
STEP: Listing all ResourceQuotas with LabelSelector 06/08/23 16:09:53.715
STEP: Patching the ResourceQuota 06/08/23 16:09:53.719
STEP: Deleting a Collection of ResourceQuotas 06/08/23 16:09:53.724
STEP: Verifying the deleted ResourceQuota 06/08/23 16:09:53.735
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  8 16:09:53.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9788" for this suite. 06/08/23 16:09:53.744
------------------------------
• [0.071 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:09:53.681
    Jun  8 16:09:53.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename resourcequota 06/08/23 16:09:53.683
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:09:53.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:09:53.703
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 06/08/23 16:09:53.706
    STEP: Getting a ResourceQuota 06/08/23 16:09:53.712
    STEP: Listing all ResourceQuotas with LabelSelector 06/08/23 16:09:53.715
    STEP: Patching the ResourceQuota 06/08/23 16:09:53.719
    STEP: Deleting a Collection of ResourceQuotas 06/08/23 16:09:53.724
    STEP: Verifying the deleted ResourceQuota 06/08/23 16:09:53.735
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:09:53.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9788" for this suite. 06/08/23 16:09:53.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:09:53.753
Jun  8 16:09:53.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename runtimeclass 06/08/23 16:09:53.754
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:09:53.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:09:53.774
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jun  8 16:09:53.790: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1053 to be scheduled
Jun  8 16:09:53.794: INFO: 1 pods are not scheduled: [runtimeclass-1053/test-runtimeclass-runtimeclass-1053-preconfigured-handler-ml6l4(4b242a6b-e59e-49f6-9288-a78154581d95)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jun  8 16:09:55.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1053" for this suite. 06/08/23 16:09:55.814
------------------------------
• [2.068 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:09:53.753
    Jun  8 16:09:53.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename runtimeclass 06/08/23 16:09:53.754
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:09:53.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:09:53.774
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jun  8 16:09:53.790: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1053 to be scheduled
    Jun  8 16:09:53.794: INFO: 1 pods are not scheduled: [runtimeclass-1053/test-runtimeclass-runtimeclass-1053-preconfigured-handler-ml6l4(4b242a6b-e59e-49f6-9288-a78154581d95)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:09:55.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1053" for this suite. 06/08/23 16:09:55.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:09:55.821
Jun  8 16:09:55.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename subpath 06/08/23 16:09:55.823
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:09:55.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:09:55.843
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/08/23 16:09:55.846
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-pv9t 06/08/23 16:09:55.857
STEP: Creating a pod to test atomic-volume-subpath 06/08/23 16:09:55.857
Jun  8 16:09:55.867: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-pv9t" in namespace "subpath-8790" to be "Succeeded or Failed"
Jun  8 16:09:55.871: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Pending", Reason="", readiness=false. Elapsed: 4.39424ms
Jun  8 16:09:57.877: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=true. Elapsed: 2.010306197s
Jun  8 16:09:59.877: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=true. Elapsed: 4.009842842s
Jun  8 16:10:01.877: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=true. Elapsed: 6.010008982s
Jun  8 16:10:03.877: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=true. Elapsed: 8.010021838s
Jun  8 16:10:05.876: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=true. Elapsed: 10.009314165s
Jun  8 16:10:07.879: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=true. Elapsed: 12.011705808s
Jun  8 16:10:09.878: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=true. Elapsed: 14.010482618s
Jun  8 16:10:11.877: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=true. Elapsed: 16.010227868s
Jun  8 16:10:13.876: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=true. Elapsed: 18.008940394s
Jun  8 16:10:15.878: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=true. Elapsed: 20.010880254s
Jun  8 16:10:17.876: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=false. Elapsed: 22.009362851s
Jun  8 16:10:19.876: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009401569s
STEP: Saw pod success 06/08/23 16:10:19.876
Jun  8 16:10:19.877: INFO: Pod "pod-subpath-test-secret-pv9t" satisfied condition "Succeeded or Failed"
Jun  8 16:10:19.881: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-subpath-test-secret-pv9t container test-container-subpath-secret-pv9t: <nil>
STEP: delete the pod 06/08/23 16:10:19.89
Jun  8 16:10:19.904: INFO: Waiting for pod pod-subpath-test-secret-pv9t to disappear
Jun  8 16:10:19.908: INFO: Pod pod-subpath-test-secret-pv9t no longer exists
STEP: Deleting pod pod-subpath-test-secret-pv9t 06/08/23 16:10:19.908
Jun  8 16:10:19.908: INFO: Deleting pod "pod-subpath-test-secret-pv9t" in namespace "subpath-8790"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jun  8 16:10:19.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8790" for this suite. 06/08/23 16:10:19.918
------------------------------
• [SLOW TEST] [24.103 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:09:55.821
    Jun  8 16:09:55.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename subpath 06/08/23 16:09:55.823
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:09:55.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:09:55.843
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/08/23 16:09:55.846
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-pv9t 06/08/23 16:09:55.857
    STEP: Creating a pod to test atomic-volume-subpath 06/08/23 16:09:55.857
    Jun  8 16:09:55.867: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-pv9t" in namespace "subpath-8790" to be "Succeeded or Failed"
    Jun  8 16:09:55.871: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Pending", Reason="", readiness=false. Elapsed: 4.39424ms
    Jun  8 16:09:57.877: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=true. Elapsed: 2.010306197s
    Jun  8 16:09:59.877: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=true. Elapsed: 4.009842842s
    Jun  8 16:10:01.877: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=true. Elapsed: 6.010008982s
    Jun  8 16:10:03.877: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=true. Elapsed: 8.010021838s
    Jun  8 16:10:05.876: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=true. Elapsed: 10.009314165s
    Jun  8 16:10:07.879: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=true. Elapsed: 12.011705808s
    Jun  8 16:10:09.878: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=true. Elapsed: 14.010482618s
    Jun  8 16:10:11.877: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=true. Elapsed: 16.010227868s
    Jun  8 16:10:13.876: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=true. Elapsed: 18.008940394s
    Jun  8 16:10:15.878: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=true. Elapsed: 20.010880254s
    Jun  8 16:10:17.876: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Running", Reason="", readiness=false. Elapsed: 22.009362851s
    Jun  8 16:10:19.876: INFO: Pod "pod-subpath-test-secret-pv9t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009401569s
    STEP: Saw pod success 06/08/23 16:10:19.876
    Jun  8 16:10:19.877: INFO: Pod "pod-subpath-test-secret-pv9t" satisfied condition "Succeeded or Failed"
    Jun  8 16:10:19.881: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-subpath-test-secret-pv9t container test-container-subpath-secret-pv9t: <nil>
    STEP: delete the pod 06/08/23 16:10:19.89
    Jun  8 16:10:19.904: INFO: Waiting for pod pod-subpath-test-secret-pv9t to disappear
    Jun  8 16:10:19.908: INFO: Pod pod-subpath-test-secret-pv9t no longer exists
    STEP: Deleting pod pod-subpath-test-secret-pv9t 06/08/23 16:10:19.908
    Jun  8 16:10:19.908: INFO: Deleting pod "pod-subpath-test-secret-pv9t" in namespace "subpath-8790"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:10:19.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8790" for this suite. 06/08/23 16:10:19.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:10:19.926
Jun  8 16:10:19.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename container-runtime 06/08/23 16:10:19.927
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:10:19.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:10:19.948
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 06/08/23 16:10:19.952
STEP: wait for the container to reach Succeeded 06/08/23 16:10:19.961
STEP: get the container status 06/08/23 16:10:22.981
STEP: the container should be terminated 06/08/23 16:10:22.985
STEP: the termination message should be set 06/08/23 16:10:22.986
Jun  8 16:10:22.986: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 06/08/23 16:10:22.986
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jun  8 16:10:23.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-4462" for this suite. 06/08/23 16:10:23.009
------------------------------
• [3.091 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:10:19.926
    Jun  8 16:10:19.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename container-runtime 06/08/23 16:10:19.927
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:10:19.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:10:19.948
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 06/08/23 16:10:19.952
    STEP: wait for the container to reach Succeeded 06/08/23 16:10:19.961
    STEP: get the container status 06/08/23 16:10:22.981
    STEP: the container should be terminated 06/08/23 16:10:22.985
    STEP: the termination message should be set 06/08/23 16:10:22.986
    Jun  8 16:10:22.986: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 06/08/23 16:10:22.986
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:10:23.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-4462" for this suite. 06/08/23 16:10:23.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:10:23.017
Jun  8 16:10:23.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename subpath 06/08/23 16:10:23.019
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:10:23.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:10:23.039
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/08/23 16:10:23.043
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-666p 06/08/23 16:10:23.053
STEP: Creating a pod to test atomic-volume-subpath 06/08/23 16:10:23.053
Jun  8 16:10:23.063: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-666p" in namespace "subpath-4775" to be "Succeeded or Failed"
Jun  8 16:10:23.067: INFO: Pod "pod-subpath-test-projected-666p": Phase="Pending", Reason="", readiness=false. Elapsed: 3.880012ms
Jun  8 16:10:25.072: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=true. Elapsed: 2.009122744s
Jun  8 16:10:27.073: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=true. Elapsed: 4.009641251s
Jun  8 16:10:29.071: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=true. Elapsed: 6.008446242s
Jun  8 16:10:31.073: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=true. Elapsed: 8.010464096s
Jun  8 16:10:33.072: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=true. Elapsed: 10.009443827s
Jun  8 16:10:35.072: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=true. Elapsed: 12.008900767s
Jun  8 16:10:37.072: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=true. Elapsed: 14.009418561s
Jun  8 16:10:39.072: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=true. Elapsed: 16.008769304s
Jun  8 16:10:41.073: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=true. Elapsed: 18.009918334s
Jun  8 16:10:43.074: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=true. Elapsed: 20.011359353s
Jun  8 16:10:45.072: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=false. Elapsed: 22.009336636s
Jun  8 16:10:47.073: INFO: Pod "pod-subpath-test-projected-666p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009884769s
STEP: Saw pod success 06/08/23 16:10:47.073
Jun  8 16:10:47.073: INFO: Pod "pod-subpath-test-projected-666p" satisfied condition "Succeeded or Failed"
Jun  8 16:10:47.077: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-subpath-test-projected-666p container test-container-subpath-projected-666p: <nil>
STEP: delete the pod 06/08/23 16:10:47.086
Jun  8 16:10:47.098: INFO: Waiting for pod pod-subpath-test-projected-666p to disappear
Jun  8 16:10:47.102: INFO: Pod pod-subpath-test-projected-666p no longer exists
STEP: Deleting pod pod-subpath-test-projected-666p 06/08/23 16:10:47.102
Jun  8 16:10:47.102: INFO: Deleting pod "pod-subpath-test-projected-666p" in namespace "subpath-4775"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jun  8 16:10:47.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4775" for this suite. 06/08/23 16:10:47.112
------------------------------
• [SLOW TEST] [24.102 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:10:23.017
    Jun  8 16:10:23.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename subpath 06/08/23 16:10:23.019
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:10:23.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:10:23.039
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/08/23 16:10:23.043
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-666p 06/08/23 16:10:23.053
    STEP: Creating a pod to test atomic-volume-subpath 06/08/23 16:10:23.053
    Jun  8 16:10:23.063: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-666p" in namespace "subpath-4775" to be "Succeeded or Failed"
    Jun  8 16:10:23.067: INFO: Pod "pod-subpath-test-projected-666p": Phase="Pending", Reason="", readiness=false. Elapsed: 3.880012ms
    Jun  8 16:10:25.072: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=true. Elapsed: 2.009122744s
    Jun  8 16:10:27.073: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=true. Elapsed: 4.009641251s
    Jun  8 16:10:29.071: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=true. Elapsed: 6.008446242s
    Jun  8 16:10:31.073: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=true. Elapsed: 8.010464096s
    Jun  8 16:10:33.072: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=true. Elapsed: 10.009443827s
    Jun  8 16:10:35.072: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=true. Elapsed: 12.008900767s
    Jun  8 16:10:37.072: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=true. Elapsed: 14.009418561s
    Jun  8 16:10:39.072: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=true. Elapsed: 16.008769304s
    Jun  8 16:10:41.073: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=true. Elapsed: 18.009918334s
    Jun  8 16:10:43.074: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=true. Elapsed: 20.011359353s
    Jun  8 16:10:45.072: INFO: Pod "pod-subpath-test-projected-666p": Phase="Running", Reason="", readiness=false. Elapsed: 22.009336636s
    Jun  8 16:10:47.073: INFO: Pod "pod-subpath-test-projected-666p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009884769s
    STEP: Saw pod success 06/08/23 16:10:47.073
    Jun  8 16:10:47.073: INFO: Pod "pod-subpath-test-projected-666p" satisfied condition "Succeeded or Failed"
    Jun  8 16:10:47.077: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-subpath-test-projected-666p container test-container-subpath-projected-666p: <nil>
    STEP: delete the pod 06/08/23 16:10:47.086
    Jun  8 16:10:47.098: INFO: Waiting for pod pod-subpath-test-projected-666p to disappear
    Jun  8 16:10:47.102: INFO: Pod pod-subpath-test-projected-666p no longer exists
    STEP: Deleting pod pod-subpath-test-projected-666p 06/08/23 16:10:47.102
    Jun  8 16:10:47.102: INFO: Deleting pod "pod-subpath-test-projected-666p" in namespace "subpath-4775"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:10:47.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4775" for this suite. 06/08/23 16:10:47.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:10:47.121
Jun  8 16:10:47.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename ingressclass 06/08/23 16:10:47.122
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:10:47.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:10:47.142
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 06/08/23 16:10:47.145
STEP: getting /apis/networking.k8s.io 06/08/23 16:10:47.148
STEP: getting /apis/networking.k8s.iov1 06/08/23 16:10:47.15
STEP: creating 06/08/23 16:10:47.151
STEP: getting 06/08/23 16:10:47.167
STEP: listing 06/08/23 16:10:47.171
STEP: watching 06/08/23 16:10:47.178
Jun  8 16:10:47.178: INFO: starting watch
STEP: patching 06/08/23 16:10:47.179
STEP: updating 06/08/23 16:10:47.186
Jun  8 16:10:47.192: INFO: waiting for watch events with expected annotations
Jun  8 16:10:47.192: INFO: saw patched and updated annotations
STEP: deleting 06/08/23 16:10:47.192
STEP: deleting a collection 06/08/23 16:10:47.206
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Jun  8 16:10:47.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-838" for this suite. 06/08/23 16:10:47.235
------------------------------
• [0.122 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:10:47.121
    Jun  8 16:10:47.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename ingressclass 06/08/23 16:10:47.122
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:10:47.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:10:47.142
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 06/08/23 16:10:47.145
    STEP: getting /apis/networking.k8s.io 06/08/23 16:10:47.148
    STEP: getting /apis/networking.k8s.iov1 06/08/23 16:10:47.15
    STEP: creating 06/08/23 16:10:47.151
    STEP: getting 06/08/23 16:10:47.167
    STEP: listing 06/08/23 16:10:47.171
    STEP: watching 06/08/23 16:10:47.178
    Jun  8 16:10:47.178: INFO: starting watch
    STEP: patching 06/08/23 16:10:47.179
    STEP: updating 06/08/23 16:10:47.186
    Jun  8 16:10:47.192: INFO: waiting for watch events with expected annotations
    Jun  8 16:10:47.192: INFO: saw patched and updated annotations
    STEP: deleting 06/08/23 16:10:47.192
    STEP: deleting a collection 06/08/23 16:10:47.206
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:10:47.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-838" for this suite. 06/08/23 16:10:47.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:10:47.245
Jun  8 16:10:47.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename webhook 06/08/23 16:10:47.246
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:10:47.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:10:47.267
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/08/23 16:10:47.286
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 16:10:47.607
STEP: Deploying the webhook pod 06/08/23 16:10:47.618
STEP: Wait for the deployment to be ready 06/08/23 16:10:47.632
Jun  8 16:10:47.641: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/08/23 16:10:49.654
STEP: Verifying the service has paired with the endpoint 06/08/23 16:10:49.671
Jun  8 16:10:50.671: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 06/08/23 16:10:50.676
STEP: Creating a custom resource definition that should be denied by the webhook 06/08/23 16:10:50.693
Jun  8 16:10:50.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:10:50.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9685" for this suite. 06/08/23 16:10:50.779
STEP: Destroying namespace "webhook-9685-markers" for this suite. 06/08/23 16:10:50.793
------------------------------
• [3.557 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:10:47.245
    Jun  8 16:10:47.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename webhook 06/08/23 16:10:47.246
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:10:47.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:10:47.267
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/08/23 16:10:47.286
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 16:10:47.607
    STEP: Deploying the webhook pod 06/08/23 16:10:47.618
    STEP: Wait for the deployment to be ready 06/08/23 16:10:47.632
    Jun  8 16:10:47.641: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/08/23 16:10:49.654
    STEP: Verifying the service has paired with the endpoint 06/08/23 16:10:49.671
    Jun  8 16:10:50.671: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 06/08/23 16:10:50.676
    STEP: Creating a custom resource definition that should be denied by the webhook 06/08/23 16:10:50.693
    Jun  8 16:10:50.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:10:50.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9685" for this suite. 06/08/23 16:10:50.779
    STEP: Destroying namespace "webhook-9685-markers" for this suite. 06/08/23 16:10:50.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:10:50.804
Jun  8 16:10:50.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 16:10:50.806
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:10:50.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:10:50.838
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 06/08/23 16:10:50.844
Jun  8 16:10:50.857: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b368c2d-40c3-43c6-9b46-452cc1668e1c" in namespace "projected-6604" to be "Succeeded or Failed"
Jun  8 16:10:50.864: INFO: Pod "downwardapi-volume-2b368c2d-40c3-43c6-9b46-452cc1668e1c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.878299ms
Jun  8 16:10:52.870: INFO: Pod "downwardapi-volume-2b368c2d-40c3-43c6-9b46-452cc1668e1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012327618s
Jun  8 16:10:54.869: INFO: Pod "downwardapi-volume-2b368c2d-40c3-43c6-9b46-452cc1668e1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012029022s
STEP: Saw pod success 06/08/23 16:10:54.869
Jun  8 16:10:54.869: INFO: Pod "downwardapi-volume-2b368c2d-40c3-43c6-9b46-452cc1668e1c" satisfied condition "Succeeded or Failed"
Jun  8 16:10:54.874: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-2b368c2d-40c3-43c6-9b46-452cc1668e1c container client-container: <nil>
STEP: delete the pod 06/08/23 16:10:54.881
Jun  8 16:10:54.902: INFO: Waiting for pod downwardapi-volume-2b368c2d-40c3-43c6-9b46-452cc1668e1c to disappear
Jun  8 16:10:54.905: INFO: Pod downwardapi-volume-2b368c2d-40c3-43c6-9b46-452cc1668e1c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  8 16:10:54.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6604" for this suite. 06/08/23 16:10:54.912
------------------------------
• [4.115 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:10:50.804
    Jun  8 16:10:50.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 16:10:50.806
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:10:50.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:10:50.838
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 06/08/23 16:10:50.844
    Jun  8 16:10:50.857: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b368c2d-40c3-43c6-9b46-452cc1668e1c" in namespace "projected-6604" to be "Succeeded or Failed"
    Jun  8 16:10:50.864: INFO: Pod "downwardapi-volume-2b368c2d-40c3-43c6-9b46-452cc1668e1c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.878299ms
    Jun  8 16:10:52.870: INFO: Pod "downwardapi-volume-2b368c2d-40c3-43c6-9b46-452cc1668e1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012327618s
    Jun  8 16:10:54.869: INFO: Pod "downwardapi-volume-2b368c2d-40c3-43c6-9b46-452cc1668e1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012029022s
    STEP: Saw pod success 06/08/23 16:10:54.869
    Jun  8 16:10:54.869: INFO: Pod "downwardapi-volume-2b368c2d-40c3-43c6-9b46-452cc1668e1c" satisfied condition "Succeeded or Failed"
    Jun  8 16:10:54.874: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-2b368c2d-40c3-43c6-9b46-452cc1668e1c container client-container: <nil>
    STEP: delete the pod 06/08/23 16:10:54.881
    Jun  8 16:10:54.902: INFO: Waiting for pod downwardapi-volume-2b368c2d-40c3-43c6-9b46-452cc1668e1c to disappear
    Jun  8 16:10:54.905: INFO: Pod downwardapi-volume-2b368c2d-40c3-43c6-9b46-452cc1668e1c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:10:54.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6604" for this suite. 06/08/23 16:10:54.912
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:10:54.92
Jun  8 16:10:54.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename events 06/08/23 16:10:54.921
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:10:54.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:10:54.942
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 06/08/23 16:10:54.945
STEP: listing events in all namespaces 06/08/23 16:10:54.953
STEP: listing events in test namespace 06/08/23 16:10:54.959
STEP: listing events with field selection filtering on source 06/08/23 16:10:54.963
STEP: listing events with field selection filtering on reportingController 06/08/23 16:10:54.966
STEP: getting the test event 06/08/23 16:10:54.97
STEP: patching the test event 06/08/23 16:10:54.973
STEP: getting the test event 06/08/23 16:10:54.985
STEP: updating the test event 06/08/23 16:10:54.989
STEP: getting the test event 06/08/23 16:10:54.996
STEP: deleting the test event 06/08/23 16:10:55
STEP: listing events in all namespaces 06/08/23 16:10:55.009
STEP: listing events in test namespace 06/08/23 16:10:55.013
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jun  8 16:10:55.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-5529" for this suite. 06/08/23 16:10:55.026
------------------------------
• [0.114 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:10:54.92
    Jun  8 16:10:54.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename events 06/08/23 16:10:54.921
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:10:54.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:10:54.942
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 06/08/23 16:10:54.945
    STEP: listing events in all namespaces 06/08/23 16:10:54.953
    STEP: listing events in test namespace 06/08/23 16:10:54.959
    STEP: listing events with field selection filtering on source 06/08/23 16:10:54.963
    STEP: listing events with field selection filtering on reportingController 06/08/23 16:10:54.966
    STEP: getting the test event 06/08/23 16:10:54.97
    STEP: patching the test event 06/08/23 16:10:54.973
    STEP: getting the test event 06/08/23 16:10:54.985
    STEP: updating the test event 06/08/23 16:10:54.989
    STEP: getting the test event 06/08/23 16:10:54.996
    STEP: deleting the test event 06/08/23 16:10:55
    STEP: listing events in all namespaces 06/08/23 16:10:55.009
    STEP: listing events in test namespace 06/08/23 16:10:55.013
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:10:55.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-5529" for this suite. 06/08/23 16:10:55.026
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:10:55.035
Jun  8 16:10:55.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename dns 06/08/23 16:10:55.036
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:10:55.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:10:55.057
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 06/08/23 16:10:55.06
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9760 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9760;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9760 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9760;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9760.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9760.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9760.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9760.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9760.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9760.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9760.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9760.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9760.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9760.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9760.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9760.svc;check="$$(dig +notcp +noall +answer +search 250.113.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.113.250_udp@PTR;check="$$(dig +tcp +noall +answer +search 250.113.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.113.250_tcp@PTR;sleep 1; done
 06/08/23 16:10:55.085
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9760 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9760;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9760 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9760;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9760.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9760.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9760.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9760.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9760.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9760.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9760.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9760.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9760.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9760.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9760.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9760.svc;check="$$(dig +notcp +noall +answer +search 250.113.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.113.250_udp@PTR;check="$$(dig +tcp +noall +answer +search 250.113.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.113.250_tcp@PTR;sleep 1; done
 06/08/23 16:10:55.085
STEP: creating a pod to probe DNS 06/08/23 16:10:55.085
STEP: submitting the pod to kubernetes 06/08/23 16:10:55.085
Jun  8 16:10:55.099: INFO: Waiting up to 15m0s for pod "dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44" in namespace "dns-9760" to be "running"
Jun  8 16:10:55.107: INFO: Pod "dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44": Phase="Pending", Reason="", readiness=false. Elapsed: 7.198345ms
Jun  8 16:10:57.114: INFO: Pod "dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44": Phase="Running", Reason="", readiness=true. Elapsed: 2.014334864s
Jun  8 16:10:57.114: INFO: Pod "dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44" satisfied condition "running"
STEP: retrieving the pod 06/08/23 16:10:57.114
STEP: looking for the results for each expected name from probers 06/08/23 16:10:57.121
Jun  8 16:10:57.130: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.137: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.143: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.150: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.164: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.170: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.182: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.189: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.196: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.202: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.209: INFO: Unable to read 10.108.113.250_udp@PTR from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.216: INFO: Unable to read 10.108.113.250_tcp@PTR from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.222: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.228: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.233: INFO: Unable to read jessie_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.239: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.245: INFO: Unable to read jessie_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.249: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.255: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.259: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.264: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.269: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.274: INFO: Unable to read 10.108.113.250_udp@PTR from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.278: INFO: Unable to read 10.108.113.250_tcp@PTR from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:10:57.278: INFO: Lookups using dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9760 wheezy_tcp@dns-test-service.dns-9760 wheezy_udp@dns-test-service.dns-9760.svc wheezy_tcp@dns-test-service.dns-9760.svc wheezy_udp@_http._tcp.dns-test-service.dns-9760.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9760.svc wheezy_udp@_http._tcp.test-service-2.dns-9760.svc wheezy_tcp@_http._tcp.test-service-2.dns-9760.svc 10.108.113.250_udp@PTR 10.108.113.250_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9760 jessie_tcp@dns-test-service.dns-9760 jessie_udp@dns-test-service.dns-9760.svc jessie_tcp@dns-test-service.dns-9760.svc jessie_udp@_http._tcp.dns-test-service.dns-9760.svc jessie_tcp@_http._tcp.dns-test-service.dns-9760.svc jessie_udp@_http._tcp.test-service-2.dns-9760.svc jessie_tcp@_http._tcp.test-service-2.dns-9760.svc 10.108.113.250_udp@PTR 10.108.113.250_tcp@PTR]

Jun  8 16:11:02.285: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:02.290: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:02.294: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:02.299: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:02.303: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:02.307: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:02.338: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:02.342: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:02.346: INFO: Unable to read jessie_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:02.350: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:02.354: INFO: Unable to read jessie_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:02.358: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:02.363: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:02.383: INFO: Lookups using dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9760 wheezy_tcp@dns-test-service.dns-9760 wheezy_udp@dns-test-service.dns-9760.svc wheezy_tcp@dns-test-service.dns-9760.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9760 jessie_tcp@dns-test-service.dns-9760 jessie_udp@dns-test-service.dns-9760.svc jessie_tcp@dns-test-service.dns-9760.svc jessie_udp@_http._tcp.dns-test-service.dns-9760.svc]

Jun  8 16:11:07.284: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:07.289: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:07.293: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:07.297: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:07.301: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:07.305: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:07.335: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:07.340: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:07.344: INFO: Unable to read jessie_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:07.348: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:07.352: INFO: Unable to read jessie_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:07.356: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:07.380: INFO: Lookups using dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9760 wheezy_tcp@dns-test-service.dns-9760 wheezy_udp@dns-test-service.dns-9760.svc wheezy_tcp@dns-test-service.dns-9760.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9760 jessie_tcp@dns-test-service.dns-9760 jessie_udp@dns-test-service.dns-9760.svc jessie_tcp@dns-test-service.dns-9760.svc]

Jun  8 16:11:12.284: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:12.289: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:12.294: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:12.298: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:12.302: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:12.306: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:12.334: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:12.339: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:12.343: INFO: Unable to read jessie_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:12.347: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:12.351: INFO: Unable to read jessie_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:12.355: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:12.380: INFO: Lookups using dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9760 wheezy_tcp@dns-test-service.dns-9760 wheezy_udp@dns-test-service.dns-9760.svc wheezy_tcp@dns-test-service.dns-9760.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9760 jessie_tcp@dns-test-service.dns-9760 jessie_udp@dns-test-service.dns-9760.svc jessie_tcp@dns-test-service.dns-9760.svc]

Jun  8 16:11:17.285: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:17.289: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:17.294: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:17.298: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:17.303: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:17.307: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:17.339: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:17.343: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:17.347: INFO: Unable to read jessie_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:17.353: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:17.357: INFO: Unable to read jessie_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:17.361: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:17.386: INFO: Lookups using dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9760 wheezy_tcp@dns-test-service.dns-9760 wheezy_udp@dns-test-service.dns-9760.svc wheezy_tcp@dns-test-service.dns-9760.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9760 jessie_tcp@dns-test-service.dns-9760 jessie_udp@dns-test-service.dns-9760.svc jessie_tcp@dns-test-service.dns-9760.svc]

Jun  8 16:11:22.286: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:22.291: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:22.295: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:22.299: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:22.304: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:22.309: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:22.339: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:22.343: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:22.348: INFO: Unable to read jessie_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:22.353: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:22.357: INFO: Unable to read jessie_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:22.361: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
Jun  8 16:11:22.386: INFO: Lookups using dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9760 wheezy_tcp@dns-test-service.dns-9760 wheezy_udp@dns-test-service.dns-9760.svc wheezy_tcp@dns-test-service.dns-9760.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9760 jessie_tcp@dns-test-service.dns-9760 jessie_udp@dns-test-service.dns-9760.svc jessie_tcp@dns-test-service.dns-9760.svc]

Jun  8 16:11:27.391: INFO: DNS probes using dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44 succeeded

STEP: deleting the pod 06/08/23 16:11:27.391
STEP: deleting the test service 06/08/23 16:11:27.409
STEP: deleting the test headless service 06/08/23 16:11:27.439
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun  8 16:11:27.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9760" for this suite. 06/08/23 16:11:27.464
------------------------------
• [SLOW TEST] [32.438 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:10:55.035
    Jun  8 16:10:55.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename dns 06/08/23 16:10:55.036
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:10:55.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:10:55.057
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 06/08/23 16:10:55.06
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9760 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9760;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9760 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9760;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9760.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9760.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9760.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9760.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9760.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9760.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9760.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9760.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9760.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9760.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9760.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9760.svc;check="$$(dig +notcp +noall +answer +search 250.113.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.113.250_udp@PTR;check="$$(dig +tcp +noall +answer +search 250.113.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.113.250_tcp@PTR;sleep 1; done
     06/08/23 16:10:55.085
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9760 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9760;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9760 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9760;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9760.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9760.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9760.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9760.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9760.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9760.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9760.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9760.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9760.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9760.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9760.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9760.svc;check="$$(dig +notcp +noall +answer +search 250.113.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.113.250_udp@PTR;check="$$(dig +tcp +noall +answer +search 250.113.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.113.250_tcp@PTR;sleep 1; done
     06/08/23 16:10:55.085
    STEP: creating a pod to probe DNS 06/08/23 16:10:55.085
    STEP: submitting the pod to kubernetes 06/08/23 16:10:55.085
    Jun  8 16:10:55.099: INFO: Waiting up to 15m0s for pod "dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44" in namespace "dns-9760" to be "running"
    Jun  8 16:10:55.107: INFO: Pod "dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44": Phase="Pending", Reason="", readiness=false. Elapsed: 7.198345ms
    Jun  8 16:10:57.114: INFO: Pod "dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44": Phase="Running", Reason="", readiness=true. Elapsed: 2.014334864s
    Jun  8 16:10:57.114: INFO: Pod "dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44" satisfied condition "running"
    STEP: retrieving the pod 06/08/23 16:10:57.114
    STEP: looking for the results for each expected name from probers 06/08/23 16:10:57.121
    Jun  8 16:10:57.130: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.137: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.143: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.150: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.164: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.170: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.182: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.189: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.196: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.202: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.209: INFO: Unable to read 10.108.113.250_udp@PTR from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.216: INFO: Unable to read 10.108.113.250_tcp@PTR from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.222: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.228: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.233: INFO: Unable to read jessie_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.239: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.245: INFO: Unable to read jessie_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.249: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.255: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.259: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.264: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.269: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.274: INFO: Unable to read 10.108.113.250_udp@PTR from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.278: INFO: Unable to read 10.108.113.250_tcp@PTR from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:10:57.278: INFO: Lookups using dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9760 wheezy_tcp@dns-test-service.dns-9760 wheezy_udp@dns-test-service.dns-9760.svc wheezy_tcp@dns-test-service.dns-9760.svc wheezy_udp@_http._tcp.dns-test-service.dns-9760.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9760.svc wheezy_udp@_http._tcp.test-service-2.dns-9760.svc wheezy_tcp@_http._tcp.test-service-2.dns-9760.svc 10.108.113.250_udp@PTR 10.108.113.250_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9760 jessie_tcp@dns-test-service.dns-9760 jessie_udp@dns-test-service.dns-9760.svc jessie_tcp@dns-test-service.dns-9760.svc jessie_udp@_http._tcp.dns-test-service.dns-9760.svc jessie_tcp@_http._tcp.dns-test-service.dns-9760.svc jessie_udp@_http._tcp.test-service-2.dns-9760.svc jessie_tcp@_http._tcp.test-service-2.dns-9760.svc 10.108.113.250_udp@PTR 10.108.113.250_tcp@PTR]

    Jun  8 16:11:02.285: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:02.290: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:02.294: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:02.299: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:02.303: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:02.307: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:02.338: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:02.342: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:02.346: INFO: Unable to read jessie_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:02.350: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:02.354: INFO: Unable to read jessie_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:02.358: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:02.363: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:02.383: INFO: Lookups using dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9760 wheezy_tcp@dns-test-service.dns-9760 wheezy_udp@dns-test-service.dns-9760.svc wheezy_tcp@dns-test-service.dns-9760.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9760 jessie_tcp@dns-test-service.dns-9760 jessie_udp@dns-test-service.dns-9760.svc jessie_tcp@dns-test-service.dns-9760.svc jessie_udp@_http._tcp.dns-test-service.dns-9760.svc]

    Jun  8 16:11:07.284: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:07.289: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:07.293: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:07.297: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:07.301: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:07.305: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:07.335: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:07.340: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:07.344: INFO: Unable to read jessie_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:07.348: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:07.352: INFO: Unable to read jessie_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:07.356: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:07.380: INFO: Lookups using dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9760 wheezy_tcp@dns-test-service.dns-9760 wheezy_udp@dns-test-service.dns-9760.svc wheezy_tcp@dns-test-service.dns-9760.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9760 jessie_tcp@dns-test-service.dns-9760 jessie_udp@dns-test-service.dns-9760.svc jessie_tcp@dns-test-service.dns-9760.svc]

    Jun  8 16:11:12.284: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:12.289: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:12.294: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:12.298: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:12.302: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:12.306: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:12.334: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:12.339: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:12.343: INFO: Unable to read jessie_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:12.347: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:12.351: INFO: Unable to read jessie_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:12.355: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:12.380: INFO: Lookups using dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9760 wheezy_tcp@dns-test-service.dns-9760 wheezy_udp@dns-test-service.dns-9760.svc wheezy_tcp@dns-test-service.dns-9760.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9760 jessie_tcp@dns-test-service.dns-9760 jessie_udp@dns-test-service.dns-9760.svc jessie_tcp@dns-test-service.dns-9760.svc]

    Jun  8 16:11:17.285: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:17.289: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:17.294: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:17.298: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:17.303: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:17.307: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:17.339: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:17.343: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:17.347: INFO: Unable to read jessie_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:17.353: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:17.357: INFO: Unable to read jessie_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:17.361: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:17.386: INFO: Lookups using dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9760 wheezy_tcp@dns-test-service.dns-9760 wheezy_udp@dns-test-service.dns-9760.svc wheezy_tcp@dns-test-service.dns-9760.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9760 jessie_tcp@dns-test-service.dns-9760 jessie_udp@dns-test-service.dns-9760.svc jessie_tcp@dns-test-service.dns-9760.svc]

    Jun  8 16:11:22.286: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:22.291: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:22.295: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:22.299: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:22.304: INFO: Unable to read wheezy_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:22.309: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:22.339: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:22.343: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:22.348: INFO: Unable to read jessie_udp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:22.353: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760 from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:22.357: INFO: Unable to read jessie_udp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:22.361: INFO: Unable to read jessie_tcp@dns-test-service.dns-9760.svc from pod dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44: the server could not find the requested resource (get pods dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44)
    Jun  8 16:11:22.386: INFO: Lookups using dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9760 wheezy_tcp@dns-test-service.dns-9760 wheezy_udp@dns-test-service.dns-9760.svc wheezy_tcp@dns-test-service.dns-9760.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9760 jessie_tcp@dns-test-service.dns-9760 jessie_udp@dns-test-service.dns-9760.svc jessie_tcp@dns-test-service.dns-9760.svc]

    Jun  8 16:11:27.391: INFO: DNS probes using dns-9760/dns-test-dcd206ad-b839-42f4-b550-2e6d74a2fb44 succeeded

    STEP: deleting the pod 06/08/23 16:11:27.391
    STEP: deleting the test service 06/08/23 16:11:27.409
    STEP: deleting the test headless service 06/08/23 16:11:27.439
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:11:27.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9760" for this suite. 06/08/23 16:11:27.464
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:11:27.477
Jun  8 16:11:27.477: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename disruption 06/08/23 16:11:27.478
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:11:27.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:11:27.505
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 06/08/23 16:11:27.509
STEP: Waiting for the pdb to be processed 06/08/23 16:11:27.515
STEP: First trying to evict a pod which shouldn't be evictable 06/08/23 16:11:29.533
STEP: Waiting for all pods to be running 06/08/23 16:11:29.533
Jun  8 16:11:29.540: INFO: pods: 0 < 3
STEP: locating a running pod 06/08/23 16:11:31.545
STEP: Updating the pdb to allow a pod to be evicted 06/08/23 16:11:31.557
STEP: Waiting for the pdb to be processed 06/08/23 16:11:31.567
STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/08/23 16:11:33.576
STEP: Waiting for all pods to be running 06/08/23 16:11:33.576
STEP: Waiting for the pdb to observed all healthy pods 06/08/23 16:11:33.581
STEP: Patching the pdb to disallow a pod to be evicted 06/08/23 16:11:33.609
STEP: Waiting for the pdb to be processed 06/08/23 16:11:33.637
STEP: Waiting for all pods to be running 06/08/23 16:11:35.646
STEP: locating a running pod 06/08/23 16:11:35.651
STEP: Deleting the pdb to allow a pod to be evicted 06/08/23 16:11:35.663
STEP: Waiting for the pdb to be deleted 06/08/23 16:11:35.67
STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/08/23 16:11:35.673
STEP: Waiting for all pods to be running 06/08/23 16:11:35.673
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jun  8 16:11:35.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6753" for this suite. 06/08/23 16:11:35.7
------------------------------
• [SLOW TEST] [8.233 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:11:27.477
    Jun  8 16:11:27.477: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename disruption 06/08/23 16:11:27.478
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:11:27.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:11:27.505
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 06/08/23 16:11:27.509
    STEP: Waiting for the pdb to be processed 06/08/23 16:11:27.515
    STEP: First trying to evict a pod which shouldn't be evictable 06/08/23 16:11:29.533
    STEP: Waiting for all pods to be running 06/08/23 16:11:29.533
    Jun  8 16:11:29.540: INFO: pods: 0 < 3
    STEP: locating a running pod 06/08/23 16:11:31.545
    STEP: Updating the pdb to allow a pod to be evicted 06/08/23 16:11:31.557
    STEP: Waiting for the pdb to be processed 06/08/23 16:11:31.567
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/08/23 16:11:33.576
    STEP: Waiting for all pods to be running 06/08/23 16:11:33.576
    STEP: Waiting for the pdb to observed all healthy pods 06/08/23 16:11:33.581
    STEP: Patching the pdb to disallow a pod to be evicted 06/08/23 16:11:33.609
    STEP: Waiting for the pdb to be processed 06/08/23 16:11:33.637
    STEP: Waiting for all pods to be running 06/08/23 16:11:35.646
    STEP: locating a running pod 06/08/23 16:11:35.651
    STEP: Deleting the pdb to allow a pod to be evicted 06/08/23 16:11:35.663
    STEP: Waiting for the pdb to be deleted 06/08/23 16:11:35.67
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/08/23 16:11:35.673
    STEP: Waiting for all pods to be running 06/08/23 16:11:35.673
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:11:35.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6753" for this suite. 06/08/23 16:11:35.7
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:11:35.711
Jun  8 16:11:35.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename namespaces 06/08/23 16:11:35.712
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:11:35.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:11:35.736
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-5595" 06/08/23 16:11:35.739
Jun  8 16:11:35.749: INFO: Namespace "namespaces-5595" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"ca71eedc-529b-4fa2-b741-e4cd93d22b67", "kubernetes.io/metadata.name":"namespaces-5595", "namespaces-5595":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:11:35.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5595" for this suite. 06/08/23 16:11:35.755
------------------------------
• [0.052 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:11:35.711
    Jun  8 16:11:35.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename namespaces 06/08/23 16:11:35.712
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:11:35.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:11:35.736
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-5595" 06/08/23 16:11:35.739
    Jun  8 16:11:35.749: INFO: Namespace "namespaces-5595" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"ca71eedc-529b-4fa2-b741-e4cd93d22b67", "kubernetes.io/metadata.name":"namespaces-5595", "namespaces-5595":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:11:35.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5595" for this suite. 06/08/23 16:11:35.755
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:11:35.763
Jun  8 16:11:35.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename secrets 06/08/23 16:11:35.764
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:11:35.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:11:35.785
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-7df76222-b89a-4145-9318-84cd79858b61 06/08/23 16:11:35.788
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  8 16:11:35.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9507" for this suite. 06/08/23 16:11:35.799
------------------------------
• [0.051 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:11:35.763
    Jun  8 16:11:35.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename secrets 06/08/23 16:11:35.764
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:11:35.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:11:35.785
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-7df76222-b89a-4145-9318-84cd79858b61 06/08/23 16:11:35.788
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:11:35.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9507" for this suite. 06/08/23 16:11:35.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:11:35.815
Jun  8 16:11:35.815: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename statefulset 06/08/23 16:11:35.816
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:11:35.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:11:35.848
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9097 06/08/23 16:11:35.852
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Jun  8 16:11:35.871: INFO: Found 0 stateful pods, waiting for 1
Jun  8 16:11:45.878: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 06/08/23 16:11:45.887
W0608 16:11:45.896210      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jun  8 16:11:45.905: INFO: Found 1 stateful pods, waiting for 2
Jun  8 16:11:55.911: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  8 16:11:55.911: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 06/08/23 16:11:55.92
STEP: Delete all of the StatefulSets 06/08/23 16:11:55.924
STEP: Verify that StatefulSets have been deleted 06/08/23 16:11:55.933
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun  8 16:11:55.937: INFO: Deleting all statefulset in ns statefulset-9097
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun  8 16:11:55.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9097" for this suite. 06/08/23 16:11:55.953
------------------------------
• [SLOW TEST] [20.146 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:11:35.815
    Jun  8 16:11:35.815: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename statefulset 06/08/23 16:11:35.816
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:11:35.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:11:35.848
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9097 06/08/23 16:11:35.852
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Jun  8 16:11:35.871: INFO: Found 0 stateful pods, waiting for 1
    Jun  8 16:11:45.878: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 06/08/23 16:11:45.887
    W0608 16:11:45.896210      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jun  8 16:11:45.905: INFO: Found 1 stateful pods, waiting for 2
    Jun  8 16:11:55.911: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun  8 16:11:55.911: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 06/08/23 16:11:55.92
    STEP: Delete all of the StatefulSets 06/08/23 16:11:55.924
    STEP: Verify that StatefulSets have been deleted 06/08/23 16:11:55.933
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun  8 16:11:55.937: INFO: Deleting all statefulset in ns statefulset-9097
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:11:55.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9097" for this suite. 06/08/23 16:11:55.953
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:11:55.961
Jun  8 16:11:55.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename pods 06/08/23 16:11:55.963
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:11:55.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:11:55.988
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 06/08/23 16:11:55.991
STEP: submitting the pod to kubernetes 06/08/23 16:11:55.992
Jun  8 16:11:56.001: INFO: Waiting up to 5m0s for pod "pod-update-ebd5dcfa-86b0-4cd1-82a1-89283503bc5e" in namespace "pods-7797" to be "running and ready"
Jun  8 16:11:56.007: INFO: Pod "pod-update-ebd5dcfa-86b0-4cd1-82a1-89283503bc5e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.354345ms
Jun  8 16:11:56.007: INFO: The phase of Pod pod-update-ebd5dcfa-86b0-4cd1-82a1-89283503bc5e is Pending, waiting for it to be Running (with Ready = true)
Jun  8 16:11:58.012: INFO: Pod "pod-update-ebd5dcfa-86b0-4cd1-82a1-89283503bc5e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010871939s
Jun  8 16:11:58.012: INFO: The phase of Pod pod-update-ebd5dcfa-86b0-4cd1-82a1-89283503bc5e is Running (Ready = true)
Jun  8 16:11:58.012: INFO: Pod "pod-update-ebd5dcfa-86b0-4cd1-82a1-89283503bc5e" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 06/08/23 16:11:58.016
STEP: updating the pod 06/08/23 16:11:58.021
Jun  8 16:11:58.534: INFO: Successfully updated pod "pod-update-ebd5dcfa-86b0-4cd1-82a1-89283503bc5e"
Jun  8 16:11:58.534: INFO: Waiting up to 5m0s for pod "pod-update-ebd5dcfa-86b0-4cd1-82a1-89283503bc5e" in namespace "pods-7797" to be "running"
Jun  8 16:11:58.538: INFO: Pod "pod-update-ebd5dcfa-86b0-4cd1-82a1-89283503bc5e": Phase="Running", Reason="", readiness=true. Elapsed: 3.750911ms
Jun  8 16:11:58.538: INFO: Pod "pod-update-ebd5dcfa-86b0-4cd1-82a1-89283503bc5e" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 06/08/23 16:11:58.538
Jun  8 16:11:58.541: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun  8 16:11:58.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7797" for this suite. 06/08/23 16:11:58.549
------------------------------
• [2.594 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:11:55.961
    Jun  8 16:11:55.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename pods 06/08/23 16:11:55.963
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:11:55.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:11:55.988
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 06/08/23 16:11:55.991
    STEP: submitting the pod to kubernetes 06/08/23 16:11:55.992
    Jun  8 16:11:56.001: INFO: Waiting up to 5m0s for pod "pod-update-ebd5dcfa-86b0-4cd1-82a1-89283503bc5e" in namespace "pods-7797" to be "running and ready"
    Jun  8 16:11:56.007: INFO: Pod "pod-update-ebd5dcfa-86b0-4cd1-82a1-89283503bc5e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.354345ms
    Jun  8 16:11:56.007: INFO: The phase of Pod pod-update-ebd5dcfa-86b0-4cd1-82a1-89283503bc5e is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 16:11:58.012: INFO: Pod "pod-update-ebd5dcfa-86b0-4cd1-82a1-89283503bc5e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010871939s
    Jun  8 16:11:58.012: INFO: The phase of Pod pod-update-ebd5dcfa-86b0-4cd1-82a1-89283503bc5e is Running (Ready = true)
    Jun  8 16:11:58.012: INFO: Pod "pod-update-ebd5dcfa-86b0-4cd1-82a1-89283503bc5e" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 06/08/23 16:11:58.016
    STEP: updating the pod 06/08/23 16:11:58.021
    Jun  8 16:11:58.534: INFO: Successfully updated pod "pod-update-ebd5dcfa-86b0-4cd1-82a1-89283503bc5e"
    Jun  8 16:11:58.534: INFO: Waiting up to 5m0s for pod "pod-update-ebd5dcfa-86b0-4cd1-82a1-89283503bc5e" in namespace "pods-7797" to be "running"
    Jun  8 16:11:58.538: INFO: Pod "pod-update-ebd5dcfa-86b0-4cd1-82a1-89283503bc5e": Phase="Running", Reason="", readiness=true. Elapsed: 3.750911ms
    Jun  8 16:11:58.538: INFO: Pod "pod-update-ebd5dcfa-86b0-4cd1-82a1-89283503bc5e" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 06/08/23 16:11:58.538
    Jun  8 16:11:58.541: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:11:58.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7797" for this suite. 06/08/23 16:11:58.549
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:11:58.557
Jun  8 16:11:58.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename statefulset 06/08/23 16:11:58.558
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:11:58.575
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:11:58.579
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-49 06/08/23 16:11:58.583
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 06/08/23 16:11:58.589
STEP: Creating stateful set ss in namespace statefulset-49 06/08/23 16:11:58.594
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-49 06/08/23 16:11:58.6
Jun  8 16:11:58.604: INFO: Found 0 stateful pods, waiting for 1
Jun  8 16:12:08.610: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 06/08/23 16:12:08.61
Jun  8 16:12:08.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-49 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  8 16:12:08.796: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  8 16:12:08.796: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  8 16:12:08.796: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  8 16:12:08.801: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun  8 16:12:18.811: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  8 16:12:18.811: INFO: Waiting for statefulset status.replicas updated to 0
Jun  8 16:12:18.832: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999688s
Jun  8 16:12:19.838: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99300397s
Jun  8 16:12:20.844: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987776593s
Jun  8 16:12:21.850: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981549374s
Jun  8 16:12:22.856: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.976657527s
Jun  8 16:12:23.862: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.970249637s
Jun  8 16:12:24.867: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.964351056s
Jun  8 16:12:25.872: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.959769482s
Jun  8 16:12:26.878: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.953962067s
Jun  8 16:12:27.882: INFO: Verifying statefulset ss doesn't scale past 1 for another 948.727468ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-49 06/08/23 16:12:28.883
Jun  8 16:12:28.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-49 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  8 16:12:29.061: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  8 16:12:29.061: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  8 16:12:29.061: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  8 16:12:29.066: INFO: Found 1 stateful pods, waiting for 3
Jun  8 16:12:39.074: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  8 16:12:39.074: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  8 16:12:39.074: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 06/08/23 16:12:39.074
STEP: Scale down will halt with unhealthy stateful pod 06/08/23 16:12:39.074
Jun  8 16:12:39.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-49 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  8 16:12:39.242: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  8 16:12:39.242: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  8 16:12:39.242: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  8 16:12:39.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-49 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  8 16:12:39.406: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  8 16:12:39.406: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  8 16:12:39.406: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  8 16:12:39.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-49 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  8 16:12:39.580: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  8 16:12:39.580: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  8 16:12:39.580: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  8 16:12:39.580: INFO: Waiting for statefulset status.replicas updated to 0
Jun  8 16:12:39.584: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jun  8 16:12:49.594: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  8 16:12:49.594: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun  8 16:12:49.594: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun  8 16:12:49.609: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999679s
Jun  8 16:12:50.614: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995728481s
Jun  8 16:12:51.620: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989795027s
Jun  8 16:12:52.626: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984353021s
Jun  8 16:12:53.632: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977568798s
Jun  8 16:12:54.637: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97197249s
Jun  8 16:12:55.643: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967334676s
Jun  8 16:12:56.649: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.960930824s
Jun  8 16:12:57.655: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.95523439s
Jun  8 16:12:58.661: INFO: Verifying statefulset ss doesn't scale past 3 for another 949.175713ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-49 06/08/23 16:12:59.661
Jun  8 16:12:59.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-49 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  8 16:12:59.826: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  8 16:12:59.826: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  8 16:12:59.826: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  8 16:12:59.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-49 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  8 16:12:59.996: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  8 16:12:59.996: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  8 16:12:59.996: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  8 16:12:59.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-49 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  8 16:13:00.165: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  8 16:13:00.165: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  8 16:13:00.165: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  8 16:13:00.165: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 06/08/23 16:13:10.184
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun  8 16:13:10.184: INFO: Deleting all statefulset in ns statefulset-49
Jun  8 16:13:10.188: INFO: Scaling statefulset ss to 0
Jun  8 16:13:10.202: INFO: Waiting for statefulset status.replicas updated to 0
Jun  8 16:13:10.206: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun  8 16:13:10.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-49" for this suite. 06/08/23 16:13:10.225
------------------------------
• [SLOW TEST] [71.675 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:11:58.557
    Jun  8 16:11:58.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename statefulset 06/08/23 16:11:58.558
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:11:58.575
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:11:58.579
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-49 06/08/23 16:11:58.583
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 06/08/23 16:11:58.589
    STEP: Creating stateful set ss in namespace statefulset-49 06/08/23 16:11:58.594
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-49 06/08/23 16:11:58.6
    Jun  8 16:11:58.604: INFO: Found 0 stateful pods, waiting for 1
    Jun  8 16:12:08.610: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 06/08/23 16:12:08.61
    Jun  8 16:12:08.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-49 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun  8 16:12:08.796: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun  8 16:12:08.796: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun  8 16:12:08.796: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun  8 16:12:08.801: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jun  8 16:12:18.811: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun  8 16:12:18.811: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  8 16:12:18.832: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999688s
    Jun  8 16:12:19.838: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99300397s
    Jun  8 16:12:20.844: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987776593s
    Jun  8 16:12:21.850: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981549374s
    Jun  8 16:12:22.856: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.976657527s
    Jun  8 16:12:23.862: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.970249637s
    Jun  8 16:12:24.867: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.964351056s
    Jun  8 16:12:25.872: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.959769482s
    Jun  8 16:12:26.878: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.953962067s
    Jun  8 16:12:27.882: INFO: Verifying statefulset ss doesn't scale past 1 for another 948.727468ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-49 06/08/23 16:12:28.883
    Jun  8 16:12:28.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-49 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun  8 16:12:29.061: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun  8 16:12:29.061: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun  8 16:12:29.061: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun  8 16:12:29.066: INFO: Found 1 stateful pods, waiting for 3
    Jun  8 16:12:39.074: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun  8 16:12:39.074: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun  8 16:12:39.074: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 06/08/23 16:12:39.074
    STEP: Scale down will halt with unhealthy stateful pod 06/08/23 16:12:39.074
    Jun  8 16:12:39.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-49 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun  8 16:12:39.242: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun  8 16:12:39.242: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun  8 16:12:39.242: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun  8 16:12:39.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-49 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun  8 16:12:39.406: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun  8 16:12:39.406: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun  8 16:12:39.406: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun  8 16:12:39.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-49 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun  8 16:12:39.580: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun  8 16:12:39.580: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun  8 16:12:39.580: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun  8 16:12:39.580: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  8 16:12:39.584: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jun  8 16:12:49.594: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun  8 16:12:49.594: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jun  8 16:12:49.594: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jun  8 16:12:49.609: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999679s
    Jun  8 16:12:50.614: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995728481s
    Jun  8 16:12:51.620: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989795027s
    Jun  8 16:12:52.626: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984353021s
    Jun  8 16:12:53.632: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977568798s
    Jun  8 16:12:54.637: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97197249s
    Jun  8 16:12:55.643: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967334676s
    Jun  8 16:12:56.649: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.960930824s
    Jun  8 16:12:57.655: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.95523439s
    Jun  8 16:12:58.661: INFO: Verifying statefulset ss doesn't scale past 3 for another 949.175713ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-49 06/08/23 16:12:59.661
    Jun  8 16:12:59.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-49 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun  8 16:12:59.826: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun  8 16:12:59.826: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun  8 16:12:59.826: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun  8 16:12:59.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-49 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun  8 16:12:59.996: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun  8 16:12:59.996: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun  8 16:12:59.996: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun  8 16:12:59.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=statefulset-49 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun  8 16:13:00.165: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun  8 16:13:00.165: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun  8 16:13:00.165: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun  8 16:13:00.165: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 06/08/23 16:13:10.184
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun  8 16:13:10.184: INFO: Deleting all statefulset in ns statefulset-49
    Jun  8 16:13:10.188: INFO: Scaling statefulset ss to 0
    Jun  8 16:13:10.202: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  8 16:13:10.206: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:13:10.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-49" for this suite. 06/08/23 16:13:10.225
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:13:10.234
Jun  8 16:13:10.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubectl 06/08/23 16:13:10.235
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:13:10.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:13:10.255
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Jun  8 16:13:10.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8441 create -f -'
Jun  8 16:13:10.933: INFO: stderr: ""
Jun  8 16:13:10.933: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jun  8 16:13:10.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8441 create -f -'
Jun  8 16:13:11.548: INFO: stderr: ""
Jun  8 16:13:11.548: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 06/08/23 16:13:11.548
Jun  8 16:13:12.553: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  8 16:13:12.553: INFO: Found 0 / 1
Jun  8 16:13:13.552: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  8 16:13:13.552: INFO: Found 1 / 1
Jun  8 16:13:13.552: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun  8 16:13:13.556: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  8 16:13:13.556: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun  8 16:13:13.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8441 describe pod agnhost-primary-5zps6'
Jun  8 16:13:13.648: INFO: stderr: ""
Jun  8 16:13:13.648: INFO: stdout: "Name:             agnhost-primary-5zps6\nNamespace:        kubectl-8441\nPriority:         0\nService Account:  default\nNode:             chl8tf-worker-001/100.100.236.215\nStart Time:       Thu, 08 Jun 2023 16:13:10 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.244.3.56\nIPs:\n  IP:           10.244.3.56\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://c5241815b3884fabe51f051339b6c57b477a6e2fb145954d29f23bb70744b415\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 08 Jun 2023 16:13:13 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xqfs5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-xqfs5:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type     Reason       Age   From               Message\n  ----     ------       ----  ----               -------\n  Normal   Scheduled    2s    default-scheduler  Successfully assigned kubectl-8441/agnhost-primary-5zps6 to chl8tf-worker-001\n  Warning  FailedMount  1s    kubelet            MountVolume.SetUp failed for volume \"kube-api-access-xqfs5\" : failed to sync configmap cache: timed out waiting for the condition\n  Normal   Pulled       1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal   Created      0s    kubelet            Created container agnhost-primary\n  Normal   Started      0s    kubelet            Started container agnhost-primary\n"
Jun  8 16:13:13.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8441 describe rc agnhost-primary'
Jun  8 16:13:13.748: INFO: stderr: ""
Jun  8 16:13:13.748: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8441\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-5zps6\n"
Jun  8 16:13:13.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8441 describe service agnhost-primary'
Jun  8 16:13:13.830: INFO: stderr: ""
Jun  8 16:13:13.830: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8441\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.99.38.239\nIPs:               10.99.38.239\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.3.56:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun  8 16:13:13.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8441 describe node chl8tf-control-plane-001'
Jun  8 16:13:13.957: INFO: stderr: ""
Jun  8 16:13:13.957: INFO: stdout: "Name:               chl8tf-control-plane-001\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/zone=US-ASHBURN-AD-2\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=chl8tf-control-plane-001\n                    kubernetes.io/os=linux\n                    oci.oraclecloud.com/fault-domain=FAULT-DOMAIN-1\n                    topology.kubernetes.io/zone=US-ASHBURN-AD-2\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 100.100.237.165\n                    csi.volume.kubernetes.io/nodeid:\n                      {\"blockvolume.csi.oraclecloud.com\":\"chl8tf-control-plane-001\",\"fss.csi.oraclecloud.com\":\"chl8tf-control-plane-001\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"52:0f:dd:52:6b:c9\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 100.100.236.3\n                    flannel.alpha.coreos.com/public-ip-overwrite: 100.100.236.3\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    oci.oraclecloud.com/compartment-id: ocid1.compartment.oc1..aaaaaaaavjdneys2lqfuppuoqhkddfn7cqni5ojc5sha5uigqiyxxgg2zdta\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 08 Jun 2023 13:31:00 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  chl8tf-control-plane-001\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 08 Jun 2023 16:13:07 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 08 Jun 2023 13:33:52 +0000   Thu, 08 Jun 2023 13:33:52 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Thu, 08 Jun 2023 16:12:33 +0000   Thu, 08 Jun 2023 13:30:57 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 08 Jun 2023 16:12:33 +0000   Thu, 08 Jun 2023 13:30:57 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 08 Jun 2023 16:12:33 +0000   Thu, 08 Jun 2023 13:30:57 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 08 Jun 2023 16:12:33 +0000   Thu, 08 Jun 2023 14:41:37 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  100.100.237.165\n  Hostname:    chl8tf-control-plane-001\nCapacity:\n  cpu:                4\n  ephemeral-storage:  72227088Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             30491276Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  66564484191\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             30388876Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 256c5c353e1d4bc08eaf6fbc173bd2f5\n  System UUID:                256c5c35-3e1d-4bc0-8eaf-6fbc173bd2f5\n  Boot ID:                    2e41a8cb-25c1-4c43-8da1-46288ad36426\n  Kernel Version:             5.15.0-101.103.2.1.el8uek.x86_64\n  OS Image:                   Oracle Linux Server 8.7\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.26.3\n  Kubelet Version:            v1.26.5+2.el8\n  Kube-Proxy Version:         v1.26.5+2.el8\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nProviderID:                   ocid1.instance.oc1.iad.anuwcljtgj4tlxyclyp7kdn4zxwghoqcjr4djl5udlr4xi2gyhn4mj26j32q\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 csi-oci-node-5p7f5                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         95m\n  kube-system                 etcd-chl8tf-control-plane-001                              100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         162m\n  kube-system                 kube-apiserver-chl8tf-control-plane-001                    250m (6%)     0 (0%)      0 (0%)           0 (0%)         162m\n  kube-system                 kube-controller-manager-chl8tf-control-plane-001           200m (5%)     0 (0%)      0 (0%)           0 (0%)         91m\n  kube-system                 kube-flannel-ds-d5bvw                                      100m (2%)     100m (2%)   50Mi (0%)        50Mi (0%)      159m\n  kube-system                 kube-proxy-j2p7z                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         161m\n  kube-system                 kube-scheduler-chl8tf-control-plane-001                    100m (2%)     0 (0%)      0 (0%)           0 (0%)         162m\n  kube-system                 oci-cloud-controller-manager-9n2zj                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         95m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-wmj9x    0 (0%)        0 (0%)      0 (0%)           0 (0%)         67m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                750m (18%)  100m (2%)\n  memory             150Mi (0%)  50Mi (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Jun  8 16:13:13.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8441 describe namespace kubectl-8441'
Jun  8 16:13:14.047: INFO: stderr: ""
Jun  8 16:13:14.047: INFO: stdout: "Name:         kubectl-8441\nLabels:       e2e-framework=kubectl\n              e2e-run=ca71eedc-529b-4fa2-b741-e4cd93d22b67\n              kubernetes.io/metadata.name=kubectl-8441\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  8 16:13:14.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8441" for this suite. 06/08/23 16:13:14.054
------------------------------
• [3.828 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:13:10.234
    Jun  8 16:13:10.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubectl 06/08/23 16:13:10.235
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:13:10.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:13:10.255
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Jun  8 16:13:10.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8441 create -f -'
    Jun  8 16:13:10.933: INFO: stderr: ""
    Jun  8 16:13:10.933: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jun  8 16:13:10.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8441 create -f -'
    Jun  8 16:13:11.548: INFO: stderr: ""
    Jun  8 16:13:11.548: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 06/08/23 16:13:11.548
    Jun  8 16:13:12.553: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun  8 16:13:12.553: INFO: Found 0 / 1
    Jun  8 16:13:13.552: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun  8 16:13:13.552: INFO: Found 1 / 1
    Jun  8 16:13:13.552: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jun  8 16:13:13.556: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun  8 16:13:13.556: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jun  8 16:13:13.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8441 describe pod agnhost-primary-5zps6'
    Jun  8 16:13:13.648: INFO: stderr: ""
    Jun  8 16:13:13.648: INFO: stdout: "Name:             agnhost-primary-5zps6\nNamespace:        kubectl-8441\nPriority:         0\nService Account:  default\nNode:             chl8tf-worker-001/100.100.236.215\nStart Time:       Thu, 08 Jun 2023 16:13:10 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.244.3.56\nIPs:\n  IP:           10.244.3.56\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://c5241815b3884fabe51f051339b6c57b477a6e2fb145954d29f23bb70744b415\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 08 Jun 2023 16:13:13 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xqfs5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-xqfs5:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type     Reason       Age   From               Message\n  ----     ------       ----  ----               -------\n  Normal   Scheduled    2s    default-scheduler  Successfully assigned kubectl-8441/agnhost-primary-5zps6 to chl8tf-worker-001\n  Warning  FailedMount  1s    kubelet            MountVolume.SetUp failed for volume \"kube-api-access-xqfs5\" : failed to sync configmap cache: timed out waiting for the condition\n  Normal   Pulled       1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal   Created      0s    kubelet            Created container agnhost-primary\n  Normal   Started      0s    kubelet            Started container agnhost-primary\n"
    Jun  8 16:13:13.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8441 describe rc agnhost-primary'
    Jun  8 16:13:13.748: INFO: stderr: ""
    Jun  8 16:13:13.748: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8441\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-5zps6\n"
    Jun  8 16:13:13.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8441 describe service agnhost-primary'
    Jun  8 16:13:13.830: INFO: stderr: ""
    Jun  8 16:13:13.830: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8441\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.99.38.239\nIPs:               10.99.38.239\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.3.56:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jun  8 16:13:13.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8441 describe node chl8tf-control-plane-001'
    Jun  8 16:13:13.957: INFO: stderr: ""
    Jun  8 16:13:13.957: INFO: stdout: "Name:               chl8tf-control-plane-001\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/zone=US-ASHBURN-AD-2\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=chl8tf-control-plane-001\n                    kubernetes.io/os=linux\n                    oci.oraclecloud.com/fault-domain=FAULT-DOMAIN-1\n                    topology.kubernetes.io/zone=US-ASHBURN-AD-2\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 100.100.237.165\n                    csi.volume.kubernetes.io/nodeid:\n                      {\"blockvolume.csi.oraclecloud.com\":\"chl8tf-control-plane-001\",\"fss.csi.oraclecloud.com\":\"chl8tf-control-plane-001\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"52:0f:dd:52:6b:c9\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 100.100.236.3\n                    flannel.alpha.coreos.com/public-ip-overwrite: 100.100.236.3\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    oci.oraclecloud.com/compartment-id: ocid1.compartment.oc1..aaaaaaaavjdneys2lqfuppuoqhkddfn7cqni5ojc5sha5uigqiyxxgg2zdta\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 08 Jun 2023 13:31:00 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  chl8tf-control-plane-001\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 08 Jun 2023 16:13:07 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 08 Jun 2023 13:33:52 +0000   Thu, 08 Jun 2023 13:33:52 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Thu, 08 Jun 2023 16:12:33 +0000   Thu, 08 Jun 2023 13:30:57 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 08 Jun 2023 16:12:33 +0000   Thu, 08 Jun 2023 13:30:57 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 08 Jun 2023 16:12:33 +0000   Thu, 08 Jun 2023 13:30:57 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 08 Jun 2023 16:12:33 +0000   Thu, 08 Jun 2023 14:41:37 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  100.100.237.165\n  Hostname:    chl8tf-control-plane-001\nCapacity:\n  cpu:                4\n  ephemeral-storage:  72227088Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             30491276Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  66564484191\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             30388876Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 256c5c353e1d4bc08eaf6fbc173bd2f5\n  System UUID:                256c5c35-3e1d-4bc0-8eaf-6fbc173bd2f5\n  Boot ID:                    2e41a8cb-25c1-4c43-8da1-46288ad36426\n  Kernel Version:             5.15.0-101.103.2.1.el8uek.x86_64\n  OS Image:                   Oracle Linux Server 8.7\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.26.3\n  Kubelet Version:            v1.26.5+2.el8\n  Kube-Proxy Version:         v1.26.5+2.el8\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nProviderID:                   ocid1.instance.oc1.iad.anuwcljtgj4tlxyclyp7kdn4zxwghoqcjr4djl5udlr4xi2gyhn4mj26j32q\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 csi-oci-node-5p7f5                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         95m\n  kube-system                 etcd-chl8tf-control-plane-001                              100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         162m\n  kube-system                 kube-apiserver-chl8tf-control-plane-001                    250m (6%)     0 (0%)      0 (0%)           0 (0%)         162m\n  kube-system                 kube-controller-manager-chl8tf-control-plane-001           200m (5%)     0 (0%)      0 (0%)           0 (0%)         91m\n  kube-system                 kube-flannel-ds-d5bvw                                      100m (2%)     100m (2%)   50Mi (0%)        50Mi (0%)      159m\n  kube-system                 kube-proxy-j2p7z                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         161m\n  kube-system                 kube-scheduler-chl8tf-control-plane-001                    100m (2%)     0 (0%)      0 (0%)           0 (0%)         162m\n  kube-system                 oci-cloud-controller-manager-9n2zj                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         95m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-wmj9x    0 (0%)        0 (0%)      0 (0%)           0 (0%)         67m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                750m (18%)  100m (2%)\n  memory             150Mi (0%)  50Mi (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
    Jun  8 16:13:13.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8441 describe namespace kubectl-8441'
    Jun  8 16:13:14.047: INFO: stderr: ""
    Jun  8 16:13:14.047: INFO: stdout: "Name:         kubectl-8441\nLabels:       e2e-framework=kubectl\n              e2e-run=ca71eedc-529b-4fa2-b741-e4cd93d22b67\n              kubernetes.io/metadata.name=kubectl-8441\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:13:14.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8441" for this suite. 06/08/23 16:13:14.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:13:14.065
Jun  8 16:13:14.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename crd-webhook 06/08/23 16:13:14.066
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:13:14.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:13:14.089
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 06/08/23 16:13:14.092
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/08/23 16:13:14.385
STEP: Deploying the custom resource conversion webhook pod 06/08/23 16:13:14.395
STEP: Wait for the deployment to be ready 06/08/23 16:13:14.407
Jun  8 16:13:14.416: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/08/23 16:13:16.43
STEP: Verifying the service has paired with the endpoint 06/08/23 16:13:16.445
Jun  8 16:13:17.445: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jun  8 16:13:17.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Creating a v1 custom resource 06/08/23 16:13:20.048
STEP: v2 custom resource should be converted 06/08/23 16:13:20.054
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:13:20.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-3256" for this suite. 06/08/23 16:13:20.641
------------------------------
• [SLOW TEST] [6.588 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:13:14.065
    Jun  8 16:13:14.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename crd-webhook 06/08/23 16:13:14.066
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:13:14.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:13:14.089
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 06/08/23 16:13:14.092
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/08/23 16:13:14.385
    STEP: Deploying the custom resource conversion webhook pod 06/08/23 16:13:14.395
    STEP: Wait for the deployment to be ready 06/08/23 16:13:14.407
    Jun  8 16:13:14.416: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/08/23 16:13:16.43
    STEP: Verifying the service has paired with the endpoint 06/08/23 16:13:16.445
    Jun  8 16:13:17.445: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jun  8 16:13:17.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Creating a v1 custom resource 06/08/23 16:13:20.048
    STEP: v2 custom resource should be converted 06/08/23 16:13:20.054
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:13:20.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-3256" for this suite. 06/08/23 16:13:20.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:13:20.656
Jun  8 16:13:20.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 16:13:20.658
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:13:20.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:13:20.69
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-47ca943b-6c7b-4611-a038-c8ce77976de4 06/08/23 16:13:20.695
STEP: Creating a pod to test consume secrets 06/08/23 16:13:20.702
Jun  8 16:13:20.713: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9a1d5e0d-2a20-4800-9c57-b7ec87189845" in namespace "projected-8852" to be "Succeeded or Failed"
Jun  8 16:13:20.718: INFO: Pod "pod-projected-secrets-9a1d5e0d-2a20-4800-9c57-b7ec87189845": Phase="Pending", Reason="", readiness=false. Elapsed: 4.477106ms
Jun  8 16:13:22.724: INFO: Pod "pod-projected-secrets-9a1d5e0d-2a20-4800-9c57-b7ec87189845": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010504226s
Jun  8 16:13:24.723: INFO: Pod "pod-projected-secrets-9a1d5e0d-2a20-4800-9c57-b7ec87189845": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009483496s
STEP: Saw pod success 06/08/23 16:13:24.723
Jun  8 16:13:24.723: INFO: Pod "pod-projected-secrets-9a1d5e0d-2a20-4800-9c57-b7ec87189845" satisfied condition "Succeeded or Failed"
Jun  8 16:13:24.726: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-secrets-9a1d5e0d-2a20-4800-9c57-b7ec87189845 container projected-secret-volume-test: <nil>
STEP: delete the pod 06/08/23 16:13:24.743
Jun  8 16:13:24.757: INFO: Waiting for pod pod-projected-secrets-9a1d5e0d-2a20-4800-9c57-b7ec87189845 to disappear
Jun  8 16:13:24.761: INFO: Pod pod-projected-secrets-9a1d5e0d-2a20-4800-9c57-b7ec87189845 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jun  8 16:13:24.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8852" for this suite. 06/08/23 16:13:24.767
------------------------------
• [4.118 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:13:20.656
    Jun  8 16:13:20.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 16:13:20.658
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:13:20.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:13:20.69
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-47ca943b-6c7b-4611-a038-c8ce77976de4 06/08/23 16:13:20.695
    STEP: Creating a pod to test consume secrets 06/08/23 16:13:20.702
    Jun  8 16:13:20.713: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9a1d5e0d-2a20-4800-9c57-b7ec87189845" in namespace "projected-8852" to be "Succeeded or Failed"
    Jun  8 16:13:20.718: INFO: Pod "pod-projected-secrets-9a1d5e0d-2a20-4800-9c57-b7ec87189845": Phase="Pending", Reason="", readiness=false. Elapsed: 4.477106ms
    Jun  8 16:13:22.724: INFO: Pod "pod-projected-secrets-9a1d5e0d-2a20-4800-9c57-b7ec87189845": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010504226s
    Jun  8 16:13:24.723: INFO: Pod "pod-projected-secrets-9a1d5e0d-2a20-4800-9c57-b7ec87189845": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009483496s
    STEP: Saw pod success 06/08/23 16:13:24.723
    Jun  8 16:13:24.723: INFO: Pod "pod-projected-secrets-9a1d5e0d-2a20-4800-9c57-b7ec87189845" satisfied condition "Succeeded or Failed"
    Jun  8 16:13:24.726: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-secrets-9a1d5e0d-2a20-4800-9c57-b7ec87189845 container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/08/23 16:13:24.743
    Jun  8 16:13:24.757: INFO: Waiting for pod pod-projected-secrets-9a1d5e0d-2a20-4800-9c57-b7ec87189845 to disappear
    Jun  8 16:13:24.761: INFO: Pod pod-projected-secrets-9a1d5e0d-2a20-4800-9c57-b7ec87189845 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:13:24.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8852" for this suite. 06/08/23 16:13:24.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:13:24.776
Jun  8 16:13:24.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename gc 06/08/23 16:13:24.778
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:13:24.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:13:24.798
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jun  8 16:13:24.831: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"fd05567e-9a94-4082-9f1d-b5a60d4c0bea", Controller:(*bool)(0xc002008686), BlockOwnerDeletion:(*bool)(0xc002008687)}}
Jun  8 16:13:24.839: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"13cfe97f-718c-45d1-8bc6-cfdbac66e796", Controller:(*bool)(0xc0033ef62e), BlockOwnerDeletion:(*bool)(0xc0033ef62f)}}
Jun  8 16:13:24.845: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"b852b011-0a03-46a3-9683-ae5e992162e4", Controller:(*bool)(0xc0033ef82a), BlockOwnerDeletion:(*bool)(0xc0033ef82b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jun  8 16:13:29.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3961" for this suite. 06/08/23 16:13:29.864
------------------------------
• [SLOW TEST] [5.095 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:13:24.776
    Jun  8 16:13:24.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename gc 06/08/23 16:13:24.778
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:13:24.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:13:24.798
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jun  8 16:13:24.831: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"fd05567e-9a94-4082-9f1d-b5a60d4c0bea", Controller:(*bool)(0xc002008686), BlockOwnerDeletion:(*bool)(0xc002008687)}}
    Jun  8 16:13:24.839: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"13cfe97f-718c-45d1-8bc6-cfdbac66e796", Controller:(*bool)(0xc0033ef62e), BlockOwnerDeletion:(*bool)(0xc0033ef62f)}}
    Jun  8 16:13:24.845: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"b852b011-0a03-46a3-9683-ae5e992162e4", Controller:(*bool)(0xc0033ef82a), BlockOwnerDeletion:(*bool)(0xc0033ef82b)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:13:29.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3961" for this suite. 06/08/23 16:13:29.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:13:29.873
Jun  8 16:13:29.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename hostport 06/08/23 16:13:29.874
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:13:29.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:13:29.894
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 06/08/23 16:13:29.903
Jun  8 16:13:29.913: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-2283" to be "running and ready"
Jun  8 16:13:29.916: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.490216ms
Jun  8 16:13:29.916: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 16:13:31.922: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009332632s
Jun  8 16:13:31.922: INFO: The phase of Pod pod1 is Running (Ready = true)
Jun  8 16:13:31.922: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 100.100.236.215 on the node which pod1 resides and expect scheduled 06/08/23 16:13:31.922
Jun  8 16:13:31.929: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-2283" to be "running and ready"
Jun  8 16:13:31.933: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.081016ms
Jun  8 16:13:31.934: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 16:13:33.938: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.008821623s
Jun  8 16:13:33.938: INFO: The phase of Pod pod2 is Running (Ready = false)
Jun  8 16:13:35.939: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.009922409s
Jun  8 16:13:35.939: INFO: The phase of Pod pod2 is Running (Ready = true)
Jun  8 16:13:35.939: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 100.100.236.215 but use UDP protocol on the node which pod2 resides 06/08/23 16:13:35.939
Jun  8 16:13:35.946: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-2283" to be "running and ready"
Jun  8 16:13:35.950: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.968398ms
Jun  8 16:13:35.950: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 16:13:37.955: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.009067382s
Jun  8 16:13:37.955: INFO: The phase of Pod pod3 is Running (Ready = false)
Jun  8 16:13:39.955: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 4.009108703s
Jun  8 16:13:39.955: INFO: The phase of Pod pod3 is Running (Ready = false)
Jun  8 16:13:41.956: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 6.009682023s
Jun  8 16:13:41.956: INFO: The phase of Pod pod3 is Running (Ready = false)
Jun  8 16:13:43.956: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 8.009499341s
Jun  8 16:13:43.956: INFO: The phase of Pod pod3 is Running (Ready = false)
Jun  8 16:13:45.956: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 10.010355971s
Jun  8 16:13:45.956: INFO: The phase of Pod pod3 is Running (Ready = false)
Jun  8 16:13:47.956: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 12.010216271s
Jun  8 16:13:47.956: INFO: The phase of Pod pod3 is Running (Ready = true)
Jun  8 16:13:47.956: INFO: Pod "pod3" satisfied condition "running and ready"
Jun  8 16:13:47.963: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-2283" to be "running and ready"
Jun  8 16:13:47.967: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053189ms
Jun  8 16:13:47.967: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jun  8 16:13:49.973: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009864976s
Jun  8 16:13:49.973: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jun  8 16:13:49.973: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 06/08/23 16:13:49.977
Jun  8 16:13:49.977: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 100.100.236.215 http://127.0.0.1:54323/hostname] Namespace:hostport-2283 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 16:13:49.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 16:13:49.978: INFO: ExecWithOptions: Clientset creation
Jun  8 16:13:49.978: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2283/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+100.100.236.215+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 100.100.236.215, port: 54323 06/08/23 16:13:50.069
Jun  8 16:13:50.069: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://100.100.236.215:54323/hostname] Namespace:hostport-2283 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 16:13:50.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 16:13:50.070: INFO: ExecWithOptions: Clientset creation
Jun  8 16:13:50.070: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2283/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F100.100.236.215%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 100.100.236.215, port: 54323 UDP 06/08/23 16:13:50.156
Jun  8 16:13:50.156: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 100.100.236.215 54323] Namespace:hostport-2283 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 16:13:50.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 16:13:50.157: INFO: ExecWithOptions: Clientset creation
Jun  8 16:13:50.157: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2283/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+100.100.236.215+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Jun  8 16:13:55.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-2283" for this suite. 06/08/23 16:13:55.242
------------------------------
• [SLOW TEST] [25.377 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:13:29.873
    Jun  8 16:13:29.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename hostport 06/08/23 16:13:29.874
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:13:29.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:13:29.894
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 06/08/23 16:13:29.903
    Jun  8 16:13:29.913: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-2283" to be "running and ready"
    Jun  8 16:13:29.916: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.490216ms
    Jun  8 16:13:29.916: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 16:13:31.922: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009332632s
    Jun  8 16:13:31.922: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jun  8 16:13:31.922: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 100.100.236.215 on the node which pod1 resides and expect scheduled 06/08/23 16:13:31.922
    Jun  8 16:13:31.929: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-2283" to be "running and ready"
    Jun  8 16:13:31.933: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.081016ms
    Jun  8 16:13:31.934: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 16:13:33.938: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.008821623s
    Jun  8 16:13:33.938: INFO: The phase of Pod pod2 is Running (Ready = false)
    Jun  8 16:13:35.939: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.009922409s
    Jun  8 16:13:35.939: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jun  8 16:13:35.939: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 100.100.236.215 but use UDP protocol on the node which pod2 resides 06/08/23 16:13:35.939
    Jun  8 16:13:35.946: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-2283" to be "running and ready"
    Jun  8 16:13:35.950: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.968398ms
    Jun  8 16:13:35.950: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 16:13:37.955: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.009067382s
    Jun  8 16:13:37.955: INFO: The phase of Pod pod3 is Running (Ready = false)
    Jun  8 16:13:39.955: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 4.009108703s
    Jun  8 16:13:39.955: INFO: The phase of Pod pod3 is Running (Ready = false)
    Jun  8 16:13:41.956: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 6.009682023s
    Jun  8 16:13:41.956: INFO: The phase of Pod pod3 is Running (Ready = false)
    Jun  8 16:13:43.956: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 8.009499341s
    Jun  8 16:13:43.956: INFO: The phase of Pod pod3 is Running (Ready = false)
    Jun  8 16:13:45.956: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 10.010355971s
    Jun  8 16:13:45.956: INFO: The phase of Pod pod3 is Running (Ready = false)
    Jun  8 16:13:47.956: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 12.010216271s
    Jun  8 16:13:47.956: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jun  8 16:13:47.956: INFO: Pod "pod3" satisfied condition "running and ready"
    Jun  8 16:13:47.963: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-2283" to be "running and ready"
    Jun  8 16:13:47.967: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053189ms
    Jun  8 16:13:47.967: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 16:13:49.973: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009864976s
    Jun  8 16:13:49.973: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jun  8 16:13:49.973: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 06/08/23 16:13:49.977
    Jun  8 16:13:49.977: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 100.100.236.215 http://127.0.0.1:54323/hostname] Namespace:hostport-2283 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 16:13:49.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 16:13:49.978: INFO: ExecWithOptions: Clientset creation
    Jun  8 16:13:49.978: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2283/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+100.100.236.215+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 100.100.236.215, port: 54323 06/08/23 16:13:50.069
    Jun  8 16:13:50.069: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://100.100.236.215:54323/hostname] Namespace:hostport-2283 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 16:13:50.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 16:13:50.070: INFO: ExecWithOptions: Clientset creation
    Jun  8 16:13:50.070: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2283/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F100.100.236.215%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 100.100.236.215, port: 54323 UDP 06/08/23 16:13:50.156
    Jun  8 16:13:50.156: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 100.100.236.215 54323] Namespace:hostport-2283 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 16:13:50.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 16:13:50.157: INFO: ExecWithOptions: Clientset creation
    Jun  8 16:13:50.157: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2283/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+100.100.236.215+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:13:55.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-2283" for this suite. 06/08/23 16:13:55.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:13:55.251
Jun  8 16:13:55.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename replication-controller 06/08/23 16:13:55.252
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:13:55.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:13:55.273
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 06/08/23 16:13:55.28
STEP: waiting for RC to be added 06/08/23 16:13:55.287
STEP: waiting for available Replicas 06/08/23 16:13:55.287
STEP: patching ReplicationController 06/08/23 16:13:56.412
STEP: waiting for RC to be modified 06/08/23 16:13:56.422
STEP: patching ReplicationController status 06/08/23 16:13:56.422
STEP: waiting for RC to be modified 06/08/23 16:13:56.429
STEP: waiting for available Replicas 06/08/23 16:13:56.429
STEP: fetching ReplicationController status 06/08/23 16:13:56.435
STEP: patching ReplicationController scale 06/08/23 16:13:56.439
STEP: waiting for RC to be modified 06/08/23 16:13:56.448
STEP: waiting for ReplicationController's scale to be the max amount 06/08/23 16:13:56.448
STEP: fetching ReplicationController; ensuring that it's patched 06/08/23 16:13:57.305
STEP: updating ReplicationController status 06/08/23 16:13:57.312
STEP: waiting for RC to be modified 06/08/23 16:13:57.319
STEP: listing all ReplicationControllers 06/08/23 16:13:57.32
STEP: checking that ReplicationController has expected values 06/08/23 16:13:57.324
STEP: deleting ReplicationControllers by collection 06/08/23 16:13:57.324
STEP: waiting for ReplicationController to have a DELETED watchEvent 06/08/23 16:13:57.334
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jun  8 16:13:57.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6391" for this suite. 06/08/23 16:13:57.405
------------------------------
• [2.162 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:13:55.251
    Jun  8 16:13:55.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename replication-controller 06/08/23 16:13:55.252
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:13:55.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:13:55.273
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 06/08/23 16:13:55.28
    STEP: waiting for RC to be added 06/08/23 16:13:55.287
    STEP: waiting for available Replicas 06/08/23 16:13:55.287
    STEP: patching ReplicationController 06/08/23 16:13:56.412
    STEP: waiting for RC to be modified 06/08/23 16:13:56.422
    STEP: patching ReplicationController status 06/08/23 16:13:56.422
    STEP: waiting for RC to be modified 06/08/23 16:13:56.429
    STEP: waiting for available Replicas 06/08/23 16:13:56.429
    STEP: fetching ReplicationController status 06/08/23 16:13:56.435
    STEP: patching ReplicationController scale 06/08/23 16:13:56.439
    STEP: waiting for RC to be modified 06/08/23 16:13:56.448
    STEP: waiting for ReplicationController's scale to be the max amount 06/08/23 16:13:56.448
    STEP: fetching ReplicationController; ensuring that it's patched 06/08/23 16:13:57.305
    STEP: updating ReplicationController status 06/08/23 16:13:57.312
    STEP: waiting for RC to be modified 06/08/23 16:13:57.319
    STEP: listing all ReplicationControllers 06/08/23 16:13:57.32
    STEP: checking that ReplicationController has expected values 06/08/23 16:13:57.324
    STEP: deleting ReplicationControllers by collection 06/08/23 16:13:57.324
    STEP: waiting for ReplicationController to have a DELETED watchEvent 06/08/23 16:13:57.334
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:13:57.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6391" for this suite. 06/08/23 16:13:57.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:13:57.413
Jun  8 16:13:57.413: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename init-container 06/08/23 16:13:57.415
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:13:57.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:13:57.439
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 06/08/23 16:13:57.442
Jun  8 16:13:57.442: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:14:01.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-838" for this suite. 06/08/23 16:14:01.439
------------------------------
• [4.033 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:13:57.413
    Jun  8 16:13:57.413: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename init-container 06/08/23 16:13:57.415
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:13:57.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:13:57.439
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 06/08/23 16:13:57.442
    Jun  8 16:13:57.442: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:14:01.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-838" for this suite. 06/08/23 16:14:01.439
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:14:01.447
Jun  8 16:14:01.447: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename resourcequota 06/08/23 16:14:01.448
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:14:01.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:14:01.469
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 06/08/23 16:14:01.472
STEP: Creating a ResourceQuota 06/08/23 16:14:06.477
STEP: Ensuring resource quota status is calculated 06/08/23 16:14:06.485
STEP: Creating a ReplicationController 06/08/23 16:14:08.491
STEP: Ensuring resource quota status captures replication controller creation 06/08/23 16:14:08.504
STEP: Deleting a ReplicationController 06/08/23 16:14:10.51
STEP: Ensuring resource quota status released usage 06/08/23 16:14:10.517
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  8 16:14:12.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4376" for this suite. 06/08/23 16:14:12.529
------------------------------
• [SLOW TEST] [11.091 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:14:01.447
    Jun  8 16:14:01.447: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename resourcequota 06/08/23 16:14:01.448
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:14:01.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:14:01.469
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 06/08/23 16:14:01.472
    STEP: Creating a ResourceQuota 06/08/23 16:14:06.477
    STEP: Ensuring resource quota status is calculated 06/08/23 16:14:06.485
    STEP: Creating a ReplicationController 06/08/23 16:14:08.491
    STEP: Ensuring resource quota status captures replication controller creation 06/08/23 16:14:08.504
    STEP: Deleting a ReplicationController 06/08/23 16:14:10.51
    STEP: Ensuring resource quota status released usage 06/08/23 16:14:10.517
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:14:12.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4376" for this suite. 06/08/23 16:14:12.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:14:12.54
Jun  8 16:14:12.540: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename configmap 06/08/23 16:14:12.541
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:14:12.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:14:12.562
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-127/configmap-test-029bdfdf-66e7-4717-b6bc-f885b828c9d6 06/08/23 16:14:12.565
STEP: Creating a pod to test consume configMaps 06/08/23 16:14:12.576
Jun  8 16:14:12.589: INFO: Waiting up to 5m0s for pod "pod-configmaps-741c3378-ffe7-4917-8091-e1a7f7f45cfe" in namespace "configmap-127" to be "Succeeded or Failed"
Jun  8 16:14:12.593: INFO: Pod "pod-configmaps-741c3378-ffe7-4917-8091-e1a7f7f45cfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.786665ms
Jun  8 16:14:14.598: INFO: Pod "pod-configmaps-741c3378-ffe7-4917-8091-e1a7f7f45cfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009884772s
Jun  8 16:14:16.599: INFO: Pod "pod-configmaps-741c3378-ffe7-4917-8091-e1a7f7f45cfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01073168s
STEP: Saw pod success 06/08/23 16:14:16.599
Jun  8 16:14:16.599: INFO: Pod "pod-configmaps-741c3378-ffe7-4917-8091-e1a7f7f45cfe" satisfied condition "Succeeded or Failed"
Jun  8 16:14:16.603: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-configmaps-741c3378-ffe7-4917-8091-e1a7f7f45cfe container env-test: <nil>
STEP: delete the pod 06/08/23 16:14:16.612
Jun  8 16:14:16.624: INFO: Waiting for pod pod-configmaps-741c3378-ffe7-4917-8091-e1a7f7f45cfe to disappear
Jun  8 16:14:16.627: INFO: Pod pod-configmaps-741c3378-ffe7-4917-8091-e1a7f7f45cfe no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  8 16:14:16.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-127" for this suite. 06/08/23 16:14:16.633
------------------------------
• [4.100 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:14:12.54
    Jun  8 16:14:12.540: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename configmap 06/08/23 16:14:12.541
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:14:12.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:14:12.562
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-127/configmap-test-029bdfdf-66e7-4717-b6bc-f885b828c9d6 06/08/23 16:14:12.565
    STEP: Creating a pod to test consume configMaps 06/08/23 16:14:12.576
    Jun  8 16:14:12.589: INFO: Waiting up to 5m0s for pod "pod-configmaps-741c3378-ffe7-4917-8091-e1a7f7f45cfe" in namespace "configmap-127" to be "Succeeded or Failed"
    Jun  8 16:14:12.593: INFO: Pod "pod-configmaps-741c3378-ffe7-4917-8091-e1a7f7f45cfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.786665ms
    Jun  8 16:14:14.598: INFO: Pod "pod-configmaps-741c3378-ffe7-4917-8091-e1a7f7f45cfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009884772s
    Jun  8 16:14:16.599: INFO: Pod "pod-configmaps-741c3378-ffe7-4917-8091-e1a7f7f45cfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01073168s
    STEP: Saw pod success 06/08/23 16:14:16.599
    Jun  8 16:14:16.599: INFO: Pod "pod-configmaps-741c3378-ffe7-4917-8091-e1a7f7f45cfe" satisfied condition "Succeeded or Failed"
    Jun  8 16:14:16.603: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-configmaps-741c3378-ffe7-4917-8091-e1a7f7f45cfe container env-test: <nil>
    STEP: delete the pod 06/08/23 16:14:16.612
    Jun  8 16:14:16.624: INFO: Waiting for pod pod-configmaps-741c3378-ffe7-4917-8091-e1a7f7f45cfe to disappear
    Jun  8 16:14:16.627: INFO: Pod pod-configmaps-741c3378-ffe7-4917-8091-e1a7f7f45cfe no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:14:16.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-127" for this suite. 06/08/23 16:14:16.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:14:16.642
Jun  8 16:14:16.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename endpointslice 06/08/23 16:14:16.643
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:14:16.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:14:16.664
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jun  8 16:14:20.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-8654" for this suite. 06/08/23 16:14:20.754
------------------------------
• [4.121 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:14:16.642
    Jun  8 16:14:16.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename endpointslice 06/08/23 16:14:16.643
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:14:16.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:14:16.664
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:14:20.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-8654" for this suite. 06/08/23 16:14:20.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:14:20.764
Jun  8 16:14:20.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename services 06/08/23 16:14:20.765
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:14:20.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:14:20.802
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8932 06/08/23 16:14:20.806
STEP: changing the ExternalName service to type=ClusterIP 06/08/23 16:14:20.812
STEP: creating replication controller externalname-service in namespace services-8932 06/08/23 16:14:20.835
I0608 16:14:20.842131      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8932, replica count: 2
I0608 16:14:23.894494      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  8 16:14:23.894: INFO: Creating new exec pod
Jun  8 16:14:23.904: INFO: Waiting up to 5m0s for pod "execpodtzgrk" in namespace "services-8932" to be "running"
Jun  8 16:14:23.909: INFO: Pod "execpodtzgrk": Phase="Pending", Reason="", readiness=false. Elapsed: 5.304472ms
Jun  8 16:14:25.915: INFO: Pod "execpodtzgrk": Phase="Running", Reason="", readiness=true. Elapsed: 2.010500529s
Jun  8 16:14:25.915: INFO: Pod "execpodtzgrk" satisfied condition "running"
Jun  8 16:14:26.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-8932 exec execpodtzgrk -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jun  8 16:14:27.084: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun  8 16:14:27.084: INFO: stdout: ""
Jun  8 16:14:27.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-8932 exec execpodtzgrk -- /bin/sh -x -c nc -v -z -w 2 10.103.150.58 80'
Jun  8 16:14:27.244: INFO: stderr: "+ nc -v -z -w 2 10.103.150.58 80\nConnection to 10.103.150.58 80 port [tcp/http] succeeded!\n"
Jun  8 16:14:27.244: INFO: stdout: ""
Jun  8 16:14:27.244: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  8 16:14:27.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8932" for this suite. 06/08/23 16:14:27.277
------------------------------
• [SLOW TEST] [6.522 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:14:20.764
    Jun  8 16:14:20.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename services 06/08/23 16:14:20.765
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:14:20.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:14:20.802
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-8932 06/08/23 16:14:20.806
    STEP: changing the ExternalName service to type=ClusterIP 06/08/23 16:14:20.812
    STEP: creating replication controller externalname-service in namespace services-8932 06/08/23 16:14:20.835
    I0608 16:14:20.842131      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8932, replica count: 2
    I0608 16:14:23.894494      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun  8 16:14:23.894: INFO: Creating new exec pod
    Jun  8 16:14:23.904: INFO: Waiting up to 5m0s for pod "execpodtzgrk" in namespace "services-8932" to be "running"
    Jun  8 16:14:23.909: INFO: Pod "execpodtzgrk": Phase="Pending", Reason="", readiness=false. Elapsed: 5.304472ms
    Jun  8 16:14:25.915: INFO: Pod "execpodtzgrk": Phase="Running", Reason="", readiness=true. Elapsed: 2.010500529s
    Jun  8 16:14:25.915: INFO: Pod "execpodtzgrk" satisfied condition "running"
    Jun  8 16:14:26.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-8932 exec execpodtzgrk -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jun  8 16:14:27.084: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun  8 16:14:27.084: INFO: stdout: ""
    Jun  8 16:14:27.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-8932 exec execpodtzgrk -- /bin/sh -x -c nc -v -z -w 2 10.103.150.58 80'
    Jun  8 16:14:27.244: INFO: stderr: "+ nc -v -z -w 2 10.103.150.58 80\nConnection to 10.103.150.58 80 port [tcp/http] succeeded!\n"
    Jun  8 16:14:27.244: INFO: stdout: ""
    Jun  8 16:14:27.244: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:14:27.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8932" for this suite. 06/08/23 16:14:27.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:14:27.287
Jun  8 16:14:27.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename sched-preemption 06/08/23 16:14:27.289
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:14:27.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:14:27.317
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jun  8 16:14:27.341: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  8 16:15:27.391: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:15:27.395
Jun  8 16:15:27.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename sched-preemption-path 06/08/23 16:15:27.396
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:27.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:27.419
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Jun  8 16:15:27.437: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jun  8 16:15:27.441: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Jun  8 16:15:27.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:15:27.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-2547" for this suite. 06/08/23 16:15:27.56
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-470" for this suite. 06/08/23 16:15:27.567
------------------------------
• [SLOW TEST] [60.287 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:14:27.287
    Jun  8 16:14:27.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename sched-preemption 06/08/23 16:14:27.289
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:14:27.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:14:27.317
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jun  8 16:14:27.341: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun  8 16:15:27.391: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:15:27.395
    Jun  8 16:15:27.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename sched-preemption-path 06/08/23 16:15:27.396
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:27.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:27.419
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Jun  8 16:15:27.437: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jun  8 16:15:27.441: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:15:27.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:15:27.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-2547" for this suite. 06/08/23 16:15:27.56
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-470" for this suite. 06/08/23 16:15:27.567
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:15:27.576
Jun  8 16:15:27.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename job 06/08/23 16:15:27.577
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:27.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:27.597
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 06/08/23 16:15:27.6
STEP: Ensure pods equal to parallelism count is attached to the job 06/08/23 16:15:27.607
STEP: patching /status 06/08/23 16:15:29.613
STEP: updating /status 06/08/23 16:15:29.622
STEP: get /status 06/08/23 16:15:29.633
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jun  8 16:15:29.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1356" for this suite. 06/08/23 16:15:29.643
------------------------------
• [2.075 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:15:27.576
    Jun  8 16:15:27.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename job 06/08/23 16:15:27.577
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:27.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:27.597
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 06/08/23 16:15:27.6
    STEP: Ensure pods equal to parallelism count is attached to the job 06/08/23 16:15:27.607
    STEP: patching /status 06/08/23 16:15:29.613
    STEP: updating /status 06/08/23 16:15:29.622
    STEP: get /status 06/08/23 16:15:29.633
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:15:29.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1356" for this suite. 06/08/23 16:15:29.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:15:29.652
Jun  8 16:15:29.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename watch 06/08/23 16:15:29.653
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:29.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:29.673
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 06/08/23 16:15:29.676
STEP: creating a new configmap 06/08/23 16:15:29.678
STEP: modifying the configmap once 06/08/23 16:15:29.683
STEP: closing the watch once it receives two notifications 06/08/23 16:15:29.692
Jun  8 16:15:29.692: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5170  78972399-460a-4d72-9a84-55d2bec2def1 51390 0 2023-06-08 16:15:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-08 16:15:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  8 16:15:29.692: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5170  78972399-460a-4d72-9a84-55d2bec2def1 51391 0 2023-06-08 16:15:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-08 16:15:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 06/08/23 16:15:29.693
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 06/08/23 16:15:29.701
STEP: deleting the configmap 06/08/23 16:15:29.703
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 06/08/23 16:15:29.709
Jun  8 16:15:29.709: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5170  78972399-460a-4d72-9a84-55d2bec2def1 51392 0 2023-06-08 16:15:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-08 16:15:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  8 16:15:29.709: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5170  78972399-460a-4d72-9a84-55d2bec2def1 51393 0 2023-06-08 16:15:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-08 16:15:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jun  8 16:15:29.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5170" for this suite. 06/08/23 16:15:29.715
------------------------------
• [0.070 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:15:29.652
    Jun  8 16:15:29.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename watch 06/08/23 16:15:29.653
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:29.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:29.673
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 06/08/23 16:15:29.676
    STEP: creating a new configmap 06/08/23 16:15:29.678
    STEP: modifying the configmap once 06/08/23 16:15:29.683
    STEP: closing the watch once it receives two notifications 06/08/23 16:15:29.692
    Jun  8 16:15:29.692: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5170  78972399-460a-4d72-9a84-55d2bec2def1 51390 0 2023-06-08 16:15:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-08 16:15:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  8 16:15:29.692: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5170  78972399-460a-4d72-9a84-55d2bec2def1 51391 0 2023-06-08 16:15:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-08 16:15:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 06/08/23 16:15:29.693
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 06/08/23 16:15:29.701
    STEP: deleting the configmap 06/08/23 16:15:29.703
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 06/08/23 16:15:29.709
    Jun  8 16:15:29.709: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5170  78972399-460a-4d72-9a84-55d2bec2def1 51392 0 2023-06-08 16:15:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-08 16:15:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  8 16:15:29.709: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5170  78972399-460a-4d72-9a84-55d2bec2def1 51393 0 2023-06-08 16:15:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-08 16:15:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:15:29.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5170" for this suite. 06/08/23 16:15:29.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:15:29.722
Jun  8 16:15:29.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 16:15:29.723
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:29.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:29.743
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-cd7758dd-fa07-43fc-987f-116395e2fbf8 06/08/23 16:15:29.746
STEP: Creating a pod to test consume secrets 06/08/23 16:15:29.752
Jun  8 16:15:29.761: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e11ac3e4-2337-4393-8f41-53478af2a091" in namespace "projected-3175" to be "Succeeded or Failed"
Jun  8 16:15:29.765: INFO: Pod "pod-projected-secrets-e11ac3e4-2337-4393-8f41-53478af2a091": Phase="Pending", Reason="", readiness=false. Elapsed: 3.811725ms
Jun  8 16:15:31.771: INFO: Pod "pod-projected-secrets-e11ac3e4-2337-4393-8f41-53478af2a091": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009605898s
Jun  8 16:15:33.770: INFO: Pod "pod-projected-secrets-e11ac3e4-2337-4393-8f41-53478af2a091": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009234772s
STEP: Saw pod success 06/08/23 16:15:33.77
Jun  8 16:15:33.771: INFO: Pod "pod-projected-secrets-e11ac3e4-2337-4393-8f41-53478af2a091" satisfied condition "Succeeded or Failed"
Jun  8 16:15:33.775: INFO: Trying to get logs from node chl8tf-worker-002 pod pod-projected-secrets-e11ac3e4-2337-4393-8f41-53478af2a091 container secret-volume-test: <nil>
STEP: delete the pod 06/08/23 16:15:33.79
Jun  8 16:15:33.804: INFO: Waiting for pod pod-projected-secrets-e11ac3e4-2337-4393-8f41-53478af2a091 to disappear
Jun  8 16:15:33.808: INFO: Pod pod-projected-secrets-e11ac3e4-2337-4393-8f41-53478af2a091 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jun  8 16:15:33.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3175" for this suite. 06/08/23 16:15:33.814
------------------------------
• [4.098 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:15:29.722
    Jun  8 16:15:29.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 16:15:29.723
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:29.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:29.743
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-cd7758dd-fa07-43fc-987f-116395e2fbf8 06/08/23 16:15:29.746
    STEP: Creating a pod to test consume secrets 06/08/23 16:15:29.752
    Jun  8 16:15:29.761: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e11ac3e4-2337-4393-8f41-53478af2a091" in namespace "projected-3175" to be "Succeeded or Failed"
    Jun  8 16:15:29.765: INFO: Pod "pod-projected-secrets-e11ac3e4-2337-4393-8f41-53478af2a091": Phase="Pending", Reason="", readiness=false. Elapsed: 3.811725ms
    Jun  8 16:15:31.771: INFO: Pod "pod-projected-secrets-e11ac3e4-2337-4393-8f41-53478af2a091": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009605898s
    Jun  8 16:15:33.770: INFO: Pod "pod-projected-secrets-e11ac3e4-2337-4393-8f41-53478af2a091": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009234772s
    STEP: Saw pod success 06/08/23 16:15:33.77
    Jun  8 16:15:33.771: INFO: Pod "pod-projected-secrets-e11ac3e4-2337-4393-8f41-53478af2a091" satisfied condition "Succeeded or Failed"
    Jun  8 16:15:33.775: INFO: Trying to get logs from node chl8tf-worker-002 pod pod-projected-secrets-e11ac3e4-2337-4393-8f41-53478af2a091 container secret-volume-test: <nil>
    STEP: delete the pod 06/08/23 16:15:33.79
    Jun  8 16:15:33.804: INFO: Waiting for pod pod-projected-secrets-e11ac3e4-2337-4393-8f41-53478af2a091 to disappear
    Jun  8 16:15:33.808: INFO: Pod pod-projected-secrets-e11ac3e4-2337-4393-8f41-53478af2a091 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:15:33.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3175" for this suite. 06/08/23 16:15:33.814
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:15:33.829
Jun  8 16:15:33.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename emptydir 06/08/23 16:15:33.83
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:33.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:33.853
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 06/08/23 16:15:33.857
Jun  8 16:15:33.866: INFO: Waiting up to 5m0s for pod "pod-852ba01a-f967-4207-8120-a797365591cb" in namespace "emptydir-8667" to be "Succeeded or Failed"
Jun  8 16:15:33.870: INFO: Pod "pod-852ba01a-f967-4207-8120-a797365591cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.362098ms
Jun  8 16:15:35.876: INFO: Pod "pod-852ba01a-f967-4207-8120-a797365591cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010459594s
Jun  8 16:15:37.876: INFO: Pod "pod-852ba01a-f967-4207-8120-a797365591cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009980421s
STEP: Saw pod success 06/08/23 16:15:37.876
Jun  8 16:15:37.876: INFO: Pod "pod-852ba01a-f967-4207-8120-a797365591cb" satisfied condition "Succeeded or Failed"
Jun  8 16:15:37.880: INFO: Trying to get logs from node chl8tf-worker-002 pod pod-852ba01a-f967-4207-8120-a797365591cb container test-container: <nil>
STEP: delete the pod 06/08/23 16:15:37.888
Jun  8 16:15:37.902: INFO: Waiting for pod pod-852ba01a-f967-4207-8120-a797365591cb to disappear
Jun  8 16:15:37.906: INFO: Pod pod-852ba01a-f967-4207-8120-a797365591cb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  8 16:15:37.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8667" for this suite. 06/08/23 16:15:37.911
------------------------------
• [4.089 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:15:33.829
    Jun  8 16:15:33.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename emptydir 06/08/23 16:15:33.83
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:33.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:33.853
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 06/08/23 16:15:33.857
    Jun  8 16:15:33.866: INFO: Waiting up to 5m0s for pod "pod-852ba01a-f967-4207-8120-a797365591cb" in namespace "emptydir-8667" to be "Succeeded or Failed"
    Jun  8 16:15:33.870: INFO: Pod "pod-852ba01a-f967-4207-8120-a797365591cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.362098ms
    Jun  8 16:15:35.876: INFO: Pod "pod-852ba01a-f967-4207-8120-a797365591cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010459594s
    Jun  8 16:15:37.876: INFO: Pod "pod-852ba01a-f967-4207-8120-a797365591cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009980421s
    STEP: Saw pod success 06/08/23 16:15:37.876
    Jun  8 16:15:37.876: INFO: Pod "pod-852ba01a-f967-4207-8120-a797365591cb" satisfied condition "Succeeded or Failed"
    Jun  8 16:15:37.880: INFO: Trying to get logs from node chl8tf-worker-002 pod pod-852ba01a-f967-4207-8120-a797365591cb container test-container: <nil>
    STEP: delete the pod 06/08/23 16:15:37.888
    Jun  8 16:15:37.902: INFO: Waiting for pod pod-852ba01a-f967-4207-8120-a797365591cb to disappear
    Jun  8 16:15:37.906: INFO: Pod pod-852ba01a-f967-4207-8120-a797365591cb no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:15:37.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8667" for this suite. 06/08/23 16:15:37.911
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:15:37.919
Jun  8 16:15:37.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 16:15:37.92
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:37.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:37.94
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 06/08/23 16:15:37.943
Jun  8 16:15:37.953: INFO: Waiting up to 5m0s for pod "annotationupdatecf430218-b34e-4669-b5e8-cfb7df54a238" in namespace "projected-3033" to be "running and ready"
Jun  8 16:15:37.956: INFO: Pod "annotationupdatecf430218-b34e-4669-b5e8-cfb7df54a238": Phase="Pending", Reason="", readiness=false. Elapsed: 3.385207ms
Jun  8 16:15:37.956: INFO: The phase of Pod annotationupdatecf430218-b34e-4669-b5e8-cfb7df54a238 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 16:15:39.962: INFO: Pod "annotationupdatecf430218-b34e-4669-b5e8-cfb7df54a238": Phase="Running", Reason="", readiness=true. Elapsed: 2.009424353s
Jun  8 16:15:39.962: INFO: The phase of Pod annotationupdatecf430218-b34e-4669-b5e8-cfb7df54a238 is Running (Ready = true)
Jun  8 16:15:39.962: INFO: Pod "annotationupdatecf430218-b34e-4669-b5e8-cfb7df54a238" satisfied condition "running and ready"
Jun  8 16:15:40.488: INFO: Successfully updated pod "annotationupdatecf430218-b34e-4669-b5e8-cfb7df54a238"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  8 16:15:44.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3033" for this suite. 06/08/23 16:15:44.523
------------------------------
• [SLOW TEST] [6.612 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:15:37.919
    Jun  8 16:15:37.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 16:15:37.92
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:37.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:37.94
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 06/08/23 16:15:37.943
    Jun  8 16:15:37.953: INFO: Waiting up to 5m0s for pod "annotationupdatecf430218-b34e-4669-b5e8-cfb7df54a238" in namespace "projected-3033" to be "running and ready"
    Jun  8 16:15:37.956: INFO: Pod "annotationupdatecf430218-b34e-4669-b5e8-cfb7df54a238": Phase="Pending", Reason="", readiness=false. Elapsed: 3.385207ms
    Jun  8 16:15:37.956: INFO: The phase of Pod annotationupdatecf430218-b34e-4669-b5e8-cfb7df54a238 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 16:15:39.962: INFO: Pod "annotationupdatecf430218-b34e-4669-b5e8-cfb7df54a238": Phase="Running", Reason="", readiness=true. Elapsed: 2.009424353s
    Jun  8 16:15:39.962: INFO: The phase of Pod annotationupdatecf430218-b34e-4669-b5e8-cfb7df54a238 is Running (Ready = true)
    Jun  8 16:15:39.962: INFO: Pod "annotationupdatecf430218-b34e-4669-b5e8-cfb7df54a238" satisfied condition "running and ready"
    Jun  8 16:15:40.488: INFO: Successfully updated pod "annotationupdatecf430218-b34e-4669-b5e8-cfb7df54a238"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:15:44.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3033" for this suite. 06/08/23 16:15:44.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:15:44.534
Jun  8 16:15:44.534: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubelet-test 06/08/23 16:15:44.535
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:44.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:44.557
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jun  8 16:15:44.571: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs4383800a-712b-41ff-8270-46839b2ef0b5" in namespace "kubelet-test-6754" to be "running and ready"
Jun  8 16:15:44.575: INFO: Pod "busybox-readonly-fs4383800a-712b-41ff-8270-46839b2ef0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.813359ms
Jun  8 16:15:44.575: INFO: The phase of Pod busybox-readonly-fs4383800a-712b-41ff-8270-46839b2ef0b5 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 16:15:46.581: INFO: Pod "busybox-readonly-fs4383800a-712b-41ff-8270-46839b2ef0b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.009485093s
Jun  8 16:15:46.581: INFO: The phase of Pod busybox-readonly-fs4383800a-712b-41ff-8270-46839b2ef0b5 is Running (Ready = true)
Jun  8 16:15:46.581: INFO: Pod "busybox-readonly-fs4383800a-712b-41ff-8270-46839b2ef0b5" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jun  8 16:15:46.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-6754" for this suite. 06/08/23 16:15:46.602
------------------------------
• [2.076 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:15:44.534
    Jun  8 16:15:44.534: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubelet-test 06/08/23 16:15:44.535
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:44.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:44.557
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jun  8 16:15:44.571: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs4383800a-712b-41ff-8270-46839b2ef0b5" in namespace "kubelet-test-6754" to be "running and ready"
    Jun  8 16:15:44.575: INFO: Pod "busybox-readonly-fs4383800a-712b-41ff-8270-46839b2ef0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.813359ms
    Jun  8 16:15:44.575: INFO: The phase of Pod busybox-readonly-fs4383800a-712b-41ff-8270-46839b2ef0b5 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 16:15:46.581: INFO: Pod "busybox-readonly-fs4383800a-712b-41ff-8270-46839b2ef0b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.009485093s
    Jun  8 16:15:46.581: INFO: The phase of Pod busybox-readonly-fs4383800a-712b-41ff-8270-46839b2ef0b5 is Running (Ready = true)
    Jun  8 16:15:46.581: INFO: Pod "busybox-readonly-fs4383800a-712b-41ff-8270-46839b2ef0b5" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:15:46.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-6754" for this suite. 06/08/23 16:15:46.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:15:46.612
Jun  8 16:15:46.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename runtimeclass 06/08/23 16:15:46.613
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:46.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:46.635
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jun  8 16:15:46.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2076" for this suite. 06/08/23 16:15:46.656
------------------------------
• [0.051 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:15:46.612
    Jun  8 16:15:46.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename runtimeclass 06/08/23 16:15:46.613
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:46.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:46.635
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:15:46.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2076" for this suite. 06/08/23 16:15:46.656
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:15:46.668
Jun  8 16:15:46.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename services 06/08/23 16:15:46.669
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:46.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:46.689
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-2217 06/08/23 16:15:46.693
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2217 to expose endpoints map[] 06/08/23 16:15:46.711
Jun  8 16:15:46.718: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jun  8 16:15:47.730: INFO: successfully validated that service multi-endpoint-test in namespace services-2217 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2217 06/08/23 16:15:47.73
Jun  8 16:15:47.739: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2217" to be "running and ready"
Jun  8 16:15:47.745: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.74767ms
Jun  8 16:15:47.745: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 16:15:49.750: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010836413s
Jun  8 16:15:49.750: INFO: The phase of Pod pod1 is Running (Ready = true)
Jun  8 16:15:49.750: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2217 to expose endpoints map[pod1:[100]] 06/08/23 16:15:49.755
Jun  8 16:15:49.767: INFO: successfully validated that service multi-endpoint-test in namespace services-2217 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-2217 06/08/23 16:15:49.767
Jun  8 16:15:49.774: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2217" to be "running and ready"
Jun  8 16:15:49.778: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.728249ms
Jun  8 16:15:49.778: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 16:15:51.783: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009235969s
Jun  8 16:15:51.783: INFO: The phase of Pod pod2 is Running (Ready = true)
Jun  8 16:15:51.783: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2217 to expose endpoints map[pod1:[100] pod2:[101]] 06/08/23 16:15:51.787
Jun  8 16:15:51.803: INFO: successfully validated that service multi-endpoint-test in namespace services-2217 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 06/08/23 16:15:51.803
Jun  8 16:15:51.803: INFO: Creating new exec pod
Jun  8 16:15:51.809: INFO: Waiting up to 5m0s for pod "execpodb82fs" in namespace "services-2217" to be "running"
Jun  8 16:15:51.814: INFO: Pod "execpodb82fs": Phase="Pending", Reason="", readiness=false. Elapsed: 4.588555ms
Jun  8 16:15:53.819: INFO: Pod "execpodb82fs": Phase="Running", Reason="", readiness=true. Elapsed: 2.009776564s
Jun  8 16:15:53.819: INFO: Pod "execpodb82fs" satisfied condition "running"
Jun  8 16:15:54.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2217 exec execpodb82fs -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Jun  8 16:15:54.992: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jun  8 16:15:54.992: INFO: stdout: ""
Jun  8 16:15:54.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2217 exec execpodb82fs -- /bin/sh -x -c nc -v -z -w 2 10.106.44.217 80'
Jun  8 16:15:55.195: INFO: stderr: "+ nc -v -z -w 2 10.106.44.217 80\nConnection to 10.106.44.217 80 port [tcp/http] succeeded!\n"
Jun  8 16:15:55.195: INFO: stdout: ""
Jun  8 16:15:55.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2217 exec execpodb82fs -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Jun  8 16:15:55.389: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jun  8 16:15:55.389: INFO: stdout: ""
Jun  8 16:15:55.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2217 exec execpodb82fs -- /bin/sh -x -c nc -v -z -w 2 10.106.44.217 81'
Jun  8 16:15:55.539: INFO: stderr: "+ nc -v -z -w 2 10.106.44.217 81\nConnection to 10.106.44.217 81 port [tcp/*] succeeded!\n"
Jun  8 16:15:55.539: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-2217 06/08/23 16:15:55.539
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2217 to expose endpoints map[pod2:[101]] 06/08/23 16:15:55.551
Jun  8 16:15:55.569: INFO: successfully validated that service multi-endpoint-test in namespace services-2217 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-2217 06/08/23 16:15:55.569
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2217 to expose endpoints map[] 06/08/23 16:15:55.592
Jun  8 16:15:55.604: INFO: successfully validated that service multi-endpoint-test in namespace services-2217 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  8 16:15:55.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2217" for this suite. 06/08/23 16:15:55.64
------------------------------
• [SLOW TEST] [8.983 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:15:46.668
    Jun  8 16:15:46.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename services 06/08/23 16:15:46.669
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:46.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:46.689
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-2217 06/08/23 16:15:46.693
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2217 to expose endpoints map[] 06/08/23 16:15:46.711
    Jun  8 16:15:46.718: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jun  8 16:15:47.730: INFO: successfully validated that service multi-endpoint-test in namespace services-2217 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-2217 06/08/23 16:15:47.73
    Jun  8 16:15:47.739: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2217" to be "running and ready"
    Jun  8 16:15:47.745: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.74767ms
    Jun  8 16:15:47.745: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 16:15:49.750: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010836413s
    Jun  8 16:15:49.750: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jun  8 16:15:49.750: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2217 to expose endpoints map[pod1:[100]] 06/08/23 16:15:49.755
    Jun  8 16:15:49.767: INFO: successfully validated that service multi-endpoint-test in namespace services-2217 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-2217 06/08/23 16:15:49.767
    Jun  8 16:15:49.774: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2217" to be "running and ready"
    Jun  8 16:15:49.778: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.728249ms
    Jun  8 16:15:49.778: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 16:15:51.783: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009235969s
    Jun  8 16:15:51.783: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jun  8 16:15:51.783: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2217 to expose endpoints map[pod1:[100] pod2:[101]] 06/08/23 16:15:51.787
    Jun  8 16:15:51.803: INFO: successfully validated that service multi-endpoint-test in namespace services-2217 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 06/08/23 16:15:51.803
    Jun  8 16:15:51.803: INFO: Creating new exec pod
    Jun  8 16:15:51.809: INFO: Waiting up to 5m0s for pod "execpodb82fs" in namespace "services-2217" to be "running"
    Jun  8 16:15:51.814: INFO: Pod "execpodb82fs": Phase="Pending", Reason="", readiness=false. Elapsed: 4.588555ms
    Jun  8 16:15:53.819: INFO: Pod "execpodb82fs": Phase="Running", Reason="", readiness=true. Elapsed: 2.009776564s
    Jun  8 16:15:53.819: INFO: Pod "execpodb82fs" satisfied condition "running"
    Jun  8 16:15:54.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2217 exec execpodb82fs -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Jun  8 16:15:54.992: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jun  8 16:15:54.992: INFO: stdout: ""
    Jun  8 16:15:54.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2217 exec execpodb82fs -- /bin/sh -x -c nc -v -z -w 2 10.106.44.217 80'
    Jun  8 16:15:55.195: INFO: stderr: "+ nc -v -z -w 2 10.106.44.217 80\nConnection to 10.106.44.217 80 port [tcp/http] succeeded!\n"
    Jun  8 16:15:55.195: INFO: stdout: ""
    Jun  8 16:15:55.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2217 exec execpodb82fs -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Jun  8 16:15:55.389: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jun  8 16:15:55.389: INFO: stdout: ""
    Jun  8 16:15:55.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-2217 exec execpodb82fs -- /bin/sh -x -c nc -v -z -w 2 10.106.44.217 81'
    Jun  8 16:15:55.539: INFO: stderr: "+ nc -v -z -w 2 10.106.44.217 81\nConnection to 10.106.44.217 81 port [tcp/*] succeeded!\n"
    Jun  8 16:15:55.539: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-2217 06/08/23 16:15:55.539
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2217 to expose endpoints map[pod2:[101]] 06/08/23 16:15:55.551
    Jun  8 16:15:55.569: INFO: successfully validated that service multi-endpoint-test in namespace services-2217 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-2217 06/08/23 16:15:55.569
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2217 to expose endpoints map[] 06/08/23 16:15:55.592
    Jun  8 16:15:55.604: INFO: successfully validated that service multi-endpoint-test in namespace services-2217 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:15:55.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2217" for this suite. 06/08/23 16:15:55.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:15:55.652
Jun  8 16:15:55.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename csistoragecapacity 06/08/23 16:15:55.654
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:55.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:55.684
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 06/08/23 16:15:55.689
STEP: getting /apis/storage.k8s.io 06/08/23 16:15:55.693
STEP: getting /apis/storage.k8s.io/v1 06/08/23 16:15:55.695
STEP: creating 06/08/23 16:15:55.697
STEP: watching 06/08/23 16:15:55.721
Jun  8 16:15:55.721: INFO: starting watch
STEP: getting 06/08/23 16:15:55.732
STEP: listing in namespace 06/08/23 16:15:55.739
STEP: listing across namespaces 06/08/23 16:15:55.745
STEP: patching 06/08/23 16:15:55.75
STEP: updating 06/08/23 16:15:55.758
Jun  8 16:15:55.766: INFO: waiting for watch events with expected annotations in namespace
Jun  8 16:15:55.767: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 06/08/23 16:15:55.767
STEP: deleting a collection 06/08/23 16:15:55.789
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Jun  8 16:15:55.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-347" for this suite. 06/08/23 16:15:55.823
------------------------------
• [0.184 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:15:55.652
    Jun  8 16:15:55.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename csistoragecapacity 06/08/23 16:15:55.654
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:55.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:55.684
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 06/08/23 16:15:55.689
    STEP: getting /apis/storage.k8s.io 06/08/23 16:15:55.693
    STEP: getting /apis/storage.k8s.io/v1 06/08/23 16:15:55.695
    STEP: creating 06/08/23 16:15:55.697
    STEP: watching 06/08/23 16:15:55.721
    Jun  8 16:15:55.721: INFO: starting watch
    STEP: getting 06/08/23 16:15:55.732
    STEP: listing in namespace 06/08/23 16:15:55.739
    STEP: listing across namespaces 06/08/23 16:15:55.745
    STEP: patching 06/08/23 16:15:55.75
    STEP: updating 06/08/23 16:15:55.758
    Jun  8 16:15:55.766: INFO: waiting for watch events with expected annotations in namespace
    Jun  8 16:15:55.767: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 06/08/23 16:15:55.767
    STEP: deleting a collection 06/08/23 16:15:55.789
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:15:55.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-347" for this suite. 06/08/23 16:15:55.823
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:15:55.84
Jun  8 16:15:55.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename init-container 06/08/23 16:15:55.841
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:55.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:55.869
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 06/08/23 16:15:55.874
Jun  8 16:15:55.874: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:15:58.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4624" for this suite. 06/08/23 16:15:58.66
------------------------------
• [2.828 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:15:55.84
    Jun  8 16:15:55.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename init-container 06/08/23 16:15:55.841
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:55.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:55.869
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 06/08/23 16:15:55.874
    Jun  8 16:15:55.874: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:15:58.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4624" for this suite. 06/08/23 16:15:58.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:15:58.67
Jun  8 16:15:58.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubectl 06/08/23 16:15:58.672
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:58.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:58.692
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 06/08/23 16:15:58.696
Jun  8 16:15:58.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6826 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun  8 16:15:58.786: INFO: stderr: ""
Jun  8 16:15:58.786: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 06/08/23 16:15:58.786
Jun  8 16:15:58.786: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun  8 16:15:58.786: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6826" to be "running and ready, or succeeded"
Jun  8 16:15:58.791: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.378222ms
Jun  8 16:15:58.791: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'chl8tf-worker-002' to be 'Running' but was 'Pending'
Jun  8 16:16:00.796: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009435495s
Jun  8 16:16:00.796: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun  8 16:16:00.796: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 06/08/23 16:16:00.796
Jun  8 16:16:00.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6826 logs logs-generator logs-generator'
Jun  8 16:16:00.949: INFO: stderr: ""
Jun  8 16:16:00.949: INFO: stdout: "I0608 16:15:59.408405       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/dpn 366\nI0608 16:15:59.608820       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/jt6 464\nI0608 16:15:59.809188       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/gw66 583\nI0608 16:16:00.008490       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/h8f 553\nI0608 16:16:00.208844       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/4gwl 355\nI0608 16:16:00.409204       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/gtp 247\nI0608 16:16:00.608497       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/74sb 309\nI0608 16:16:00.808827       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/mts 482\n"
STEP: limiting log lines 06/08/23 16:16:00.949
Jun  8 16:16:00.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6826 logs logs-generator logs-generator --tail=1'
Jun  8 16:16:01.078: INFO: stderr: ""
Jun  8 16:16:01.078: INFO: stdout: "I0608 16:16:01.009177       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/v8fd 205\n"
Jun  8 16:16:01.078: INFO: got output "I0608 16:16:01.009177       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/v8fd 205\n"
STEP: limiting log bytes 06/08/23 16:16:01.078
Jun  8 16:16:01.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6826 logs logs-generator logs-generator --limit-bytes=1'
Jun  8 16:16:01.165: INFO: stderr: ""
Jun  8 16:16:01.165: INFO: stdout: "I"
Jun  8 16:16:01.165: INFO: got output "I"
STEP: exposing timestamps 06/08/23 16:16:01.165
Jun  8 16:16:01.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6826 logs logs-generator logs-generator --tail=1 --timestamps'
Jun  8 16:16:01.258: INFO: stderr: ""
Jun  8 16:16:01.258: INFO: stdout: "2023-06-08T16:16:01.208520431Z I0608 16:16:01.208449       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/nrx 441\n"
Jun  8 16:16:01.258: INFO: got output "2023-06-08T16:16:01.208520431Z I0608 16:16:01.208449       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/nrx 441\n"
STEP: restricting to a time range 06/08/23 16:16:01.258
Jun  8 16:16:03.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6826 logs logs-generator logs-generator --since=1s'
Jun  8 16:16:03.851: INFO: stderr: ""
Jun  8 16:16:03.851: INFO: stdout: "I0608 16:16:03.009547       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/9bfg 411\nI0608 16:16:03.208864       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/htw4 527\nI0608 16:16:03.409210       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/ksb 344\nI0608 16:16:03.608491       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/jkpl 244\nI0608 16:16:03.808853       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/zbvr 470\n"
Jun  8 16:16:03.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6826 logs logs-generator logs-generator --since=24h'
Jun  8 16:16:03.995: INFO: stderr: ""
Jun  8 16:16:03.995: INFO: stdout: "I0608 16:15:59.408405       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/dpn 366\nI0608 16:15:59.608820       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/jt6 464\nI0608 16:15:59.809188       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/gw66 583\nI0608 16:16:00.008490       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/h8f 553\nI0608 16:16:00.208844       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/4gwl 355\nI0608 16:16:00.409204       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/gtp 247\nI0608 16:16:00.608497       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/74sb 309\nI0608 16:16:00.808827       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/mts 482\nI0608 16:16:01.009177       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/v8fd 205\nI0608 16:16:01.208449       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/nrx 441\nI0608 16:16:01.408825       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/dwdb 584\nI0608 16:16:01.609163       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/nzkv 221\nI0608 16:16:01.808454       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/ms2 282\nI0608 16:16:02.008834       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/jbq 566\nI0608 16:16:02.209179       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/njwh 542\nI0608 16:16:02.408469       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/dkgx 431\nI0608 16:16:02.608812       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/spcg 402\nI0608 16:16:02.809164       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/xds 559\nI0608 16:16:03.009547       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/9bfg 411\nI0608 16:16:03.208864       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/htw4 527\nI0608 16:16:03.409210       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/ksb 344\nI0608 16:16:03.608491       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/jkpl 244\nI0608 16:16:03.808853       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/zbvr 470\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Jun  8 16:16:03.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6826 delete pod logs-generator'
Jun  8 16:16:04.737: INFO: stderr: ""
Jun  8 16:16:04.737: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  8 16:16:04.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6826" for this suite. 06/08/23 16:16:04.744
------------------------------
• [SLOW TEST] [6.081 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:15:58.67
    Jun  8 16:15:58.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubectl 06/08/23 16:15:58.672
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:15:58.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:15:58.692
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 06/08/23 16:15:58.696
    Jun  8 16:15:58.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6826 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jun  8 16:15:58.786: INFO: stderr: ""
    Jun  8 16:15:58.786: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 06/08/23 16:15:58.786
    Jun  8 16:15:58.786: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jun  8 16:15:58.786: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6826" to be "running and ready, or succeeded"
    Jun  8 16:15:58.791: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.378222ms
    Jun  8 16:15:58.791: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'chl8tf-worker-002' to be 'Running' but was 'Pending'
    Jun  8 16:16:00.796: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009435495s
    Jun  8 16:16:00.796: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jun  8 16:16:00.796: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 06/08/23 16:16:00.796
    Jun  8 16:16:00.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6826 logs logs-generator logs-generator'
    Jun  8 16:16:00.949: INFO: stderr: ""
    Jun  8 16:16:00.949: INFO: stdout: "I0608 16:15:59.408405       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/dpn 366\nI0608 16:15:59.608820       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/jt6 464\nI0608 16:15:59.809188       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/gw66 583\nI0608 16:16:00.008490       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/h8f 553\nI0608 16:16:00.208844       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/4gwl 355\nI0608 16:16:00.409204       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/gtp 247\nI0608 16:16:00.608497       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/74sb 309\nI0608 16:16:00.808827       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/mts 482\n"
    STEP: limiting log lines 06/08/23 16:16:00.949
    Jun  8 16:16:00.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6826 logs logs-generator logs-generator --tail=1'
    Jun  8 16:16:01.078: INFO: stderr: ""
    Jun  8 16:16:01.078: INFO: stdout: "I0608 16:16:01.009177       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/v8fd 205\n"
    Jun  8 16:16:01.078: INFO: got output "I0608 16:16:01.009177       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/v8fd 205\n"
    STEP: limiting log bytes 06/08/23 16:16:01.078
    Jun  8 16:16:01.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6826 logs logs-generator logs-generator --limit-bytes=1'
    Jun  8 16:16:01.165: INFO: stderr: ""
    Jun  8 16:16:01.165: INFO: stdout: "I"
    Jun  8 16:16:01.165: INFO: got output "I"
    STEP: exposing timestamps 06/08/23 16:16:01.165
    Jun  8 16:16:01.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6826 logs logs-generator logs-generator --tail=1 --timestamps'
    Jun  8 16:16:01.258: INFO: stderr: ""
    Jun  8 16:16:01.258: INFO: stdout: "2023-06-08T16:16:01.208520431Z I0608 16:16:01.208449       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/nrx 441\n"
    Jun  8 16:16:01.258: INFO: got output "2023-06-08T16:16:01.208520431Z I0608 16:16:01.208449       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/nrx 441\n"
    STEP: restricting to a time range 06/08/23 16:16:01.258
    Jun  8 16:16:03.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6826 logs logs-generator logs-generator --since=1s'
    Jun  8 16:16:03.851: INFO: stderr: ""
    Jun  8 16:16:03.851: INFO: stdout: "I0608 16:16:03.009547       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/9bfg 411\nI0608 16:16:03.208864       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/htw4 527\nI0608 16:16:03.409210       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/ksb 344\nI0608 16:16:03.608491       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/jkpl 244\nI0608 16:16:03.808853       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/zbvr 470\n"
    Jun  8 16:16:03.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6826 logs logs-generator logs-generator --since=24h'
    Jun  8 16:16:03.995: INFO: stderr: ""
    Jun  8 16:16:03.995: INFO: stdout: "I0608 16:15:59.408405       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/dpn 366\nI0608 16:15:59.608820       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/jt6 464\nI0608 16:15:59.809188       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/gw66 583\nI0608 16:16:00.008490       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/h8f 553\nI0608 16:16:00.208844       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/4gwl 355\nI0608 16:16:00.409204       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/gtp 247\nI0608 16:16:00.608497       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/74sb 309\nI0608 16:16:00.808827       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/mts 482\nI0608 16:16:01.009177       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/v8fd 205\nI0608 16:16:01.208449       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/nrx 441\nI0608 16:16:01.408825       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/dwdb 584\nI0608 16:16:01.609163       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/nzkv 221\nI0608 16:16:01.808454       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/ms2 282\nI0608 16:16:02.008834       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/jbq 566\nI0608 16:16:02.209179       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/njwh 542\nI0608 16:16:02.408469       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/dkgx 431\nI0608 16:16:02.608812       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/spcg 402\nI0608 16:16:02.809164       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/xds 559\nI0608 16:16:03.009547       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/9bfg 411\nI0608 16:16:03.208864       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/htw4 527\nI0608 16:16:03.409210       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/ksb 344\nI0608 16:16:03.608491       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/jkpl 244\nI0608 16:16:03.808853       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/zbvr 470\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Jun  8 16:16:03.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-6826 delete pod logs-generator'
    Jun  8 16:16:04.737: INFO: stderr: ""
    Jun  8 16:16:04.737: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:16:04.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6826" for this suite. 06/08/23 16:16:04.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:16:04.752
Jun  8 16:16:04.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename webhook 06/08/23 16:16:04.753
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:16:04.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:16:04.772
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/08/23 16:16:04.792
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 16:16:05.734
STEP: Deploying the webhook pod 06/08/23 16:16:05.744
STEP: Wait for the deployment to be ready 06/08/23 16:16:05.756
Jun  8 16:16:05.765: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/08/23 16:16:07.778
STEP: Verifying the service has paired with the endpoint 06/08/23 16:16:07.793
Jun  8 16:16:08.794: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 06/08/23 16:16:08.798
STEP: create a namespace for the webhook 06/08/23 16:16:08.816
STEP: create a configmap should be unconditionally rejected by the webhook 06/08/23 16:16:08.825
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:16:08.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7546" for this suite. 06/08/23 16:16:08.903
STEP: Destroying namespace "webhook-7546-markers" for this suite. 06/08/23 16:16:08.914
------------------------------
• [4.175 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:16:04.752
    Jun  8 16:16:04.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename webhook 06/08/23 16:16:04.753
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:16:04.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:16:04.772
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/08/23 16:16:04.792
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 16:16:05.734
    STEP: Deploying the webhook pod 06/08/23 16:16:05.744
    STEP: Wait for the deployment to be ready 06/08/23 16:16:05.756
    Jun  8 16:16:05.765: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/08/23 16:16:07.778
    STEP: Verifying the service has paired with the endpoint 06/08/23 16:16:07.793
    Jun  8 16:16:08.794: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 06/08/23 16:16:08.798
    STEP: create a namespace for the webhook 06/08/23 16:16:08.816
    STEP: create a configmap should be unconditionally rejected by the webhook 06/08/23 16:16:08.825
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:16:08.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7546" for this suite. 06/08/23 16:16:08.903
    STEP: Destroying namespace "webhook-7546-markers" for this suite. 06/08/23 16:16:08.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:16:08.927
Jun  8 16:16:08.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename emptydir 06/08/23 16:16:08.929
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:16:08.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:16:08.957
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 06/08/23 16:16:08.962
Jun  8 16:16:08.974: INFO: Waiting up to 5m0s for pod "pod-8fd026c4-6bb4-49ad-a20f-b891a40dc82e" in namespace "emptydir-3123" to be "Succeeded or Failed"
Jun  8 16:16:08.980: INFO: Pod "pod-8fd026c4-6bb4-49ad-a20f-b891a40dc82e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.249687ms
Jun  8 16:16:10.987: INFO: Pod "pod-8fd026c4-6bb4-49ad-a20f-b891a40dc82e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01271436s
Jun  8 16:16:12.986: INFO: Pod "pod-8fd026c4-6bb4-49ad-a20f-b891a40dc82e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011542775s
STEP: Saw pod success 06/08/23 16:16:12.986
Jun  8 16:16:12.986: INFO: Pod "pod-8fd026c4-6bb4-49ad-a20f-b891a40dc82e" satisfied condition "Succeeded or Failed"
Jun  8 16:16:12.991: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-8fd026c4-6bb4-49ad-a20f-b891a40dc82e container test-container: <nil>
STEP: delete the pod 06/08/23 16:16:12.998
Jun  8 16:16:13.012: INFO: Waiting for pod pod-8fd026c4-6bb4-49ad-a20f-b891a40dc82e to disappear
Jun  8 16:16:13.016: INFO: Pod pod-8fd026c4-6bb4-49ad-a20f-b891a40dc82e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  8 16:16:13.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3123" for this suite. 06/08/23 16:16:13.021
------------------------------
• [4.101 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:16:08.927
    Jun  8 16:16:08.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename emptydir 06/08/23 16:16:08.929
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:16:08.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:16:08.957
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 06/08/23 16:16:08.962
    Jun  8 16:16:08.974: INFO: Waiting up to 5m0s for pod "pod-8fd026c4-6bb4-49ad-a20f-b891a40dc82e" in namespace "emptydir-3123" to be "Succeeded or Failed"
    Jun  8 16:16:08.980: INFO: Pod "pod-8fd026c4-6bb4-49ad-a20f-b891a40dc82e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.249687ms
    Jun  8 16:16:10.987: INFO: Pod "pod-8fd026c4-6bb4-49ad-a20f-b891a40dc82e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01271436s
    Jun  8 16:16:12.986: INFO: Pod "pod-8fd026c4-6bb4-49ad-a20f-b891a40dc82e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011542775s
    STEP: Saw pod success 06/08/23 16:16:12.986
    Jun  8 16:16:12.986: INFO: Pod "pod-8fd026c4-6bb4-49ad-a20f-b891a40dc82e" satisfied condition "Succeeded or Failed"
    Jun  8 16:16:12.991: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-8fd026c4-6bb4-49ad-a20f-b891a40dc82e container test-container: <nil>
    STEP: delete the pod 06/08/23 16:16:12.998
    Jun  8 16:16:13.012: INFO: Waiting for pod pod-8fd026c4-6bb4-49ad-a20f-b891a40dc82e to disappear
    Jun  8 16:16:13.016: INFO: Pod pod-8fd026c4-6bb4-49ad-a20f-b891a40dc82e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:16:13.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3123" for this suite. 06/08/23 16:16:13.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:16:13.03
Jun  8 16:16:13.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename svc-latency 06/08/23 16:16:13.031
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:16:13.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:16:13.052
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jun  8 16:16:13.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6605 06/08/23 16:16:13.056
I0608 16:16:13.062692      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6605, replica count: 1
I0608 16:16:14.114140      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  8 16:16:14.232: INFO: Created: latency-svc-7p2r4
Jun  8 16:16:14.239: INFO: Got endpoints: latency-svc-7p2r4 [24.783707ms]
Jun  8 16:16:14.261: INFO: Created: latency-svc-m856m
Jun  8 16:16:14.272: INFO: Got endpoints: latency-svc-m856m [33.245105ms]
Jun  8 16:16:14.280: INFO: Created: latency-svc-tpc7d
Jun  8 16:16:14.290: INFO: Got endpoints: latency-svc-tpc7d [50.456107ms]
Jun  8 16:16:14.297: INFO: Created: latency-svc-vztrk
Jun  8 16:16:14.310: INFO: Got endpoints: latency-svc-vztrk [71.437824ms]
Jun  8 16:16:14.319: INFO: Created: latency-svc-mc2ph
Jun  8 16:16:14.339: INFO: Got endpoints: latency-svc-mc2ph [99.511255ms]
Jun  8 16:16:14.340: INFO: Created: latency-svc-8ghw6
Jun  8 16:16:14.343: INFO: Got endpoints: latency-svc-8ghw6 [103.636721ms]
Jun  8 16:16:14.357: INFO: Created: latency-svc-4s7b5
Jun  8 16:16:14.368: INFO: Got endpoints: latency-svc-4s7b5 [128.034102ms]
Jun  8 16:16:14.376: INFO: Created: latency-svc-nj8kr
Jun  8 16:16:14.384: INFO: Got endpoints: latency-svc-nj8kr [143.955405ms]
Jun  8 16:16:14.395: INFO: Created: latency-svc-jqhhp
Jun  8 16:16:14.407: INFO: Created: latency-svc-jwnvr
Jun  8 16:16:14.415: INFO: Got endpoints: latency-svc-jqhhp [176.172997ms]
Jun  8 16:16:14.424: INFO: Got endpoints: latency-svc-jwnvr [184.388082ms]
Jun  8 16:16:14.433: INFO: Created: latency-svc-mx6tl
Jun  8 16:16:14.446: INFO: Got endpoints: latency-svc-mx6tl [206.017263ms]
Jun  8 16:16:14.455: INFO: Created: latency-svc-87mn7
Jun  8 16:16:14.479: INFO: Created: latency-svc-5nlkg
Jun  8 16:16:14.485: INFO: Got endpoints: latency-svc-87mn7 [244.724347ms]
Jun  8 16:16:14.492: INFO: Got endpoints: latency-svc-5nlkg [251.77213ms]
Jun  8 16:16:14.493: INFO: Created: latency-svc-svqv7
Jun  8 16:16:14.502: INFO: Got endpoints: latency-svc-svqv7 [262.396295ms]
Jun  8 16:16:14.518: INFO: Created: latency-svc-2b9tl
Jun  8 16:16:14.534: INFO: Got endpoints: latency-svc-2b9tl [293.204713ms]
Jun  8 16:16:14.552: INFO: Created: latency-svc-cqkgr
Jun  8 16:16:14.559: INFO: Got endpoints: latency-svc-cqkgr [318.740775ms]
Jun  8 16:16:14.570: INFO: Created: latency-svc-77hmv
Jun  8 16:16:14.583: INFO: Got endpoints: latency-svc-77hmv [310.924355ms]
Jun  8 16:16:14.590: INFO: Created: latency-svc-9lcnc
Jun  8 16:16:14.599: INFO: Got endpoints: latency-svc-9lcnc [309.381486ms]
Jun  8 16:16:14.604: INFO: Created: latency-svc-rjgp7
Jun  8 16:16:14.611: INFO: Got endpoints: latency-svc-rjgp7 [300.944073ms]
Jun  8 16:16:14.622: INFO: Created: latency-svc-9w2gn
Jun  8 16:16:14.633: INFO: Got endpoints: latency-svc-9w2gn [293.642065ms]
Jun  8 16:16:14.642: INFO: Created: latency-svc-5slnh
Jun  8 16:16:14.650: INFO: Got endpoints: latency-svc-5slnh [306.715633ms]
Jun  8 16:16:14.668: INFO: Created: latency-svc-bhlbl
Jun  8 16:16:14.675: INFO: Got endpoints: latency-svc-bhlbl [307.367939ms]
Jun  8 16:16:14.687: INFO: Created: latency-svc-tlsfh
Jun  8 16:16:14.694: INFO: Got endpoints: latency-svc-tlsfh [310.156638ms]
Jun  8 16:16:14.709: INFO: Created: latency-svc-gj697
Jun  8 16:16:14.723: INFO: Got endpoints: latency-svc-gj697 [307.988168ms]
Jun  8 16:16:14.744: INFO: Created: latency-svc-qxbz6
Jun  8 16:16:14.754: INFO: Got endpoints: latency-svc-qxbz6 [329.082292ms]
Jun  8 16:16:14.799: INFO: Created: latency-svc-45972
Jun  8 16:16:14.806: INFO: Got endpoints: latency-svc-45972 [359.580515ms]
Jun  8 16:16:14.812: INFO: Created: latency-svc-qw6v9
Jun  8 16:16:14.818: INFO: Got endpoints: latency-svc-qw6v9 [332.848305ms]
Jun  8 16:16:14.840: INFO: Created: latency-svc-x45xq
Jun  8 16:16:14.849: INFO: Got endpoints: latency-svc-x45xq [357.198583ms]
Jun  8 16:16:14.857: INFO: Created: latency-svc-2whgw
Jun  8 16:16:14.874: INFO: Created: latency-svc-dfmsk
Jun  8 16:16:14.876: INFO: Got endpoints: latency-svc-2whgw [373.606679ms]
Jun  8 16:16:14.887: INFO: Got endpoints: latency-svc-dfmsk [353.792109ms]
Jun  8 16:16:14.892: INFO: Created: latency-svc-dwnsj
Jun  8 16:16:14.906: INFO: Got endpoints: latency-svc-dwnsj [347.399658ms]
Jun  8 16:16:14.916: INFO: Created: latency-svc-sv4zc
Jun  8 16:16:14.922: INFO: Got endpoints: latency-svc-sv4zc [338.56728ms]
Jun  8 16:16:14.932: INFO: Created: latency-svc-khsq6
Jun  8 16:16:14.943: INFO: Got endpoints: latency-svc-khsq6 [343.35969ms]
Jun  8 16:16:14.948: INFO: Created: latency-svc-fdmzd
Jun  8 16:16:14.965: INFO: Got endpoints: latency-svc-fdmzd [353.856674ms]
Jun  8 16:16:14.968: INFO: Created: latency-svc-2svs8
Jun  8 16:16:14.980: INFO: Got endpoints: latency-svc-2svs8 [347.442787ms]
Jun  8 16:16:14.995: INFO: Created: latency-svc-jgnbd
Jun  8 16:16:14.997: INFO: Got endpoints: latency-svc-jgnbd [346.981209ms]
Jun  8 16:16:15.012: INFO: Created: latency-svc-xsshj
Jun  8 16:16:15.025: INFO: Got endpoints: latency-svc-xsshj [350.034666ms]
Jun  8 16:16:15.038: INFO: Created: latency-svc-bl9tw
Jun  8 16:16:15.059: INFO: Got endpoints: latency-svc-bl9tw [364.412943ms]
Jun  8 16:16:15.067: INFO: Created: latency-svc-47879
Jun  8 16:16:15.090: INFO: Got endpoints: latency-svc-47879 [365.852786ms]
Jun  8 16:16:15.092: INFO: Created: latency-svc-d7vhg
Jun  8 16:16:15.104: INFO: Got endpoints: latency-svc-d7vhg [350.197813ms]
Jun  8 16:16:15.110: INFO: Created: latency-svc-m8xc6
Jun  8 16:16:15.124: INFO: Got endpoints: latency-svc-m8xc6 [317.831331ms]
Jun  8 16:16:15.131: INFO: Created: latency-svc-6xnh7
Jun  8 16:16:15.140: INFO: Got endpoints: latency-svc-6xnh7 [322.061172ms]
Jun  8 16:16:15.146: INFO: Created: latency-svc-rc468
Jun  8 16:16:15.154: INFO: Got endpoints: latency-svc-rc468 [304.979653ms]
Jun  8 16:16:15.170: INFO: Created: latency-svc-7lsz8
Jun  8 16:16:15.188: INFO: Created: latency-svc-x2ws8
Jun  8 16:16:15.189: INFO: Got endpoints: latency-svc-7lsz8 [312.854965ms]
Jun  8 16:16:15.201: INFO: Got endpoints: latency-svc-x2ws8 [313.989574ms]
Jun  8 16:16:15.212: INFO: Created: latency-svc-bj7j6
Jun  8 16:16:15.221: INFO: Got endpoints: latency-svc-bj7j6 [314.568124ms]
Jun  8 16:16:15.228: INFO: Created: latency-svc-wfgvk
Jun  8 16:16:15.235: INFO: Got endpoints: latency-svc-wfgvk [313.031393ms]
Jun  8 16:16:15.247: INFO: Created: latency-svc-g77z7
Jun  8 16:16:15.256: INFO: Got endpoints: latency-svc-g77z7 [313.597093ms]
Jun  8 16:16:15.265: INFO: Created: latency-svc-8lsbs
Jun  8 16:16:15.274: INFO: Got endpoints: latency-svc-8lsbs [308.585465ms]
Jun  8 16:16:15.287: INFO: Created: latency-svc-25mbh
Jun  8 16:16:15.298: INFO: Got endpoints: latency-svc-25mbh [318.075604ms]
Jun  8 16:16:15.303: INFO: Created: latency-svc-zgd7r
Jun  8 16:16:15.320: INFO: Got endpoints: latency-svc-zgd7r [322.710261ms]
Jun  8 16:16:15.326: INFO: Created: latency-svc-bff57
Jun  8 16:16:15.338: INFO: Got endpoints: latency-svc-bff57 [312.543563ms]
Jun  8 16:16:15.343: INFO: Created: latency-svc-jvgmf
Jun  8 16:16:15.357: INFO: Created: latency-svc-n6r9c
Jun  8 16:16:15.372: INFO: Created: latency-svc-fw2s8
Jun  8 16:16:15.389: INFO: Got endpoints: latency-svc-jvgmf [330.111708ms]
Jun  8 16:16:15.389: INFO: Created: latency-svc-nvq7h
Jun  8 16:16:15.408: INFO: Created: latency-svc-krw4x
Jun  8 16:16:15.433: INFO: Created: latency-svc-84gcm
Jun  8 16:16:15.441: INFO: Got endpoints: latency-svc-n6r9c [351.342741ms]
Jun  8 16:16:15.452: INFO: Created: latency-svc-74qzk
Jun  8 16:16:15.471: INFO: Created: latency-svc-slvnh
Jun  8 16:16:15.489: INFO: Got endpoints: latency-svc-fw2s8 [385.351369ms]
Jun  8 16:16:15.493: INFO: Created: latency-svc-q4nsm
Jun  8 16:16:15.523: INFO: Created: latency-svc-9kzcm
Jun  8 16:16:15.541: INFO: Got endpoints: latency-svc-nvq7h [417.748955ms]
Jun  8 16:16:15.550: INFO: Created: latency-svc-snjsz
Jun  8 16:16:15.576: INFO: Created: latency-svc-wcwk4
Jun  8 16:16:15.590: INFO: Got endpoints: latency-svc-krw4x [450.048352ms]
Jun  8 16:16:15.596: INFO: Created: latency-svc-njzzl
Jun  8 16:16:15.614: INFO: Created: latency-svc-r6d2d
Jun  8 16:16:15.637: INFO: Created: latency-svc-g6k56
Jun  8 16:16:15.649: INFO: Got endpoints: latency-svc-84gcm [494.460043ms]
Jun  8 16:16:15.656: INFO: Created: latency-svc-8dzj4
Jun  8 16:16:15.672: INFO: Created: latency-svc-555kq
Jun  8 16:16:15.691: INFO: Got endpoints: latency-svc-74qzk [501.745038ms]
Jun  8 16:16:15.691: INFO: Created: latency-svc-p258h
Jun  8 16:16:15.708: INFO: Created: latency-svc-vvcrz
Jun  8 16:16:15.727: INFO: Created: latency-svc-ts65b
Jun  8 16:16:15.740: INFO: Got endpoints: latency-svc-slvnh [539.066522ms]
Jun  8 16:16:15.747: INFO: Created: latency-svc-9nk4m
Jun  8 16:16:15.765: INFO: Created: latency-svc-xqb6p
Jun  8 16:16:15.782: INFO: Created: latency-svc-h85rk
Jun  8 16:16:15.789: INFO: Got endpoints: latency-svc-q4nsm [567.808308ms]
Jun  8 16:16:15.815: INFO: Created: latency-svc-mwdss
Jun  8 16:16:15.840: INFO: Got endpoints: latency-svc-9kzcm [605.145027ms]
Jun  8 16:16:15.868: INFO: Created: latency-svc-gfmnq
Jun  8 16:16:15.888: INFO: Got endpoints: latency-svc-snjsz [631.369512ms]
Jun  8 16:16:15.913: INFO: Created: latency-svc-7bnkh
Jun  8 16:16:15.940: INFO: Got endpoints: latency-svc-wcwk4 [665.90882ms]
Jun  8 16:16:15.960: INFO: Created: latency-svc-67h2d
Jun  8 16:16:15.989: INFO: Got endpoints: latency-svc-njzzl [690.598544ms]
Jun  8 16:16:16.012: INFO: Created: latency-svc-6vxp8
Jun  8 16:16:16.038: INFO: Got endpoints: latency-svc-r6d2d [718.486566ms]
Jun  8 16:16:16.060: INFO: Created: latency-svc-kx226
Jun  8 16:16:16.088: INFO: Got endpoints: latency-svc-g6k56 [749.997667ms]
Jun  8 16:16:16.107: INFO: Created: latency-svc-vwx6b
Jun  8 16:16:16.139: INFO: Got endpoints: latency-svc-8dzj4 [750.351033ms]
Jun  8 16:16:16.158: INFO: Created: latency-svc-tz58s
Jun  8 16:16:16.188: INFO: Got endpoints: latency-svc-555kq [747.143044ms]
Jun  8 16:16:16.206: INFO: Created: latency-svc-kpwc6
Jun  8 16:16:16.237: INFO: Got endpoints: latency-svc-p258h [747.806943ms]
Jun  8 16:16:16.257: INFO: Created: latency-svc-4bwp2
Jun  8 16:16:16.293: INFO: Got endpoints: latency-svc-vvcrz [751.630682ms]
Jun  8 16:16:16.317: INFO: Created: latency-svc-pnv5n
Jun  8 16:16:16.338: INFO: Got endpoints: latency-svc-ts65b [747.268531ms]
Jun  8 16:16:16.360: INFO: Created: latency-svc-cjrk4
Jun  8 16:16:16.389: INFO: Got endpoints: latency-svc-9nk4m [739.776159ms]
Jun  8 16:16:16.410: INFO: Created: latency-svc-7n7d8
Jun  8 16:16:16.438: INFO: Got endpoints: latency-svc-xqb6p [747.395796ms]
Jun  8 16:16:16.458: INFO: Created: latency-svc-95sgl
Jun  8 16:16:16.490: INFO: Got endpoints: latency-svc-h85rk [749.104821ms]
Jun  8 16:16:16.527: INFO: Created: latency-svc-shm7j
Jun  8 16:16:16.540: INFO: Got endpoints: latency-svc-mwdss [751.582235ms]
Jun  8 16:16:16.566: INFO: Created: latency-svc-sp9gp
Jun  8 16:16:16.592: INFO: Got endpoints: latency-svc-gfmnq [751.959682ms]
Jun  8 16:16:16.614: INFO: Created: latency-svc-njkcj
Jun  8 16:16:16.638: INFO: Got endpoints: latency-svc-7bnkh [750.220155ms]
Jun  8 16:16:16.657: INFO: Created: latency-svc-nz7rb
Jun  8 16:16:16.691: INFO: Got endpoints: latency-svc-67h2d [751.121939ms]
Jun  8 16:16:16.714: INFO: Created: latency-svc-748bp
Jun  8 16:16:16.739: INFO: Got endpoints: latency-svc-6vxp8 [749.859338ms]
Jun  8 16:16:16.757: INFO: Created: latency-svc-n476g
Jun  8 16:16:16.788: INFO: Got endpoints: latency-svc-kx226 [749.812997ms]
Jun  8 16:16:16.822: INFO: Created: latency-svc-9f99z
Jun  8 16:16:16.841: INFO: Got endpoints: latency-svc-vwx6b [752.633684ms]
Jun  8 16:16:16.863: INFO: Created: latency-svc-sw2pq
Jun  8 16:16:16.890: INFO: Got endpoints: latency-svc-tz58s [751.049669ms]
Jun  8 16:16:16.907: INFO: Created: latency-svc-rkhsf
Jun  8 16:16:16.936: INFO: Got endpoints: latency-svc-kpwc6 [747.962882ms]
Jun  8 16:16:16.952: INFO: Created: latency-svc-hlcmg
Jun  8 16:16:16.988: INFO: Got endpoints: latency-svc-4bwp2 [750.420919ms]
Jun  8 16:16:17.004: INFO: Created: latency-svc-pbgt2
Jun  8 16:16:17.038: INFO: Got endpoints: latency-svc-pnv5n [744.871979ms]
Jun  8 16:16:17.054: INFO: Created: latency-svc-4m5vx
Jun  8 16:16:17.087: INFO: Got endpoints: latency-svc-cjrk4 [749.485636ms]
Jun  8 16:16:17.103: INFO: Created: latency-svc-27jmk
Jun  8 16:16:17.137: INFO: Got endpoints: latency-svc-7n7d8 [747.87029ms]
Jun  8 16:16:17.153: INFO: Created: latency-svc-b5brs
Jun  8 16:16:17.188: INFO: Got endpoints: latency-svc-95sgl [750.089624ms]
Jun  8 16:16:17.207: INFO: Created: latency-svc-jtlfl
Jun  8 16:16:17.241: INFO: Got endpoints: latency-svc-shm7j [751.299979ms]
Jun  8 16:16:17.259: INFO: Created: latency-svc-4hrn4
Jun  8 16:16:17.290: INFO: Got endpoints: latency-svc-sp9gp [749.21197ms]
Jun  8 16:16:17.314: INFO: Created: latency-svc-fzz4f
Jun  8 16:16:17.338: INFO: Got endpoints: latency-svc-njkcj [745.450928ms]
Jun  8 16:16:17.362: INFO: Created: latency-svc-k46rq
Jun  8 16:16:17.390: INFO: Got endpoints: latency-svc-nz7rb [751.478253ms]
Jun  8 16:16:17.412: INFO: Created: latency-svc-fqnhx
Jun  8 16:16:17.440: INFO: Got endpoints: latency-svc-748bp [749.213566ms]
Jun  8 16:16:17.467: INFO: Created: latency-svc-zwgwd
Jun  8 16:16:17.490: INFO: Got endpoints: latency-svc-n476g [751.587601ms]
Jun  8 16:16:17.509: INFO: Created: latency-svc-g8c6w
Jun  8 16:16:17.543: INFO: Got endpoints: latency-svc-9f99z [754.395635ms]
Jun  8 16:16:17.567: INFO: Created: latency-svc-hkz27
Jun  8 16:16:17.588: INFO: Got endpoints: latency-svc-sw2pq [747.532994ms]
Jun  8 16:16:17.611: INFO: Created: latency-svc-k27mz
Jun  8 16:16:17.641: INFO: Got endpoints: latency-svc-rkhsf [751.018649ms]
Jun  8 16:16:17.665: INFO: Created: latency-svc-mllmc
Jun  8 16:16:17.690: INFO: Got endpoints: latency-svc-hlcmg [753.586604ms]
Jun  8 16:16:17.709: INFO: Created: latency-svc-4z27v
Jun  8 16:16:17.738: INFO: Got endpoints: latency-svc-pbgt2 [750.735725ms]
Jun  8 16:16:17.757: INFO: Created: latency-svc-jfxvs
Jun  8 16:16:17.791: INFO: Got endpoints: latency-svc-4m5vx [752.986799ms]
Jun  8 16:16:17.809: INFO: Created: latency-svc-7qf2w
Jun  8 16:16:17.839: INFO: Got endpoints: latency-svc-27jmk [752.224383ms]
Jun  8 16:16:17.857: INFO: Created: latency-svc-6h4kr
Jun  8 16:16:17.887: INFO: Got endpoints: latency-svc-b5brs [750.405569ms]
Jun  8 16:16:17.903: INFO: Created: latency-svc-ns9fl
Jun  8 16:16:17.939: INFO: Got endpoints: latency-svc-jtlfl [751.066581ms]
Jun  8 16:16:17.955: INFO: Created: latency-svc-h5hb5
Jun  8 16:16:17.987: INFO: Got endpoints: latency-svc-4hrn4 [745.970636ms]
Jun  8 16:16:18.002: INFO: Created: latency-svc-jpcfg
Jun  8 16:16:18.037: INFO: Got endpoints: latency-svc-fzz4f [747.334789ms]
Jun  8 16:16:18.054: INFO: Created: latency-svc-gln8r
Jun  8 16:16:18.087: INFO: Got endpoints: latency-svc-k46rq [749.266872ms]
Jun  8 16:16:18.104: INFO: Created: latency-svc-xpzpn
Jun  8 16:16:18.137: INFO: Got endpoints: latency-svc-fqnhx [747.557503ms]
Jun  8 16:16:18.154: INFO: Created: latency-svc-xpdh9
Jun  8 16:16:18.187: INFO: Got endpoints: latency-svc-zwgwd [746.600473ms]
Jun  8 16:16:18.203: INFO: Created: latency-svc-9ff4q
Jun  8 16:16:18.238: INFO: Got endpoints: latency-svc-g8c6w [747.560109ms]
Jun  8 16:16:18.256: INFO: Created: latency-svc-8kwxb
Jun  8 16:16:18.296: INFO: Got endpoints: latency-svc-hkz27 [753.436414ms]
Jun  8 16:16:18.319: INFO: Created: latency-svc-h6948
Jun  8 16:16:18.339: INFO: Got endpoints: latency-svc-k27mz [751.022413ms]
Jun  8 16:16:18.358: INFO: Created: latency-svc-9v8vl
Jun  8 16:16:18.397: INFO: Got endpoints: latency-svc-mllmc [755.591285ms]
Jun  8 16:16:18.420: INFO: Created: latency-svc-dp6dw
Jun  8 16:16:18.439: INFO: Got endpoints: latency-svc-4z27v [749.008047ms]
Jun  8 16:16:18.459: INFO: Created: latency-svc-ptg2f
Jun  8 16:16:18.500: INFO: Got endpoints: latency-svc-jfxvs [761.834866ms]
Jun  8 16:16:18.523: INFO: Created: latency-svc-44h5t
Jun  8 16:16:18.547: INFO: Got endpoints: latency-svc-7qf2w [756.080127ms]
Jun  8 16:16:18.572: INFO: Created: latency-svc-8qmmp
Jun  8 16:16:18.591: INFO: Got endpoints: latency-svc-6h4kr [751.226085ms]
Jun  8 16:16:18.612: INFO: Created: latency-svc-v8frw
Jun  8 16:16:18.641: INFO: Got endpoints: latency-svc-ns9fl [753.452739ms]
Jun  8 16:16:18.659: INFO: Created: latency-svc-nmvnh
Jun  8 16:16:18.693: INFO: Got endpoints: latency-svc-h5hb5 [753.440897ms]
Jun  8 16:16:18.720: INFO: Created: latency-svc-fcffr
Jun  8 16:16:18.738: INFO: Got endpoints: latency-svc-jpcfg [751.413371ms]
Jun  8 16:16:18.758: INFO: Created: latency-svc-5sgdw
Jun  8 16:16:18.791: INFO: Got endpoints: latency-svc-gln8r [753.109205ms]
Jun  8 16:16:18.816: INFO: Created: latency-svc-4ccm9
Jun  8 16:16:18.837: INFO: Got endpoints: latency-svc-xpzpn [750.01316ms]
Jun  8 16:16:18.855: INFO: Created: latency-svc-nxqtx
Jun  8 16:16:18.889: INFO: Got endpoints: latency-svc-xpdh9 [751.506989ms]
Jun  8 16:16:18.911: INFO: Created: latency-svc-86wpx
Jun  8 16:16:18.939: INFO: Got endpoints: latency-svc-9ff4q [751.726208ms]
Jun  8 16:16:18.955: INFO: Created: latency-svc-pm4wb
Jun  8 16:16:18.989: INFO: Got endpoints: latency-svc-8kwxb [750.997003ms]
Jun  8 16:16:19.006: INFO: Created: latency-svc-96bwk
Jun  8 16:16:19.037: INFO: Got endpoints: latency-svc-h6948 [740.945063ms]
Jun  8 16:16:19.055: INFO: Created: latency-svc-dtb2t
Jun  8 16:16:19.087: INFO: Got endpoints: latency-svc-9v8vl [748.105644ms]
Jun  8 16:16:19.109: INFO: Created: latency-svc-6gzxc
Jun  8 16:16:19.138: INFO: Got endpoints: latency-svc-dp6dw [740.969205ms]
Jun  8 16:16:19.155: INFO: Created: latency-svc-vch7j
Jun  8 16:16:19.187: INFO: Got endpoints: latency-svc-ptg2f [748.662848ms]
Jun  8 16:16:19.205: INFO: Created: latency-svc-qkl9h
Jun  8 16:16:19.238: INFO: Got endpoints: latency-svc-44h5t [737.866615ms]
Jun  8 16:16:19.257: INFO: Created: latency-svc-6k6jw
Jun  8 16:16:19.290: INFO: Got endpoints: latency-svc-8qmmp [742.66301ms]
Jun  8 16:16:19.314: INFO: Created: latency-svc-v7kzg
Jun  8 16:16:19.340: INFO: Got endpoints: latency-svc-v8frw [748.957357ms]
Jun  8 16:16:19.360: INFO: Created: latency-svc-9xsfp
Jun  8 16:16:19.389: INFO: Got endpoints: latency-svc-nmvnh [748.640193ms]
Jun  8 16:16:19.410: INFO: Created: latency-svc-472nl
Jun  8 16:16:19.445: INFO: Got endpoints: latency-svc-fcffr [751.588375ms]
Jun  8 16:16:19.469: INFO: Created: latency-svc-8rxch
Jun  8 16:16:19.488: INFO: Got endpoints: latency-svc-5sgdw [749.654592ms]
Jun  8 16:16:19.511: INFO: Created: latency-svc-qzqjk
Jun  8 16:16:19.540: INFO: Got endpoints: latency-svc-4ccm9 [749.859485ms]
Jun  8 16:16:19.573: INFO: Created: latency-svc-gmqkn
Jun  8 16:16:19.590: INFO: Got endpoints: latency-svc-nxqtx [752.55757ms]
Jun  8 16:16:19.612: INFO: Created: latency-svc-5f25p
Jun  8 16:16:19.639: INFO: Got endpoints: latency-svc-86wpx [750.087704ms]
Jun  8 16:16:19.667: INFO: Created: latency-svc-jlvbn
Jun  8 16:16:19.689: INFO: Got endpoints: latency-svc-pm4wb [749.734193ms]
Jun  8 16:16:19.709: INFO: Created: latency-svc-qnxvx
Jun  8 16:16:19.739: INFO: Got endpoints: latency-svc-96bwk [750.242754ms]
Jun  8 16:16:19.761: INFO: Created: latency-svc-xnj76
Jun  8 16:16:19.788: INFO: Got endpoints: latency-svc-dtb2t [750.784174ms]
Jun  8 16:16:19.805: INFO: Created: latency-svc-jnthh
Jun  8 16:16:19.838: INFO: Got endpoints: latency-svc-6gzxc [750.1209ms]
Jun  8 16:16:19.857: INFO: Created: latency-svc-vb5lr
Jun  8 16:16:19.889: INFO: Got endpoints: latency-svc-vch7j [750.755771ms]
Jun  8 16:16:19.909: INFO: Created: latency-svc-9hqfx
Jun  8 16:16:19.937: INFO: Got endpoints: latency-svc-qkl9h [749.163788ms]
Jun  8 16:16:19.956: INFO: Created: latency-svc-grd2b
Jun  8 16:16:19.987: INFO: Got endpoints: latency-svc-6k6jw [749.047323ms]
Jun  8 16:16:20.006: INFO: Created: latency-svc-mfvtv
Jun  8 16:16:20.040: INFO: Got endpoints: latency-svc-v7kzg [749.678826ms]
Jun  8 16:16:20.056: INFO: Created: latency-svc-b5lx8
Jun  8 16:16:20.088: INFO: Got endpoints: latency-svc-9xsfp [748.433732ms]
Jun  8 16:16:20.105: INFO: Created: latency-svc-xlzqd
Jun  8 16:16:20.139: INFO: Got endpoints: latency-svc-472nl [749.204894ms]
Jun  8 16:16:20.173: INFO: Created: latency-svc-hm8qm
Jun  8 16:16:20.199: INFO: Got endpoints: latency-svc-8rxch [754.143543ms]
Jun  8 16:16:20.229: INFO: Created: latency-svc-s2v75
Jun  8 16:16:20.241: INFO: Got endpoints: latency-svc-qzqjk [752.964243ms]
Jun  8 16:16:20.271: INFO: Created: latency-svc-l4n2c
Jun  8 16:16:20.289: INFO: Got endpoints: latency-svc-gmqkn [748.694472ms]
Jun  8 16:16:20.312: INFO: Created: latency-svc-6jdrc
Jun  8 16:16:20.339: INFO: Got endpoints: latency-svc-5f25p [749.315432ms]
Jun  8 16:16:20.359: INFO: Created: latency-svc-hwq9x
Jun  8 16:16:20.389: INFO: Got endpoints: latency-svc-jlvbn [749.628883ms]
Jun  8 16:16:20.411: INFO: Created: latency-svc-nb62w
Jun  8 16:16:20.440: INFO: Got endpoints: latency-svc-qnxvx [750.96849ms]
Jun  8 16:16:20.464: INFO: Created: latency-svc-gthr5
Jun  8 16:16:20.492: INFO: Got endpoints: latency-svc-xnj76 [752.46585ms]
Jun  8 16:16:20.516: INFO: Created: latency-svc-sgjjq
Jun  8 16:16:20.542: INFO: Got endpoints: latency-svc-jnthh [753.7461ms]
Jun  8 16:16:20.564: INFO: Created: latency-svc-xdltj
Jun  8 16:16:20.591: INFO: Got endpoints: latency-svc-vb5lr [753.2583ms]
Jun  8 16:16:20.611: INFO: Created: latency-svc-mxqw6
Jun  8 16:16:20.640: INFO: Got endpoints: latency-svc-9hqfx [751.493956ms]
Jun  8 16:16:20.665: INFO: Created: latency-svc-nfnvn
Jun  8 16:16:20.689: INFO: Got endpoints: latency-svc-grd2b [752.320752ms]
Jun  8 16:16:20.712: INFO: Created: latency-svc-45qxk
Jun  8 16:16:20.741: INFO: Got endpoints: latency-svc-mfvtv [753.56008ms]
Jun  8 16:16:20.764: INFO: Created: latency-svc-tq4vb
Jun  8 16:16:20.790: INFO: Got endpoints: latency-svc-b5lx8 [749.915742ms]
Jun  8 16:16:20.809: INFO: Created: latency-svc-khhns
Jun  8 16:16:20.838: INFO: Got endpoints: latency-svc-xlzqd [750.186865ms]
Jun  8 16:16:20.864: INFO: Created: latency-svc-jqjql
Jun  8 16:16:20.889: INFO: Got endpoints: latency-svc-hm8qm [750.003317ms]
Jun  8 16:16:20.905: INFO: Created: latency-svc-b5zxl
Jun  8 16:16:20.938: INFO: Got endpoints: latency-svc-s2v75 [739.39638ms]
Jun  8 16:16:20.955: INFO: Created: latency-svc-rfq2f
Jun  8 16:16:20.988: INFO: Got endpoints: latency-svc-l4n2c [746.746235ms]
Jun  8 16:16:21.005: INFO: Created: latency-svc-zmnzg
Jun  8 16:16:21.039: INFO: Got endpoints: latency-svc-6jdrc [749.819595ms]
Jun  8 16:16:21.057: INFO: Created: latency-svc-78gt6
Jun  8 16:16:21.088: INFO: Got endpoints: latency-svc-hwq9x [748.140733ms]
Jun  8 16:16:21.104: INFO: Created: latency-svc-wbhhg
Jun  8 16:16:21.139: INFO: Got endpoints: latency-svc-nb62w [750.172282ms]
Jun  8 16:16:21.160: INFO: Created: latency-svc-t84gg
Jun  8 16:16:21.188: INFO: Got endpoints: latency-svc-gthr5 [748.381313ms]
Jun  8 16:16:21.205: INFO: Created: latency-svc-m62kb
Jun  8 16:16:21.238: INFO: Got endpoints: latency-svc-sgjjq [745.507827ms]
Jun  8 16:16:21.260: INFO: Created: latency-svc-t4g28
Jun  8 16:16:21.289: INFO: Got endpoints: latency-svc-xdltj [747.663667ms]
Jun  8 16:16:21.312: INFO: Created: latency-svc-l7bsr
Jun  8 16:16:21.339: INFO: Got endpoints: latency-svc-mxqw6 [747.607542ms]
Jun  8 16:16:21.366: INFO: Created: latency-svc-x7nq2
Jun  8 16:16:21.389: INFO: Got endpoints: latency-svc-nfnvn [748.939586ms]
Jun  8 16:16:21.412: INFO: Created: latency-svc-gk5jc
Jun  8 16:16:21.440: INFO: Got endpoints: latency-svc-45qxk [750.813344ms]
Jun  8 16:16:21.466: INFO: Created: latency-svc-f4pl2
Jun  8 16:16:21.492: INFO: Got endpoints: latency-svc-tq4vb [751.286685ms]
Jun  8 16:16:21.513: INFO: Created: latency-svc-gkglt
Jun  8 16:16:21.540: INFO: Got endpoints: latency-svc-khhns [750.806511ms]
Jun  8 16:16:21.561: INFO: Created: latency-svc-qvrdm
Jun  8 16:16:21.588: INFO: Got endpoints: latency-svc-jqjql [749.890223ms]
Jun  8 16:16:21.610: INFO: Created: latency-svc-bskks
Jun  8 16:16:21.643: INFO: Got endpoints: latency-svc-b5zxl [753.781612ms]
Jun  8 16:16:21.667: INFO: Created: latency-svc-cdcjr
Jun  8 16:16:21.690: INFO: Got endpoints: latency-svc-rfq2f [751.595607ms]
Jun  8 16:16:21.720: INFO: Created: latency-svc-q54th
Jun  8 16:16:21.740: INFO: Got endpoints: latency-svc-zmnzg [751.869643ms]
Jun  8 16:16:21.761: INFO: Created: latency-svc-rqbng
Jun  8 16:16:21.789: INFO: Got endpoints: latency-svc-78gt6 [749.487378ms]
Jun  8 16:16:21.808: INFO: Created: latency-svc-hzbbl
Jun  8 16:16:21.839: INFO: Got endpoints: latency-svc-wbhhg [751.22514ms]
Jun  8 16:16:21.860: INFO: Created: latency-svc-bsjwl
Jun  8 16:16:21.887: INFO: Got endpoints: latency-svc-t84gg [748.584337ms]
Jun  8 16:16:21.904: INFO: Created: latency-svc-kmbtg
Jun  8 16:16:21.938: INFO: Got endpoints: latency-svc-m62kb [750.039925ms]
Jun  8 16:16:21.954: INFO: Created: latency-svc-gpl5k
Jun  8 16:16:21.990: INFO: Got endpoints: latency-svc-t4g28 [752.490606ms]
Jun  8 16:16:22.006: INFO: Created: latency-svc-bwktx
Jun  8 16:16:22.038: INFO: Got endpoints: latency-svc-l7bsr [748.033561ms]
Jun  8 16:16:22.054: INFO: Created: latency-svc-wl9d2
Jun  8 16:16:22.088: INFO: Got endpoints: latency-svc-x7nq2 [749.034143ms]
Jun  8 16:16:22.138: INFO: Got endpoints: latency-svc-gk5jc [748.484507ms]
Jun  8 16:16:22.188: INFO: Got endpoints: latency-svc-f4pl2 [747.756843ms]
Jun  8 16:16:22.238: INFO: Got endpoints: latency-svc-gkglt [746.124741ms]
Jun  8 16:16:22.289: INFO: Got endpoints: latency-svc-qvrdm [748.19933ms]
Jun  8 16:16:22.341: INFO: Got endpoints: latency-svc-bskks [752.893197ms]
Jun  8 16:16:22.389: INFO: Got endpoints: latency-svc-cdcjr [746.339448ms]
Jun  8 16:16:22.440: INFO: Got endpoints: latency-svc-q54th [749.996426ms]
Jun  8 16:16:22.488: INFO: Got endpoints: latency-svc-rqbng [748.112436ms]
Jun  8 16:16:22.541: INFO: Got endpoints: latency-svc-hzbbl [752.468049ms]
Jun  8 16:16:22.589: INFO: Got endpoints: latency-svc-bsjwl [750.434047ms]
Jun  8 16:16:22.639: INFO: Got endpoints: latency-svc-kmbtg [751.893636ms]
Jun  8 16:16:22.689: INFO: Got endpoints: latency-svc-gpl5k [751.076685ms]
Jun  8 16:16:22.739: INFO: Got endpoints: latency-svc-bwktx [748.861633ms]
Jun  8 16:16:22.788: INFO: Got endpoints: latency-svc-wl9d2 [750.016673ms]
Jun  8 16:16:22.788: INFO: Latencies: [33.245105ms 50.456107ms 71.437824ms 99.511255ms 103.636721ms 128.034102ms 143.955405ms 176.172997ms 184.388082ms 206.017263ms 244.724347ms 251.77213ms 262.396295ms 293.204713ms 293.642065ms 300.944073ms 304.979653ms 306.715633ms 307.367939ms 307.988168ms 308.585465ms 309.381486ms 310.156638ms 310.924355ms 312.543563ms 312.854965ms 313.031393ms 313.597093ms 313.989574ms 314.568124ms 317.831331ms 318.075604ms 318.740775ms 322.061172ms 322.710261ms 329.082292ms 330.111708ms 332.848305ms 338.56728ms 343.35969ms 346.981209ms 347.399658ms 347.442787ms 350.034666ms 350.197813ms 351.342741ms 353.792109ms 353.856674ms 357.198583ms 359.580515ms 364.412943ms 365.852786ms 373.606679ms 385.351369ms 417.748955ms 450.048352ms 494.460043ms 501.745038ms 539.066522ms 567.808308ms 605.145027ms 631.369512ms 665.90882ms 690.598544ms 718.486566ms 737.866615ms 739.39638ms 739.776159ms 740.945063ms 740.969205ms 742.66301ms 744.871979ms 745.450928ms 745.507827ms 745.970636ms 746.124741ms 746.339448ms 746.600473ms 746.746235ms 747.143044ms 747.268531ms 747.334789ms 747.395796ms 747.532994ms 747.557503ms 747.560109ms 747.607542ms 747.663667ms 747.756843ms 747.806943ms 747.87029ms 747.962882ms 748.033561ms 748.105644ms 748.112436ms 748.140733ms 748.19933ms 748.381313ms 748.433732ms 748.484507ms 748.584337ms 748.640193ms 748.662848ms 748.694472ms 748.861633ms 748.939586ms 748.957357ms 749.008047ms 749.034143ms 749.047323ms 749.104821ms 749.163788ms 749.204894ms 749.21197ms 749.213566ms 749.266872ms 749.315432ms 749.485636ms 749.487378ms 749.628883ms 749.654592ms 749.678826ms 749.734193ms 749.812997ms 749.819595ms 749.859338ms 749.859485ms 749.890223ms 749.915742ms 749.996426ms 749.997667ms 750.003317ms 750.01316ms 750.016673ms 750.039925ms 750.087704ms 750.089624ms 750.1209ms 750.172282ms 750.186865ms 750.220155ms 750.242754ms 750.351033ms 750.405569ms 750.420919ms 750.434047ms 750.735725ms 750.755771ms 750.784174ms 750.806511ms 750.813344ms 750.96849ms 750.997003ms 751.018649ms 751.022413ms 751.049669ms 751.066581ms 751.076685ms 751.121939ms 751.22514ms 751.226085ms 751.286685ms 751.299979ms 751.413371ms 751.478253ms 751.493956ms 751.506989ms 751.582235ms 751.587601ms 751.588375ms 751.595607ms 751.630682ms 751.726208ms 751.869643ms 751.893636ms 751.959682ms 752.224383ms 752.320752ms 752.46585ms 752.468049ms 752.490606ms 752.55757ms 752.633684ms 752.893197ms 752.964243ms 752.986799ms 753.109205ms 753.2583ms 753.436414ms 753.440897ms 753.452739ms 753.56008ms 753.586604ms 753.7461ms 753.781612ms 754.143543ms 754.395635ms 755.591285ms 756.080127ms 761.834866ms]
Jun  8 16:16:22.788: INFO: 50 %ile: 748.584337ms
Jun  8 16:16:22.788: INFO: 90 %ile: 752.490606ms
Jun  8 16:16:22.788: INFO: 99 %ile: 756.080127ms
Jun  8 16:16:22.788: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Jun  8 16:16:22.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-6605" for this suite. 06/08/23 16:16:22.796
------------------------------
• [SLOW TEST] [9.774 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:16:13.03
    Jun  8 16:16:13.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename svc-latency 06/08/23 16:16:13.031
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:16:13.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:16:13.052
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jun  8 16:16:13.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-6605 06/08/23 16:16:13.056
    I0608 16:16:13.062692      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6605, replica count: 1
    I0608 16:16:14.114140      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun  8 16:16:14.232: INFO: Created: latency-svc-7p2r4
    Jun  8 16:16:14.239: INFO: Got endpoints: latency-svc-7p2r4 [24.783707ms]
    Jun  8 16:16:14.261: INFO: Created: latency-svc-m856m
    Jun  8 16:16:14.272: INFO: Got endpoints: latency-svc-m856m [33.245105ms]
    Jun  8 16:16:14.280: INFO: Created: latency-svc-tpc7d
    Jun  8 16:16:14.290: INFO: Got endpoints: latency-svc-tpc7d [50.456107ms]
    Jun  8 16:16:14.297: INFO: Created: latency-svc-vztrk
    Jun  8 16:16:14.310: INFO: Got endpoints: latency-svc-vztrk [71.437824ms]
    Jun  8 16:16:14.319: INFO: Created: latency-svc-mc2ph
    Jun  8 16:16:14.339: INFO: Got endpoints: latency-svc-mc2ph [99.511255ms]
    Jun  8 16:16:14.340: INFO: Created: latency-svc-8ghw6
    Jun  8 16:16:14.343: INFO: Got endpoints: latency-svc-8ghw6 [103.636721ms]
    Jun  8 16:16:14.357: INFO: Created: latency-svc-4s7b5
    Jun  8 16:16:14.368: INFO: Got endpoints: latency-svc-4s7b5 [128.034102ms]
    Jun  8 16:16:14.376: INFO: Created: latency-svc-nj8kr
    Jun  8 16:16:14.384: INFO: Got endpoints: latency-svc-nj8kr [143.955405ms]
    Jun  8 16:16:14.395: INFO: Created: latency-svc-jqhhp
    Jun  8 16:16:14.407: INFO: Created: latency-svc-jwnvr
    Jun  8 16:16:14.415: INFO: Got endpoints: latency-svc-jqhhp [176.172997ms]
    Jun  8 16:16:14.424: INFO: Got endpoints: latency-svc-jwnvr [184.388082ms]
    Jun  8 16:16:14.433: INFO: Created: latency-svc-mx6tl
    Jun  8 16:16:14.446: INFO: Got endpoints: latency-svc-mx6tl [206.017263ms]
    Jun  8 16:16:14.455: INFO: Created: latency-svc-87mn7
    Jun  8 16:16:14.479: INFO: Created: latency-svc-5nlkg
    Jun  8 16:16:14.485: INFO: Got endpoints: latency-svc-87mn7 [244.724347ms]
    Jun  8 16:16:14.492: INFO: Got endpoints: latency-svc-5nlkg [251.77213ms]
    Jun  8 16:16:14.493: INFO: Created: latency-svc-svqv7
    Jun  8 16:16:14.502: INFO: Got endpoints: latency-svc-svqv7 [262.396295ms]
    Jun  8 16:16:14.518: INFO: Created: latency-svc-2b9tl
    Jun  8 16:16:14.534: INFO: Got endpoints: latency-svc-2b9tl [293.204713ms]
    Jun  8 16:16:14.552: INFO: Created: latency-svc-cqkgr
    Jun  8 16:16:14.559: INFO: Got endpoints: latency-svc-cqkgr [318.740775ms]
    Jun  8 16:16:14.570: INFO: Created: latency-svc-77hmv
    Jun  8 16:16:14.583: INFO: Got endpoints: latency-svc-77hmv [310.924355ms]
    Jun  8 16:16:14.590: INFO: Created: latency-svc-9lcnc
    Jun  8 16:16:14.599: INFO: Got endpoints: latency-svc-9lcnc [309.381486ms]
    Jun  8 16:16:14.604: INFO: Created: latency-svc-rjgp7
    Jun  8 16:16:14.611: INFO: Got endpoints: latency-svc-rjgp7 [300.944073ms]
    Jun  8 16:16:14.622: INFO: Created: latency-svc-9w2gn
    Jun  8 16:16:14.633: INFO: Got endpoints: latency-svc-9w2gn [293.642065ms]
    Jun  8 16:16:14.642: INFO: Created: latency-svc-5slnh
    Jun  8 16:16:14.650: INFO: Got endpoints: latency-svc-5slnh [306.715633ms]
    Jun  8 16:16:14.668: INFO: Created: latency-svc-bhlbl
    Jun  8 16:16:14.675: INFO: Got endpoints: latency-svc-bhlbl [307.367939ms]
    Jun  8 16:16:14.687: INFO: Created: latency-svc-tlsfh
    Jun  8 16:16:14.694: INFO: Got endpoints: latency-svc-tlsfh [310.156638ms]
    Jun  8 16:16:14.709: INFO: Created: latency-svc-gj697
    Jun  8 16:16:14.723: INFO: Got endpoints: latency-svc-gj697 [307.988168ms]
    Jun  8 16:16:14.744: INFO: Created: latency-svc-qxbz6
    Jun  8 16:16:14.754: INFO: Got endpoints: latency-svc-qxbz6 [329.082292ms]
    Jun  8 16:16:14.799: INFO: Created: latency-svc-45972
    Jun  8 16:16:14.806: INFO: Got endpoints: latency-svc-45972 [359.580515ms]
    Jun  8 16:16:14.812: INFO: Created: latency-svc-qw6v9
    Jun  8 16:16:14.818: INFO: Got endpoints: latency-svc-qw6v9 [332.848305ms]
    Jun  8 16:16:14.840: INFO: Created: latency-svc-x45xq
    Jun  8 16:16:14.849: INFO: Got endpoints: latency-svc-x45xq [357.198583ms]
    Jun  8 16:16:14.857: INFO: Created: latency-svc-2whgw
    Jun  8 16:16:14.874: INFO: Created: latency-svc-dfmsk
    Jun  8 16:16:14.876: INFO: Got endpoints: latency-svc-2whgw [373.606679ms]
    Jun  8 16:16:14.887: INFO: Got endpoints: latency-svc-dfmsk [353.792109ms]
    Jun  8 16:16:14.892: INFO: Created: latency-svc-dwnsj
    Jun  8 16:16:14.906: INFO: Got endpoints: latency-svc-dwnsj [347.399658ms]
    Jun  8 16:16:14.916: INFO: Created: latency-svc-sv4zc
    Jun  8 16:16:14.922: INFO: Got endpoints: latency-svc-sv4zc [338.56728ms]
    Jun  8 16:16:14.932: INFO: Created: latency-svc-khsq6
    Jun  8 16:16:14.943: INFO: Got endpoints: latency-svc-khsq6 [343.35969ms]
    Jun  8 16:16:14.948: INFO: Created: latency-svc-fdmzd
    Jun  8 16:16:14.965: INFO: Got endpoints: latency-svc-fdmzd [353.856674ms]
    Jun  8 16:16:14.968: INFO: Created: latency-svc-2svs8
    Jun  8 16:16:14.980: INFO: Got endpoints: latency-svc-2svs8 [347.442787ms]
    Jun  8 16:16:14.995: INFO: Created: latency-svc-jgnbd
    Jun  8 16:16:14.997: INFO: Got endpoints: latency-svc-jgnbd [346.981209ms]
    Jun  8 16:16:15.012: INFO: Created: latency-svc-xsshj
    Jun  8 16:16:15.025: INFO: Got endpoints: latency-svc-xsshj [350.034666ms]
    Jun  8 16:16:15.038: INFO: Created: latency-svc-bl9tw
    Jun  8 16:16:15.059: INFO: Got endpoints: latency-svc-bl9tw [364.412943ms]
    Jun  8 16:16:15.067: INFO: Created: latency-svc-47879
    Jun  8 16:16:15.090: INFO: Got endpoints: latency-svc-47879 [365.852786ms]
    Jun  8 16:16:15.092: INFO: Created: latency-svc-d7vhg
    Jun  8 16:16:15.104: INFO: Got endpoints: latency-svc-d7vhg [350.197813ms]
    Jun  8 16:16:15.110: INFO: Created: latency-svc-m8xc6
    Jun  8 16:16:15.124: INFO: Got endpoints: latency-svc-m8xc6 [317.831331ms]
    Jun  8 16:16:15.131: INFO: Created: latency-svc-6xnh7
    Jun  8 16:16:15.140: INFO: Got endpoints: latency-svc-6xnh7 [322.061172ms]
    Jun  8 16:16:15.146: INFO: Created: latency-svc-rc468
    Jun  8 16:16:15.154: INFO: Got endpoints: latency-svc-rc468 [304.979653ms]
    Jun  8 16:16:15.170: INFO: Created: latency-svc-7lsz8
    Jun  8 16:16:15.188: INFO: Created: latency-svc-x2ws8
    Jun  8 16:16:15.189: INFO: Got endpoints: latency-svc-7lsz8 [312.854965ms]
    Jun  8 16:16:15.201: INFO: Got endpoints: latency-svc-x2ws8 [313.989574ms]
    Jun  8 16:16:15.212: INFO: Created: latency-svc-bj7j6
    Jun  8 16:16:15.221: INFO: Got endpoints: latency-svc-bj7j6 [314.568124ms]
    Jun  8 16:16:15.228: INFO: Created: latency-svc-wfgvk
    Jun  8 16:16:15.235: INFO: Got endpoints: latency-svc-wfgvk [313.031393ms]
    Jun  8 16:16:15.247: INFO: Created: latency-svc-g77z7
    Jun  8 16:16:15.256: INFO: Got endpoints: latency-svc-g77z7 [313.597093ms]
    Jun  8 16:16:15.265: INFO: Created: latency-svc-8lsbs
    Jun  8 16:16:15.274: INFO: Got endpoints: latency-svc-8lsbs [308.585465ms]
    Jun  8 16:16:15.287: INFO: Created: latency-svc-25mbh
    Jun  8 16:16:15.298: INFO: Got endpoints: latency-svc-25mbh [318.075604ms]
    Jun  8 16:16:15.303: INFO: Created: latency-svc-zgd7r
    Jun  8 16:16:15.320: INFO: Got endpoints: latency-svc-zgd7r [322.710261ms]
    Jun  8 16:16:15.326: INFO: Created: latency-svc-bff57
    Jun  8 16:16:15.338: INFO: Got endpoints: latency-svc-bff57 [312.543563ms]
    Jun  8 16:16:15.343: INFO: Created: latency-svc-jvgmf
    Jun  8 16:16:15.357: INFO: Created: latency-svc-n6r9c
    Jun  8 16:16:15.372: INFO: Created: latency-svc-fw2s8
    Jun  8 16:16:15.389: INFO: Got endpoints: latency-svc-jvgmf [330.111708ms]
    Jun  8 16:16:15.389: INFO: Created: latency-svc-nvq7h
    Jun  8 16:16:15.408: INFO: Created: latency-svc-krw4x
    Jun  8 16:16:15.433: INFO: Created: latency-svc-84gcm
    Jun  8 16:16:15.441: INFO: Got endpoints: latency-svc-n6r9c [351.342741ms]
    Jun  8 16:16:15.452: INFO: Created: latency-svc-74qzk
    Jun  8 16:16:15.471: INFO: Created: latency-svc-slvnh
    Jun  8 16:16:15.489: INFO: Got endpoints: latency-svc-fw2s8 [385.351369ms]
    Jun  8 16:16:15.493: INFO: Created: latency-svc-q4nsm
    Jun  8 16:16:15.523: INFO: Created: latency-svc-9kzcm
    Jun  8 16:16:15.541: INFO: Got endpoints: latency-svc-nvq7h [417.748955ms]
    Jun  8 16:16:15.550: INFO: Created: latency-svc-snjsz
    Jun  8 16:16:15.576: INFO: Created: latency-svc-wcwk4
    Jun  8 16:16:15.590: INFO: Got endpoints: latency-svc-krw4x [450.048352ms]
    Jun  8 16:16:15.596: INFO: Created: latency-svc-njzzl
    Jun  8 16:16:15.614: INFO: Created: latency-svc-r6d2d
    Jun  8 16:16:15.637: INFO: Created: latency-svc-g6k56
    Jun  8 16:16:15.649: INFO: Got endpoints: latency-svc-84gcm [494.460043ms]
    Jun  8 16:16:15.656: INFO: Created: latency-svc-8dzj4
    Jun  8 16:16:15.672: INFO: Created: latency-svc-555kq
    Jun  8 16:16:15.691: INFO: Got endpoints: latency-svc-74qzk [501.745038ms]
    Jun  8 16:16:15.691: INFO: Created: latency-svc-p258h
    Jun  8 16:16:15.708: INFO: Created: latency-svc-vvcrz
    Jun  8 16:16:15.727: INFO: Created: latency-svc-ts65b
    Jun  8 16:16:15.740: INFO: Got endpoints: latency-svc-slvnh [539.066522ms]
    Jun  8 16:16:15.747: INFO: Created: latency-svc-9nk4m
    Jun  8 16:16:15.765: INFO: Created: latency-svc-xqb6p
    Jun  8 16:16:15.782: INFO: Created: latency-svc-h85rk
    Jun  8 16:16:15.789: INFO: Got endpoints: latency-svc-q4nsm [567.808308ms]
    Jun  8 16:16:15.815: INFO: Created: latency-svc-mwdss
    Jun  8 16:16:15.840: INFO: Got endpoints: latency-svc-9kzcm [605.145027ms]
    Jun  8 16:16:15.868: INFO: Created: latency-svc-gfmnq
    Jun  8 16:16:15.888: INFO: Got endpoints: latency-svc-snjsz [631.369512ms]
    Jun  8 16:16:15.913: INFO: Created: latency-svc-7bnkh
    Jun  8 16:16:15.940: INFO: Got endpoints: latency-svc-wcwk4 [665.90882ms]
    Jun  8 16:16:15.960: INFO: Created: latency-svc-67h2d
    Jun  8 16:16:15.989: INFO: Got endpoints: latency-svc-njzzl [690.598544ms]
    Jun  8 16:16:16.012: INFO: Created: latency-svc-6vxp8
    Jun  8 16:16:16.038: INFO: Got endpoints: latency-svc-r6d2d [718.486566ms]
    Jun  8 16:16:16.060: INFO: Created: latency-svc-kx226
    Jun  8 16:16:16.088: INFO: Got endpoints: latency-svc-g6k56 [749.997667ms]
    Jun  8 16:16:16.107: INFO: Created: latency-svc-vwx6b
    Jun  8 16:16:16.139: INFO: Got endpoints: latency-svc-8dzj4 [750.351033ms]
    Jun  8 16:16:16.158: INFO: Created: latency-svc-tz58s
    Jun  8 16:16:16.188: INFO: Got endpoints: latency-svc-555kq [747.143044ms]
    Jun  8 16:16:16.206: INFO: Created: latency-svc-kpwc6
    Jun  8 16:16:16.237: INFO: Got endpoints: latency-svc-p258h [747.806943ms]
    Jun  8 16:16:16.257: INFO: Created: latency-svc-4bwp2
    Jun  8 16:16:16.293: INFO: Got endpoints: latency-svc-vvcrz [751.630682ms]
    Jun  8 16:16:16.317: INFO: Created: latency-svc-pnv5n
    Jun  8 16:16:16.338: INFO: Got endpoints: latency-svc-ts65b [747.268531ms]
    Jun  8 16:16:16.360: INFO: Created: latency-svc-cjrk4
    Jun  8 16:16:16.389: INFO: Got endpoints: latency-svc-9nk4m [739.776159ms]
    Jun  8 16:16:16.410: INFO: Created: latency-svc-7n7d8
    Jun  8 16:16:16.438: INFO: Got endpoints: latency-svc-xqb6p [747.395796ms]
    Jun  8 16:16:16.458: INFO: Created: latency-svc-95sgl
    Jun  8 16:16:16.490: INFO: Got endpoints: latency-svc-h85rk [749.104821ms]
    Jun  8 16:16:16.527: INFO: Created: latency-svc-shm7j
    Jun  8 16:16:16.540: INFO: Got endpoints: latency-svc-mwdss [751.582235ms]
    Jun  8 16:16:16.566: INFO: Created: latency-svc-sp9gp
    Jun  8 16:16:16.592: INFO: Got endpoints: latency-svc-gfmnq [751.959682ms]
    Jun  8 16:16:16.614: INFO: Created: latency-svc-njkcj
    Jun  8 16:16:16.638: INFO: Got endpoints: latency-svc-7bnkh [750.220155ms]
    Jun  8 16:16:16.657: INFO: Created: latency-svc-nz7rb
    Jun  8 16:16:16.691: INFO: Got endpoints: latency-svc-67h2d [751.121939ms]
    Jun  8 16:16:16.714: INFO: Created: latency-svc-748bp
    Jun  8 16:16:16.739: INFO: Got endpoints: latency-svc-6vxp8 [749.859338ms]
    Jun  8 16:16:16.757: INFO: Created: latency-svc-n476g
    Jun  8 16:16:16.788: INFO: Got endpoints: latency-svc-kx226 [749.812997ms]
    Jun  8 16:16:16.822: INFO: Created: latency-svc-9f99z
    Jun  8 16:16:16.841: INFO: Got endpoints: latency-svc-vwx6b [752.633684ms]
    Jun  8 16:16:16.863: INFO: Created: latency-svc-sw2pq
    Jun  8 16:16:16.890: INFO: Got endpoints: latency-svc-tz58s [751.049669ms]
    Jun  8 16:16:16.907: INFO: Created: latency-svc-rkhsf
    Jun  8 16:16:16.936: INFO: Got endpoints: latency-svc-kpwc6 [747.962882ms]
    Jun  8 16:16:16.952: INFO: Created: latency-svc-hlcmg
    Jun  8 16:16:16.988: INFO: Got endpoints: latency-svc-4bwp2 [750.420919ms]
    Jun  8 16:16:17.004: INFO: Created: latency-svc-pbgt2
    Jun  8 16:16:17.038: INFO: Got endpoints: latency-svc-pnv5n [744.871979ms]
    Jun  8 16:16:17.054: INFO: Created: latency-svc-4m5vx
    Jun  8 16:16:17.087: INFO: Got endpoints: latency-svc-cjrk4 [749.485636ms]
    Jun  8 16:16:17.103: INFO: Created: latency-svc-27jmk
    Jun  8 16:16:17.137: INFO: Got endpoints: latency-svc-7n7d8 [747.87029ms]
    Jun  8 16:16:17.153: INFO: Created: latency-svc-b5brs
    Jun  8 16:16:17.188: INFO: Got endpoints: latency-svc-95sgl [750.089624ms]
    Jun  8 16:16:17.207: INFO: Created: latency-svc-jtlfl
    Jun  8 16:16:17.241: INFO: Got endpoints: latency-svc-shm7j [751.299979ms]
    Jun  8 16:16:17.259: INFO: Created: latency-svc-4hrn4
    Jun  8 16:16:17.290: INFO: Got endpoints: latency-svc-sp9gp [749.21197ms]
    Jun  8 16:16:17.314: INFO: Created: latency-svc-fzz4f
    Jun  8 16:16:17.338: INFO: Got endpoints: latency-svc-njkcj [745.450928ms]
    Jun  8 16:16:17.362: INFO: Created: latency-svc-k46rq
    Jun  8 16:16:17.390: INFO: Got endpoints: latency-svc-nz7rb [751.478253ms]
    Jun  8 16:16:17.412: INFO: Created: latency-svc-fqnhx
    Jun  8 16:16:17.440: INFO: Got endpoints: latency-svc-748bp [749.213566ms]
    Jun  8 16:16:17.467: INFO: Created: latency-svc-zwgwd
    Jun  8 16:16:17.490: INFO: Got endpoints: latency-svc-n476g [751.587601ms]
    Jun  8 16:16:17.509: INFO: Created: latency-svc-g8c6w
    Jun  8 16:16:17.543: INFO: Got endpoints: latency-svc-9f99z [754.395635ms]
    Jun  8 16:16:17.567: INFO: Created: latency-svc-hkz27
    Jun  8 16:16:17.588: INFO: Got endpoints: latency-svc-sw2pq [747.532994ms]
    Jun  8 16:16:17.611: INFO: Created: latency-svc-k27mz
    Jun  8 16:16:17.641: INFO: Got endpoints: latency-svc-rkhsf [751.018649ms]
    Jun  8 16:16:17.665: INFO: Created: latency-svc-mllmc
    Jun  8 16:16:17.690: INFO: Got endpoints: latency-svc-hlcmg [753.586604ms]
    Jun  8 16:16:17.709: INFO: Created: latency-svc-4z27v
    Jun  8 16:16:17.738: INFO: Got endpoints: latency-svc-pbgt2 [750.735725ms]
    Jun  8 16:16:17.757: INFO: Created: latency-svc-jfxvs
    Jun  8 16:16:17.791: INFO: Got endpoints: latency-svc-4m5vx [752.986799ms]
    Jun  8 16:16:17.809: INFO: Created: latency-svc-7qf2w
    Jun  8 16:16:17.839: INFO: Got endpoints: latency-svc-27jmk [752.224383ms]
    Jun  8 16:16:17.857: INFO: Created: latency-svc-6h4kr
    Jun  8 16:16:17.887: INFO: Got endpoints: latency-svc-b5brs [750.405569ms]
    Jun  8 16:16:17.903: INFO: Created: latency-svc-ns9fl
    Jun  8 16:16:17.939: INFO: Got endpoints: latency-svc-jtlfl [751.066581ms]
    Jun  8 16:16:17.955: INFO: Created: latency-svc-h5hb5
    Jun  8 16:16:17.987: INFO: Got endpoints: latency-svc-4hrn4 [745.970636ms]
    Jun  8 16:16:18.002: INFO: Created: latency-svc-jpcfg
    Jun  8 16:16:18.037: INFO: Got endpoints: latency-svc-fzz4f [747.334789ms]
    Jun  8 16:16:18.054: INFO: Created: latency-svc-gln8r
    Jun  8 16:16:18.087: INFO: Got endpoints: latency-svc-k46rq [749.266872ms]
    Jun  8 16:16:18.104: INFO: Created: latency-svc-xpzpn
    Jun  8 16:16:18.137: INFO: Got endpoints: latency-svc-fqnhx [747.557503ms]
    Jun  8 16:16:18.154: INFO: Created: latency-svc-xpdh9
    Jun  8 16:16:18.187: INFO: Got endpoints: latency-svc-zwgwd [746.600473ms]
    Jun  8 16:16:18.203: INFO: Created: latency-svc-9ff4q
    Jun  8 16:16:18.238: INFO: Got endpoints: latency-svc-g8c6w [747.560109ms]
    Jun  8 16:16:18.256: INFO: Created: latency-svc-8kwxb
    Jun  8 16:16:18.296: INFO: Got endpoints: latency-svc-hkz27 [753.436414ms]
    Jun  8 16:16:18.319: INFO: Created: latency-svc-h6948
    Jun  8 16:16:18.339: INFO: Got endpoints: latency-svc-k27mz [751.022413ms]
    Jun  8 16:16:18.358: INFO: Created: latency-svc-9v8vl
    Jun  8 16:16:18.397: INFO: Got endpoints: latency-svc-mllmc [755.591285ms]
    Jun  8 16:16:18.420: INFO: Created: latency-svc-dp6dw
    Jun  8 16:16:18.439: INFO: Got endpoints: latency-svc-4z27v [749.008047ms]
    Jun  8 16:16:18.459: INFO: Created: latency-svc-ptg2f
    Jun  8 16:16:18.500: INFO: Got endpoints: latency-svc-jfxvs [761.834866ms]
    Jun  8 16:16:18.523: INFO: Created: latency-svc-44h5t
    Jun  8 16:16:18.547: INFO: Got endpoints: latency-svc-7qf2w [756.080127ms]
    Jun  8 16:16:18.572: INFO: Created: latency-svc-8qmmp
    Jun  8 16:16:18.591: INFO: Got endpoints: latency-svc-6h4kr [751.226085ms]
    Jun  8 16:16:18.612: INFO: Created: latency-svc-v8frw
    Jun  8 16:16:18.641: INFO: Got endpoints: latency-svc-ns9fl [753.452739ms]
    Jun  8 16:16:18.659: INFO: Created: latency-svc-nmvnh
    Jun  8 16:16:18.693: INFO: Got endpoints: latency-svc-h5hb5 [753.440897ms]
    Jun  8 16:16:18.720: INFO: Created: latency-svc-fcffr
    Jun  8 16:16:18.738: INFO: Got endpoints: latency-svc-jpcfg [751.413371ms]
    Jun  8 16:16:18.758: INFO: Created: latency-svc-5sgdw
    Jun  8 16:16:18.791: INFO: Got endpoints: latency-svc-gln8r [753.109205ms]
    Jun  8 16:16:18.816: INFO: Created: latency-svc-4ccm9
    Jun  8 16:16:18.837: INFO: Got endpoints: latency-svc-xpzpn [750.01316ms]
    Jun  8 16:16:18.855: INFO: Created: latency-svc-nxqtx
    Jun  8 16:16:18.889: INFO: Got endpoints: latency-svc-xpdh9 [751.506989ms]
    Jun  8 16:16:18.911: INFO: Created: latency-svc-86wpx
    Jun  8 16:16:18.939: INFO: Got endpoints: latency-svc-9ff4q [751.726208ms]
    Jun  8 16:16:18.955: INFO: Created: latency-svc-pm4wb
    Jun  8 16:16:18.989: INFO: Got endpoints: latency-svc-8kwxb [750.997003ms]
    Jun  8 16:16:19.006: INFO: Created: latency-svc-96bwk
    Jun  8 16:16:19.037: INFO: Got endpoints: latency-svc-h6948 [740.945063ms]
    Jun  8 16:16:19.055: INFO: Created: latency-svc-dtb2t
    Jun  8 16:16:19.087: INFO: Got endpoints: latency-svc-9v8vl [748.105644ms]
    Jun  8 16:16:19.109: INFO: Created: latency-svc-6gzxc
    Jun  8 16:16:19.138: INFO: Got endpoints: latency-svc-dp6dw [740.969205ms]
    Jun  8 16:16:19.155: INFO: Created: latency-svc-vch7j
    Jun  8 16:16:19.187: INFO: Got endpoints: latency-svc-ptg2f [748.662848ms]
    Jun  8 16:16:19.205: INFO: Created: latency-svc-qkl9h
    Jun  8 16:16:19.238: INFO: Got endpoints: latency-svc-44h5t [737.866615ms]
    Jun  8 16:16:19.257: INFO: Created: latency-svc-6k6jw
    Jun  8 16:16:19.290: INFO: Got endpoints: latency-svc-8qmmp [742.66301ms]
    Jun  8 16:16:19.314: INFO: Created: latency-svc-v7kzg
    Jun  8 16:16:19.340: INFO: Got endpoints: latency-svc-v8frw [748.957357ms]
    Jun  8 16:16:19.360: INFO: Created: latency-svc-9xsfp
    Jun  8 16:16:19.389: INFO: Got endpoints: latency-svc-nmvnh [748.640193ms]
    Jun  8 16:16:19.410: INFO: Created: latency-svc-472nl
    Jun  8 16:16:19.445: INFO: Got endpoints: latency-svc-fcffr [751.588375ms]
    Jun  8 16:16:19.469: INFO: Created: latency-svc-8rxch
    Jun  8 16:16:19.488: INFO: Got endpoints: latency-svc-5sgdw [749.654592ms]
    Jun  8 16:16:19.511: INFO: Created: latency-svc-qzqjk
    Jun  8 16:16:19.540: INFO: Got endpoints: latency-svc-4ccm9 [749.859485ms]
    Jun  8 16:16:19.573: INFO: Created: latency-svc-gmqkn
    Jun  8 16:16:19.590: INFO: Got endpoints: latency-svc-nxqtx [752.55757ms]
    Jun  8 16:16:19.612: INFO: Created: latency-svc-5f25p
    Jun  8 16:16:19.639: INFO: Got endpoints: latency-svc-86wpx [750.087704ms]
    Jun  8 16:16:19.667: INFO: Created: latency-svc-jlvbn
    Jun  8 16:16:19.689: INFO: Got endpoints: latency-svc-pm4wb [749.734193ms]
    Jun  8 16:16:19.709: INFO: Created: latency-svc-qnxvx
    Jun  8 16:16:19.739: INFO: Got endpoints: latency-svc-96bwk [750.242754ms]
    Jun  8 16:16:19.761: INFO: Created: latency-svc-xnj76
    Jun  8 16:16:19.788: INFO: Got endpoints: latency-svc-dtb2t [750.784174ms]
    Jun  8 16:16:19.805: INFO: Created: latency-svc-jnthh
    Jun  8 16:16:19.838: INFO: Got endpoints: latency-svc-6gzxc [750.1209ms]
    Jun  8 16:16:19.857: INFO: Created: latency-svc-vb5lr
    Jun  8 16:16:19.889: INFO: Got endpoints: latency-svc-vch7j [750.755771ms]
    Jun  8 16:16:19.909: INFO: Created: latency-svc-9hqfx
    Jun  8 16:16:19.937: INFO: Got endpoints: latency-svc-qkl9h [749.163788ms]
    Jun  8 16:16:19.956: INFO: Created: latency-svc-grd2b
    Jun  8 16:16:19.987: INFO: Got endpoints: latency-svc-6k6jw [749.047323ms]
    Jun  8 16:16:20.006: INFO: Created: latency-svc-mfvtv
    Jun  8 16:16:20.040: INFO: Got endpoints: latency-svc-v7kzg [749.678826ms]
    Jun  8 16:16:20.056: INFO: Created: latency-svc-b5lx8
    Jun  8 16:16:20.088: INFO: Got endpoints: latency-svc-9xsfp [748.433732ms]
    Jun  8 16:16:20.105: INFO: Created: latency-svc-xlzqd
    Jun  8 16:16:20.139: INFO: Got endpoints: latency-svc-472nl [749.204894ms]
    Jun  8 16:16:20.173: INFO: Created: latency-svc-hm8qm
    Jun  8 16:16:20.199: INFO: Got endpoints: latency-svc-8rxch [754.143543ms]
    Jun  8 16:16:20.229: INFO: Created: latency-svc-s2v75
    Jun  8 16:16:20.241: INFO: Got endpoints: latency-svc-qzqjk [752.964243ms]
    Jun  8 16:16:20.271: INFO: Created: latency-svc-l4n2c
    Jun  8 16:16:20.289: INFO: Got endpoints: latency-svc-gmqkn [748.694472ms]
    Jun  8 16:16:20.312: INFO: Created: latency-svc-6jdrc
    Jun  8 16:16:20.339: INFO: Got endpoints: latency-svc-5f25p [749.315432ms]
    Jun  8 16:16:20.359: INFO: Created: latency-svc-hwq9x
    Jun  8 16:16:20.389: INFO: Got endpoints: latency-svc-jlvbn [749.628883ms]
    Jun  8 16:16:20.411: INFO: Created: latency-svc-nb62w
    Jun  8 16:16:20.440: INFO: Got endpoints: latency-svc-qnxvx [750.96849ms]
    Jun  8 16:16:20.464: INFO: Created: latency-svc-gthr5
    Jun  8 16:16:20.492: INFO: Got endpoints: latency-svc-xnj76 [752.46585ms]
    Jun  8 16:16:20.516: INFO: Created: latency-svc-sgjjq
    Jun  8 16:16:20.542: INFO: Got endpoints: latency-svc-jnthh [753.7461ms]
    Jun  8 16:16:20.564: INFO: Created: latency-svc-xdltj
    Jun  8 16:16:20.591: INFO: Got endpoints: latency-svc-vb5lr [753.2583ms]
    Jun  8 16:16:20.611: INFO: Created: latency-svc-mxqw6
    Jun  8 16:16:20.640: INFO: Got endpoints: latency-svc-9hqfx [751.493956ms]
    Jun  8 16:16:20.665: INFO: Created: latency-svc-nfnvn
    Jun  8 16:16:20.689: INFO: Got endpoints: latency-svc-grd2b [752.320752ms]
    Jun  8 16:16:20.712: INFO: Created: latency-svc-45qxk
    Jun  8 16:16:20.741: INFO: Got endpoints: latency-svc-mfvtv [753.56008ms]
    Jun  8 16:16:20.764: INFO: Created: latency-svc-tq4vb
    Jun  8 16:16:20.790: INFO: Got endpoints: latency-svc-b5lx8 [749.915742ms]
    Jun  8 16:16:20.809: INFO: Created: latency-svc-khhns
    Jun  8 16:16:20.838: INFO: Got endpoints: latency-svc-xlzqd [750.186865ms]
    Jun  8 16:16:20.864: INFO: Created: latency-svc-jqjql
    Jun  8 16:16:20.889: INFO: Got endpoints: latency-svc-hm8qm [750.003317ms]
    Jun  8 16:16:20.905: INFO: Created: latency-svc-b5zxl
    Jun  8 16:16:20.938: INFO: Got endpoints: latency-svc-s2v75 [739.39638ms]
    Jun  8 16:16:20.955: INFO: Created: latency-svc-rfq2f
    Jun  8 16:16:20.988: INFO: Got endpoints: latency-svc-l4n2c [746.746235ms]
    Jun  8 16:16:21.005: INFO: Created: latency-svc-zmnzg
    Jun  8 16:16:21.039: INFO: Got endpoints: latency-svc-6jdrc [749.819595ms]
    Jun  8 16:16:21.057: INFO: Created: latency-svc-78gt6
    Jun  8 16:16:21.088: INFO: Got endpoints: latency-svc-hwq9x [748.140733ms]
    Jun  8 16:16:21.104: INFO: Created: latency-svc-wbhhg
    Jun  8 16:16:21.139: INFO: Got endpoints: latency-svc-nb62w [750.172282ms]
    Jun  8 16:16:21.160: INFO: Created: latency-svc-t84gg
    Jun  8 16:16:21.188: INFO: Got endpoints: latency-svc-gthr5 [748.381313ms]
    Jun  8 16:16:21.205: INFO: Created: latency-svc-m62kb
    Jun  8 16:16:21.238: INFO: Got endpoints: latency-svc-sgjjq [745.507827ms]
    Jun  8 16:16:21.260: INFO: Created: latency-svc-t4g28
    Jun  8 16:16:21.289: INFO: Got endpoints: latency-svc-xdltj [747.663667ms]
    Jun  8 16:16:21.312: INFO: Created: latency-svc-l7bsr
    Jun  8 16:16:21.339: INFO: Got endpoints: latency-svc-mxqw6 [747.607542ms]
    Jun  8 16:16:21.366: INFO: Created: latency-svc-x7nq2
    Jun  8 16:16:21.389: INFO: Got endpoints: latency-svc-nfnvn [748.939586ms]
    Jun  8 16:16:21.412: INFO: Created: latency-svc-gk5jc
    Jun  8 16:16:21.440: INFO: Got endpoints: latency-svc-45qxk [750.813344ms]
    Jun  8 16:16:21.466: INFO: Created: latency-svc-f4pl2
    Jun  8 16:16:21.492: INFO: Got endpoints: latency-svc-tq4vb [751.286685ms]
    Jun  8 16:16:21.513: INFO: Created: latency-svc-gkglt
    Jun  8 16:16:21.540: INFO: Got endpoints: latency-svc-khhns [750.806511ms]
    Jun  8 16:16:21.561: INFO: Created: latency-svc-qvrdm
    Jun  8 16:16:21.588: INFO: Got endpoints: latency-svc-jqjql [749.890223ms]
    Jun  8 16:16:21.610: INFO: Created: latency-svc-bskks
    Jun  8 16:16:21.643: INFO: Got endpoints: latency-svc-b5zxl [753.781612ms]
    Jun  8 16:16:21.667: INFO: Created: latency-svc-cdcjr
    Jun  8 16:16:21.690: INFO: Got endpoints: latency-svc-rfq2f [751.595607ms]
    Jun  8 16:16:21.720: INFO: Created: latency-svc-q54th
    Jun  8 16:16:21.740: INFO: Got endpoints: latency-svc-zmnzg [751.869643ms]
    Jun  8 16:16:21.761: INFO: Created: latency-svc-rqbng
    Jun  8 16:16:21.789: INFO: Got endpoints: latency-svc-78gt6 [749.487378ms]
    Jun  8 16:16:21.808: INFO: Created: latency-svc-hzbbl
    Jun  8 16:16:21.839: INFO: Got endpoints: latency-svc-wbhhg [751.22514ms]
    Jun  8 16:16:21.860: INFO: Created: latency-svc-bsjwl
    Jun  8 16:16:21.887: INFO: Got endpoints: latency-svc-t84gg [748.584337ms]
    Jun  8 16:16:21.904: INFO: Created: latency-svc-kmbtg
    Jun  8 16:16:21.938: INFO: Got endpoints: latency-svc-m62kb [750.039925ms]
    Jun  8 16:16:21.954: INFO: Created: latency-svc-gpl5k
    Jun  8 16:16:21.990: INFO: Got endpoints: latency-svc-t4g28 [752.490606ms]
    Jun  8 16:16:22.006: INFO: Created: latency-svc-bwktx
    Jun  8 16:16:22.038: INFO: Got endpoints: latency-svc-l7bsr [748.033561ms]
    Jun  8 16:16:22.054: INFO: Created: latency-svc-wl9d2
    Jun  8 16:16:22.088: INFO: Got endpoints: latency-svc-x7nq2 [749.034143ms]
    Jun  8 16:16:22.138: INFO: Got endpoints: latency-svc-gk5jc [748.484507ms]
    Jun  8 16:16:22.188: INFO: Got endpoints: latency-svc-f4pl2 [747.756843ms]
    Jun  8 16:16:22.238: INFO: Got endpoints: latency-svc-gkglt [746.124741ms]
    Jun  8 16:16:22.289: INFO: Got endpoints: latency-svc-qvrdm [748.19933ms]
    Jun  8 16:16:22.341: INFO: Got endpoints: latency-svc-bskks [752.893197ms]
    Jun  8 16:16:22.389: INFO: Got endpoints: latency-svc-cdcjr [746.339448ms]
    Jun  8 16:16:22.440: INFO: Got endpoints: latency-svc-q54th [749.996426ms]
    Jun  8 16:16:22.488: INFO: Got endpoints: latency-svc-rqbng [748.112436ms]
    Jun  8 16:16:22.541: INFO: Got endpoints: latency-svc-hzbbl [752.468049ms]
    Jun  8 16:16:22.589: INFO: Got endpoints: latency-svc-bsjwl [750.434047ms]
    Jun  8 16:16:22.639: INFO: Got endpoints: latency-svc-kmbtg [751.893636ms]
    Jun  8 16:16:22.689: INFO: Got endpoints: latency-svc-gpl5k [751.076685ms]
    Jun  8 16:16:22.739: INFO: Got endpoints: latency-svc-bwktx [748.861633ms]
    Jun  8 16:16:22.788: INFO: Got endpoints: latency-svc-wl9d2 [750.016673ms]
    Jun  8 16:16:22.788: INFO: Latencies: [33.245105ms 50.456107ms 71.437824ms 99.511255ms 103.636721ms 128.034102ms 143.955405ms 176.172997ms 184.388082ms 206.017263ms 244.724347ms 251.77213ms 262.396295ms 293.204713ms 293.642065ms 300.944073ms 304.979653ms 306.715633ms 307.367939ms 307.988168ms 308.585465ms 309.381486ms 310.156638ms 310.924355ms 312.543563ms 312.854965ms 313.031393ms 313.597093ms 313.989574ms 314.568124ms 317.831331ms 318.075604ms 318.740775ms 322.061172ms 322.710261ms 329.082292ms 330.111708ms 332.848305ms 338.56728ms 343.35969ms 346.981209ms 347.399658ms 347.442787ms 350.034666ms 350.197813ms 351.342741ms 353.792109ms 353.856674ms 357.198583ms 359.580515ms 364.412943ms 365.852786ms 373.606679ms 385.351369ms 417.748955ms 450.048352ms 494.460043ms 501.745038ms 539.066522ms 567.808308ms 605.145027ms 631.369512ms 665.90882ms 690.598544ms 718.486566ms 737.866615ms 739.39638ms 739.776159ms 740.945063ms 740.969205ms 742.66301ms 744.871979ms 745.450928ms 745.507827ms 745.970636ms 746.124741ms 746.339448ms 746.600473ms 746.746235ms 747.143044ms 747.268531ms 747.334789ms 747.395796ms 747.532994ms 747.557503ms 747.560109ms 747.607542ms 747.663667ms 747.756843ms 747.806943ms 747.87029ms 747.962882ms 748.033561ms 748.105644ms 748.112436ms 748.140733ms 748.19933ms 748.381313ms 748.433732ms 748.484507ms 748.584337ms 748.640193ms 748.662848ms 748.694472ms 748.861633ms 748.939586ms 748.957357ms 749.008047ms 749.034143ms 749.047323ms 749.104821ms 749.163788ms 749.204894ms 749.21197ms 749.213566ms 749.266872ms 749.315432ms 749.485636ms 749.487378ms 749.628883ms 749.654592ms 749.678826ms 749.734193ms 749.812997ms 749.819595ms 749.859338ms 749.859485ms 749.890223ms 749.915742ms 749.996426ms 749.997667ms 750.003317ms 750.01316ms 750.016673ms 750.039925ms 750.087704ms 750.089624ms 750.1209ms 750.172282ms 750.186865ms 750.220155ms 750.242754ms 750.351033ms 750.405569ms 750.420919ms 750.434047ms 750.735725ms 750.755771ms 750.784174ms 750.806511ms 750.813344ms 750.96849ms 750.997003ms 751.018649ms 751.022413ms 751.049669ms 751.066581ms 751.076685ms 751.121939ms 751.22514ms 751.226085ms 751.286685ms 751.299979ms 751.413371ms 751.478253ms 751.493956ms 751.506989ms 751.582235ms 751.587601ms 751.588375ms 751.595607ms 751.630682ms 751.726208ms 751.869643ms 751.893636ms 751.959682ms 752.224383ms 752.320752ms 752.46585ms 752.468049ms 752.490606ms 752.55757ms 752.633684ms 752.893197ms 752.964243ms 752.986799ms 753.109205ms 753.2583ms 753.436414ms 753.440897ms 753.452739ms 753.56008ms 753.586604ms 753.7461ms 753.781612ms 754.143543ms 754.395635ms 755.591285ms 756.080127ms 761.834866ms]
    Jun  8 16:16:22.788: INFO: 50 %ile: 748.584337ms
    Jun  8 16:16:22.788: INFO: 90 %ile: 752.490606ms
    Jun  8 16:16:22.788: INFO: 99 %ile: 756.080127ms
    Jun  8 16:16:22.788: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:16:22.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-6605" for this suite. 06/08/23 16:16:22.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:16:22.805
Jun  8 16:16:22.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename podtemplate 06/08/23 16:16:22.806
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:16:22.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:16:22.827
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jun  8 16:16:22.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-4548" for this suite. 06/08/23 16:16:22.867
------------------------------
• [0.069 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:16:22.805
    Jun  8 16:16:22.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename podtemplate 06/08/23 16:16:22.806
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:16:22.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:16:22.827
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:16:22.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-4548" for this suite. 06/08/23 16:16:22.867
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:16:22.876
Jun  8 16:16:22.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename pods 06/08/23 16:16:22.878
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:16:22.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:16:22.899
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 06/08/23 16:16:22.904
STEP: submitting the pod to kubernetes 06/08/23 16:16:22.904
Jun  8 16:16:22.914: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289" in namespace "pods-480" to be "running and ready"
Jun  8 16:16:22.919: INFO: Pod "pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289": Phase="Pending", Reason="", readiness=false. Elapsed: 4.938263ms
Jun  8 16:16:22.919: INFO: The phase of Pod pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 16:16:24.924: INFO: Pod "pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289": Phase="Running", Reason="", readiness=true. Elapsed: 2.010263952s
Jun  8 16:16:24.924: INFO: The phase of Pod pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289 is Running (Ready = true)
Jun  8 16:16:24.924: INFO: Pod "pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 06/08/23 16:16:24.928
STEP: updating the pod 06/08/23 16:16:24.932
Jun  8 16:16:25.446: INFO: Successfully updated pod "pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289"
Jun  8 16:16:25.446: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289" in namespace "pods-480" to be "terminated with reason DeadlineExceeded"
Jun  8 16:16:25.450: INFO: Pod "pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289": Phase="Running", Reason="", readiness=true. Elapsed: 4.18889ms
Jun  8 16:16:27.456: INFO: Pod "pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289": Phase="Running", Reason="", readiness=true. Elapsed: 2.010471684s
Jun  8 16:16:29.456: INFO: Pod "pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.010037798s
Jun  8 16:16:29.456: INFO: Pod "pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun  8 16:16:29.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-480" for this suite. 06/08/23 16:16:29.465
------------------------------
• [SLOW TEST] [6.597 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:16:22.876
    Jun  8 16:16:22.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename pods 06/08/23 16:16:22.878
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:16:22.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:16:22.899
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 06/08/23 16:16:22.904
    STEP: submitting the pod to kubernetes 06/08/23 16:16:22.904
    Jun  8 16:16:22.914: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289" in namespace "pods-480" to be "running and ready"
    Jun  8 16:16:22.919: INFO: Pod "pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289": Phase="Pending", Reason="", readiness=false. Elapsed: 4.938263ms
    Jun  8 16:16:22.919: INFO: The phase of Pod pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 16:16:24.924: INFO: Pod "pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289": Phase="Running", Reason="", readiness=true. Elapsed: 2.010263952s
    Jun  8 16:16:24.924: INFO: The phase of Pod pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289 is Running (Ready = true)
    Jun  8 16:16:24.924: INFO: Pod "pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 06/08/23 16:16:24.928
    STEP: updating the pod 06/08/23 16:16:24.932
    Jun  8 16:16:25.446: INFO: Successfully updated pod "pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289"
    Jun  8 16:16:25.446: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289" in namespace "pods-480" to be "terminated with reason DeadlineExceeded"
    Jun  8 16:16:25.450: INFO: Pod "pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289": Phase="Running", Reason="", readiness=true. Elapsed: 4.18889ms
    Jun  8 16:16:27.456: INFO: Pod "pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289": Phase="Running", Reason="", readiness=true. Elapsed: 2.010471684s
    Jun  8 16:16:29.456: INFO: Pod "pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.010037798s
    Jun  8 16:16:29.456: INFO: Pod "pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:16:29.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-480" for this suite. 06/08/23 16:16:29.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:16:29.475
Jun  8 16:16:29.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename daemonsets 06/08/23 16:16:29.477
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:16:29.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:16:29.514
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 06/08/23 16:16:29.574
STEP: Check that daemon pods launch on every node of the cluster. 06/08/23 16:16:29.582
Jun  8 16:16:29.598: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  8 16:16:29.598: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
Jun  8 16:16:30.613: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  8 16:16:30.613: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
Jun  8 16:16:31.611: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jun  8 16:16:31.611: INFO: Node chl8tf-control-plane-002 is running 0 daemon pod, expected 1
Jun  8 16:16:32.612: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jun  8 16:16:32.612: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: listing all DeamonSets 06/08/23 16:16:32.617
STEP: DeleteCollection of the DaemonSets 06/08/23 16:16:32.622
STEP: Verify that ReplicaSets have been deleted 06/08/23 16:16:32.637
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Jun  8 16:16:32.653: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"53557"},"items":null}

Jun  8 16:16:32.659: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"53557"},"items":[{"metadata":{"name":"daemon-set-98c6p","generateName":"daemon-set-","namespace":"daemonsets-8881","uid":"16fe4293-77b2-427b-9fe9-b8800fbb08e5","resourceVersion":"53331","creationTimestamp":"2023-06-08T16:16:29Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ee6da5a6-6514-478d-a016-619e909303d8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-08T16:16:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee6da5a6-6514-478d-a016-619e909303d8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-08T16:16:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.142\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gqq8r","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gqq8r","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"chl8tf-worker-002","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["chl8tf-worker-002"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:29Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:30Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:30Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:29Z"}],"hostIP":"100.100.237.90","podIP":"10.244.4.142","podIPs":[{"ip":"10.244.4.142"}],"startTime":"2023-06-08T16:16:29Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-08T16:16:30Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://829c2430fd39eac38e3ef19cdf4ab5e102aa795419d085aefc9ce060612f624f","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fk76w","generateName":"daemon-set-","namespace":"daemonsets-8881","uid":"274b9933-e550-4a4c-9334-0744747fe4dd","resourceVersion":"53324","creationTimestamp":"2023-06-08T16:16:29Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ee6da5a6-6514-478d-a016-619e909303d8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-08T16:16:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee6da5a6-6514-478d-a016-619e909303d8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-08T16:16:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qj6r6","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qj6r6","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"chl8tf-worker-001","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["chl8tf-worker-001"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:29Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:30Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:30Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:29Z"}],"hostIP":"100.100.236.215","podIP":"10.244.3.79","podIPs":[{"ip":"10.244.3.79"}],"startTime":"2023-06-08T16:16:29Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-08T16:16:30Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://9626ecb2f038a22953b1382bd7d43fca368d6de148fa197c66acbb24321cdac6","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-h6tnr","generateName":"daemon-set-","namespace":"daemonsets-8881","uid":"582cffb7-044c-4538-b119-941c1ae2edf4","resourceVersion":"53328","creationTimestamp":"2023-06-08T16:16:29Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ee6da5a6-6514-478d-a016-619e909303d8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-08T16:16:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee6da5a6-6514-478d-a016-619e909303d8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-08T16:16:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-f99jw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-f99jw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"chl8tf-control-plane-001","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["chl8tf-control-plane-001"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:29Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:30Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:30Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:29Z"}],"hostIP":"100.100.237.165","podIP":"10.244.0.84","podIPs":[{"ip":"10.244.0.84"}],"startTime":"2023-06-08T16:16:29Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-08T16:16:30Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://0e3111a14265f3067242b6e77ec91a7756f50e4ec17c167b998d6d60a3f0323f","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-hzfdt","generateName":"daemon-set-","namespace":"daemonsets-8881","uid":"6c27b930-876e-4025-8c56-412e37869bdf","resourceVersion":"53404","creationTimestamp":"2023-06-08T16:16:29Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ee6da5a6-6514-478d-a016-619e909303d8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-08T16:16:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee6da5a6-6514-478d-a016-619e909303d8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-08T16:16:31Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-bjcjm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-bjcjm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"chl8tf-control-plane-003","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["chl8tf-control-plane-003"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:29Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:31Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:31Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:29Z"}],"hostIP":"100.100.237.235","podIP":"10.244.2.78","podIPs":[{"ip":"10.244.2.78"}],"startTime":"2023-06-08T16:16:29Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-08T16:16:30Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://d8f304eaf3ce65153a3028a5da34ef65ed868640c26a512f8fe645c3f3f9f9d5","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-ljzjp","generateName":"daemon-set-","namespace":"daemonsets-8881","uid":"d54b4ca8-d8eb-4c03-81ac-f0565572a117","resourceVersion":"53468","creationTimestamp":"2023-06-08T16:16:29Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ee6da5a6-6514-478d-a016-619e909303d8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-08T16:16:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee6da5a6-6514-478d-a016-619e909303d8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-08T16:16:31Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-fkc79","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-fkc79","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"chl8tf-control-plane-002","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["chl8tf-control-plane-002"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:29Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:31Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:31Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:29Z"}],"hostIP":"100.100.236.41","podIP":"10.244.1.81","podIPs":[{"ip":"10.244.1.81"}],"startTime":"2023-06-08T16:16:29Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-08T16:16:30Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://87f8be35326f1895d24939f09c4dd8dad461632574b1c8ae421e4e26377f5474","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:16:32.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8881" for this suite. 06/08/23 16:16:32.717
------------------------------
• [3.253 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:16:29.475
    Jun  8 16:16:29.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename daemonsets 06/08/23 16:16:29.477
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:16:29.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:16:29.514
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 06/08/23 16:16:29.574
    STEP: Check that daemon pods launch on every node of the cluster. 06/08/23 16:16:29.582
    Jun  8 16:16:29.598: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  8 16:16:29.598: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
    Jun  8 16:16:30.613: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  8 16:16:30.613: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
    Jun  8 16:16:31.611: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Jun  8 16:16:31.611: INFO: Node chl8tf-control-plane-002 is running 0 daemon pod, expected 1
    Jun  8 16:16:32.612: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jun  8 16:16:32.612: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: listing all DeamonSets 06/08/23 16:16:32.617
    STEP: DeleteCollection of the DaemonSets 06/08/23 16:16:32.622
    STEP: Verify that ReplicaSets have been deleted 06/08/23 16:16:32.637
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Jun  8 16:16:32.653: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"53557"},"items":null}

    Jun  8 16:16:32.659: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"53557"},"items":[{"metadata":{"name":"daemon-set-98c6p","generateName":"daemon-set-","namespace":"daemonsets-8881","uid":"16fe4293-77b2-427b-9fe9-b8800fbb08e5","resourceVersion":"53331","creationTimestamp":"2023-06-08T16:16:29Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ee6da5a6-6514-478d-a016-619e909303d8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-08T16:16:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee6da5a6-6514-478d-a016-619e909303d8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-08T16:16:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.142\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gqq8r","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gqq8r","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"chl8tf-worker-002","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["chl8tf-worker-002"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:29Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:30Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:30Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:29Z"}],"hostIP":"100.100.237.90","podIP":"10.244.4.142","podIPs":[{"ip":"10.244.4.142"}],"startTime":"2023-06-08T16:16:29Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-08T16:16:30Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://829c2430fd39eac38e3ef19cdf4ab5e102aa795419d085aefc9ce060612f624f","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fk76w","generateName":"daemon-set-","namespace":"daemonsets-8881","uid":"274b9933-e550-4a4c-9334-0744747fe4dd","resourceVersion":"53324","creationTimestamp":"2023-06-08T16:16:29Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ee6da5a6-6514-478d-a016-619e909303d8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-08T16:16:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee6da5a6-6514-478d-a016-619e909303d8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-08T16:16:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qj6r6","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qj6r6","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"chl8tf-worker-001","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["chl8tf-worker-001"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:29Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:30Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:30Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:29Z"}],"hostIP":"100.100.236.215","podIP":"10.244.3.79","podIPs":[{"ip":"10.244.3.79"}],"startTime":"2023-06-08T16:16:29Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-08T16:16:30Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://9626ecb2f038a22953b1382bd7d43fca368d6de148fa197c66acbb24321cdac6","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-h6tnr","generateName":"daemon-set-","namespace":"daemonsets-8881","uid":"582cffb7-044c-4538-b119-941c1ae2edf4","resourceVersion":"53328","creationTimestamp":"2023-06-08T16:16:29Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ee6da5a6-6514-478d-a016-619e909303d8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-08T16:16:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee6da5a6-6514-478d-a016-619e909303d8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-08T16:16:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-f99jw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-f99jw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"chl8tf-control-plane-001","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["chl8tf-control-plane-001"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:29Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:30Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:30Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:29Z"}],"hostIP":"100.100.237.165","podIP":"10.244.0.84","podIPs":[{"ip":"10.244.0.84"}],"startTime":"2023-06-08T16:16:29Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-08T16:16:30Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://0e3111a14265f3067242b6e77ec91a7756f50e4ec17c167b998d6d60a3f0323f","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-hzfdt","generateName":"daemon-set-","namespace":"daemonsets-8881","uid":"6c27b930-876e-4025-8c56-412e37869bdf","resourceVersion":"53404","creationTimestamp":"2023-06-08T16:16:29Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ee6da5a6-6514-478d-a016-619e909303d8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-08T16:16:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee6da5a6-6514-478d-a016-619e909303d8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-08T16:16:31Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-bjcjm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-bjcjm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"chl8tf-control-plane-003","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["chl8tf-control-plane-003"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:29Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:31Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:31Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:29Z"}],"hostIP":"100.100.237.235","podIP":"10.244.2.78","podIPs":[{"ip":"10.244.2.78"}],"startTime":"2023-06-08T16:16:29Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-08T16:16:30Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://d8f304eaf3ce65153a3028a5da34ef65ed868640c26a512f8fe645c3f3f9f9d5","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-ljzjp","generateName":"daemon-set-","namespace":"daemonsets-8881","uid":"d54b4ca8-d8eb-4c03-81ac-f0565572a117","resourceVersion":"53468","creationTimestamp":"2023-06-08T16:16:29Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ee6da5a6-6514-478d-a016-619e909303d8","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-08T16:16:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee6da5a6-6514-478d-a016-619e909303d8\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-08T16:16:31Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-fkc79","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-fkc79","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"chl8tf-control-plane-002","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["chl8tf-control-plane-002"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:29Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:31Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:31Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-08T16:16:29Z"}],"hostIP":"100.100.236.41","podIP":"10.244.1.81","podIPs":[{"ip":"10.244.1.81"}],"startTime":"2023-06-08T16:16:29Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-08T16:16:30Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://87f8be35326f1895d24939f09c4dd8dad461632574b1c8ae421e4e26377f5474","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:16:32.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8881" for this suite. 06/08/23 16:16:32.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:16:32.729
Jun  8 16:16:32.729: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename sched-pred 06/08/23 16:16:32.731
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:16:32.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:16:32.765
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jun  8 16:16:32.769: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  8 16:16:32.788: INFO: Waiting for terminating namespaces to be deleted...
Jun  8 16:16:32.793: INFO: 
Logging pods the apiserver thinks is on node chl8tf-control-plane-001 before test
Jun  8 16:16:32.805: INFO: daemon-set-h6tnr from daemonsets-8881 started at 2023-06-08 16:16:29 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.805: INFO: 	Container app ready: true, restart count 0
Jun  8 16:16:32.805: INFO: csi-oci-node-5p7f5 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
Jun  8 16:16:32.805: INFO: 	Container csi-node-registrar ready: true, restart count 0
Jun  8 16:16:32.805: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
Jun  8 16:16:32.805: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
Jun  8 16:16:32.805: INFO: etcd-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:04 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.805: INFO: 	Container etcd ready: true, restart count 0
Jun  8 16:16:32.805: INFO: kube-apiserver-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:21 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.805: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun  8 16:16:32.805: INFO: kube-controller-manager-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:21 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.805: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun  8 16:16:32.805: INFO: kube-flannel-ds-d5bvw from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.805: INFO: 	Container kube-flannel ready: true, restart count 0
Jun  8 16:16:32.805: INFO: kube-proxy-j2p7z from kube-system started at 2023-06-08 13:31:27 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.805: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  8 16:16:32.805: INFO: kube-scheduler-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:04 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.805: INFO: 	Container kube-scheduler ready: true, restart count 1
Jun  8 16:16:32.805: INFO: oci-cloud-controller-manager-9n2zj from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.805: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
Jun  8 16:16:32.805: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-wmj9x from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 16:16:32.805: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 16:16:32.805: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  8 16:16:32.805: INFO: 
Logging pods the apiserver thinks is on node chl8tf-control-plane-002 before test
Jun  8 16:16:32.816: INFO: daemon-set-ljzjp from daemonsets-8881 started at 2023-06-08 16:16:29 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.816: INFO: 	Container app ready: true, restart count 0
Jun  8 16:16:32.816: INFO: coredns-55b8ccd764-56hlv from kube-system started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.816: INFO: 	Container coredns ready: true, restart count 0
Jun  8 16:16:32.816: INFO: coredns-55b8ccd764-jpqkc from kube-system started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.816: INFO: 	Container coredns ready: true, restart count 0
Jun  8 16:16:32.816: INFO: csi-oci-node-xbn7q from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
Jun  8 16:16:32.816: INFO: 	Container csi-node-registrar ready: true, restart count 0
Jun  8 16:16:32.816: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
Jun  8 16:16:32.816: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
Jun  8 16:16:32.816: INFO: etcd-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.816: INFO: 	Container etcd ready: true, restart count 0
Jun  8 16:16:32.816: INFO: kube-apiserver-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:10 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.816: INFO: 	Container kube-apiserver ready: true, restart count 1
Jun  8 16:16:32.816: INFO: kube-controller-manager-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.816: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun  8 16:16:32.816: INFO: kube-flannel-ds-6fgbp from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.816: INFO: 	Container kube-flannel ready: true, restart count 0
Jun  8 16:16:32.816: INFO: kube-proxy-cgwfj from kube-system started at 2023-06-08 13:32:05 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.816: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  8 16:16:32.816: INFO: kube-scheduler-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.816: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun  8 16:16:32.816: INFO: oci-cloud-controller-manager-dpkmx from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.816: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
Jun  8 16:16:32.816: INFO: kubernetes-dashboard-8c85c4f9-9l4pq from kubernetes-dashboard started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.816: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jun  8 16:16:32.816: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-74mph from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 16:16:32.816: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 16:16:32.816: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  8 16:16:32.816: INFO: 
Logging pods the apiserver thinks is on node chl8tf-control-plane-003 before test
Jun  8 16:16:32.827: INFO: daemon-set-hzfdt from daemonsets-8881 started at 2023-06-08 16:16:29 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.827: INFO: 	Container app ready: true, restart count 0
Jun  8 16:16:32.827: INFO: csi-oci-node-6zdfs from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
Jun  8 16:16:32.827: INFO: 	Container csi-node-registrar ready: true, restart count 0
Jun  8 16:16:32.827: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
Jun  8 16:16:32.827: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
Jun  8 16:16:32.827: INFO: etcd-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.827: INFO: 	Container etcd ready: true, restart count 0
Jun  8 16:16:32.827: INFO: kube-apiserver-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.827: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun  8 16:16:32.827: INFO: kube-controller-manager-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.827: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun  8 16:16:32.827: INFO: kube-flannel-ds-qf6c6 from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.827: INFO: 	Container kube-flannel ready: true, restart count 0
Jun  8 16:16:32.827: INFO: kube-proxy-9kw5t from kube-system started at 2023-06-08 13:33:04 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.827: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  8 16:16:32.827: INFO: kube-scheduler-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.827: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun  8 16:16:32.827: INFO: oci-cloud-controller-manager-2sdzm from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.827: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
Jun  8 16:16:32.827: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-jpq4c from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 16:16:32.827: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 16:16:32.827: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  8 16:16:32.827: INFO: 
Logging pods the apiserver thinks is on node chl8tf-worker-001 before test
Jun  8 16:16:32.838: INFO: daemon-set-fk76w from daemonsets-8881 started at 2023-06-08 16:16:29 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.838: INFO: 	Container app ready: true, restart count 0
Jun  8 16:16:32.838: INFO: csi-oci-node-7ww4x from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
Jun  8 16:16:32.838: INFO: 	Container csi-node-registrar ready: true, restart count 0
Jun  8 16:16:32.838: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
Jun  8 16:16:32.838: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
Jun  8 16:16:32.838: INFO: kube-flannel-ds-vg6nz from kube-system started at 2023-06-08 16:01:36 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.838: INFO: 	Container kube-flannel ready: true, restart count 0
Jun  8 16:16:32.838: INFO: kube-proxy-6kfv2 from kube-system started at 2023-06-08 13:33:27 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.838: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  8 16:16:32.838: INFO: oci-cloud-controller-manager-fchj2 from kube-system started at 2023-06-08 16:01:36 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.838: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
Jun  8 16:16:32.838: INFO: pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289 from pods-480 started at 2023-06-08 16:16:22 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.838: INFO: 	Container pause ready: false, restart count 0
Jun  8 16:16:32.838: INFO: sonobuoy from sonobuoy started at 2023-06-08 15:05:13 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.838: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  8 16:16:32.838: INFO: sonobuoy-e2e-job-e329b7fb80aa4b40 from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 16:16:32.838: INFO: 	Container e2e ready: true, restart count 0
Jun  8 16:16:32.838: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 16:16:32.838: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-rs4qz from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 16:16:32.838: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 16:16:32.838: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  8 16:16:32.838: INFO: 
Logging pods the apiserver thinks is on node chl8tf-worker-002 before test
Jun  8 16:16:32.854: INFO: daemon-set-98c6p from daemonsets-8881 started at 2023-06-08 16:16:29 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.854: INFO: 	Container app ready: true, restart count 0
Jun  8 16:16:32.854: INFO: csi-oci-controller-69f8b488fc-n8th6 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (5 container statuses recorded)
Jun  8 16:16:32.854: INFO: 	Container csi-attacher ready: true, restart count 1
Jun  8 16:16:32.854: INFO: 	Container csi-fss-volume-provisioner ready: true, restart count 1
Jun  8 16:16:32.854: INFO: 	Container csi-resizer ready: true, restart count 0
Jun  8 16:16:32.854: INFO: 	Container csi-volume-provisioner ready: true, restart count 0
Jun  8 16:16:32.854: INFO: 	Container oci-csi-controller-driver ready: true, restart count 0
Jun  8 16:16:32.854: INFO: csi-oci-node-thcvn from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
Jun  8 16:16:32.854: INFO: 	Container csi-node-registrar ready: true, restart count 0
Jun  8 16:16:32.854: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
Jun  8 16:16:32.854: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
Jun  8 16:16:32.854: INFO: kube-flannel-ds-74q2b from kube-system started at 2023-06-08 13:33:47 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.854: INFO: 	Container kube-flannel ready: true, restart count 0
Jun  8 16:16:32.854: INFO: kube-proxy-hjjpt from kube-system started at 2023-06-08 13:33:47 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.854: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  8 16:16:32.854: INFO: oci-cloud-controller-manager-lwnq4 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
Jun  8 16:16:32.854: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
Jun  8 16:16:32.854: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-6nv2x from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 16:16:32.854: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 16:16:32.854: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 06/08/23 16:16:32.854
Jun  8 16:16:32.873: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8492" to be "running"
Jun  8 16:16:32.879: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.710478ms
Jun  8 16:16:34.884: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.010852841s
Jun  8 16:16:34.884: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 06/08/23 16:16:34.889
STEP: Trying to apply a random label on the found node. 06/08/23 16:16:34.913
STEP: verifying the node has the label kubernetes.io/e2e-5043b0f8-19f3-4816-9fb6-dd7edbd7bc55 95 06/08/23 16:16:34.93
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 06/08/23 16:16:34.935
Jun  8 16:16:34.941: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-8492" to be "not pending"
Jun  8 16:16:34.947: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.921468ms
Jun  8 16:16:36.952: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.011045307s
Jun  8 16:16:36.952: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 100.100.236.215 on the node which pod4 resides and expect not scheduled 06/08/23 16:16:36.952
Jun  8 16:16:36.961: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-8492" to be "not pending"
Jun  8 16:16:36.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.785976ms
Jun  8 16:16:38.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009895147s
Jun  8 16:16:40.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009116244s
Jun  8 16:16:42.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008561565s
Jun  8 16:16:44.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010656832s
Jun  8 16:16:46.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009617735s
Jun  8 16:16:48.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.010259441s
Jun  8 16:16:50.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009660109s
Jun  8 16:16:52.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.009994871s
Jun  8 16:16:54.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008724768s
Jun  8 16:16:56.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009509913s
Jun  8 16:16:58.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009989005s
Jun  8 16:17:00.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009511253s
Jun  8 16:17:02.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010304557s
Jun  8 16:17:04.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008709235s
Jun  8 16:17:06.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.008868945s
Jun  8 16:17:08.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010905003s
Jun  8 16:17:10.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009254237s
Jun  8 16:17:12.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009671521s
Jun  8 16:17:14.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008442423s
Jun  8 16:17:16.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.008618633s
Jun  8 16:17:18.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010118372s
Jun  8 16:17:20.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.010023004s
Jun  8 16:17:22.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009437467s
Jun  8 16:17:24.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.008428799s
Jun  8 16:17:26.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009861279s
Jun  8 16:17:28.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.010451908s
Jun  8 16:17:30.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010712963s
Jun  8 16:17:32.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.009536096s
Jun  8 16:17:34.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.010055744s
Jun  8 16:17:36.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008606097s
Jun  8 16:17:38.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009009082s
Jun  8 16:17:40.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009258326s
Jun  8 16:17:42.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.009961771s
Jun  8 16:17:44.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.009531539s
Jun  8 16:17:46.973: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.011385047s
Jun  8 16:17:48.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009945745s
Jun  8 16:17:50.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009832326s
Jun  8 16:17:52.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.009418002s
Jun  8 16:17:54.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009478097s
Jun  8 16:17:56.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008397414s
Jun  8 16:17:58.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.010354184s
Jun  8 16:18:00.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010030705s
Jun  8 16:18:02.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.00895499s
Jun  8 16:18:04.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008346051s
Jun  8 16:18:06.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.010053277s
Jun  8 16:18:08.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009680854s
Jun  8 16:18:10.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008708061s
Jun  8 16:18:12.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009788445s
Jun  8 16:18:14.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.00925423s
Jun  8 16:18:16.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.00851028s
Jun  8 16:18:18.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010934857s
Jun  8 16:18:20.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008780645s
Jun  8 16:18:22.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008974907s
Jun  8 16:18:24.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.009329072s
Jun  8 16:18:26.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009906668s
Jun  8 16:18:28.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.00839432s
Jun  8 16:18:30.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009650912s
Jun  8 16:18:32.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008467244s
Jun  8 16:18:34.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.00924956s
Jun  8 16:18:36.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008283646s
Jun  8 16:18:38.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.009033927s
Jun  8 16:18:40.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.010229282s
Jun  8 16:18:42.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.010509494s
Jun  8 16:18:44.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.008251204s
Jun  8 16:18:46.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.00858929s
Jun  8 16:18:48.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.010667962s
Jun  8 16:18:50.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.009184841s
Jun  8 16:18:52.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.010269304s
Jun  8 16:18:54.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.008564768s
Jun  8 16:18:56.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.009705762s
Jun  8 16:18:58.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.010008719s
Jun  8 16:19:00.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.009362459s
Jun  8 16:19:02.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.008845347s
Jun  8 16:19:04.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.008805963s
Jun  8 16:19:06.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.00944015s
Jun  8 16:19:08.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.008745031s
Jun  8 16:19:10.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.010643604s
Jun  8 16:19:12.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.009405607s
Jun  8 16:19:14.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.008806259s
Jun  8 16:19:16.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.010329637s
Jun  8 16:19:18.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.008578s
Jun  8 16:19:20.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.010124973s
Jun  8 16:19:22.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.009898915s
Jun  8 16:19:24.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.008425564s
Jun  8 16:19:26.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.008479944s
Jun  8 16:19:28.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.009971246s
Jun  8 16:19:30.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.009709962s
Jun  8 16:19:32.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.008707227s
Jun  8 16:19:34.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.00892325s
Jun  8 16:19:36.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.009222529s
Jun  8 16:19:38.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.010033595s
Jun  8 16:19:40.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.010444635s
Jun  8 16:19:42.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.010447684s
Jun  8 16:19:44.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.008988585s
Jun  8 16:19:46.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.009031165s
Jun  8 16:19:48.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.008731897s
Jun  8 16:19:50.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.008360134s
Jun  8 16:19:52.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.010178115s
Jun  8 16:19:54.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.00991175s
Jun  8 16:19:56.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008980302s
Jun  8 16:19:58.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.009270042s
Jun  8 16:20:00.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.008951565s
Jun  8 16:20:02.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.009542614s
Jun  8 16:20:04.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.009628877s
Jun  8 16:20:06.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.009437039s
Jun  8 16:20:08.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.010303192s
Jun  8 16:20:10.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.009827922s
Jun  8 16:20:12.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.008323655s
Jun  8 16:20:14.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.010729555s
Jun  8 16:20:16.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.009906198s
Jun  8 16:20:18.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008647151s
Jun  8 16:20:20.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.009919377s
Jun  8 16:20:22.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.009489061s
Jun  8 16:20:24.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.009178236s
Jun  8 16:20:26.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.00905988s
Jun  8 16:20:28.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.008581489s
Jun  8 16:20:30.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.00843386s
Jun  8 16:20:32.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.010276979s
Jun  8 16:20:34.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.00829358s
Jun  8 16:20:36.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.008811318s
Jun  8 16:20:38.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.009015503s
Jun  8 16:20:40.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009340724s
Jun  8 16:20:42.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.010429506s
Jun  8 16:20:44.969: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.007905324s
Jun  8 16:20:46.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.010177977s
Jun  8 16:20:48.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.00923801s
Jun  8 16:20:50.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.010019955s
Jun  8 16:20:52.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.009936629s
Jun  8 16:20:54.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.00950417s
Jun  8 16:20:56.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.009363259s
Jun  8 16:20:58.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.010507229s
Jun  8 16:21:00.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.009188755s
Jun  8 16:21:02.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.009441136s
Jun  8 16:21:04.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.008063848s
Jun  8 16:21:06.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008648107s
Jun  8 16:21:08.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.00868804s
Jun  8 16:21:10.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.010508927s
Jun  8 16:21:12.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.010488532s
Jun  8 16:21:14.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.00954196s
Jun  8 16:21:16.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.009018782s
Jun  8 16:21:18.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.008762571s
Jun  8 16:21:20.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.008181983s
Jun  8 16:21:22.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.009762019s
Jun  8 16:21:24.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.010367025s
Jun  8 16:21:26.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.009780927s
Jun  8 16:21:28.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.010320566s
Jun  8 16:21:30.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.008517614s
Jun  8 16:21:32.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.00940021s
Jun  8 16:21:34.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.009479242s
Jun  8 16:21:36.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008548594s
Jun  8 16:21:36.974: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012575989s
STEP: removing the label kubernetes.io/e2e-5043b0f8-19f3-4816-9fb6-dd7edbd7bc55 off the node chl8tf-worker-001 06/08/23 16:21:36.974
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5043b0f8-19f3-4816-9fb6-dd7edbd7bc55 06/08/23 16:21:36.99
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:21:36.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-8492" for this suite. 06/08/23 16:21:37.002
------------------------------
• [SLOW TEST] [304.283 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:16:32.729
    Jun  8 16:16:32.729: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename sched-pred 06/08/23 16:16:32.731
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:16:32.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:16:32.765
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jun  8 16:16:32.769: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun  8 16:16:32.788: INFO: Waiting for terminating namespaces to be deleted...
    Jun  8 16:16:32.793: INFO: 
    Logging pods the apiserver thinks is on node chl8tf-control-plane-001 before test
    Jun  8 16:16:32.805: INFO: daemon-set-h6tnr from daemonsets-8881 started at 2023-06-08 16:16:29 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.805: INFO: 	Container app ready: true, restart count 0
    Jun  8 16:16:32.805: INFO: csi-oci-node-5p7f5 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
    Jun  8 16:16:32.805: INFO: 	Container csi-node-registrar ready: true, restart count 0
    Jun  8 16:16:32.805: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
    Jun  8 16:16:32.805: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
    Jun  8 16:16:32.805: INFO: etcd-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:04 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.805: INFO: 	Container etcd ready: true, restart count 0
    Jun  8 16:16:32.805: INFO: kube-apiserver-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:21 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.805: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jun  8 16:16:32.805: INFO: kube-controller-manager-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:21 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.805: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jun  8 16:16:32.805: INFO: kube-flannel-ds-d5bvw from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.805: INFO: 	Container kube-flannel ready: true, restart count 0
    Jun  8 16:16:32.805: INFO: kube-proxy-j2p7z from kube-system started at 2023-06-08 13:31:27 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.805: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  8 16:16:32.805: INFO: kube-scheduler-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:04 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.805: INFO: 	Container kube-scheduler ready: true, restart count 1
    Jun  8 16:16:32.805: INFO: oci-cloud-controller-manager-9n2zj from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.805: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
    Jun  8 16:16:32.805: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-wmj9x from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 16:16:32.805: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 16:16:32.805: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  8 16:16:32.805: INFO: 
    Logging pods the apiserver thinks is on node chl8tf-control-plane-002 before test
    Jun  8 16:16:32.816: INFO: daemon-set-ljzjp from daemonsets-8881 started at 2023-06-08 16:16:29 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.816: INFO: 	Container app ready: true, restart count 0
    Jun  8 16:16:32.816: INFO: coredns-55b8ccd764-56hlv from kube-system started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.816: INFO: 	Container coredns ready: true, restart count 0
    Jun  8 16:16:32.816: INFO: coredns-55b8ccd764-jpqkc from kube-system started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.816: INFO: 	Container coredns ready: true, restart count 0
    Jun  8 16:16:32.816: INFO: csi-oci-node-xbn7q from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
    Jun  8 16:16:32.816: INFO: 	Container csi-node-registrar ready: true, restart count 0
    Jun  8 16:16:32.816: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
    Jun  8 16:16:32.816: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
    Jun  8 16:16:32.816: INFO: etcd-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.816: INFO: 	Container etcd ready: true, restart count 0
    Jun  8 16:16:32.816: INFO: kube-apiserver-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:10 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.816: INFO: 	Container kube-apiserver ready: true, restart count 1
    Jun  8 16:16:32.816: INFO: kube-controller-manager-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.816: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jun  8 16:16:32.816: INFO: kube-flannel-ds-6fgbp from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.816: INFO: 	Container kube-flannel ready: true, restart count 0
    Jun  8 16:16:32.816: INFO: kube-proxy-cgwfj from kube-system started at 2023-06-08 13:32:05 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.816: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  8 16:16:32.816: INFO: kube-scheduler-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.816: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jun  8 16:16:32.816: INFO: oci-cloud-controller-manager-dpkmx from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.816: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
    Jun  8 16:16:32.816: INFO: kubernetes-dashboard-8c85c4f9-9l4pq from kubernetes-dashboard started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.816: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Jun  8 16:16:32.816: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-74mph from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 16:16:32.816: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 16:16:32.816: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  8 16:16:32.816: INFO: 
    Logging pods the apiserver thinks is on node chl8tf-control-plane-003 before test
    Jun  8 16:16:32.827: INFO: daemon-set-hzfdt from daemonsets-8881 started at 2023-06-08 16:16:29 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.827: INFO: 	Container app ready: true, restart count 0
    Jun  8 16:16:32.827: INFO: csi-oci-node-6zdfs from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
    Jun  8 16:16:32.827: INFO: 	Container csi-node-registrar ready: true, restart count 0
    Jun  8 16:16:32.827: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
    Jun  8 16:16:32.827: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
    Jun  8 16:16:32.827: INFO: etcd-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.827: INFO: 	Container etcd ready: true, restart count 0
    Jun  8 16:16:32.827: INFO: kube-apiserver-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.827: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jun  8 16:16:32.827: INFO: kube-controller-manager-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.827: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jun  8 16:16:32.827: INFO: kube-flannel-ds-qf6c6 from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.827: INFO: 	Container kube-flannel ready: true, restart count 0
    Jun  8 16:16:32.827: INFO: kube-proxy-9kw5t from kube-system started at 2023-06-08 13:33:04 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.827: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  8 16:16:32.827: INFO: kube-scheduler-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.827: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jun  8 16:16:32.827: INFO: oci-cloud-controller-manager-2sdzm from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.827: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
    Jun  8 16:16:32.827: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-jpq4c from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 16:16:32.827: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 16:16:32.827: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  8 16:16:32.827: INFO: 
    Logging pods the apiserver thinks is on node chl8tf-worker-001 before test
    Jun  8 16:16:32.838: INFO: daemon-set-fk76w from daemonsets-8881 started at 2023-06-08 16:16:29 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.838: INFO: 	Container app ready: true, restart count 0
    Jun  8 16:16:32.838: INFO: csi-oci-node-7ww4x from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
    Jun  8 16:16:32.838: INFO: 	Container csi-node-registrar ready: true, restart count 0
    Jun  8 16:16:32.838: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
    Jun  8 16:16:32.838: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
    Jun  8 16:16:32.838: INFO: kube-flannel-ds-vg6nz from kube-system started at 2023-06-08 16:01:36 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.838: INFO: 	Container kube-flannel ready: true, restart count 0
    Jun  8 16:16:32.838: INFO: kube-proxy-6kfv2 from kube-system started at 2023-06-08 13:33:27 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.838: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  8 16:16:32.838: INFO: oci-cloud-controller-manager-fchj2 from kube-system started at 2023-06-08 16:01:36 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.838: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
    Jun  8 16:16:32.838: INFO: pod-update-activedeadlineseconds-c834f450-6557-40a1-a926-33e1498a0289 from pods-480 started at 2023-06-08 16:16:22 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.838: INFO: 	Container pause ready: false, restart count 0
    Jun  8 16:16:32.838: INFO: sonobuoy from sonobuoy started at 2023-06-08 15:05:13 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.838: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun  8 16:16:32.838: INFO: sonobuoy-e2e-job-e329b7fb80aa4b40 from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 16:16:32.838: INFO: 	Container e2e ready: true, restart count 0
    Jun  8 16:16:32.838: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 16:16:32.838: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-rs4qz from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 16:16:32.838: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 16:16:32.838: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  8 16:16:32.838: INFO: 
    Logging pods the apiserver thinks is on node chl8tf-worker-002 before test
    Jun  8 16:16:32.854: INFO: daemon-set-98c6p from daemonsets-8881 started at 2023-06-08 16:16:29 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.854: INFO: 	Container app ready: true, restart count 0
    Jun  8 16:16:32.854: INFO: csi-oci-controller-69f8b488fc-n8th6 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (5 container statuses recorded)
    Jun  8 16:16:32.854: INFO: 	Container csi-attacher ready: true, restart count 1
    Jun  8 16:16:32.854: INFO: 	Container csi-fss-volume-provisioner ready: true, restart count 1
    Jun  8 16:16:32.854: INFO: 	Container csi-resizer ready: true, restart count 0
    Jun  8 16:16:32.854: INFO: 	Container csi-volume-provisioner ready: true, restart count 0
    Jun  8 16:16:32.854: INFO: 	Container oci-csi-controller-driver ready: true, restart count 0
    Jun  8 16:16:32.854: INFO: csi-oci-node-thcvn from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
    Jun  8 16:16:32.854: INFO: 	Container csi-node-registrar ready: true, restart count 0
    Jun  8 16:16:32.854: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
    Jun  8 16:16:32.854: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
    Jun  8 16:16:32.854: INFO: kube-flannel-ds-74q2b from kube-system started at 2023-06-08 13:33:47 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.854: INFO: 	Container kube-flannel ready: true, restart count 0
    Jun  8 16:16:32.854: INFO: kube-proxy-hjjpt from kube-system started at 2023-06-08 13:33:47 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.854: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  8 16:16:32.854: INFO: oci-cloud-controller-manager-lwnq4 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
    Jun  8 16:16:32.854: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
    Jun  8 16:16:32.854: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-6nv2x from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 16:16:32.854: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 16:16:32.854: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 06/08/23 16:16:32.854
    Jun  8 16:16:32.873: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8492" to be "running"
    Jun  8 16:16:32.879: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.710478ms
    Jun  8 16:16:34.884: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.010852841s
    Jun  8 16:16:34.884: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 06/08/23 16:16:34.889
    STEP: Trying to apply a random label on the found node. 06/08/23 16:16:34.913
    STEP: verifying the node has the label kubernetes.io/e2e-5043b0f8-19f3-4816-9fb6-dd7edbd7bc55 95 06/08/23 16:16:34.93
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 06/08/23 16:16:34.935
    Jun  8 16:16:34.941: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-8492" to be "not pending"
    Jun  8 16:16:34.947: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.921468ms
    Jun  8 16:16:36.952: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.011045307s
    Jun  8 16:16:36.952: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 100.100.236.215 on the node which pod4 resides and expect not scheduled 06/08/23 16:16:36.952
    Jun  8 16:16:36.961: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-8492" to be "not pending"
    Jun  8 16:16:36.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.785976ms
    Jun  8 16:16:38.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009895147s
    Jun  8 16:16:40.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009116244s
    Jun  8 16:16:42.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008561565s
    Jun  8 16:16:44.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010656832s
    Jun  8 16:16:46.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009617735s
    Jun  8 16:16:48.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.010259441s
    Jun  8 16:16:50.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009660109s
    Jun  8 16:16:52.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.009994871s
    Jun  8 16:16:54.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008724768s
    Jun  8 16:16:56.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009509913s
    Jun  8 16:16:58.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009989005s
    Jun  8 16:17:00.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009511253s
    Jun  8 16:17:02.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010304557s
    Jun  8 16:17:04.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008709235s
    Jun  8 16:17:06.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.008868945s
    Jun  8 16:17:08.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010905003s
    Jun  8 16:17:10.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009254237s
    Jun  8 16:17:12.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009671521s
    Jun  8 16:17:14.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008442423s
    Jun  8 16:17:16.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.008618633s
    Jun  8 16:17:18.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010118372s
    Jun  8 16:17:20.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.010023004s
    Jun  8 16:17:22.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009437467s
    Jun  8 16:17:24.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.008428799s
    Jun  8 16:17:26.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009861279s
    Jun  8 16:17:28.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.010451908s
    Jun  8 16:17:30.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010712963s
    Jun  8 16:17:32.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.009536096s
    Jun  8 16:17:34.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.010055744s
    Jun  8 16:17:36.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008606097s
    Jun  8 16:17:38.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009009082s
    Jun  8 16:17:40.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009258326s
    Jun  8 16:17:42.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.009961771s
    Jun  8 16:17:44.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.009531539s
    Jun  8 16:17:46.973: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.011385047s
    Jun  8 16:17:48.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009945745s
    Jun  8 16:17:50.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009832326s
    Jun  8 16:17:52.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.009418002s
    Jun  8 16:17:54.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009478097s
    Jun  8 16:17:56.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008397414s
    Jun  8 16:17:58.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.010354184s
    Jun  8 16:18:00.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010030705s
    Jun  8 16:18:02.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.00895499s
    Jun  8 16:18:04.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008346051s
    Jun  8 16:18:06.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.010053277s
    Jun  8 16:18:08.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009680854s
    Jun  8 16:18:10.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008708061s
    Jun  8 16:18:12.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009788445s
    Jun  8 16:18:14.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.00925423s
    Jun  8 16:18:16.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.00851028s
    Jun  8 16:18:18.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010934857s
    Jun  8 16:18:20.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008780645s
    Jun  8 16:18:22.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008974907s
    Jun  8 16:18:24.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.009329072s
    Jun  8 16:18:26.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009906668s
    Jun  8 16:18:28.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.00839432s
    Jun  8 16:18:30.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009650912s
    Jun  8 16:18:32.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008467244s
    Jun  8 16:18:34.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.00924956s
    Jun  8 16:18:36.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008283646s
    Jun  8 16:18:38.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.009033927s
    Jun  8 16:18:40.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.010229282s
    Jun  8 16:18:42.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.010509494s
    Jun  8 16:18:44.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.008251204s
    Jun  8 16:18:46.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.00858929s
    Jun  8 16:18:48.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.010667962s
    Jun  8 16:18:50.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.009184841s
    Jun  8 16:18:52.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.010269304s
    Jun  8 16:18:54.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.008564768s
    Jun  8 16:18:56.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.009705762s
    Jun  8 16:18:58.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.010008719s
    Jun  8 16:19:00.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.009362459s
    Jun  8 16:19:02.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.008845347s
    Jun  8 16:19:04.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.008805963s
    Jun  8 16:19:06.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.00944015s
    Jun  8 16:19:08.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.008745031s
    Jun  8 16:19:10.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.010643604s
    Jun  8 16:19:12.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.009405607s
    Jun  8 16:19:14.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.008806259s
    Jun  8 16:19:16.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.010329637s
    Jun  8 16:19:18.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.008578s
    Jun  8 16:19:20.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.010124973s
    Jun  8 16:19:22.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.009898915s
    Jun  8 16:19:24.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.008425564s
    Jun  8 16:19:26.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.008479944s
    Jun  8 16:19:28.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.009971246s
    Jun  8 16:19:30.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.009709962s
    Jun  8 16:19:32.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.008707227s
    Jun  8 16:19:34.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.00892325s
    Jun  8 16:19:36.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.009222529s
    Jun  8 16:19:38.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.010033595s
    Jun  8 16:19:40.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.010444635s
    Jun  8 16:19:42.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.010447684s
    Jun  8 16:19:44.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.008988585s
    Jun  8 16:19:46.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.009031165s
    Jun  8 16:19:48.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.008731897s
    Jun  8 16:19:50.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.008360134s
    Jun  8 16:19:52.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.010178115s
    Jun  8 16:19:54.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.00991175s
    Jun  8 16:19:56.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008980302s
    Jun  8 16:19:58.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.009270042s
    Jun  8 16:20:00.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.008951565s
    Jun  8 16:20:02.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.009542614s
    Jun  8 16:20:04.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.009628877s
    Jun  8 16:20:06.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.009437039s
    Jun  8 16:20:08.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.010303192s
    Jun  8 16:20:10.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.009827922s
    Jun  8 16:20:12.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.008323655s
    Jun  8 16:20:14.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.010729555s
    Jun  8 16:20:16.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.009906198s
    Jun  8 16:20:18.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008647151s
    Jun  8 16:20:20.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.009919377s
    Jun  8 16:20:22.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.009489061s
    Jun  8 16:20:24.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.009178236s
    Jun  8 16:20:26.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.00905988s
    Jun  8 16:20:28.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.008581489s
    Jun  8 16:20:30.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.00843386s
    Jun  8 16:20:32.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.010276979s
    Jun  8 16:20:34.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.00829358s
    Jun  8 16:20:36.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.008811318s
    Jun  8 16:20:38.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.009015503s
    Jun  8 16:20:40.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009340724s
    Jun  8 16:20:42.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.010429506s
    Jun  8 16:20:44.969: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.007905324s
    Jun  8 16:20:46.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.010177977s
    Jun  8 16:20:48.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.00923801s
    Jun  8 16:20:50.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.010019955s
    Jun  8 16:20:52.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.009936629s
    Jun  8 16:20:54.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.00950417s
    Jun  8 16:20:56.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.009363259s
    Jun  8 16:20:58.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.010507229s
    Jun  8 16:21:00.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.009188755s
    Jun  8 16:21:02.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.009441136s
    Jun  8 16:21:04.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.008063848s
    Jun  8 16:21:06.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008648107s
    Jun  8 16:21:08.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.00868804s
    Jun  8 16:21:10.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.010508927s
    Jun  8 16:21:12.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.010488532s
    Jun  8 16:21:14.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.00954196s
    Jun  8 16:21:16.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.009018782s
    Jun  8 16:21:18.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.008762571s
    Jun  8 16:21:20.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.008181983s
    Jun  8 16:21:22.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.009762019s
    Jun  8 16:21:24.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.010367025s
    Jun  8 16:21:26.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.009780927s
    Jun  8 16:21:28.972: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.010320566s
    Jun  8 16:21:30.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.008517614s
    Jun  8 16:21:32.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.00940021s
    Jun  8 16:21:34.971: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.009479242s
    Jun  8 16:21:36.970: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008548594s
    Jun  8 16:21:36.974: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012575989s
    STEP: removing the label kubernetes.io/e2e-5043b0f8-19f3-4816-9fb6-dd7edbd7bc55 off the node chl8tf-worker-001 06/08/23 16:21:36.974
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-5043b0f8-19f3-4816-9fb6-dd7edbd7bc55 06/08/23 16:21:36.99
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:21:36.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-8492" for this suite. 06/08/23 16:21:37.002
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:21:37.015
Jun  8 16:21:37.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename replicaset 06/08/23 16:21:37.017
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:21:37.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:21:37.038
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 06/08/23 16:21:37.043
Jun  8 16:21:37.053: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-2199" to be "running and ready"
Jun  8 16:21:37.061: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 8.473629ms
Jun  8 16:21:37.062: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jun  8 16:21:39.068: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.015513778s
Jun  8 16:21:39.069: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jun  8 16:21:39.069: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 06/08/23 16:21:39.073
STEP: Then the orphan pod is adopted 06/08/23 16:21:39.079
STEP: When the matched label of one of its pods change 06/08/23 16:21:40.089
Jun  8 16:21:40.093: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 06/08/23 16:21:40.105
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jun  8 16:21:41.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2199" for this suite. 06/08/23 16:21:41.12
------------------------------
• [4.113 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:21:37.015
    Jun  8 16:21:37.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename replicaset 06/08/23 16:21:37.017
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:21:37.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:21:37.038
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 06/08/23 16:21:37.043
    Jun  8 16:21:37.053: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-2199" to be "running and ready"
    Jun  8 16:21:37.061: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 8.473629ms
    Jun  8 16:21:37.062: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 16:21:39.068: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.015513778s
    Jun  8 16:21:39.069: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jun  8 16:21:39.069: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 06/08/23 16:21:39.073
    STEP: Then the orphan pod is adopted 06/08/23 16:21:39.079
    STEP: When the matched label of one of its pods change 06/08/23 16:21:40.089
    Jun  8 16:21:40.093: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 06/08/23 16:21:40.105
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:21:41.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2199" for this suite. 06/08/23 16:21:41.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:21:41.128
Jun  8 16:21:41.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubectl 06/08/23 16:21:41.13
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:21:41.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:21:41.152
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 06/08/23 16:21:41.155
Jun  8 16:21:41.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-3007 create -f -'
Jun  8 16:21:42.043: INFO: stderr: ""
Jun  8 16:21:42.043: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 06/08/23 16:21:42.043
Jun  8 16:21:42.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-3007 diff -f -'
Jun  8 16:21:42.247: INFO: rc: 1
Jun  8 16:21:42.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-3007 delete -f -'
Jun  8 16:21:42.362: INFO: stderr: ""
Jun  8 16:21:42.362: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  8 16:21:42.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3007" for this suite. 06/08/23 16:21:42.369
------------------------------
• [1.248 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:21:41.128
    Jun  8 16:21:41.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubectl 06/08/23 16:21:41.13
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:21:41.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:21:41.152
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 06/08/23 16:21:41.155
    Jun  8 16:21:41.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-3007 create -f -'
    Jun  8 16:21:42.043: INFO: stderr: ""
    Jun  8 16:21:42.043: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 06/08/23 16:21:42.043
    Jun  8 16:21:42.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-3007 diff -f -'
    Jun  8 16:21:42.247: INFO: rc: 1
    Jun  8 16:21:42.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-3007 delete -f -'
    Jun  8 16:21:42.362: INFO: stderr: ""
    Jun  8 16:21:42.362: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:21:42.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3007" for this suite. 06/08/23 16:21:42.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:21:42.377
Jun  8 16:21:42.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename gc 06/08/23 16:21:42.379
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:21:42.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:21:42.401
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 06/08/23 16:21:42.405
STEP: delete the rc 06/08/23 16:21:47.418
STEP: wait for all pods to be garbage collected 06/08/23 16:21:47.427
STEP: Gathering metrics 06/08/23 16:21:52.436
Jun  8 16:21:52.463: INFO: Waiting up to 5m0s for pod "kube-controller-manager-chl8tf-control-plane-003" in namespace "kube-system" to be "running and ready"
Jun  8 16:21:52.467: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003": Phase="Running", Reason="", readiness=true. Elapsed: 3.993766ms
Jun  8 16:21:52.467: INFO: The phase of Pod kube-controller-manager-chl8tf-control-plane-003 is Running (Ready = true)
Jun  8 16:21:52.468: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003" satisfied condition "running and ready"
Jun  8 16:21:52.534: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jun  8 16:21:52.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1340" for this suite. 06/08/23 16:21:52.543
------------------------------
• [SLOW TEST] [10.173 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:21:42.377
    Jun  8 16:21:42.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename gc 06/08/23 16:21:42.379
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:21:42.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:21:42.401
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 06/08/23 16:21:42.405
    STEP: delete the rc 06/08/23 16:21:47.418
    STEP: wait for all pods to be garbage collected 06/08/23 16:21:47.427
    STEP: Gathering metrics 06/08/23 16:21:52.436
    Jun  8 16:21:52.463: INFO: Waiting up to 5m0s for pod "kube-controller-manager-chl8tf-control-plane-003" in namespace "kube-system" to be "running and ready"
    Jun  8 16:21:52.467: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003": Phase="Running", Reason="", readiness=true. Elapsed: 3.993766ms
    Jun  8 16:21:52.467: INFO: The phase of Pod kube-controller-manager-chl8tf-control-plane-003 is Running (Ready = true)
    Jun  8 16:21:52.468: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003" satisfied condition "running and ready"
    Jun  8 16:21:52.534: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:21:52.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1340" for this suite. 06/08/23 16:21:52.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:21:52.555
Jun  8 16:21:52.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename configmap 06/08/23 16:21:52.556
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:21:52.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:21:52.576
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-945a4714-3896-4164-a2c6-1585860ff65a 06/08/23 16:21:52.586
STEP: Creating configMap with name cm-test-opt-upd-01d7e42b-4726-4fd7-82d7-62594f37ae57 06/08/23 16:21:52.591
STEP: Creating the pod 06/08/23 16:21:52.596
Jun  8 16:21:52.606: INFO: Waiting up to 5m0s for pod "pod-configmaps-a706578c-f3b6-43f4-a77d-c1dd5b43c36e" in namespace "configmap-7425" to be "running and ready"
Jun  8 16:21:52.610: INFO: Pod "pod-configmaps-a706578c-f3b6-43f4-a77d-c1dd5b43c36e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.164021ms
Jun  8 16:21:52.610: INFO: The phase of Pod pod-configmaps-a706578c-f3b6-43f4-a77d-c1dd5b43c36e is Pending, waiting for it to be Running (with Ready = true)
Jun  8 16:21:54.616: INFO: Pod "pod-configmaps-a706578c-f3b6-43f4-a77d-c1dd5b43c36e": Phase="Running", Reason="", readiness=true. Elapsed: 2.009547651s
Jun  8 16:21:54.616: INFO: The phase of Pod pod-configmaps-a706578c-f3b6-43f4-a77d-c1dd5b43c36e is Running (Ready = true)
Jun  8 16:21:54.616: INFO: Pod "pod-configmaps-a706578c-f3b6-43f4-a77d-c1dd5b43c36e" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-945a4714-3896-4164-a2c6-1585860ff65a 06/08/23 16:21:54.651
STEP: Updating configmap cm-test-opt-upd-01d7e42b-4726-4fd7-82d7-62594f37ae57 06/08/23 16:21:54.659
STEP: Creating configMap with name cm-test-opt-create-f1bf98fb-02fb-400b-8d8f-2425e1d3e0de 06/08/23 16:21:54.664
STEP: waiting to observe update in volume 06/08/23 16:21:54.67
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  8 16:21:58.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7425" for this suite. 06/08/23 16:21:58.718
------------------------------
• [SLOW TEST] [6.171 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:21:52.555
    Jun  8 16:21:52.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename configmap 06/08/23 16:21:52.556
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:21:52.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:21:52.576
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-945a4714-3896-4164-a2c6-1585860ff65a 06/08/23 16:21:52.586
    STEP: Creating configMap with name cm-test-opt-upd-01d7e42b-4726-4fd7-82d7-62594f37ae57 06/08/23 16:21:52.591
    STEP: Creating the pod 06/08/23 16:21:52.596
    Jun  8 16:21:52.606: INFO: Waiting up to 5m0s for pod "pod-configmaps-a706578c-f3b6-43f4-a77d-c1dd5b43c36e" in namespace "configmap-7425" to be "running and ready"
    Jun  8 16:21:52.610: INFO: Pod "pod-configmaps-a706578c-f3b6-43f4-a77d-c1dd5b43c36e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.164021ms
    Jun  8 16:21:52.610: INFO: The phase of Pod pod-configmaps-a706578c-f3b6-43f4-a77d-c1dd5b43c36e is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 16:21:54.616: INFO: Pod "pod-configmaps-a706578c-f3b6-43f4-a77d-c1dd5b43c36e": Phase="Running", Reason="", readiness=true. Elapsed: 2.009547651s
    Jun  8 16:21:54.616: INFO: The phase of Pod pod-configmaps-a706578c-f3b6-43f4-a77d-c1dd5b43c36e is Running (Ready = true)
    Jun  8 16:21:54.616: INFO: Pod "pod-configmaps-a706578c-f3b6-43f4-a77d-c1dd5b43c36e" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-945a4714-3896-4164-a2c6-1585860ff65a 06/08/23 16:21:54.651
    STEP: Updating configmap cm-test-opt-upd-01d7e42b-4726-4fd7-82d7-62594f37ae57 06/08/23 16:21:54.659
    STEP: Creating configMap with name cm-test-opt-create-f1bf98fb-02fb-400b-8d8f-2425e1d3e0de 06/08/23 16:21:54.664
    STEP: waiting to observe update in volume 06/08/23 16:21:54.67
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:21:58.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7425" for this suite. 06/08/23 16:21:58.718
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:21:58.727
Jun  8 16:21:58.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename gc 06/08/23 16:21:58.728
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:21:58.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:21:58.751
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 06/08/23 16:21:58.755
STEP: Wait for the Deployment to create new ReplicaSet 06/08/23 16:21:58.762
STEP: delete the deployment 06/08/23 16:21:59.271
STEP: wait for all rs to be garbage collected 06/08/23 16:21:59.279
STEP: expected 0 rs, got 1 rs 06/08/23 16:21:59.288
STEP: expected 0 pods, got 2 pods 06/08/23 16:21:59.293
STEP: Gathering metrics 06/08/23 16:21:59.805
Jun  8 16:21:59.825: INFO: Waiting up to 5m0s for pod "kube-controller-manager-chl8tf-control-plane-003" in namespace "kube-system" to be "running and ready"
Jun  8 16:21:59.829: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003": Phase="Running", Reason="", readiness=true. Elapsed: 3.508737ms
Jun  8 16:21:59.829: INFO: The phase of Pod kube-controller-manager-chl8tf-control-plane-003 is Running (Ready = true)
Jun  8 16:21:59.829: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003" satisfied condition "running and ready"
Jun  8 16:21:59.880: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jun  8 16:21:59.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5921" for this suite. 06/08/23 16:21:59.886
------------------------------
• [1.167 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:21:58.727
    Jun  8 16:21:58.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename gc 06/08/23 16:21:58.728
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:21:58.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:21:58.751
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 06/08/23 16:21:58.755
    STEP: Wait for the Deployment to create new ReplicaSet 06/08/23 16:21:58.762
    STEP: delete the deployment 06/08/23 16:21:59.271
    STEP: wait for all rs to be garbage collected 06/08/23 16:21:59.279
    STEP: expected 0 rs, got 1 rs 06/08/23 16:21:59.288
    STEP: expected 0 pods, got 2 pods 06/08/23 16:21:59.293
    STEP: Gathering metrics 06/08/23 16:21:59.805
    Jun  8 16:21:59.825: INFO: Waiting up to 5m0s for pod "kube-controller-manager-chl8tf-control-plane-003" in namespace "kube-system" to be "running and ready"
    Jun  8 16:21:59.829: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003": Phase="Running", Reason="", readiness=true. Elapsed: 3.508737ms
    Jun  8 16:21:59.829: INFO: The phase of Pod kube-controller-manager-chl8tf-control-plane-003 is Running (Ready = true)
    Jun  8 16:21:59.829: INFO: Pod "kube-controller-manager-chl8tf-control-plane-003" satisfied condition "running and ready"
    Jun  8 16:21:59.880: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:21:59.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5921" for this suite. 06/08/23 16:21:59.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:21:59.895
Jun  8 16:21:59.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename job 06/08/23 16:21:59.896
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:21:59.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:21:59.918
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 06/08/23 16:21:59.926
STEP: Patching the Job 06/08/23 16:21:59.932
STEP: Watching for Job to be patched 06/08/23 16:21:59.952
Jun  8 16:21:59.953: INFO: Event ADDED observed for Job e2e-bqbxv in namespace job-4598 with labels: map[e2e-job-label:e2e-bqbxv] and annotations: map[batch.kubernetes.io/job-tracking:]
Jun  8 16:21:59.953: INFO: Event MODIFIED observed for Job e2e-bqbxv in namespace job-4598 with labels: map[e2e-job-label:e2e-bqbxv] and annotations: map[batch.kubernetes.io/job-tracking:]
Jun  8 16:21:59.954: INFO: Event MODIFIED found for Job e2e-bqbxv in namespace job-4598 with labels: map[e2e-bqbxv:patched e2e-job-label:e2e-bqbxv] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 06/08/23 16:21:59.954
STEP: Watching for Job to be updated 06/08/23 16:21:59.966
Jun  8 16:21:59.967: INFO: Event MODIFIED found for Job e2e-bqbxv in namespace job-4598 with labels: map[e2e-bqbxv:patched e2e-job-label:e2e-bqbxv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun  8 16:21:59.968: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 06/08/23 16:21:59.968
Jun  8 16:21:59.972: INFO: Job: e2e-bqbxv as labels: map[e2e-bqbxv:patched e2e-job-label:e2e-bqbxv]
STEP: Waiting for job to complete 06/08/23 16:21:59.972
STEP: Delete a job collection with a labelselector 06/08/23 16:22:09.978
STEP: Watching for Job to be deleted 06/08/23 16:22:09.988
Jun  8 16:22:09.990: INFO: Event MODIFIED observed for Job e2e-bqbxv in namespace job-4598 with labels: map[e2e-bqbxv:patched e2e-job-label:e2e-bqbxv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun  8 16:22:09.990: INFO: Event MODIFIED observed for Job e2e-bqbxv in namespace job-4598 with labels: map[e2e-bqbxv:patched e2e-job-label:e2e-bqbxv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun  8 16:22:09.990: INFO: Event MODIFIED observed for Job e2e-bqbxv in namespace job-4598 with labels: map[e2e-bqbxv:patched e2e-job-label:e2e-bqbxv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun  8 16:22:09.990: INFO: Event MODIFIED observed for Job e2e-bqbxv in namespace job-4598 with labels: map[e2e-bqbxv:patched e2e-job-label:e2e-bqbxv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun  8 16:22:09.990: INFO: Event MODIFIED observed for Job e2e-bqbxv in namespace job-4598 with labels: map[e2e-bqbxv:patched e2e-job-label:e2e-bqbxv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun  8 16:22:09.990: INFO: Event DELETED found for Job e2e-bqbxv in namespace job-4598 with labels: map[e2e-bqbxv:patched e2e-job-label:e2e-bqbxv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 06/08/23 16:22:09.99
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jun  8 16:22:09.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4598" for this suite. 06/08/23 16:22:10
------------------------------
• [SLOW TEST] [10.112 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:21:59.895
    Jun  8 16:21:59.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename job 06/08/23 16:21:59.896
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:21:59.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:21:59.918
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 06/08/23 16:21:59.926
    STEP: Patching the Job 06/08/23 16:21:59.932
    STEP: Watching for Job to be patched 06/08/23 16:21:59.952
    Jun  8 16:21:59.953: INFO: Event ADDED observed for Job e2e-bqbxv in namespace job-4598 with labels: map[e2e-job-label:e2e-bqbxv] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jun  8 16:21:59.953: INFO: Event MODIFIED observed for Job e2e-bqbxv in namespace job-4598 with labels: map[e2e-job-label:e2e-bqbxv] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jun  8 16:21:59.954: INFO: Event MODIFIED found for Job e2e-bqbxv in namespace job-4598 with labels: map[e2e-bqbxv:patched e2e-job-label:e2e-bqbxv] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 06/08/23 16:21:59.954
    STEP: Watching for Job to be updated 06/08/23 16:21:59.966
    Jun  8 16:21:59.967: INFO: Event MODIFIED found for Job e2e-bqbxv in namespace job-4598 with labels: map[e2e-bqbxv:patched e2e-job-label:e2e-bqbxv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun  8 16:21:59.968: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 06/08/23 16:21:59.968
    Jun  8 16:21:59.972: INFO: Job: e2e-bqbxv as labels: map[e2e-bqbxv:patched e2e-job-label:e2e-bqbxv]
    STEP: Waiting for job to complete 06/08/23 16:21:59.972
    STEP: Delete a job collection with a labelselector 06/08/23 16:22:09.978
    STEP: Watching for Job to be deleted 06/08/23 16:22:09.988
    Jun  8 16:22:09.990: INFO: Event MODIFIED observed for Job e2e-bqbxv in namespace job-4598 with labels: map[e2e-bqbxv:patched e2e-job-label:e2e-bqbxv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun  8 16:22:09.990: INFO: Event MODIFIED observed for Job e2e-bqbxv in namespace job-4598 with labels: map[e2e-bqbxv:patched e2e-job-label:e2e-bqbxv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun  8 16:22:09.990: INFO: Event MODIFIED observed for Job e2e-bqbxv in namespace job-4598 with labels: map[e2e-bqbxv:patched e2e-job-label:e2e-bqbxv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun  8 16:22:09.990: INFO: Event MODIFIED observed for Job e2e-bqbxv in namespace job-4598 with labels: map[e2e-bqbxv:patched e2e-job-label:e2e-bqbxv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun  8 16:22:09.990: INFO: Event MODIFIED observed for Job e2e-bqbxv in namespace job-4598 with labels: map[e2e-bqbxv:patched e2e-job-label:e2e-bqbxv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun  8 16:22:09.990: INFO: Event DELETED found for Job e2e-bqbxv in namespace job-4598 with labels: map[e2e-bqbxv:patched e2e-job-label:e2e-bqbxv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 06/08/23 16:22:09.99
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:22:09.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4598" for this suite. 06/08/23 16:22:10
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:22:10.008
Jun  8 16:22:10.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename podtemplate 06/08/23 16:22:10.009
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:22:10.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:22:10.035
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 06/08/23 16:22:10.038
Jun  8 16:22:10.044: INFO: created test-podtemplate-1
Jun  8 16:22:10.050: INFO: created test-podtemplate-2
Jun  8 16:22:10.055: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 06/08/23 16:22:10.055
STEP: delete collection of pod templates 06/08/23 16:22:10.059
Jun  8 16:22:10.059: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 06/08/23 16:22:10.078
Jun  8 16:22:10.078: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jun  8 16:22:10.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-1724" for this suite. 06/08/23 16:22:10.088
------------------------------
• [0.087 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:22:10.008
    Jun  8 16:22:10.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename podtemplate 06/08/23 16:22:10.009
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:22:10.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:22:10.035
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 06/08/23 16:22:10.038
    Jun  8 16:22:10.044: INFO: created test-podtemplate-1
    Jun  8 16:22:10.050: INFO: created test-podtemplate-2
    Jun  8 16:22:10.055: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 06/08/23 16:22:10.055
    STEP: delete collection of pod templates 06/08/23 16:22:10.059
    Jun  8 16:22:10.059: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 06/08/23 16:22:10.078
    Jun  8 16:22:10.078: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:22:10.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-1724" for this suite. 06/08/23 16:22:10.088
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:22:10.096
Jun  8 16:22:10.096: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename sched-preemption 06/08/23 16:22:10.097
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:22:10.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:22:10.117
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jun  8 16:22:10.135: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  8 16:23:10.193: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 06/08/23 16:23:10.197
Jun  8 16:23:10.225: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jun  8 16:23:10.232: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jun  8 16:23:10.257: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jun  8 16:23:10.266: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jun  8 16:23:10.293: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jun  8 16:23:10.301: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Jun  8 16:23:10.331: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Jun  8 16:23:10.346: INFO: Created pod: pod3-1-sched-preemption-medium-priority
Jun  8 16:23:10.371: INFO: Created pod: pod4-0-sched-preemption-medium-priority
Jun  8 16:23:10.378: INFO: Created pod: pod4-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 06/08/23 16:23:10.378
Jun  8 16:23:10.378: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8283" to be "running"
Jun  8 16:23:10.383: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.923259ms
Jun  8 16:23:12.389: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.010984665s
Jun  8 16:23:12.389: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jun  8 16:23:12.389: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8283" to be "running"
Jun  8 16:23:12.393: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.711881ms
Jun  8 16:23:12.393: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jun  8 16:23:12.393: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8283" to be "running"
Jun  8 16:23:12.396: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.464438ms
Jun  8 16:23:12.396: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jun  8 16:23:12.396: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8283" to be "running"
Jun  8 16:23:12.400: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.546144ms
Jun  8 16:23:12.400: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jun  8 16:23:12.400: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8283" to be "running"
Jun  8 16:23:12.403: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.3346ms
Jun  8 16:23:12.403: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jun  8 16:23:12.403: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8283" to be "running"
Jun  8 16:23:12.407: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.023273ms
Jun  8 16:23:12.407: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Jun  8 16:23:12.407: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-8283" to be "running"
Jun  8 16:23:12.411: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.585691ms
Jun  8 16:23:12.411: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Jun  8 16:23:12.411: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-8283" to be "running"
Jun  8 16:23:12.415: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.447596ms
Jun  8 16:23:12.415: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
Jun  8 16:23:12.415: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-8283" to be "running"
Jun  8 16:23:12.418: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.553886ms
Jun  8 16:23:12.418: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
Jun  8 16:23:12.418: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-8283" to be "running"
Jun  8 16:23:12.421: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.1918ms
Jun  8 16:23:12.421: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 06/08/23 16:23:12.421
Jun  8 16:23:12.427: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8283" to be "running"
Jun  8 16:23:12.431: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.753384ms
Jun  8 16:23:14.437: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009390596s
Jun  8 16:23:16.437: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009549478s
Jun  8 16:23:18.436: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.008938613s
Jun  8 16:23:18.436: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:23:18.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-8283" for this suite. 06/08/23 16:23:18.556
------------------------------
• [SLOW TEST] [68.467 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:22:10.096
    Jun  8 16:22:10.096: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename sched-preemption 06/08/23 16:22:10.097
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:22:10.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:22:10.117
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jun  8 16:22:10.135: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun  8 16:23:10.193: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 06/08/23 16:23:10.197
    Jun  8 16:23:10.225: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jun  8 16:23:10.232: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jun  8 16:23:10.257: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jun  8 16:23:10.266: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jun  8 16:23:10.293: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jun  8 16:23:10.301: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Jun  8 16:23:10.331: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Jun  8 16:23:10.346: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    Jun  8 16:23:10.371: INFO: Created pod: pod4-0-sched-preemption-medium-priority
    Jun  8 16:23:10.378: INFO: Created pod: pod4-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 06/08/23 16:23:10.378
    Jun  8 16:23:10.378: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8283" to be "running"
    Jun  8 16:23:10.383: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.923259ms
    Jun  8 16:23:12.389: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.010984665s
    Jun  8 16:23:12.389: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jun  8 16:23:12.389: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8283" to be "running"
    Jun  8 16:23:12.393: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.711881ms
    Jun  8 16:23:12.393: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun  8 16:23:12.393: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8283" to be "running"
    Jun  8 16:23:12.396: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.464438ms
    Jun  8 16:23:12.396: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun  8 16:23:12.396: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8283" to be "running"
    Jun  8 16:23:12.400: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.546144ms
    Jun  8 16:23:12.400: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun  8 16:23:12.400: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8283" to be "running"
    Jun  8 16:23:12.403: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.3346ms
    Jun  8 16:23:12.403: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun  8 16:23:12.403: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8283" to be "running"
    Jun  8 16:23:12.407: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.023273ms
    Jun  8 16:23:12.407: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun  8 16:23:12.407: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-8283" to be "running"
    Jun  8 16:23:12.411: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.585691ms
    Jun  8 16:23:12.411: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun  8 16:23:12.411: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-8283" to be "running"
    Jun  8 16:23:12.415: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.447596ms
    Jun  8 16:23:12.415: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun  8 16:23:12.415: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-8283" to be "running"
    Jun  8 16:23:12.418: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.553886ms
    Jun  8 16:23:12.418: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun  8 16:23:12.418: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-8283" to be "running"
    Jun  8 16:23:12.421: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.1918ms
    Jun  8 16:23:12.421: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 06/08/23 16:23:12.421
    Jun  8 16:23:12.427: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8283" to be "running"
    Jun  8 16:23:12.431: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.753384ms
    Jun  8 16:23:14.437: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009390596s
    Jun  8 16:23:16.437: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009549478s
    Jun  8 16:23:18.436: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.008938613s
    Jun  8 16:23:18.436: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:23:18.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-8283" for this suite. 06/08/23 16:23:18.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:23:18.565
Jun  8 16:23:18.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename namespaces 06/08/23 16:23:18.565
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:23:18.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:23:18.585
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-rjrrg" 06/08/23 16:23:18.589
Jun  8 16:23:18.606: INFO: Namespace "e2e-ns-rjrrg-1171" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-rjrrg-1171" 06/08/23 16:23:18.606
Jun  8 16:23:18.617: INFO: Namespace "e2e-ns-rjrrg-1171" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-rjrrg-1171" 06/08/23 16:23:18.617
Jun  8 16:23:18.626: INFO: Namespace "e2e-ns-rjrrg-1171" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:23:18.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-970" for this suite. 06/08/23 16:23:18.632
STEP: Destroying namespace "e2e-ns-rjrrg-1171" for this suite. 06/08/23 16:23:18.639
------------------------------
• [0.081 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:23:18.565
    Jun  8 16:23:18.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename namespaces 06/08/23 16:23:18.565
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:23:18.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:23:18.585
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-rjrrg" 06/08/23 16:23:18.589
    Jun  8 16:23:18.606: INFO: Namespace "e2e-ns-rjrrg-1171" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-rjrrg-1171" 06/08/23 16:23:18.606
    Jun  8 16:23:18.617: INFO: Namespace "e2e-ns-rjrrg-1171" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-rjrrg-1171" 06/08/23 16:23:18.617
    Jun  8 16:23:18.626: INFO: Namespace "e2e-ns-rjrrg-1171" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:23:18.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-970" for this suite. 06/08/23 16:23:18.632
    STEP: Destroying namespace "e2e-ns-rjrrg-1171" for this suite. 06/08/23 16:23:18.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:23:18.646
Jun  8 16:23:18.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubelet-test 06/08/23 16:23:18.647
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:23:18.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:23:18.67
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jun  8 16:23:18.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7198" for this suite. 06/08/23 16:23:18.703
------------------------------
• [0.065 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:23:18.646
    Jun  8 16:23:18.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubelet-test 06/08/23 16:23:18.647
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:23:18.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:23:18.67
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:23:18.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7198" for this suite. 06/08/23 16:23:18.703
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:23:18.714
Jun  8 16:23:18.714: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename proxy 06/08/23 16:23:18.715
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:23:18.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:23:18.742
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jun  8 16:23:18.747: INFO: Creating pod...
Jun  8 16:23:18.757: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8729" to be "running"
Jun  8 16:23:18.761: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.496198ms
Jun  8 16:23:20.767: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.010185821s
Jun  8 16:23:20.767: INFO: Pod "agnhost" satisfied condition "running"
Jun  8 16:23:20.767: INFO: Creating service...
Jun  8 16:23:20.783: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/pods/agnhost/proxy/some/path/with/DELETE
Jun  8 16:23:20.789: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun  8 16:23:20.789: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/pods/agnhost/proxy/some/path/with/GET
Jun  8 16:23:20.794: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jun  8 16:23:20.794: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/pods/agnhost/proxy/some/path/with/HEAD
Jun  8 16:23:20.799: INFO: http.Client request:HEAD | StatusCode:200
Jun  8 16:23:20.799: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/pods/agnhost/proxy/some/path/with/OPTIONS
Jun  8 16:23:20.806: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun  8 16:23:20.806: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/pods/agnhost/proxy/some/path/with/PATCH
Jun  8 16:23:20.811: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun  8 16:23:20.811: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/pods/agnhost/proxy/some/path/with/POST
Jun  8 16:23:20.816: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun  8 16:23:20.816: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/pods/agnhost/proxy/some/path/with/PUT
Jun  8 16:23:20.822: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun  8 16:23:20.822: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/services/test-service/proxy/some/path/with/DELETE
Jun  8 16:23:20.830: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun  8 16:23:20.830: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/services/test-service/proxy/some/path/with/GET
Jun  8 16:23:20.838: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jun  8 16:23:20.838: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/services/test-service/proxy/some/path/with/HEAD
Jun  8 16:23:20.846: INFO: http.Client request:HEAD | StatusCode:200
Jun  8 16:23:20.846: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/services/test-service/proxy/some/path/with/OPTIONS
Jun  8 16:23:20.856: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun  8 16:23:20.856: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/services/test-service/proxy/some/path/with/PATCH
Jun  8 16:23:20.864: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun  8 16:23:20.864: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/services/test-service/proxy/some/path/with/POST
Jun  8 16:23:20.872: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun  8 16:23:20.872: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/services/test-service/proxy/some/path/with/PUT
Jun  8 16:23:20.879: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jun  8 16:23:20.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-8729" for this suite. 06/08/23 16:23:20.887
------------------------------
• [2.183 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:23:18.714
    Jun  8 16:23:18.714: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename proxy 06/08/23 16:23:18.715
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:23:18.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:23:18.742
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jun  8 16:23:18.747: INFO: Creating pod...
    Jun  8 16:23:18.757: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8729" to be "running"
    Jun  8 16:23:18.761: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.496198ms
    Jun  8 16:23:20.767: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.010185821s
    Jun  8 16:23:20.767: INFO: Pod "agnhost" satisfied condition "running"
    Jun  8 16:23:20.767: INFO: Creating service...
    Jun  8 16:23:20.783: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/pods/agnhost/proxy/some/path/with/DELETE
    Jun  8 16:23:20.789: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun  8 16:23:20.789: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/pods/agnhost/proxy/some/path/with/GET
    Jun  8 16:23:20.794: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jun  8 16:23:20.794: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/pods/agnhost/proxy/some/path/with/HEAD
    Jun  8 16:23:20.799: INFO: http.Client request:HEAD | StatusCode:200
    Jun  8 16:23:20.799: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/pods/agnhost/proxy/some/path/with/OPTIONS
    Jun  8 16:23:20.806: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun  8 16:23:20.806: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/pods/agnhost/proxy/some/path/with/PATCH
    Jun  8 16:23:20.811: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun  8 16:23:20.811: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/pods/agnhost/proxy/some/path/with/POST
    Jun  8 16:23:20.816: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun  8 16:23:20.816: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/pods/agnhost/proxy/some/path/with/PUT
    Jun  8 16:23:20.822: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jun  8 16:23:20.822: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/services/test-service/proxy/some/path/with/DELETE
    Jun  8 16:23:20.830: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun  8 16:23:20.830: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/services/test-service/proxy/some/path/with/GET
    Jun  8 16:23:20.838: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jun  8 16:23:20.838: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/services/test-service/proxy/some/path/with/HEAD
    Jun  8 16:23:20.846: INFO: http.Client request:HEAD | StatusCode:200
    Jun  8 16:23:20.846: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/services/test-service/proxy/some/path/with/OPTIONS
    Jun  8 16:23:20.856: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun  8 16:23:20.856: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/services/test-service/proxy/some/path/with/PATCH
    Jun  8 16:23:20.864: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun  8 16:23:20.864: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/services/test-service/proxy/some/path/with/POST
    Jun  8 16:23:20.872: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun  8 16:23:20.872: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8729/services/test-service/proxy/some/path/with/PUT
    Jun  8 16:23:20.879: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:23:20.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-8729" for this suite. 06/08/23 16:23:20.887
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:23:20.898
Jun  8 16:23:20.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename security-context 06/08/23 16:23:20.9
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:23:20.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:23:20.93
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/08/23 16:23:20.934
Jun  8 16:23:20.946: INFO: Waiting up to 5m0s for pod "security-context-936e7683-4b10-4d86-8d54-5ee2d4b31d76" in namespace "security-context-4294" to be "Succeeded or Failed"
Jun  8 16:23:20.951: INFO: Pod "security-context-936e7683-4b10-4d86-8d54-5ee2d4b31d76": Phase="Pending", Reason="", readiness=false. Elapsed: 5.387673ms
Jun  8 16:23:22.957: INFO: Pod "security-context-936e7683-4b10-4d86-8d54-5ee2d4b31d76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01058101s
Jun  8 16:23:24.956: INFO: Pod "security-context-936e7683-4b10-4d86-8d54-5ee2d4b31d76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009886883s
STEP: Saw pod success 06/08/23 16:23:24.956
Jun  8 16:23:24.956: INFO: Pod "security-context-936e7683-4b10-4d86-8d54-5ee2d4b31d76" satisfied condition "Succeeded or Failed"
Jun  8 16:23:24.960: INFO: Trying to get logs from node chl8tf-worker-001 pod security-context-936e7683-4b10-4d86-8d54-5ee2d4b31d76 container test-container: <nil>
STEP: delete the pod 06/08/23 16:23:24.967
Jun  8 16:23:24.978: INFO: Waiting for pod security-context-936e7683-4b10-4d86-8d54-5ee2d4b31d76 to disappear
Jun  8 16:23:24.982: INFO: Pod security-context-936e7683-4b10-4d86-8d54-5ee2d4b31d76 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jun  8 16:23:24.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-4294" for this suite. 06/08/23 16:23:24.987
------------------------------
• [4.096 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:23:20.898
    Jun  8 16:23:20.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename security-context 06/08/23 16:23:20.9
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:23:20.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:23:20.93
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/08/23 16:23:20.934
    Jun  8 16:23:20.946: INFO: Waiting up to 5m0s for pod "security-context-936e7683-4b10-4d86-8d54-5ee2d4b31d76" in namespace "security-context-4294" to be "Succeeded or Failed"
    Jun  8 16:23:20.951: INFO: Pod "security-context-936e7683-4b10-4d86-8d54-5ee2d4b31d76": Phase="Pending", Reason="", readiness=false. Elapsed: 5.387673ms
    Jun  8 16:23:22.957: INFO: Pod "security-context-936e7683-4b10-4d86-8d54-5ee2d4b31d76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01058101s
    Jun  8 16:23:24.956: INFO: Pod "security-context-936e7683-4b10-4d86-8d54-5ee2d4b31d76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009886883s
    STEP: Saw pod success 06/08/23 16:23:24.956
    Jun  8 16:23:24.956: INFO: Pod "security-context-936e7683-4b10-4d86-8d54-5ee2d4b31d76" satisfied condition "Succeeded or Failed"
    Jun  8 16:23:24.960: INFO: Trying to get logs from node chl8tf-worker-001 pod security-context-936e7683-4b10-4d86-8d54-5ee2d4b31d76 container test-container: <nil>
    STEP: delete the pod 06/08/23 16:23:24.967
    Jun  8 16:23:24.978: INFO: Waiting for pod security-context-936e7683-4b10-4d86-8d54-5ee2d4b31d76 to disappear
    Jun  8 16:23:24.982: INFO: Pod security-context-936e7683-4b10-4d86-8d54-5ee2d4b31d76 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:23:24.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-4294" for this suite. 06/08/23 16:23:24.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:23:24.995
Jun  8 16:23:24.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename webhook 06/08/23 16:23:24.996
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:23:25.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:23:25.019
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/08/23 16:23:25.037
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 16:23:25.736
STEP: Deploying the webhook pod 06/08/23 16:23:25.746
STEP: Wait for the deployment to be ready 06/08/23 16:23:25.758
Jun  8 16:23:25.767: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/08/23 16:23:27.78
STEP: Verifying the service has paired with the endpoint 06/08/23 16:23:27.797
Jun  8 16:23:28.798: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Jun  8 16:23:28.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4847-crds.webhook.example.com via the AdmissionRegistration API 06/08/23 16:23:29.317
STEP: Creating a custom resource that should be mutated by the webhook 06/08/23 16:23:29.338
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:23:31.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4805" for this suite. 06/08/23 16:23:31.988
STEP: Destroying namespace "webhook-4805-markers" for this suite. 06/08/23 16:23:32.004
------------------------------
• [SLOW TEST] [7.018 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:23:24.995
    Jun  8 16:23:24.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename webhook 06/08/23 16:23:24.996
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:23:25.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:23:25.019
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/08/23 16:23:25.037
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 16:23:25.736
    STEP: Deploying the webhook pod 06/08/23 16:23:25.746
    STEP: Wait for the deployment to be ready 06/08/23 16:23:25.758
    Jun  8 16:23:25.767: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/08/23 16:23:27.78
    STEP: Verifying the service has paired with the endpoint 06/08/23 16:23:27.797
    Jun  8 16:23:28.798: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Jun  8 16:23:28.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4847-crds.webhook.example.com via the AdmissionRegistration API 06/08/23 16:23:29.317
    STEP: Creating a custom resource that should be mutated by the webhook 06/08/23 16:23:29.338
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:23:31.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4805" for this suite. 06/08/23 16:23:31.988
    STEP: Destroying namespace "webhook-4805-markers" for this suite. 06/08/23 16:23:32.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:23:32.017
Jun  8 16:23:32.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename container-runtime 06/08/23 16:23:32.019
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:23:32.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:23:32.044
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 06/08/23 16:23:32.049
STEP: wait for the container to reach Failed 06/08/23 16:23:32.063
STEP: get the container status 06/08/23 16:23:36.09
STEP: the container should be terminated 06/08/23 16:23:36.094
STEP: the termination message should be set 06/08/23 16:23:36.095
Jun  8 16:23:36.095: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 06/08/23 16:23:36.095
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jun  8 16:23:36.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-4454" for this suite. 06/08/23 16:23:36.115
------------------------------
• [4.105 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:23:32.017
    Jun  8 16:23:32.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename container-runtime 06/08/23 16:23:32.019
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:23:32.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:23:32.044
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 06/08/23 16:23:32.049
    STEP: wait for the container to reach Failed 06/08/23 16:23:32.063
    STEP: get the container status 06/08/23 16:23:36.09
    STEP: the container should be terminated 06/08/23 16:23:36.094
    STEP: the termination message should be set 06/08/23 16:23:36.095
    Jun  8 16:23:36.095: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 06/08/23 16:23:36.095
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:23:36.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-4454" for this suite. 06/08/23 16:23:36.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:23:36.124
Jun  8 16:23:36.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename crd-publish-openapi 06/08/23 16:23:36.125
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:23:36.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:23:36.147
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 06/08/23 16:23:36.15
Jun  8 16:23:36.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: rename a version 06/08/23 16:23:40.606
STEP: check the new version name is served 06/08/23 16:23:40.625
STEP: check the old version name is removed 06/08/23 16:23:42.816
STEP: check the other version is not changed 06/08/23 16:23:43.577
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:23:47.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2733" for this suite. 06/08/23 16:23:47.141
------------------------------
• [SLOW TEST] [11.024 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:23:36.124
    Jun  8 16:23:36.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename crd-publish-openapi 06/08/23 16:23:36.125
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:23:36.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:23:36.147
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 06/08/23 16:23:36.15
    Jun  8 16:23:36.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: rename a version 06/08/23 16:23:40.606
    STEP: check the new version name is served 06/08/23 16:23:40.625
    STEP: check the old version name is removed 06/08/23 16:23:42.816
    STEP: check the other version is not changed 06/08/23 16:23:43.577
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:23:47.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2733" for this suite. 06/08/23 16:23:47.141
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:23:47.15
Jun  8 16:23:47.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename webhook 06/08/23 16:23:47.151
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:23:47.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:23:47.169
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/08/23 16:23:47.187
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 16:23:48.007
STEP: Deploying the webhook pod 06/08/23 16:23:48.017
STEP: Wait for the deployment to be ready 06/08/23 16:23:48.03
Jun  8 16:23:48.040: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/08/23 16:23:50.053
STEP: Verifying the service has paired with the endpoint 06/08/23 16:23:50.069
Jun  8 16:23:51.070: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Jun  8 16:23:51.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6557-crds.webhook.example.com via the AdmissionRegistration API 06/08/23 16:23:51.586
STEP: Creating a custom resource that should be mutated by the webhook 06/08/23 16:23:51.607
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:23:54.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3794" for this suite. 06/08/23 16:23:54.265
STEP: Destroying namespace "webhook-3794-markers" for this suite. 06/08/23 16:23:54.294
------------------------------
• [SLOW TEST] [7.156 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:23:47.15
    Jun  8 16:23:47.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename webhook 06/08/23 16:23:47.151
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:23:47.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:23:47.169
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/08/23 16:23:47.187
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 16:23:48.007
    STEP: Deploying the webhook pod 06/08/23 16:23:48.017
    STEP: Wait for the deployment to be ready 06/08/23 16:23:48.03
    Jun  8 16:23:48.040: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/08/23 16:23:50.053
    STEP: Verifying the service has paired with the endpoint 06/08/23 16:23:50.069
    Jun  8 16:23:51.070: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Jun  8 16:23:51.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6557-crds.webhook.example.com via the AdmissionRegistration API 06/08/23 16:23:51.586
    STEP: Creating a custom resource that should be mutated by the webhook 06/08/23 16:23:51.607
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:23:54.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3794" for this suite. 06/08/23 16:23:54.265
    STEP: Destroying namespace "webhook-3794-markers" for this suite. 06/08/23 16:23:54.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:23:54.314
Jun  8 16:23:54.314: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename namespaces 06/08/23 16:23:54.316
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:23:54.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:23:54.358
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 06/08/23 16:23:54.362
STEP: patching the Namespace 06/08/23 16:23:54.38
STEP: get the Namespace and ensuring it has the label 06/08/23 16:23:54.387
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:23:54.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3392" for this suite. 06/08/23 16:23:54.402
STEP: Destroying namespace "nspatchtest-bf4e4b73-0295-4462-9aab-1b63d3771a91-9134" for this suite. 06/08/23 16:23:54.414
------------------------------
• [0.112 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:23:54.314
    Jun  8 16:23:54.314: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename namespaces 06/08/23 16:23:54.316
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:23:54.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:23:54.358
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 06/08/23 16:23:54.362
    STEP: patching the Namespace 06/08/23 16:23:54.38
    STEP: get the Namespace and ensuring it has the label 06/08/23 16:23:54.387
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:23:54.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3392" for this suite. 06/08/23 16:23:54.402
    STEP: Destroying namespace "nspatchtest-bf4e4b73-0295-4462-9aab-1b63d3771a91-9134" for this suite. 06/08/23 16:23:54.414
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:23:54.427
Jun  8 16:23:54.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename resourcequota 06/08/23 16:23:54.429
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:23:54.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:23:54.453
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 06/08/23 16:23:54.458
STEP: Creating a ResourceQuota 06/08/23 16:23:59.468
STEP: Ensuring resource quota status is calculated 06/08/23 16:23:59.477
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  8 16:24:01.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7912" for this suite. 06/08/23 16:24:01.496
------------------------------
• [SLOW TEST] [7.076 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:23:54.427
    Jun  8 16:23:54.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename resourcequota 06/08/23 16:23:54.429
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:23:54.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:23:54.453
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 06/08/23 16:23:54.458
    STEP: Creating a ResourceQuota 06/08/23 16:23:59.468
    STEP: Ensuring resource quota status is calculated 06/08/23 16:23:59.477
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:24:01.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7912" for this suite. 06/08/23 16:24:01.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:24:01.504
Jun  8 16:24:01.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename container-probe 06/08/23 16:24:01.505
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:24:01.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:24:01.522
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-46d40cb6-3ece-445a-b36f-4ae85611a5e5 in namespace container-probe-6905 06/08/23 16:24:01.526
Jun  8 16:24:01.535: INFO: Waiting up to 5m0s for pod "busybox-46d40cb6-3ece-445a-b36f-4ae85611a5e5" in namespace "container-probe-6905" to be "not pending"
Jun  8 16:24:01.538: INFO: Pod "busybox-46d40cb6-3ece-445a-b36f-4ae85611a5e5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.569652ms
Jun  8 16:24:03.544: INFO: Pod "busybox-46d40cb6-3ece-445a-b36f-4ae85611a5e5": Phase="Running", Reason="", readiness=true. Elapsed: 2.009104678s
Jun  8 16:24:03.544: INFO: Pod "busybox-46d40cb6-3ece-445a-b36f-4ae85611a5e5" satisfied condition "not pending"
Jun  8 16:24:03.544: INFO: Started pod busybox-46d40cb6-3ece-445a-b36f-4ae85611a5e5 in namespace container-probe-6905
STEP: checking the pod's current state and verifying that restartCount is present 06/08/23 16:24:03.544
Jun  8 16:24:03.548: INFO: Initial restart count of pod busybox-46d40cb6-3ece-445a-b36f-4ae85611a5e5 is 0
Jun  8 16:24:53.697: INFO: Restart count of pod container-probe-6905/busybox-46d40cb6-3ece-445a-b36f-4ae85611a5e5 is now 1 (50.148404426s elapsed)
STEP: deleting the pod 06/08/23 16:24:53.697
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun  8 16:24:53.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6905" for this suite. 06/08/23 16:24:53.719
------------------------------
• [SLOW TEST] [52.222 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:24:01.504
    Jun  8 16:24:01.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename container-probe 06/08/23 16:24:01.505
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:24:01.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:24:01.522
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-46d40cb6-3ece-445a-b36f-4ae85611a5e5 in namespace container-probe-6905 06/08/23 16:24:01.526
    Jun  8 16:24:01.535: INFO: Waiting up to 5m0s for pod "busybox-46d40cb6-3ece-445a-b36f-4ae85611a5e5" in namespace "container-probe-6905" to be "not pending"
    Jun  8 16:24:01.538: INFO: Pod "busybox-46d40cb6-3ece-445a-b36f-4ae85611a5e5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.569652ms
    Jun  8 16:24:03.544: INFO: Pod "busybox-46d40cb6-3ece-445a-b36f-4ae85611a5e5": Phase="Running", Reason="", readiness=true. Elapsed: 2.009104678s
    Jun  8 16:24:03.544: INFO: Pod "busybox-46d40cb6-3ece-445a-b36f-4ae85611a5e5" satisfied condition "not pending"
    Jun  8 16:24:03.544: INFO: Started pod busybox-46d40cb6-3ece-445a-b36f-4ae85611a5e5 in namespace container-probe-6905
    STEP: checking the pod's current state and verifying that restartCount is present 06/08/23 16:24:03.544
    Jun  8 16:24:03.548: INFO: Initial restart count of pod busybox-46d40cb6-3ece-445a-b36f-4ae85611a5e5 is 0
    Jun  8 16:24:53.697: INFO: Restart count of pod container-probe-6905/busybox-46d40cb6-3ece-445a-b36f-4ae85611a5e5 is now 1 (50.148404426s elapsed)
    STEP: deleting the pod 06/08/23 16:24:53.697
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:24:53.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6905" for this suite. 06/08/23 16:24:53.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:24:53.727
Jun  8 16:24:53.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename dns 06/08/23 16:24:53.729
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:24:53.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:24:53.745
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 06/08/23 16:24:53.748
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 06/08/23 16:24:53.749
STEP: creating a pod to probe DNS 06/08/23 16:24:53.749
STEP: submitting the pod to kubernetes 06/08/23 16:24:53.749
Jun  8 16:24:53.758: INFO: Waiting up to 15m0s for pod "dns-test-4c354ced-e660-45d1-a245-b76e673be241" in namespace "dns-939" to be "running"
Jun  8 16:24:53.762: INFO: Pod "dns-test-4c354ced-e660-45d1-a245-b76e673be241": Phase="Pending", Reason="", readiness=false. Elapsed: 4.301187ms
Jun  8 16:24:55.768: INFO: Pod "dns-test-4c354ced-e660-45d1-a245-b76e673be241": Phase="Running", Reason="", readiness=true. Elapsed: 2.009754851s
Jun  8 16:24:55.768: INFO: Pod "dns-test-4c354ced-e660-45d1-a245-b76e673be241" satisfied condition "running"
STEP: retrieving the pod 06/08/23 16:24:55.768
STEP: looking for the results for each expected name from probers 06/08/23 16:24:55.772
Jun  8 16:24:55.791: INFO: DNS probes using dns-939/dns-test-4c354ced-e660-45d1-a245-b76e673be241 succeeded

STEP: deleting the pod 06/08/23 16:24:55.791
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun  8 16:24:55.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-939" for this suite. 06/08/23 16:24:55.811
------------------------------
• [2.091 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:24:53.727
    Jun  8 16:24:53.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename dns 06/08/23 16:24:53.729
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:24:53.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:24:53.745
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     06/08/23 16:24:53.748
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     06/08/23 16:24:53.749
    STEP: creating a pod to probe DNS 06/08/23 16:24:53.749
    STEP: submitting the pod to kubernetes 06/08/23 16:24:53.749
    Jun  8 16:24:53.758: INFO: Waiting up to 15m0s for pod "dns-test-4c354ced-e660-45d1-a245-b76e673be241" in namespace "dns-939" to be "running"
    Jun  8 16:24:53.762: INFO: Pod "dns-test-4c354ced-e660-45d1-a245-b76e673be241": Phase="Pending", Reason="", readiness=false. Elapsed: 4.301187ms
    Jun  8 16:24:55.768: INFO: Pod "dns-test-4c354ced-e660-45d1-a245-b76e673be241": Phase="Running", Reason="", readiness=true. Elapsed: 2.009754851s
    Jun  8 16:24:55.768: INFO: Pod "dns-test-4c354ced-e660-45d1-a245-b76e673be241" satisfied condition "running"
    STEP: retrieving the pod 06/08/23 16:24:55.768
    STEP: looking for the results for each expected name from probers 06/08/23 16:24:55.772
    Jun  8 16:24:55.791: INFO: DNS probes using dns-939/dns-test-4c354ced-e660-45d1-a245-b76e673be241 succeeded

    STEP: deleting the pod 06/08/23 16:24:55.791
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:24:55.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-939" for this suite. 06/08/23 16:24:55.811
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:24:55.819
Jun  8 16:24:55.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename downward-api 06/08/23 16:24:55.82
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:24:55.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:24:55.84
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 06/08/23 16:24:55.843
Jun  8 16:24:55.852: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7eb9c884-7803-49a0-b828-cc25410e08ba" in namespace "downward-api-9756" to be "Succeeded or Failed"
Jun  8 16:24:55.856: INFO: Pod "downwardapi-volume-7eb9c884-7803-49a0-b828-cc25410e08ba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.560335ms
Jun  8 16:24:57.862: INFO: Pod "downwardapi-volume-7eb9c884-7803-49a0-b828-cc25410e08ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009461486s
Jun  8 16:24:59.862: INFO: Pod "downwardapi-volume-7eb9c884-7803-49a0-b828-cc25410e08ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009559822s
STEP: Saw pod success 06/08/23 16:24:59.862
Jun  8 16:24:59.862: INFO: Pod "downwardapi-volume-7eb9c884-7803-49a0-b828-cc25410e08ba" satisfied condition "Succeeded or Failed"
Jun  8 16:24:59.866: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-7eb9c884-7803-49a0-b828-cc25410e08ba container client-container: <nil>
STEP: delete the pod 06/08/23 16:24:59.884
Jun  8 16:24:59.896: INFO: Waiting for pod downwardapi-volume-7eb9c884-7803-49a0-b828-cc25410e08ba to disappear
Jun  8 16:24:59.900: INFO: Pod downwardapi-volume-7eb9c884-7803-49a0-b828-cc25410e08ba no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  8 16:24:59.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9756" for this suite. 06/08/23 16:24:59.906
------------------------------
• [4.095 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:24:55.819
    Jun  8 16:24:55.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename downward-api 06/08/23 16:24:55.82
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:24:55.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:24:55.84
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 06/08/23 16:24:55.843
    Jun  8 16:24:55.852: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7eb9c884-7803-49a0-b828-cc25410e08ba" in namespace "downward-api-9756" to be "Succeeded or Failed"
    Jun  8 16:24:55.856: INFO: Pod "downwardapi-volume-7eb9c884-7803-49a0-b828-cc25410e08ba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.560335ms
    Jun  8 16:24:57.862: INFO: Pod "downwardapi-volume-7eb9c884-7803-49a0-b828-cc25410e08ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009461486s
    Jun  8 16:24:59.862: INFO: Pod "downwardapi-volume-7eb9c884-7803-49a0-b828-cc25410e08ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009559822s
    STEP: Saw pod success 06/08/23 16:24:59.862
    Jun  8 16:24:59.862: INFO: Pod "downwardapi-volume-7eb9c884-7803-49a0-b828-cc25410e08ba" satisfied condition "Succeeded or Failed"
    Jun  8 16:24:59.866: INFO: Trying to get logs from node chl8tf-worker-001 pod downwardapi-volume-7eb9c884-7803-49a0-b828-cc25410e08ba container client-container: <nil>
    STEP: delete the pod 06/08/23 16:24:59.884
    Jun  8 16:24:59.896: INFO: Waiting for pod downwardapi-volume-7eb9c884-7803-49a0-b828-cc25410e08ba to disappear
    Jun  8 16:24:59.900: INFO: Pod downwardapi-volume-7eb9c884-7803-49a0-b828-cc25410e08ba no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:24:59.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9756" for this suite. 06/08/23 16:24:59.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:24:59.915
Jun  8 16:24:59.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename disruption 06/08/23 16:24:59.916
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:24:59.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:24:59.934
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 06/08/23 16:24:59.937
STEP: Waiting for the pdb to be processed 06/08/23 16:24:59.943
STEP: updating the pdb 06/08/23 16:25:01.952
STEP: Waiting for the pdb to be processed 06/08/23 16:25:01.961
STEP: patching the pdb 06/08/23 16:25:01.969
STEP: Waiting for the pdb to be processed 06/08/23 16:25:01.98
STEP: Waiting for the pdb to be deleted 06/08/23 16:25:01.993
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jun  8 16:25:01.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6383" for this suite. 06/08/23 16:25:02.004
------------------------------
• [2.096 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:24:59.915
    Jun  8 16:24:59.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename disruption 06/08/23 16:24:59.916
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:24:59.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:24:59.934
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 06/08/23 16:24:59.937
    STEP: Waiting for the pdb to be processed 06/08/23 16:24:59.943
    STEP: updating the pdb 06/08/23 16:25:01.952
    STEP: Waiting for the pdb to be processed 06/08/23 16:25:01.961
    STEP: patching the pdb 06/08/23 16:25:01.969
    STEP: Waiting for the pdb to be processed 06/08/23 16:25:01.98
    STEP: Waiting for the pdb to be deleted 06/08/23 16:25:01.993
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:25:01.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6383" for this suite. 06/08/23 16:25:02.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:25:02.012
Jun  8 16:25:02.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename cronjob 06/08/23 16:25:02.013
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:25:02.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:25:02.032
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 06/08/23 16:25:02.035
STEP: Ensuring no jobs are scheduled 06/08/23 16:25:02.041
STEP: Ensuring no job exists by listing jobs explicitly 06/08/23 16:30:02.051
STEP: Removing cronjob 06/08/23 16:30:02.055
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jun  8 16:30:02.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9480" for this suite. 06/08/23 16:30:02.068
------------------------------
• [SLOW TEST] [300.064 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:25:02.012
    Jun  8 16:25:02.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename cronjob 06/08/23 16:25:02.013
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:25:02.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:25:02.032
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 06/08/23 16:25:02.035
    STEP: Ensuring no jobs are scheduled 06/08/23 16:25:02.041
    STEP: Ensuring no job exists by listing jobs explicitly 06/08/23 16:30:02.051
    STEP: Removing cronjob 06/08/23 16:30:02.055
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:30:02.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9480" for this suite. 06/08/23 16:30:02.068
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:30:02.077
Jun  8 16:30:02.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename conformance-tests 06/08/23 16:30:02.079
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:30:02.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:30:02.098
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 06/08/23 16:30:02.101
Jun  8 16:30:02.101: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Jun  8 16:30:02.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-5715" for this suite. 06/08/23 16:30:02.118
------------------------------
• [0.051 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:30:02.077
    Jun  8 16:30:02.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename conformance-tests 06/08/23 16:30:02.079
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:30:02.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:30:02.098
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 06/08/23 16:30:02.101
    Jun  8 16:30:02.101: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:30:02.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-5715" for this suite. 06/08/23 16:30:02.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:30:02.129
Jun  8 16:30:02.129: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename replicaset 06/08/23 16:30:02.13
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:30:02.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:30:02.148
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jun  8 16:30:02.166: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun  8 16:30:07.172: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/08/23 16:30:07.172
STEP: Scaling up "test-rs" replicaset  06/08/23 16:30:07.172
Jun  8 16:30:07.185: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 06/08/23 16:30:07.185
W0608 16:30:07.196484      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jun  8 16:30:07.199: INFO: observed ReplicaSet test-rs in namespace replicaset-7928 with ReadyReplicas 1, AvailableReplicas 1
Jun  8 16:30:07.212: INFO: observed ReplicaSet test-rs in namespace replicaset-7928 with ReadyReplicas 1, AvailableReplicas 1
Jun  8 16:30:07.241: INFO: observed ReplicaSet test-rs in namespace replicaset-7928 with ReadyReplicas 1, AvailableReplicas 1
Jun  8 16:30:07.248: INFO: observed ReplicaSet test-rs in namespace replicaset-7928 with ReadyReplicas 1, AvailableReplicas 1
Jun  8 16:30:08.161: INFO: observed ReplicaSet test-rs in namespace replicaset-7928 with ReadyReplicas 2, AvailableReplicas 2
Jun  8 16:30:08.574: INFO: observed Replicaset test-rs in namespace replicaset-7928 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jun  8 16:30:08.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7928" for this suite. 06/08/23 16:30:08.581
------------------------------
• [SLOW TEST] [6.461 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:30:02.129
    Jun  8 16:30:02.129: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename replicaset 06/08/23 16:30:02.13
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:30:02.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:30:02.148
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jun  8 16:30:02.166: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun  8 16:30:07.172: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/08/23 16:30:07.172
    STEP: Scaling up "test-rs" replicaset  06/08/23 16:30:07.172
    Jun  8 16:30:07.185: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 06/08/23 16:30:07.185
    W0608 16:30:07.196484      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jun  8 16:30:07.199: INFO: observed ReplicaSet test-rs in namespace replicaset-7928 with ReadyReplicas 1, AvailableReplicas 1
    Jun  8 16:30:07.212: INFO: observed ReplicaSet test-rs in namespace replicaset-7928 with ReadyReplicas 1, AvailableReplicas 1
    Jun  8 16:30:07.241: INFO: observed ReplicaSet test-rs in namespace replicaset-7928 with ReadyReplicas 1, AvailableReplicas 1
    Jun  8 16:30:07.248: INFO: observed ReplicaSet test-rs in namespace replicaset-7928 with ReadyReplicas 1, AvailableReplicas 1
    Jun  8 16:30:08.161: INFO: observed ReplicaSet test-rs in namespace replicaset-7928 with ReadyReplicas 2, AvailableReplicas 2
    Jun  8 16:30:08.574: INFO: observed Replicaset test-rs in namespace replicaset-7928 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:30:08.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7928" for this suite. 06/08/23 16:30:08.581
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:30:08.59
Jun  8 16:30:08.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubectl 06/08/23 16:30:08.591
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:30:08.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:30:08.61
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 06/08/23 16:30:08.614
Jun  8 16:30:08.614: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-5250 proxy --unix-socket=/tmp/kubectl-proxy-unix3434974398/test'
STEP: retrieving proxy /api/ output 06/08/23 16:30:08.686
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  8 16:30:08.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5250" for this suite. 06/08/23 16:30:08.694
------------------------------
• [0.112 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:30:08.59
    Jun  8 16:30:08.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubectl 06/08/23 16:30:08.591
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:30:08.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:30:08.61
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 06/08/23 16:30:08.614
    Jun  8 16:30:08.614: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-5250 proxy --unix-socket=/tmp/kubectl-proxy-unix3434974398/test'
    STEP: retrieving proxy /api/ output 06/08/23 16:30:08.686
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:30:08.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5250" for this suite. 06/08/23 16:30:08.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:30:08.702
Jun  8 16:30:08.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename configmap 06/08/23 16:30:08.703
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:30:08.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:30:08.722
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-47ffa517-f1dc-4e46-b0ed-397a0284a747 06/08/23 16:30:08.725
STEP: Creating a pod to test consume configMaps 06/08/23 16:30:08.731
Jun  8 16:30:08.741: INFO: Waiting up to 5m0s for pod "pod-configmaps-d52ef041-d7a1-4c03-be37-3a9adb3287d4" in namespace "configmap-43" to be "Succeeded or Failed"
Jun  8 16:30:08.745: INFO: Pod "pod-configmaps-d52ef041-d7a1-4c03-be37-3a9adb3287d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053528ms
Jun  8 16:30:10.753: INFO: Pod "pod-configmaps-d52ef041-d7a1-4c03-be37-3a9adb3287d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012488289s
Jun  8 16:30:12.750: INFO: Pod "pod-configmaps-d52ef041-d7a1-4c03-be37-3a9adb3287d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009344423s
STEP: Saw pod success 06/08/23 16:30:12.75
Jun  8 16:30:12.750: INFO: Pod "pod-configmaps-d52ef041-d7a1-4c03-be37-3a9adb3287d4" satisfied condition "Succeeded or Failed"
Jun  8 16:30:12.754: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-configmaps-d52ef041-d7a1-4c03-be37-3a9adb3287d4 container agnhost-container: <nil>
STEP: delete the pod 06/08/23 16:30:12.772
Jun  8 16:30:12.788: INFO: Waiting for pod pod-configmaps-d52ef041-d7a1-4c03-be37-3a9adb3287d4 to disappear
Jun  8 16:30:12.792: INFO: Pod pod-configmaps-d52ef041-d7a1-4c03-be37-3a9adb3287d4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  8 16:30:12.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-43" for this suite. 06/08/23 16:30:12.798
------------------------------
• [4.104 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:30:08.702
    Jun  8 16:30:08.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename configmap 06/08/23 16:30:08.703
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:30:08.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:30:08.722
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-47ffa517-f1dc-4e46-b0ed-397a0284a747 06/08/23 16:30:08.725
    STEP: Creating a pod to test consume configMaps 06/08/23 16:30:08.731
    Jun  8 16:30:08.741: INFO: Waiting up to 5m0s for pod "pod-configmaps-d52ef041-d7a1-4c03-be37-3a9adb3287d4" in namespace "configmap-43" to be "Succeeded or Failed"
    Jun  8 16:30:08.745: INFO: Pod "pod-configmaps-d52ef041-d7a1-4c03-be37-3a9adb3287d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053528ms
    Jun  8 16:30:10.753: INFO: Pod "pod-configmaps-d52ef041-d7a1-4c03-be37-3a9adb3287d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012488289s
    Jun  8 16:30:12.750: INFO: Pod "pod-configmaps-d52ef041-d7a1-4c03-be37-3a9adb3287d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009344423s
    STEP: Saw pod success 06/08/23 16:30:12.75
    Jun  8 16:30:12.750: INFO: Pod "pod-configmaps-d52ef041-d7a1-4c03-be37-3a9adb3287d4" satisfied condition "Succeeded or Failed"
    Jun  8 16:30:12.754: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-configmaps-d52ef041-d7a1-4c03-be37-3a9adb3287d4 container agnhost-container: <nil>
    STEP: delete the pod 06/08/23 16:30:12.772
    Jun  8 16:30:12.788: INFO: Waiting for pod pod-configmaps-d52ef041-d7a1-4c03-be37-3a9adb3287d4 to disappear
    Jun  8 16:30:12.792: INFO: Pod pod-configmaps-d52ef041-d7a1-4c03-be37-3a9adb3287d4 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:30:12.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-43" for this suite. 06/08/23 16:30:12.798
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:30:12.806
Jun  8 16:30:12.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename container-probe 06/08/23 16:30:12.808
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:30:12.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:30:12.829
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-cb3d8acb-e386-43e0-9508-8cda5c5f9ac0 in namespace container-probe-2656 06/08/23 16:30:12.833
Jun  8 16:30:12.844: INFO: Waiting up to 5m0s for pod "test-webserver-cb3d8acb-e386-43e0-9508-8cda5c5f9ac0" in namespace "container-probe-2656" to be "not pending"
Jun  8 16:30:12.849: INFO: Pod "test-webserver-cb3d8acb-e386-43e0-9508-8cda5c5f9ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.959231ms
Jun  8 16:30:14.855: INFO: Pod "test-webserver-cb3d8acb-e386-43e0-9508-8cda5c5f9ac0": Phase="Running", Reason="", readiness=true. Elapsed: 2.010398815s
Jun  8 16:30:14.855: INFO: Pod "test-webserver-cb3d8acb-e386-43e0-9508-8cda5c5f9ac0" satisfied condition "not pending"
Jun  8 16:30:14.855: INFO: Started pod test-webserver-cb3d8acb-e386-43e0-9508-8cda5c5f9ac0 in namespace container-probe-2656
STEP: checking the pod's current state and verifying that restartCount is present 06/08/23 16:30:14.855
Jun  8 16:30:14.859: INFO: Initial restart count of pod test-webserver-cb3d8acb-e386-43e0-9508-8cda5c5f9ac0 is 0
STEP: deleting the pod 06/08/23 16:34:15.561
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun  8 16:34:15.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2656" for this suite. 06/08/23 16:34:15.586
------------------------------
• [SLOW TEST] [242.789 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:30:12.806
    Jun  8 16:30:12.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename container-probe 06/08/23 16:30:12.808
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:30:12.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:30:12.829
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-cb3d8acb-e386-43e0-9508-8cda5c5f9ac0 in namespace container-probe-2656 06/08/23 16:30:12.833
    Jun  8 16:30:12.844: INFO: Waiting up to 5m0s for pod "test-webserver-cb3d8acb-e386-43e0-9508-8cda5c5f9ac0" in namespace "container-probe-2656" to be "not pending"
    Jun  8 16:30:12.849: INFO: Pod "test-webserver-cb3d8acb-e386-43e0-9508-8cda5c5f9ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.959231ms
    Jun  8 16:30:14.855: INFO: Pod "test-webserver-cb3d8acb-e386-43e0-9508-8cda5c5f9ac0": Phase="Running", Reason="", readiness=true. Elapsed: 2.010398815s
    Jun  8 16:30:14.855: INFO: Pod "test-webserver-cb3d8acb-e386-43e0-9508-8cda5c5f9ac0" satisfied condition "not pending"
    Jun  8 16:30:14.855: INFO: Started pod test-webserver-cb3d8acb-e386-43e0-9508-8cda5c5f9ac0 in namespace container-probe-2656
    STEP: checking the pod's current state and verifying that restartCount is present 06/08/23 16:30:14.855
    Jun  8 16:30:14.859: INFO: Initial restart count of pod test-webserver-cb3d8acb-e386-43e0-9508-8cda5c5f9ac0 is 0
    STEP: deleting the pod 06/08/23 16:34:15.561
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:34:15.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2656" for this suite. 06/08/23 16:34:15.586
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:34:15.598
Jun  8 16:34:15.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename controllerrevisions 06/08/23 16:34:15.599
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:34:15.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:34:15.62
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-vt9qm-daemon-set" 06/08/23 16:34:15.655
STEP: Check that daemon pods launch on every node of the cluster. 06/08/23 16:34:15.662
Jun  8 16:34:15.676: INFO: Number of nodes with available pods controlled by daemonset e2e-vt9qm-daemon-set: 0
Jun  8 16:34:15.676: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
Jun  8 16:34:16.689: INFO: Number of nodes with available pods controlled by daemonset e2e-vt9qm-daemon-set: 0
Jun  8 16:34:16.689: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
Jun  8 16:34:17.688: INFO: Number of nodes with available pods controlled by daemonset e2e-vt9qm-daemon-set: 5
Jun  8 16:34:17.688: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset e2e-vt9qm-daemon-set
STEP: Confirm DaemonSet "e2e-vt9qm-daemon-set" successfully created with "daemonset-name=e2e-vt9qm-daemon-set" label 06/08/23 16:34:17.691
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-vt9qm-daemon-set" 06/08/23 16:34:17.699
Jun  8 16:34:17.704: INFO: Located ControllerRevision: "e2e-vt9qm-daemon-set-6494d8975b"
STEP: Patching ControllerRevision "e2e-vt9qm-daemon-set-6494d8975b" 06/08/23 16:34:17.707
Jun  8 16:34:17.714: INFO: e2e-vt9qm-daemon-set-6494d8975b has been patched
STEP: Create a new ControllerRevision 06/08/23 16:34:17.714
Jun  8 16:34:17.720: INFO: Created ControllerRevision: e2e-vt9qm-daemon-set-7cd9d4c5db
STEP: Confirm that there are two ControllerRevisions 06/08/23 16:34:17.72
Jun  8 16:34:17.720: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun  8 16:34:17.724: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-vt9qm-daemon-set-6494d8975b" 06/08/23 16:34:17.724
STEP: Confirm that there is only one ControllerRevision 06/08/23 16:34:17.731
Jun  8 16:34:17.731: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun  8 16:34:17.735: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-vt9qm-daemon-set-7cd9d4c5db" 06/08/23 16:34:17.738
Jun  8 16:34:17.748: INFO: e2e-vt9qm-daemon-set-7cd9d4c5db has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 06/08/23 16:34:17.748
W0608 16:34:17.758575      23 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 06/08/23 16:34:17.758
Jun  8 16:34:17.758: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun  8 16:34:18.762: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun  8 16:34:18.766: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-vt9qm-daemon-set-7cd9d4c5db=updated" 06/08/23 16:34:18.766
STEP: Confirm that there is only one ControllerRevision 06/08/23 16:34:18.776
Jun  8 16:34:18.776: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun  8 16:34:18.780: INFO: Found 1 ControllerRevisions
Jun  8 16:34:18.783: INFO: ControllerRevision "e2e-vt9qm-daemon-set-86574bbf75" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-vt9qm-daemon-set" 06/08/23 16:34:18.787
STEP: deleting DaemonSet.extensions e2e-vt9qm-daemon-set in namespace controllerrevisions-4004, will wait for the garbage collector to delete the pods 06/08/23 16:34:18.787
Jun  8 16:34:18.849: INFO: Deleting DaemonSet.extensions e2e-vt9qm-daemon-set took: 8.138777ms
Jun  8 16:34:18.950: INFO: Terminating DaemonSet.extensions e2e-vt9qm-daemon-set pods took: 100.573101ms
Jun  8 16:34:20.355: INFO: Number of nodes with available pods controlled by daemonset e2e-vt9qm-daemon-set: 0
Jun  8 16:34:20.355: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-vt9qm-daemon-set
Jun  8 16:34:20.358: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"59392"},"items":null}

Jun  8 16:34:20.362: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"59392"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:34:20.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-4004" for this suite. 06/08/23 16:34:20.395
------------------------------
• [4.804 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:34:15.598
    Jun  8 16:34:15.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename controllerrevisions 06/08/23 16:34:15.599
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:34:15.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:34:15.62
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-vt9qm-daemon-set" 06/08/23 16:34:15.655
    STEP: Check that daemon pods launch on every node of the cluster. 06/08/23 16:34:15.662
    Jun  8 16:34:15.676: INFO: Number of nodes with available pods controlled by daemonset e2e-vt9qm-daemon-set: 0
    Jun  8 16:34:15.676: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
    Jun  8 16:34:16.689: INFO: Number of nodes with available pods controlled by daemonset e2e-vt9qm-daemon-set: 0
    Jun  8 16:34:16.689: INFO: Node chl8tf-control-plane-001 is running 0 daemon pod, expected 1
    Jun  8 16:34:17.688: INFO: Number of nodes with available pods controlled by daemonset e2e-vt9qm-daemon-set: 5
    Jun  8 16:34:17.688: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset e2e-vt9qm-daemon-set
    STEP: Confirm DaemonSet "e2e-vt9qm-daemon-set" successfully created with "daemonset-name=e2e-vt9qm-daemon-set" label 06/08/23 16:34:17.691
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-vt9qm-daemon-set" 06/08/23 16:34:17.699
    Jun  8 16:34:17.704: INFO: Located ControllerRevision: "e2e-vt9qm-daemon-set-6494d8975b"
    STEP: Patching ControllerRevision "e2e-vt9qm-daemon-set-6494d8975b" 06/08/23 16:34:17.707
    Jun  8 16:34:17.714: INFO: e2e-vt9qm-daemon-set-6494d8975b has been patched
    STEP: Create a new ControllerRevision 06/08/23 16:34:17.714
    Jun  8 16:34:17.720: INFO: Created ControllerRevision: e2e-vt9qm-daemon-set-7cd9d4c5db
    STEP: Confirm that there are two ControllerRevisions 06/08/23 16:34:17.72
    Jun  8 16:34:17.720: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun  8 16:34:17.724: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-vt9qm-daemon-set-6494d8975b" 06/08/23 16:34:17.724
    STEP: Confirm that there is only one ControllerRevision 06/08/23 16:34:17.731
    Jun  8 16:34:17.731: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun  8 16:34:17.735: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-vt9qm-daemon-set-7cd9d4c5db" 06/08/23 16:34:17.738
    Jun  8 16:34:17.748: INFO: e2e-vt9qm-daemon-set-7cd9d4c5db has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 06/08/23 16:34:17.748
    W0608 16:34:17.758575      23 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 06/08/23 16:34:17.758
    Jun  8 16:34:17.758: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun  8 16:34:18.762: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun  8 16:34:18.766: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-vt9qm-daemon-set-7cd9d4c5db=updated" 06/08/23 16:34:18.766
    STEP: Confirm that there is only one ControllerRevision 06/08/23 16:34:18.776
    Jun  8 16:34:18.776: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun  8 16:34:18.780: INFO: Found 1 ControllerRevisions
    Jun  8 16:34:18.783: INFO: ControllerRevision "e2e-vt9qm-daemon-set-86574bbf75" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-vt9qm-daemon-set" 06/08/23 16:34:18.787
    STEP: deleting DaemonSet.extensions e2e-vt9qm-daemon-set in namespace controllerrevisions-4004, will wait for the garbage collector to delete the pods 06/08/23 16:34:18.787
    Jun  8 16:34:18.849: INFO: Deleting DaemonSet.extensions e2e-vt9qm-daemon-set took: 8.138777ms
    Jun  8 16:34:18.950: INFO: Terminating DaemonSet.extensions e2e-vt9qm-daemon-set pods took: 100.573101ms
    Jun  8 16:34:20.355: INFO: Number of nodes with available pods controlled by daemonset e2e-vt9qm-daemon-set: 0
    Jun  8 16:34:20.355: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-vt9qm-daemon-set
    Jun  8 16:34:20.358: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"59392"},"items":null}

    Jun  8 16:34:20.362: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"59392"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:34:20.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-4004" for this suite. 06/08/23 16:34:20.395
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:34:20.403
Jun  8 16:34:20.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename replication-controller 06/08/23 16:34:20.404
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:34:20.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:34:20.421
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 06/08/23 16:34:20.424
STEP: When the matched label of one of its pods change 06/08/23 16:34:20.43
Jun  8 16:34:20.434: INFO: Pod name pod-release: Found 0 pods out of 1
Jun  8 16:34:25.439: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 06/08/23 16:34:25.451
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jun  8 16:34:26.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5197" for this suite. 06/08/23 16:34:26.468
------------------------------
• [SLOW TEST] [6.073 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:34:20.403
    Jun  8 16:34:20.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename replication-controller 06/08/23 16:34:20.404
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:34:20.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:34:20.421
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 06/08/23 16:34:20.424
    STEP: When the matched label of one of its pods change 06/08/23 16:34:20.43
    Jun  8 16:34:20.434: INFO: Pod name pod-release: Found 0 pods out of 1
    Jun  8 16:34:25.439: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 06/08/23 16:34:25.451
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:34:26.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5197" for this suite. 06/08/23 16:34:26.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:34:26.48
Jun  8 16:34:26.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename resourcequota 06/08/23 16:34:26.481
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:34:26.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:34:26.499
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 06/08/23 16:34:26.502
STEP: Creating a ResourceQuota 06/08/23 16:34:31.507
STEP: Ensuring resource quota status is calculated 06/08/23 16:34:31.513
STEP: Creating a Service 06/08/23 16:34:33.518
STEP: Creating a NodePort Service 06/08/23 16:34:33.543
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 06/08/23 16:34:33.583
STEP: Ensuring resource quota status captures service creation 06/08/23 16:34:33.629
STEP: Deleting Services 06/08/23 16:34:35.636
STEP: Ensuring resource quota status released usage 06/08/23 16:34:35.691
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  8 16:34:37.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3071" for this suite. 06/08/23 16:34:37.702
------------------------------
• [SLOW TEST] [11.230 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:34:26.48
    Jun  8 16:34:26.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename resourcequota 06/08/23 16:34:26.481
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:34:26.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:34:26.499
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 06/08/23 16:34:26.502
    STEP: Creating a ResourceQuota 06/08/23 16:34:31.507
    STEP: Ensuring resource quota status is calculated 06/08/23 16:34:31.513
    STEP: Creating a Service 06/08/23 16:34:33.518
    STEP: Creating a NodePort Service 06/08/23 16:34:33.543
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 06/08/23 16:34:33.583
    STEP: Ensuring resource quota status captures service creation 06/08/23 16:34:33.629
    STEP: Deleting Services 06/08/23 16:34:35.636
    STEP: Ensuring resource quota status released usage 06/08/23 16:34:35.691
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:34:37.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3071" for this suite. 06/08/23 16:34:37.702
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:34:37.71
Jun  8 16:34:37.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubectl 06/08/23 16:34:37.712
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:34:37.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:34:37.73
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 06/08/23 16:34:37.733
Jun  8 16:34:37.733: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8754 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 06/08/23 16:34:37.793
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  8 16:34:37.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8754" for this suite. 06/08/23 16:34:37.809
------------------------------
• [0.106 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:34:37.71
    Jun  8 16:34:37.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubectl 06/08/23 16:34:37.712
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:34:37.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:34:37.73
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 06/08/23 16:34:37.733
    Jun  8 16:34:37.733: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-8754 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 06/08/23 16:34:37.793
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:34:37.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8754" for this suite. 06/08/23 16:34:37.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:34:37.817
Jun  8 16:34:37.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename container-runtime 06/08/23 16:34:37.818
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:34:37.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:34:37.838
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 06/08/23 16:34:37.851
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 06/08/23 16:34:52.933
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 06/08/23 16:34:52.937
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 06/08/23 16:34:52.945
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 06/08/23 16:34:52.945
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 06/08/23 16:34:52.968
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 06/08/23 16:34:55.987
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 06/08/23 16:34:58.001
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 06/08/23 16:34:58.009
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 06/08/23 16:34:58.009
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 06/08/23 16:34:58.03
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 06/08/23 16:34:59.04
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 06/08/23 16:35:02.06
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 06/08/23 16:35:02.068
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 06/08/23 16:35:02.068
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jun  8 16:35:02.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5216" for this suite. 06/08/23 16:35:02.103
------------------------------
• [SLOW TEST] [24.293 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:34:37.817
    Jun  8 16:34:37.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename container-runtime 06/08/23 16:34:37.818
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:34:37.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:34:37.838
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 06/08/23 16:34:37.851
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 06/08/23 16:34:52.933
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 06/08/23 16:34:52.937
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 06/08/23 16:34:52.945
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 06/08/23 16:34:52.945
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 06/08/23 16:34:52.968
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 06/08/23 16:34:55.987
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 06/08/23 16:34:58.001
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 06/08/23 16:34:58.009
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 06/08/23 16:34:58.009
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 06/08/23 16:34:58.03
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 06/08/23 16:34:59.04
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 06/08/23 16:35:02.06
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 06/08/23 16:35:02.068
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 06/08/23 16:35:02.068
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:35:02.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5216" for this suite. 06/08/23 16:35:02.103
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:35:02.111
Jun  8 16:35:02.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename sched-preemption 06/08/23 16:35:02.112
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:35:02.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:35:02.13
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jun  8 16:35:02.150: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  8 16:36:02.197: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:36:02.201
Jun  8 16:36:02.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename sched-preemption-path 06/08/23 16:36:02.202
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:36:02.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:36:02.221
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 06/08/23 16:36:02.224
STEP: Trying to launch a pod without a label to get a node which can launch it. 06/08/23 16:36:02.224
Jun  8 16:36:02.233: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3213" to be "running"
Jun  8 16:36:02.237: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.838625ms
Jun  8 16:36:04.243: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009380981s
Jun  8 16:36:04.243: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 06/08/23 16:36:04.247
Jun  8 16:36:04.263: INFO: found a healthy node: chl8tf-worker-001
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Jun  8 16:36:10.349: INFO: pods created so far: [1 1 1]
Jun  8 16:36:10.349: INFO: length of pods created so far: 3
Jun  8 16:36:12.362: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Jun  8 16:36:19.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:36:19.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-3213" for this suite. 06/08/23 16:36:19.498
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9566" for this suite. 06/08/23 16:36:19.505
------------------------------
• [SLOW TEST] [77.402 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:35:02.111
    Jun  8 16:35:02.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename sched-preemption 06/08/23 16:35:02.112
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:35:02.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:35:02.13
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jun  8 16:35:02.150: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun  8 16:36:02.197: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:36:02.201
    Jun  8 16:36:02.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename sched-preemption-path 06/08/23 16:36:02.202
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:36:02.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:36:02.221
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 06/08/23 16:36:02.224
    STEP: Trying to launch a pod without a label to get a node which can launch it. 06/08/23 16:36:02.224
    Jun  8 16:36:02.233: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3213" to be "running"
    Jun  8 16:36:02.237: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.838625ms
    Jun  8 16:36:04.243: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009380981s
    Jun  8 16:36:04.243: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 06/08/23 16:36:04.247
    Jun  8 16:36:04.263: INFO: found a healthy node: chl8tf-worker-001
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Jun  8 16:36:10.349: INFO: pods created so far: [1 1 1]
    Jun  8 16:36:10.349: INFO: length of pods created so far: 3
    Jun  8 16:36:12.362: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:36:19.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:36:19.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-3213" for this suite. 06/08/23 16:36:19.498
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9566" for this suite. 06/08/23 16:36:19.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:36:19.513
Jun  8 16:36:19.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename sched-preemption 06/08/23 16:36:19.515
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:36:19.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:36:19.534
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jun  8 16:36:19.552: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  8 16:37:19.599: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 06/08/23 16:37:19.603
Jun  8 16:37:19.628: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jun  8 16:37:19.637: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jun  8 16:37:19.659: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jun  8 16:37:19.668: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jun  8 16:37:19.695: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jun  8 16:37:19.711: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Jun  8 16:37:19.752: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Jun  8 16:37:19.758: INFO: Created pod: pod3-1-sched-preemption-medium-priority
Jun  8 16:37:19.779: INFO: Created pod: pod4-0-sched-preemption-medium-priority
Jun  8 16:37:19.788: INFO: Created pod: pod4-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 06/08/23 16:37:19.788
Jun  8 16:37:19.788: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-830" to be "running"
Jun  8 16:37:19.792: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.16721ms
Jun  8 16:37:21.799: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.01060453s
Jun  8 16:37:21.799: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jun  8 16:37:21.799: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-830" to be "running"
Jun  8 16:37:21.803: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.835566ms
Jun  8 16:37:21.803: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jun  8 16:37:21.803: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-830" to be "running"
Jun  8 16:37:21.807: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.721614ms
Jun  8 16:37:21.807: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jun  8 16:37:21.807: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-830" to be "running"
Jun  8 16:37:21.810: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.49679ms
Jun  8 16:37:21.810: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jun  8 16:37:21.810: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-830" to be "running"
Jun  8 16:37:21.814: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.58271ms
Jun  8 16:37:21.814: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jun  8 16:37:21.814: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-830" to be "running"
Jun  8 16:37:21.817: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.631687ms
Jun  8 16:37:21.817: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Jun  8 16:37:21.817: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-830" to be "running"
Jun  8 16:37:21.821: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.487187ms
Jun  8 16:37:21.821: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Jun  8 16:37:21.821: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-830" to be "running"
Jun  8 16:37:21.827: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.465643ms
Jun  8 16:37:21.827: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
Jun  8 16:37:21.827: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-830" to be "running"
Jun  8 16:37:21.831: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.715253ms
Jun  8 16:37:21.831: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
Jun  8 16:37:21.831: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-830" to be "running"
Jun  8 16:37:21.835: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.491205ms
Jun  8 16:37:21.835: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 06/08/23 16:37:21.835
Jun  8 16:37:21.846: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jun  8 16:37:21.849: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.310099ms
Jun  8 16:37:23.855: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008814198s
Jun  8 16:37:25.856: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009848165s
Jun  8 16:37:27.856: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009789919s
Jun  8 16:37:27.856: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:37:27.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-830" for this suite. 06/08/23 16:37:27.995
------------------------------
• [SLOW TEST] [68.488 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:36:19.513
    Jun  8 16:36:19.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename sched-preemption 06/08/23 16:36:19.515
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:36:19.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:36:19.534
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jun  8 16:36:19.552: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun  8 16:37:19.599: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 06/08/23 16:37:19.603
    Jun  8 16:37:19.628: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jun  8 16:37:19.637: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jun  8 16:37:19.659: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jun  8 16:37:19.668: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jun  8 16:37:19.695: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jun  8 16:37:19.711: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Jun  8 16:37:19.752: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Jun  8 16:37:19.758: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    Jun  8 16:37:19.779: INFO: Created pod: pod4-0-sched-preemption-medium-priority
    Jun  8 16:37:19.788: INFO: Created pod: pod4-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 06/08/23 16:37:19.788
    Jun  8 16:37:19.788: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-830" to be "running"
    Jun  8 16:37:19.792: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.16721ms
    Jun  8 16:37:21.799: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.01060453s
    Jun  8 16:37:21.799: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jun  8 16:37:21.799: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-830" to be "running"
    Jun  8 16:37:21.803: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.835566ms
    Jun  8 16:37:21.803: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun  8 16:37:21.803: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-830" to be "running"
    Jun  8 16:37:21.807: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.721614ms
    Jun  8 16:37:21.807: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun  8 16:37:21.807: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-830" to be "running"
    Jun  8 16:37:21.810: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.49679ms
    Jun  8 16:37:21.810: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun  8 16:37:21.810: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-830" to be "running"
    Jun  8 16:37:21.814: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.58271ms
    Jun  8 16:37:21.814: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun  8 16:37:21.814: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-830" to be "running"
    Jun  8 16:37:21.817: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.631687ms
    Jun  8 16:37:21.817: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun  8 16:37:21.817: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-830" to be "running"
    Jun  8 16:37:21.821: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.487187ms
    Jun  8 16:37:21.821: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun  8 16:37:21.821: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-830" to be "running"
    Jun  8 16:37:21.827: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.465643ms
    Jun  8 16:37:21.827: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun  8 16:37:21.827: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-830" to be "running"
    Jun  8 16:37:21.831: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.715253ms
    Jun  8 16:37:21.831: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun  8 16:37:21.831: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-830" to be "running"
    Jun  8 16:37:21.835: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.491205ms
    Jun  8 16:37:21.835: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 06/08/23 16:37:21.835
    Jun  8 16:37:21.846: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jun  8 16:37:21.849: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.310099ms
    Jun  8 16:37:23.855: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008814198s
    Jun  8 16:37:25.856: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009848165s
    Jun  8 16:37:27.856: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009789919s
    Jun  8 16:37:27.856: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:37:27.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-830" for this suite. 06/08/23 16:37:27.995
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:37:28.004
Jun  8 16:37:28.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename webhook 06/08/23 16:37:28.005
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:37:28.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:37:28.024
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/08/23 16:37:28.042
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 16:37:28.621
STEP: Deploying the webhook pod 06/08/23 16:37:28.628
STEP: Wait for the deployment to be ready 06/08/23 16:37:28.642
Jun  8 16:37:28.652: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  8 16:37:30.665: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  8 16:37:32.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  8 16:37:34.671: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  8 16:37:36.671: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  8 16:37:38.670: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/08/23 16:37:40.671
STEP: Verifying the service has paired with the endpoint 06/08/23 16:37:40.687
Jun  8 16:37:41.688: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 06/08/23 16:37:41.693
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 06/08/23 16:37:41.695
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 06/08/23 16:37:41.695
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 06/08/23 16:37:41.695
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 06/08/23 16:37:41.697
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 06/08/23 16:37:41.697
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 06/08/23 16:37:41.698
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:37:41.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6910" for this suite. 06/08/23 16:37:41.768
STEP: Destroying namespace "webhook-6910-markers" for this suite. 06/08/23 16:37:41.779
------------------------------
• [SLOW TEST] [13.791 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:37:28.004
    Jun  8 16:37:28.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename webhook 06/08/23 16:37:28.005
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:37:28.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:37:28.024
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/08/23 16:37:28.042
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 16:37:28.621
    STEP: Deploying the webhook pod 06/08/23 16:37:28.628
    STEP: Wait for the deployment to be ready 06/08/23 16:37:28.642
    Jun  8 16:37:28.652: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jun  8 16:37:30.665: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  8 16:37:32.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  8 16:37:34.671: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  8 16:37:36.671: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  8 16:37:38.670: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 8, 16, 37, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/08/23 16:37:40.671
    STEP: Verifying the service has paired with the endpoint 06/08/23 16:37:40.687
    Jun  8 16:37:41.688: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 06/08/23 16:37:41.693
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 06/08/23 16:37:41.695
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 06/08/23 16:37:41.695
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 06/08/23 16:37:41.695
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 06/08/23 16:37:41.697
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 06/08/23 16:37:41.697
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 06/08/23 16:37:41.698
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:37:41.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6910" for this suite. 06/08/23 16:37:41.768
    STEP: Destroying namespace "webhook-6910-markers" for this suite. 06/08/23 16:37:41.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:37:41.8
Jun  8 16:37:41.800: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename endpointslice 06/08/23 16:37:41.802
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:37:41.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:37:41.83
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 06/08/23 16:37:46.954
STEP: referencing matching pods with named port 06/08/23 16:37:51.965
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 06/08/23 16:37:56.974
STEP: recreating EndpointSlices after they've been deleted 06/08/23 16:38:01.984
Jun  8 16:38:02.010: INFO: EndpointSlice for Service endpointslice-4240/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jun  8 16:38:12.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-4240" for this suite. 06/08/23 16:38:12.029
------------------------------
• [SLOW TEST] [30.237 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:37:41.8
    Jun  8 16:37:41.800: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename endpointslice 06/08/23 16:37:41.802
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:37:41.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:37:41.83
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 06/08/23 16:37:46.954
    STEP: referencing matching pods with named port 06/08/23 16:37:51.965
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 06/08/23 16:37:56.974
    STEP: recreating EndpointSlices after they've been deleted 06/08/23 16:38:01.984
    Jun  8 16:38:02.010: INFO: EndpointSlice for Service endpointslice-4240/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:38:12.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-4240" for this suite. 06/08/23 16:38:12.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:38:12.039
Jun  8 16:38:12.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename pod-network-test 06/08/23 16:38:12.04
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:38:12.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:38:12.059
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-7633 06/08/23 16:38:12.062
STEP: creating a selector 06/08/23 16:38:12.062
STEP: Creating the service pods in kubernetes 06/08/23 16:38:12.062
Jun  8 16:38:12.062: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun  8 16:38:12.119: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7633" to be "running and ready"
Jun  8 16:38:12.129: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.631931ms
Jun  8 16:38:12.129: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 16:38:14.135: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016450605s
Jun  8 16:38:14.135: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 16:38:16.136: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017045732s
Jun  8 16:38:16.136: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 16:38:18.137: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.018473621s
Jun  8 16:38:18.137: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 16:38:20.135: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016276072s
Jun  8 16:38:20.135: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 16:38:22.134: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015776233s
Jun  8 16:38:22.134: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 16:38:24.134: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.015473537s
Jun  8 16:38:24.134: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun  8 16:38:24.134: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun  8 16:38:24.138: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7633" to be "running and ready"
Jun  8 16:38:24.142: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.777633ms
Jun  8 16:38:24.142: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun  8 16:38:24.142: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun  8 16:38:24.145: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7633" to be "running and ready"
Jun  8 16:38:24.149: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.717419ms
Jun  8 16:38:24.149: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun  8 16:38:24.149: INFO: Pod "netserver-2" satisfied condition "running and ready"
Jun  8 16:38:24.153: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-7633" to be "running and ready"
Jun  8 16:38:24.156: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.419963ms
Jun  8 16:38:24.156: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Jun  8 16:38:24.156: INFO: Pod "netserver-3" satisfied condition "running and ready"
Jun  8 16:38:24.160: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-7633" to be "running and ready"
Jun  8 16:38:24.164: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.569395ms
Jun  8 16:38:24.164: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Jun  8 16:38:24.164: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 06/08/23 16:38:24.167
Jun  8 16:38:24.180: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7633" to be "running"
Jun  8 16:38:24.185: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.793141ms
Jun  8 16:38:26.190: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010136657s
Jun  8 16:38:26.190: INFO: Pod "test-container-pod" satisfied condition "running"
Jun  8 16:38:26.194: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7633" to be "running"
Jun  8 16:38:26.198: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.947223ms
Jun  8 16:38:26.198: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jun  8 16:38:26.202: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Jun  8 16:38:26.202: INFO: Going to poll 10.244.0.92 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jun  8 16:38:26.205: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.92 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7633 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 16:38:26.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 16:38:26.206: INFO: ExecWithOptions: Clientset creation
Jun  8 16:38:26.206: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7633/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.0.92+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun  8 16:38:27.296: INFO: Found all 1 expected endpoints: [netserver-0]
Jun  8 16:38:27.297: INFO: Going to poll 10.244.1.87 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jun  8 16:38:27.301: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.87 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7633 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 16:38:27.301: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 16:38:27.302: INFO: ExecWithOptions: Clientset creation
Jun  8 16:38:27.302: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7633/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.87+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun  8 16:38:28.386: INFO: Found all 1 expected endpoints: [netserver-1]
Jun  8 16:38:28.386: INFO: Going to poll 10.244.2.85 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jun  8 16:38:28.391: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.85 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7633 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 16:38:28.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 16:38:28.392: INFO: ExecWithOptions: Clientset creation
Jun  8 16:38:28.392: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7633/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.2.85+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun  8 16:38:29.477: INFO: Found all 1 expected endpoints: [netserver-2]
Jun  8 16:38:29.477: INFO: Going to poll 10.244.3.118 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jun  8 16:38:29.482: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.118 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7633 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 16:38:29.482: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 16:38:29.483: INFO: ExecWithOptions: Clientset creation
Jun  8 16:38:29.483: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7633/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.3.118+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun  8 16:38:30.570: INFO: Found all 1 expected endpoints: [netserver-3]
Jun  8 16:38:30.570: INFO: Going to poll 10.244.4.155 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jun  8 16:38:30.575: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.4.155 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7633 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 16:38:30.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 16:38:30.575: INFO: ExecWithOptions: Clientset creation
Jun  8 16:38:30.575: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7633/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.4.155+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun  8 16:38:31.651: INFO: Found all 1 expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jun  8 16:38:31.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7633" for this suite. 06/08/23 16:38:31.658
------------------------------
• [SLOW TEST] [19.627 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:38:12.039
    Jun  8 16:38:12.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename pod-network-test 06/08/23 16:38:12.04
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:38:12.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:38:12.059
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-7633 06/08/23 16:38:12.062
    STEP: creating a selector 06/08/23 16:38:12.062
    STEP: Creating the service pods in kubernetes 06/08/23 16:38:12.062
    Jun  8 16:38:12.062: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jun  8 16:38:12.119: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7633" to be "running and ready"
    Jun  8 16:38:12.129: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.631931ms
    Jun  8 16:38:12.129: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 16:38:14.135: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016450605s
    Jun  8 16:38:14.135: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 16:38:16.136: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017045732s
    Jun  8 16:38:16.136: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 16:38:18.137: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.018473621s
    Jun  8 16:38:18.137: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 16:38:20.135: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016276072s
    Jun  8 16:38:20.135: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 16:38:22.134: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015776233s
    Jun  8 16:38:22.134: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 16:38:24.134: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.015473537s
    Jun  8 16:38:24.134: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun  8 16:38:24.134: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun  8 16:38:24.138: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7633" to be "running and ready"
    Jun  8 16:38:24.142: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.777633ms
    Jun  8 16:38:24.142: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun  8 16:38:24.142: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun  8 16:38:24.145: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7633" to be "running and ready"
    Jun  8 16:38:24.149: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.717419ms
    Jun  8 16:38:24.149: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun  8 16:38:24.149: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Jun  8 16:38:24.153: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-7633" to be "running and ready"
    Jun  8 16:38:24.156: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.419963ms
    Jun  8 16:38:24.156: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Jun  8 16:38:24.156: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Jun  8 16:38:24.160: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-7633" to be "running and ready"
    Jun  8 16:38:24.164: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.569395ms
    Jun  8 16:38:24.164: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Jun  8 16:38:24.164: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 06/08/23 16:38:24.167
    Jun  8 16:38:24.180: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7633" to be "running"
    Jun  8 16:38:24.185: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.793141ms
    Jun  8 16:38:26.190: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010136657s
    Jun  8 16:38:26.190: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun  8 16:38:26.194: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7633" to be "running"
    Jun  8 16:38:26.198: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.947223ms
    Jun  8 16:38:26.198: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jun  8 16:38:26.202: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Jun  8 16:38:26.202: INFO: Going to poll 10.244.0.92 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Jun  8 16:38:26.205: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.92 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7633 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 16:38:26.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 16:38:26.206: INFO: ExecWithOptions: Clientset creation
    Jun  8 16:38:26.206: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7633/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.0.92+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun  8 16:38:27.296: INFO: Found all 1 expected endpoints: [netserver-0]
    Jun  8 16:38:27.297: INFO: Going to poll 10.244.1.87 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Jun  8 16:38:27.301: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.87 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7633 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 16:38:27.301: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 16:38:27.302: INFO: ExecWithOptions: Clientset creation
    Jun  8 16:38:27.302: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7633/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.87+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun  8 16:38:28.386: INFO: Found all 1 expected endpoints: [netserver-1]
    Jun  8 16:38:28.386: INFO: Going to poll 10.244.2.85 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Jun  8 16:38:28.391: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.85 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7633 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 16:38:28.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 16:38:28.392: INFO: ExecWithOptions: Clientset creation
    Jun  8 16:38:28.392: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7633/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.2.85+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun  8 16:38:29.477: INFO: Found all 1 expected endpoints: [netserver-2]
    Jun  8 16:38:29.477: INFO: Going to poll 10.244.3.118 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Jun  8 16:38:29.482: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.118 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7633 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 16:38:29.482: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 16:38:29.483: INFO: ExecWithOptions: Clientset creation
    Jun  8 16:38:29.483: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7633/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.3.118+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun  8 16:38:30.570: INFO: Found all 1 expected endpoints: [netserver-3]
    Jun  8 16:38:30.570: INFO: Going to poll 10.244.4.155 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Jun  8 16:38:30.575: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.4.155 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7633 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 16:38:30.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 16:38:30.575: INFO: ExecWithOptions: Clientset creation
    Jun  8 16:38:30.575: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7633/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.4.155+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun  8 16:38:31.651: INFO: Found all 1 expected endpoints: [netserver-4]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:38:31.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7633" for this suite. 06/08/23 16:38:31.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:38:31.667
Jun  8 16:38:31.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename downward-api 06/08/23 16:38:31.669
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:38:31.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:38:31.686
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 06/08/23 16:38:31.69
Jun  8 16:38:31.699: INFO: Waiting up to 5m0s for pod "labelsupdate840f2b9f-f8e8-46f7-80cd-de219e6a0280" in namespace "downward-api-2998" to be "running and ready"
Jun  8 16:38:31.703: INFO: Pod "labelsupdate840f2b9f-f8e8-46f7-80cd-de219e6a0280": Phase="Pending", Reason="", readiness=false. Elapsed: 3.691106ms
Jun  8 16:38:31.703: INFO: The phase of Pod labelsupdate840f2b9f-f8e8-46f7-80cd-de219e6a0280 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 16:38:33.708: INFO: Pod "labelsupdate840f2b9f-f8e8-46f7-80cd-de219e6a0280": Phase="Running", Reason="", readiness=true. Elapsed: 2.008887288s
Jun  8 16:38:33.708: INFO: The phase of Pod labelsupdate840f2b9f-f8e8-46f7-80cd-de219e6a0280 is Running (Ready = true)
Jun  8 16:38:33.708: INFO: Pod "labelsupdate840f2b9f-f8e8-46f7-80cd-de219e6a0280" satisfied condition "running and ready"
Jun  8 16:38:34.242: INFO: Successfully updated pod "labelsupdate840f2b9f-f8e8-46f7-80cd-de219e6a0280"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  8 16:38:38.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2998" for this suite. 06/08/23 16:38:38.275
------------------------------
• [SLOW TEST] [6.616 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:38:31.667
    Jun  8 16:38:31.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename downward-api 06/08/23 16:38:31.669
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:38:31.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:38:31.686
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 06/08/23 16:38:31.69
    Jun  8 16:38:31.699: INFO: Waiting up to 5m0s for pod "labelsupdate840f2b9f-f8e8-46f7-80cd-de219e6a0280" in namespace "downward-api-2998" to be "running and ready"
    Jun  8 16:38:31.703: INFO: Pod "labelsupdate840f2b9f-f8e8-46f7-80cd-de219e6a0280": Phase="Pending", Reason="", readiness=false. Elapsed: 3.691106ms
    Jun  8 16:38:31.703: INFO: The phase of Pod labelsupdate840f2b9f-f8e8-46f7-80cd-de219e6a0280 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 16:38:33.708: INFO: Pod "labelsupdate840f2b9f-f8e8-46f7-80cd-de219e6a0280": Phase="Running", Reason="", readiness=true. Elapsed: 2.008887288s
    Jun  8 16:38:33.708: INFO: The phase of Pod labelsupdate840f2b9f-f8e8-46f7-80cd-de219e6a0280 is Running (Ready = true)
    Jun  8 16:38:33.708: INFO: Pod "labelsupdate840f2b9f-f8e8-46f7-80cd-de219e6a0280" satisfied condition "running and ready"
    Jun  8 16:38:34.242: INFO: Successfully updated pod "labelsupdate840f2b9f-f8e8-46f7-80cd-de219e6a0280"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:38:38.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2998" for this suite. 06/08/23 16:38:38.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:38:38.284
Jun  8 16:38:38.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 16:38:38.285
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:38:38.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:38:38.304
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-a0ff8d64-4aba-43dc-99aa-824fa2428a7f 06/08/23 16:38:38.308
STEP: Creating a pod to test consume configMaps 06/08/23 16:38:38.313
Jun  8 16:38:38.325: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6bedde5a-6461-4ecf-b2ad-c0dad9d2dc52" in namespace "projected-4435" to be "Succeeded or Failed"
Jun  8 16:38:38.330: INFO: Pod "pod-projected-configmaps-6bedde5a-6461-4ecf-b2ad-c0dad9d2dc52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.29771ms
Jun  8 16:38:40.335: INFO: Pod "pod-projected-configmaps-6bedde5a-6461-4ecf-b2ad-c0dad9d2dc52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009809967s
Jun  8 16:38:42.335: INFO: Pod "pod-projected-configmaps-6bedde5a-6461-4ecf-b2ad-c0dad9d2dc52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009392501s
STEP: Saw pod success 06/08/23 16:38:42.335
Jun  8 16:38:42.335: INFO: Pod "pod-projected-configmaps-6bedde5a-6461-4ecf-b2ad-c0dad9d2dc52" satisfied condition "Succeeded or Failed"
Jun  8 16:38:42.339: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-configmaps-6bedde5a-6461-4ecf-b2ad-c0dad9d2dc52 container agnhost-container: <nil>
STEP: delete the pod 06/08/23 16:38:42.347
Jun  8 16:38:42.362: INFO: Waiting for pod pod-projected-configmaps-6bedde5a-6461-4ecf-b2ad-c0dad9d2dc52 to disappear
Jun  8 16:38:42.365: INFO: Pod pod-projected-configmaps-6bedde5a-6461-4ecf-b2ad-c0dad9d2dc52 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun  8 16:38:42.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4435" for this suite. 06/08/23 16:38:42.371
------------------------------
• [4.094 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:38:38.284
    Jun  8 16:38:38.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 16:38:38.285
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:38:38.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:38:38.304
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-a0ff8d64-4aba-43dc-99aa-824fa2428a7f 06/08/23 16:38:38.308
    STEP: Creating a pod to test consume configMaps 06/08/23 16:38:38.313
    Jun  8 16:38:38.325: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6bedde5a-6461-4ecf-b2ad-c0dad9d2dc52" in namespace "projected-4435" to be "Succeeded or Failed"
    Jun  8 16:38:38.330: INFO: Pod "pod-projected-configmaps-6bedde5a-6461-4ecf-b2ad-c0dad9d2dc52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.29771ms
    Jun  8 16:38:40.335: INFO: Pod "pod-projected-configmaps-6bedde5a-6461-4ecf-b2ad-c0dad9d2dc52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009809967s
    Jun  8 16:38:42.335: INFO: Pod "pod-projected-configmaps-6bedde5a-6461-4ecf-b2ad-c0dad9d2dc52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009392501s
    STEP: Saw pod success 06/08/23 16:38:42.335
    Jun  8 16:38:42.335: INFO: Pod "pod-projected-configmaps-6bedde5a-6461-4ecf-b2ad-c0dad9d2dc52" satisfied condition "Succeeded or Failed"
    Jun  8 16:38:42.339: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-projected-configmaps-6bedde5a-6461-4ecf-b2ad-c0dad9d2dc52 container agnhost-container: <nil>
    STEP: delete the pod 06/08/23 16:38:42.347
    Jun  8 16:38:42.362: INFO: Waiting for pod pod-projected-configmaps-6bedde5a-6461-4ecf-b2ad-c0dad9d2dc52 to disappear
    Jun  8 16:38:42.365: INFO: Pod pod-projected-configmaps-6bedde5a-6461-4ecf-b2ad-c0dad9d2dc52 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:38:42.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4435" for this suite. 06/08/23 16:38:42.371
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:38:42.379
Jun  8 16:38:42.379: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubectl 06/08/23 16:38:42.38
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:38:42.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:38:42.398
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/08/23 16:38:42.401
Jun  8 16:38:42.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-9122 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Jun  8 16:38:42.488: INFO: stderr: ""
Jun  8 16:38:42.488: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 06/08/23 16:38:42.488
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Jun  8 16:38:42.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-9122 delete pods e2e-test-httpd-pod'
Jun  8 16:38:45.055: INFO: stderr: ""
Jun  8 16:38:45.055: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  8 16:38:45.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9122" for this suite. 06/08/23 16:38:45.062
------------------------------
• [2.691 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:38:42.379
    Jun  8 16:38:42.379: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubectl 06/08/23 16:38:42.38
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:38:42.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:38:42.398
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/08/23 16:38:42.401
    Jun  8 16:38:42.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-9122 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Jun  8 16:38:42.488: INFO: stderr: ""
    Jun  8 16:38:42.488: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 06/08/23 16:38:42.488
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Jun  8 16:38:42.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-9122 delete pods e2e-test-httpd-pod'
    Jun  8 16:38:45.055: INFO: stderr: ""
    Jun  8 16:38:45.055: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:38:45.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9122" for this suite. 06/08/23 16:38:45.062
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:38:45.071
Jun  8 16:38:45.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename dns 06/08/23 16:38:45.072
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:38:45.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:38:45.09
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 06/08/23 16:38:45.094
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4190.svc.cluster.local;sleep 1; done
 06/08/23 16:38:45.099
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4190.svc.cluster.local;sleep 1; done
 06/08/23 16:38:45.099
STEP: creating a pod to probe DNS 06/08/23 16:38:45.099
STEP: submitting the pod to kubernetes 06/08/23 16:38:45.099
Jun  8 16:38:45.110: INFO: Waiting up to 15m0s for pod "dns-test-7bd34739-47da-4784-9c39-aa9c78d39130" in namespace "dns-4190" to be "running"
Jun  8 16:38:45.116: INFO: Pod "dns-test-7bd34739-47da-4784-9c39-aa9c78d39130": Phase="Pending", Reason="", readiness=false. Elapsed: 5.844593ms
Jun  8 16:38:47.121: INFO: Pod "dns-test-7bd34739-47da-4784-9c39-aa9c78d39130": Phase="Running", Reason="", readiness=true. Elapsed: 2.011182817s
Jun  8 16:38:47.121: INFO: Pod "dns-test-7bd34739-47da-4784-9c39-aa9c78d39130" satisfied condition "running"
STEP: retrieving the pod 06/08/23 16:38:47.121
STEP: looking for the results for each expected name from probers 06/08/23 16:38:47.125
Jun  8 16:38:47.131: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:38:47.136: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:38:47.140: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:38:47.144: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:38:47.149: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:38:47.154: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:38:47.159: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:38:47.164: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:38:47.164: INFO: Lookups using dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4190.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4190.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_udp@dns-test-service-2.dns-4190.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4190.svc.cluster.local]

Jun  8 16:38:52.171: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:38:52.176: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:38:52.189: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:38:52.193: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:38:52.201: INFO: Lookups using dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local]

Jun  8 16:38:57.171: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:38:57.175: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:38:57.188: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:38:57.192: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:38:57.201: INFO: Lookups using dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local]

Jun  8 16:39:02.170: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:39:02.175: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:39:02.188: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:39:02.192: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:39:02.201: INFO: Lookups using dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local]

Jun  8 16:39:07.170: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:39:07.174: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:39:07.187: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:39:07.191: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:39:07.200: INFO: Lookups using dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local]

Jun  8 16:39:12.172: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:39:12.178: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:39:12.192: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:39:12.197: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
Jun  8 16:39:12.206: INFO: Lookups using dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local]

Jun  8 16:39:17.202: INFO: DNS probes using dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130 succeeded

STEP: deleting the pod 06/08/23 16:39:17.202
STEP: deleting the test headless service 06/08/23 16:39:17.219
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun  8 16:39:17.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4190" for this suite. 06/08/23 16:39:17.249
------------------------------
• [SLOW TEST] [32.187 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:38:45.071
    Jun  8 16:38:45.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename dns 06/08/23 16:38:45.072
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:38:45.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:38:45.09
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 06/08/23 16:38:45.094
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4190.svc.cluster.local;sleep 1; done
     06/08/23 16:38:45.099
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4190.svc.cluster.local;sleep 1; done
     06/08/23 16:38:45.099
    STEP: creating a pod to probe DNS 06/08/23 16:38:45.099
    STEP: submitting the pod to kubernetes 06/08/23 16:38:45.099
    Jun  8 16:38:45.110: INFO: Waiting up to 15m0s for pod "dns-test-7bd34739-47da-4784-9c39-aa9c78d39130" in namespace "dns-4190" to be "running"
    Jun  8 16:38:45.116: INFO: Pod "dns-test-7bd34739-47da-4784-9c39-aa9c78d39130": Phase="Pending", Reason="", readiness=false. Elapsed: 5.844593ms
    Jun  8 16:38:47.121: INFO: Pod "dns-test-7bd34739-47da-4784-9c39-aa9c78d39130": Phase="Running", Reason="", readiness=true. Elapsed: 2.011182817s
    Jun  8 16:38:47.121: INFO: Pod "dns-test-7bd34739-47da-4784-9c39-aa9c78d39130" satisfied condition "running"
    STEP: retrieving the pod 06/08/23 16:38:47.121
    STEP: looking for the results for each expected name from probers 06/08/23 16:38:47.125
    Jun  8 16:38:47.131: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:38:47.136: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:38:47.140: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:38:47.144: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:38:47.149: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:38:47.154: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:38:47.159: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:38:47.164: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:38:47.164: INFO: Lookups using dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4190.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4190.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_udp@dns-test-service-2.dns-4190.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4190.svc.cluster.local]

    Jun  8 16:38:52.171: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:38:52.176: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:38:52.189: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:38:52.193: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:38:52.201: INFO: Lookups using dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local]

    Jun  8 16:38:57.171: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:38:57.175: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:38:57.188: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:38:57.192: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:38:57.201: INFO: Lookups using dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local]

    Jun  8 16:39:02.170: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:39:02.175: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:39:02.188: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:39:02.192: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:39:02.201: INFO: Lookups using dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local]

    Jun  8 16:39:07.170: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:39:07.174: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:39:07.187: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:39:07.191: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:39:07.200: INFO: Lookups using dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local]

    Jun  8 16:39:12.172: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:39:12.178: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:39:12.192: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:39:12.197: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local from pod dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130: the server could not find the requested resource (get pods dns-test-7bd34739-47da-4784-9c39-aa9c78d39130)
    Jun  8 16:39:12.206: INFO: Lookups using dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4190.svc.cluster.local]

    Jun  8 16:39:17.202: INFO: DNS probes using dns-4190/dns-test-7bd34739-47da-4784-9c39-aa9c78d39130 succeeded

    STEP: deleting the pod 06/08/23 16:39:17.202
    STEP: deleting the test headless service 06/08/23 16:39:17.219
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:39:17.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4190" for this suite. 06/08/23 16:39:17.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:39:17.259
Jun  8 16:39:17.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename container-probe 06/08/23 16:39:17.26
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:39:17.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:39:17.279
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Jun  8 16:39:17.291: INFO: Waiting up to 5m0s for pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3" in namespace "container-probe-9831" to be "running and ready"
Jun  8 16:39:17.297: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.347333ms
Jun  8 16:39:17.297: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 16:39:19.302: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 2.010860698s
Jun  8 16:39:19.302: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
Jun  8 16:39:21.302: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 4.010482957s
Jun  8 16:39:21.302: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
Jun  8 16:39:23.303: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 6.011716151s
Jun  8 16:39:23.303: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
Jun  8 16:39:25.302: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 8.010559294s
Jun  8 16:39:25.302: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
Jun  8 16:39:27.302: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 10.01086663s
Jun  8 16:39:27.302: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
Jun  8 16:39:29.303: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 12.011826056s
Jun  8 16:39:29.303: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
Jun  8 16:39:31.304: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 14.012350415s
Jun  8 16:39:31.304: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
Jun  8 16:39:33.304: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 16.012150164s
Jun  8 16:39:33.304: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
Jun  8 16:39:35.302: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 18.010910586s
Jun  8 16:39:35.302: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
Jun  8 16:39:37.303: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 20.011390484s
Jun  8 16:39:37.303: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
Jun  8 16:39:39.302: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 22.011079024s
Jun  8 16:39:39.302: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
Jun  8 16:39:41.304: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 24.01227476s
Jun  8 16:39:41.304: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
Jun  8 16:39:43.303: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 26.011139398s
Jun  8 16:39:43.303: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
Jun  8 16:39:45.303: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 28.01157051s
Jun  8 16:39:45.303: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
Jun  8 16:39:47.304: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 30.012403085s
Jun  8 16:39:47.304: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
Jun  8 16:39:49.302: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=true. Elapsed: 32.010722304s
Jun  8 16:39:49.302: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = true)
Jun  8 16:39:49.302: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3" satisfied condition "running and ready"
Jun  8 16:39:49.306: INFO: Container started at 2023-06-08 16:39:17 +0000 UTC, pod became ready at 2023-06-08 16:39:47 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun  8 16:39:49.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9831" for this suite. 06/08/23 16:39:49.313
------------------------------
• [SLOW TEST] [32.062 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:39:17.259
    Jun  8 16:39:17.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename container-probe 06/08/23 16:39:17.26
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:39:17.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:39:17.279
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Jun  8 16:39:17.291: INFO: Waiting up to 5m0s for pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3" in namespace "container-probe-9831" to be "running and ready"
    Jun  8 16:39:17.297: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.347333ms
    Jun  8 16:39:17.297: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 16:39:19.302: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 2.010860698s
    Jun  8 16:39:19.302: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
    Jun  8 16:39:21.302: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 4.010482957s
    Jun  8 16:39:21.302: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
    Jun  8 16:39:23.303: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 6.011716151s
    Jun  8 16:39:23.303: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
    Jun  8 16:39:25.302: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 8.010559294s
    Jun  8 16:39:25.302: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
    Jun  8 16:39:27.302: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 10.01086663s
    Jun  8 16:39:27.302: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
    Jun  8 16:39:29.303: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 12.011826056s
    Jun  8 16:39:29.303: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
    Jun  8 16:39:31.304: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 14.012350415s
    Jun  8 16:39:31.304: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
    Jun  8 16:39:33.304: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 16.012150164s
    Jun  8 16:39:33.304: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
    Jun  8 16:39:35.302: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 18.010910586s
    Jun  8 16:39:35.302: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
    Jun  8 16:39:37.303: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 20.011390484s
    Jun  8 16:39:37.303: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
    Jun  8 16:39:39.302: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 22.011079024s
    Jun  8 16:39:39.302: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
    Jun  8 16:39:41.304: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 24.01227476s
    Jun  8 16:39:41.304: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
    Jun  8 16:39:43.303: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 26.011139398s
    Jun  8 16:39:43.303: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
    Jun  8 16:39:45.303: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 28.01157051s
    Jun  8 16:39:45.303: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
    Jun  8 16:39:47.304: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=false. Elapsed: 30.012403085s
    Jun  8 16:39:47.304: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = false)
    Jun  8 16:39:49.302: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3": Phase="Running", Reason="", readiness=true. Elapsed: 32.010722304s
    Jun  8 16:39:49.302: INFO: The phase of Pod test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3 is Running (Ready = true)
    Jun  8 16:39:49.302: INFO: Pod "test-webserver-5a8cbff4-99d5-41fc-934f-396df0a47de3" satisfied condition "running and ready"
    Jun  8 16:39:49.306: INFO: Container started at 2023-06-08 16:39:17 +0000 UTC, pod became ready at 2023-06-08 16:39:47 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:39:49.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9831" for this suite. 06/08/23 16:39:49.313
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:39:49.321
Jun  8 16:39:49.321: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename services 06/08/23 16:39:49.322
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:39:49.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:39:49.341
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7431 06/08/23 16:39:49.344
STEP: changing the ExternalName service to type=NodePort 06/08/23 16:39:49.349
STEP: creating replication controller externalname-service in namespace services-7431 06/08/23 16:39:49.38
I0608 16:39:49.386817      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7431, replica count: 2
I0608 16:39:52.438416      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  8 16:39:52.438: INFO: Creating new exec pod
Jun  8 16:39:52.445: INFO: Waiting up to 5m0s for pod "execpodjfkd8" in namespace "services-7431" to be "running"
Jun  8 16:39:52.450: INFO: Pod "execpodjfkd8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.999834ms
Jun  8 16:39:54.455: INFO: Pod "execpodjfkd8": Phase="Running", Reason="", readiness=true. Elapsed: 2.010322556s
Jun  8 16:39:54.455: INFO: Pod "execpodjfkd8" satisfied condition "running"
Jun  8 16:39:55.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-7431 exec execpodjfkd8 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jun  8 16:39:55.628: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun  8 16:39:55.628: INFO: stdout: ""
Jun  8 16:39:55.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-7431 exec execpodjfkd8 -- /bin/sh -x -c nc -v -z -w 2 10.104.21.236 80'
Jun  8 16:39:55.797: INFO: stderr: "+ nc -v -z -w 2 10.104.21.236 80\nConnection to 10.104.21.236 80 port [tcp/http] succeeded!\n"
Jun  8 16:39:55.797: INFO: stdout: ""
Jun  8 16:39:55.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-7431 exec execpodjfkd8 -- /bin/sh -x -c nc -v -z -w 2 100.100.236.41 32414'
Jun  8 16:39:55.958: INFO: stderr: "+ nc -v -z -w 2 100.100.236.41 32414\nConnection to 100.100.236.41 32414 port [tcp/*] succeeded!\n"
Jun  8 16:39:55.958: INFO: stdout: ""
Jun  8 16:39:55.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-7431 exec execpodjfkd8 -- /bin/sh -x -c nc -v -z -w 2 100.100.237.235 32414'
Jun  8 16:39:56.128: INFO: stderr: "+ nc -v -z -w 2 100.100.237.235 32414\nConnection to 100.100.237.235 32414 port [tcp/*] succeeded!\n"
Jun  8 16:39:56.129: INFO: stdout: ""
Jun  8 16:39:56.129: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  8 16:39:56.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7431" for this suite. 06/08/23 16:39:56.179
------------------------------
• [SLOW TEST] [6.866 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:39:49.321
    Jun  8 16:39:49.321: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename services 06/08/23 16:39:49.322
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:39:49.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:39:49.341
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7431 06/08/23 16:39:49.344
    STEP: changing the ExternalName service to type=NodePort 06/08/23 16:39:49.349
    STEP: creating replication controller externalname-service in namespace services-7431 06/08/23 16:39:49.38
    I0608 16:39:49.386817      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7431, replica count: 2
    I0608 16:39:52.438416      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun  8 16:39:52.438: INFO: Creating new exec pod
    Jun  8 16:39:52.445: INFO: Waiting up to 5m0s for pod "execpodjfkd8" in namespace "services-7431" to be "running"
    Jun  8 16:39:52.450: INFO: Pod "execpodjfkd8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.999834ms
    Jun  8 16:39:54.455: INFO: Pod "execpodjfkd8": Phase="Running", Reason="", readiness=true. Elapsed: 2.010322556s
    Jun  8 16:39:54.455: INFO: Pod "execpodjfkd8" satisfied condition "running"
    Jun  8 16:39:55.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-7431 exec execpodjfkd8 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jun  8 16:39:55.628: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun  8 16:39:55.628: INFO: stdout: ""
    Jun  8 16:39:55.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-7431 exec execpodjfkd8 -- /bin/sh -x -c nc -v -z -w 2 10.104.21.236 80'
    Jun  8 16:39:55.797: INFO: stderr: "+ nc -v -z -w 2 10.104.21.236 80\nConnection to 10.104.21.236 80 port [tcp/http] succeeded!\n"
    Jun  8 16:39:55.797: INFO: stdout: ""
    Jun  8 16:39:55.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-7431 exec execpodjfkd8 -- /bin/sh -x -c nc -v -z -w 2 100.100.236.41 32414'
    Jun  8 16:39:55.958: INFO: stderr: "+ nc -v -z -w 2 100.100.236.41 32414\nConnection to 100.100.236.41 32414 port [tcp/*] succeeded!\n"
    Jun  8 16:39:55.958: INFO: stdout: ""
    Jun  8 16:39:55.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=services-7431 exec execpodjfkd8 -- /bin/sh -x -c nc -v -z -w 2 100.100.237.235 32414'
    Jun  8 16:39:56.128: INFO: stderr: "+ nc -v -z -w 2 100.100.237.235 32414\nConnection to 100.100.237.235 32414 port [tcp/*] succeeded!\n"
    Jun  8 16:39:56.129: INFO: stdout: ""
    Jun  8 16:39:56.129: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:39:56.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7431" for this suite. 06/08/23 16:39:56.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:39:56.188
Jun  8 16:39:56.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename dns 06/08/23 16:39:56.19
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:39:56.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:39:56.213
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 06/08/23 16:39:56.218
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5382.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5382.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 06/08/23 16:39:56.225
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5382.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5382.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 06/08/23 16:39:56.225
STEP: creating a pod to probe DNS 06/08/23 16:39:56.225
STEP: submitting the pod to kubernetes 06/08/23 16:39:56.225
Jun  8 16:39:56.240: INFO: Waiting up to 15m0s for pod "dns-test-0812478d-6b83-4926-a778-f93cb6aedea4" in namespace "dns-5382" to be "running"
Jun  8 16:39:56.247: INFO: Pod "dns-test-0812478d-6b83-4926-a778-f93cb6aedea4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.049666ms
Jun  8 16:39:58.253: INFO: Pod "dns-test-0812478d-6b83-4926-a778-f93cb6aedea4": Phase="Running", Reason="", readiness=true. Elapsed: 2.013119894s
Jun  8 16:39:58.253: INFO: Pod "dns-test-0812478d-6b83-4926-a778-f93cb6aedea4" satisfied condition "running"
STEP: retrieving the pod 06/08/23 16:39:58.253
STEP: looking for the results for each expected name from probers 06/08/23 16:39:58.257
Jun  8 16:39:58.277: INFO: DNS probes using dns-5382/dns-test-0812478d-6b83-4926-a778-f93cb6aedea4 succeeded

STEP: deleting the pod 06/08/23 16:39:58.277
STEP: deleting the test headless service 06/08/23 16:39:58.296
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun  8 16:39:58.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5382" for this suite. 06/08/23 16:39:58.32
------------------------------
• [2.139 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:39:56.188
    Jun  8 16:39:56.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename dns 06/08/23 16:39:56.19
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:39:56.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:39:56.213
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 06/08/23 16:39:56.218
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5382.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5382.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     06/08/23 16:39:56.225
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5382.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5382.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     06/08/23 16:39:56.225
    STEP: creating a pod to probe DNS 06/08/23 16:39:56.225
    STEP: submitting the pod to kubernetes 06/08/23 16:39:56.225
    Jun  8 16:39:56.240: INFO: Waiting up to 15m0s for pod "dns-test-0812478d-6b83-4926-a778-f93cb6aedea4" in namespace "dns-5382" to be "running"
    Jun  8 16:39:56.247: INFO: Pod "dns-test-0812478d-6b83-4926-a778-f93cb6aedea4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.049666ms
    Jun  8 16:39:58.253: INFO: Pod "dns-test-0812478d-6b83-4926-a778-f93cb6aedea4": Phase="Running", Reason="", readiness=true. Elapsed: 2.013119894s
    Jun  8 16:39:58.253: INFO: Pod "dns-test-0812478d-6b83-4926-a778-f93cb6aedea4" satisfied condition "running"
    STEP: retrieving the pod 06/08/23 16:39:58.253
    STEP: looking for the results for each expected name from probers 06/08/23 16:39:58.257
    Jun  8 16:39:58.277: INFO: DNS probes using dns-5382/dns-test-0812478d-6b83-4926-a778-f93cb6aedea4 succeeded

    STEP: deleting the pod 06/08/23 16:39:58.277
    STEP: deleting the test headless service 06/08/23 16:39:58.296
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:39:58.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5382" for this suite. 06/08/23 16:39:58.32
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:39:58.329
Jun  8 16:39:58.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 16:39:58.33
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:39:58.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:39:58.347
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-c0ed1d2c-ff8c-4d3f-ae78-a630f87bf596 06/08/23 16:39:58.357
STEP: Creating configMap with name cm-test-opt-upd-0131d83d-d2f0-47e9-8b09-54a71f68e52b 06/08/23 16:39:58.362
STEP: Creating the pod 06/08/23 16:39:58.367
Jun  8 16:39:58.377: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-381239ce-064f-4424-aebe-344c87d63d66" in namespace "projected-7076" to be "running and ready"
Jun  8 16:39:58.381: INFO: Pod "pod-projected-configmaps-381239ce-064f-4424-aebe-344c87d63d66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.175894ms
Jun  8 16:39:58.381: INFO: The phase of Pod pod-projected-configmaps-381239ce-064f-4424-aebe-344c87d63d66 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 16:40:00.387: INFO: Pod "pod-projected-configmaps-381239ce-064f-4424-aebe-344c87d63d66": Phase="Running", Reason="", readiness=true. Elapsed: 2.009955421s
Jun  8 16:40:00.387: INFO: The phase of Pod pod-projected-configmaps-381239ce-064f-4424-aebe-344c87d63d66 is Running (Ready = true)
Jun  8 16:40:00.387: INFO: Pod "pod-projected-configmaps-381239ce-064f-4424-aebe-344c87d63d66" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-c0ed1d2c-ff8c-4d3f-ae78-a630f87bf596 06/08/23 16:40:00.414
STEP: Updating configmap cm-test-opt-upd-0131d83d-d2f0-47e9-8b09-54a71f68e52b 06/08/23 16:40:00.421
STEP: Creating configMap with name cm-test-opt-create-5e453131-69df-4f07-bec7-1a3496a03cbc 06/08/23 16:40:00.426
STEP: waiting to observe update in volume 06/08/23 16:40:00.431
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun  8 16:40:04.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7076" for this suite. 06/08/23 16:40:04.482
------------------------------
• [SLOW TEST] [6.161 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:39:58.329
    Jun  8 16:39:58.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 16:39:58.33
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:39:58.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:39:58.347
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-c0ed1d2c-ff8c-4d3f-ae78-a630f87bf596 06/08/23 16:39:58.357
    STEP: Creating configMap with name cm-test-opt-upd-0131d83d-d2f0-47e9-8b09-54a71f68e52b 06/08/23 16:39:58.362
    STEP: Creating the pod 06/08/23 16:39:58.367
    Jun  8 16:39:58.377: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-381239ce-064f-4424-aebe-344c87d63d66" in namespace "projected-7076" to be "running and ready"
    Jun  8 16:39:58.381: INFO: Pod "pod-projected-configmaps-381239ce-064f-4424-aebe-344c87d63d66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.175894ms
    Jun  8 16:39:58.381: INFO: The phase of Pod pod-projected-configmaps-381239ce-064f-4424-aebe-344c87d63d66 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 16:40:00.387: INFO: Pod "pod-projected-configmaps-381239ce-064f-4424-aebe-344c87d63d66": Phase="Running", Reason="", readiness=true. Elapsed: 2.009955421s
    Jun  8 16:40:00.387: INFO: The phase of Pod pod-projected-configmaps-381239ce-064f-4424-aebe-344c87d63d66 is Running (Ready = true)
    Jun  8 16:40:00.387: INFO: Pod "pod-projected-configmaps-381239ce-064f-4424-aebe-344c87d63d66" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-c0ed1d2c-ff8c-4d3f-ae78-a630f87bf596 06/08/23 16:40:00.414
    STEP: Updating configmap cm-test-opt-upd-0131d83d-d2f0-47e9-8b09-54a71f68e52b 06/08/23 16:40:00.421
    STEP: Creating configMap with name cm-test-opt-create-5e453131-69df-4f07-bec7-1a3496a03cbc 06/08/23 16:40:00.426
    STEP: waiting to observe update in volume 06/08/23 16:40:00.431
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:40:04.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7076" for this suite. 06/08/23 16:40:04.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:40:04.491
Jun  8 16:40:04.491: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename proxy 06/08/23 16:40:04.492
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:40:04.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:40:04.509
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jun  8 16:40:04.512: INFO: Creating pod...
Jun  8 16:40:04.521: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2444" to be "running"
Jun  8 16:40:04.525: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.700266ms
Jun  8 16:40:06.530: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.009008256s
Jun  8 16:40:06.531: INFO: Pod "agnhost" satisfied condition "running"
Jun  8 16:40:06.531: INFO: Creating service...
Jun  8 16:40:06.547: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/pods/agnhost/proxy?method=DELETE
Jun  8 16:40:06.555: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun  8 16:40:06.555: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/pods/agnhost/proxy?method=OPTIONS
Jun  8 16:40:06.561: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun  8 16:40:06.561: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/pods/agnhost/proxy?method=PATCH
Jun  8 16:40:06.566: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun  8 16:40:06.566: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/pods/agnhost/proxy?method=POST
Jun  8 16:40:06.572: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun  8 16:40:06.572: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/pods/agnhost/proxy?method=PUT
Jun  8 16:40:06.577: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun  8 16:40:06.577: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/services/e2e-proxy-test-service/proxy?method=DELETE
Jun  8 16:40:06.585: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun  8 16:40:06.585: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jun  8 16:40:06.593: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun  8 16:40:06.593: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/services/e2e-proxy-test-service/proxy?method=PATCH
Jun  8 16:40:06.603: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun  8 16:40:06.603: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/services/e2e-proxy-test-service/proxy?method=POST
Jun  8 16:40:06.611: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun  8 16:40:06.611: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/services/e2e-proxy-test-service/proxy?method=PUT
Jun  8 16:40:06.620: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun  8 16:40:06.620: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/pods/agnhost/proxy?method=GET
Jun  8 16:40:06.625: INFO: http.Client request:GET StatusCode:301
Jun  8 16:40:06.625: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/services/e2e-proxy-test-service/proxy?method=GET
Jun  8 16:40:06.632: INFO: http.Client request:GET StatusCode:301
Jun  8 16:40:06.633: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/pods/agnhost/proxy?method=HEAD
Jun  8 16:40:06.637: INFO: http.Client request:HEAD StatusCode:301
Jun  8 16:40:06.637: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/services/e2e-proxy-test-service/proxy?method=HEAD
Jun  8 16:40:06.645: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jun  8 16:40:06.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-2444" for this suite. 06/08/23 16:40:06.652
------------------------------
• [2.172 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:40:04.491
    Jun  8 16:40:04.491: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename proxy 06/08/23 16:40:04.492
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:40:04.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:40:04.509
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jun  8 16:40:04.512: INFO: Creating pod...
    Jun  8 16:40:04.521: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2444" to be "running"
    Jun  8 16:40:04.525: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.700266ms
    Jun  8 16:40:06.530: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.009008256s
    Jun  8 16:40:06.531: INFO: Pod "agnhost" satisfied condition "running"
    Jun  8 16:40:06.531: INFO: Creating service...
    Jun  8 16:40:06.547: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/pods/agnhost/proxy?method=DELETE
    Jun  8 16:40:06.555: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun  8 16:40:06.555: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/pods/agnhost/proxy?method=OPTIONS
    Jun  8 16:40:06.561: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun  8 16:40:06.561: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/pods/agnhost/proxy?method=PATCH
    Jun  8 16:40:06.566: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun  8 16:40:06.566: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/pods/agnhost/proxy?method=POST
    Jun  8 16:40:06.572: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun  8 16:40:06.572: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/pods/agnhost/proxy?method=PUT
    Jun  8 16:40:06.577: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jun  8 16:40:06.577: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/services/e2e-proxy-test-service/proxy?method=DELETE
    Jun  8 16:40:06.585: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun  8 16:40:06.585: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jun  8 16:40:06.593: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun  8 16:40:06.593: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/services/e2e-proxy-test-service/proxy?method=PATCH
    Jun  8 16:40:06.603: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun  8 16:40:06.603: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/services/e2e-proxy-test-service/proxy?method=POST
    Jun  8 16:40:06.611: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun  8 16:40:06.611: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/services/e2e-proxy-test-service/proxy?method=PUT
    Jun  8 16:40:06.620: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jun  8 16:40:06.620: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/pods/agnhost/proxy?method=GET
    Jun  8 16:40:06.625: INFO: http.Client request:GET StatusCode:301
    Jun  8 16:40:06.625: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/services/e2e-proxy-test-service/proxy?method=GET
    Jun  8 16:40:06.632: INFO: http.Client request:GET StatusCode:301
    Jun  8 16:40:06.633: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/pods/agnhost/proxy?method=HEAD
    Jun  8 16:40:06.637: INFO: http.Client request:HEAD StatusCode:301
    Jun  8 16:40:06.637: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2444/services/e2e-proxy-test-service/proxy?method=HEAD
    Jun  8 16:40:06.645: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:40:06.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-2444" for this suite. 06/08/23 16:40:06.652
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:40:06.664
Jun  8 16:40:06.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename sched-pred 06/08/23 16:40:06.666
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:40:06.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:40:06.69
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jun  8 16:40:06.694: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  8 16:40:06.710: INFO: Waiting for terminating namespaces to be deleted...
Jun  8 16:40:06.715: INFO: 
Logging pods the apiserver thinks is on node chl8tf-control-plane-001 before test
Jun  8 16:40:06.728: INFO: csi-oci-node-5p7f5 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
Jun  8 16:40:06.728: INFO: 	Container csi-node-registrar ready: true, restart count 0
Jun  8 16:40:06.728: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
Jun  8 16:40:06.728: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
Jun  8 16:40:06.728: INFO: etcd-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:04 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.729: INFO: 	Container etcd ready: true, restart count 0
Jun  8 16:40:06.729: INFO: kube-apiserver-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:21 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.729: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun  8 16:40:06.729: INFO: kube-controller-manager-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:21 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.729: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun  8 16:40:06.729: INFO: kube-flannel-ds-d5bvw from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.729: INFO: 	Container kube-flannel ready: true, restart count 0
Jun  8 16:40:06.729: INFO: kube-proxy-j2p7z from kube-system started at 2023-06-08 13:31:27 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.729: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  8 16:40:06.729: INFO: kube-scheduler-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:04 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.729: INFO: 	Container kube-scheduler ready: true, restart count 1
Jun  8 16:40:06.729: INFO: oci-cloud-controller-manager-9n2zj from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.729: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
Jun  8 16:40:06.729: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-wmj9x from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 16:40:06.729: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 16:40:06.729: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  8 16:40:06.729: INFO: 
Logging pods the apiserver thinks is on node chl8tf-control-plane-002 before test
Jun  8 16:40:06.749: INFO: coredns-55b8ccd764-56hlv from kube-system started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.749: INFO: 	Container coredns ready: true, restart count 0
Jun  8 16:40:06.749: INFO: coredns-55b8ccd764-jpqkc from kube-system started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.749: INFO: 	Container coredns ready: true, restart count 0
Jun  8 16:40:06.749: INFO: csi-oci-node-xbn7q from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
Jun  8 16:40:06.749: INFO: 	Container csi-node-registrar ready: true, restart count 0
Jun  8 16:40:06.749: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
Jun  8 16:40:06.749: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
Jun  8 16:40:06.749: INFO: etcd-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.749: INFO: 	Container etcd ready: true, restart count 0
Jun  8 16:40:06.749: INFO: kube-apiserver-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:10 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.749: INFO: 	Container kube-apiserver ready: true, restart count 1
Jun  8 16:40:06.749: INFO: kube-controller-manager-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.749: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun  8 16:40:06.749: INFO: kube-flannel-ds-6fgbp from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.749: INFO: 	Container kube-flannel ready: true, restart count 0
Jun  8 16:40:06.749: INFO: kube-proxy-cgwfj from kube-system started at 2023-06-08 13:32:05 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.749: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  8 16:40:06.749: INFO: kube-scheduler-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.749: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun  8 16:40:06.749: INFO: oci-cloud-controller-manager-dpkmx from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.749: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
Jun  8 16:40:06.749: INFO: kubernetes-dashboard-8c85c4f9-9l4pq from kubernetes-dashboard started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.749: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jun  8 16:40:06.749: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-74mph from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 16:40:06.749: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 16:40:06.749: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  8 16:40:06.749: INFO: 
Logging pods the apiserver thinks is on node chl8tf-control-plane-003 before test
Jun  8 16:40:06.761: INFO: csi-oci-node-6zdfs from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
Jun  8 16:40:06.761: INFO: 	Container csi-node-registrar ready: true, restart count 0
Jun  8 16:40:06.761: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
Jun  8 16:40:06.761: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
Jun  8 16:40:06.761: INFO: etcd-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.761: INFO: 	Container etcd ready: true, restart count 0
Jun  8 16:40:06.761: INFO: kube-apiserver-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.761: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun  8 16:40:06.761: INFO: kube-controller-manager-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.761: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun  8 16:40:06.761: INFO: kube-flannel-ds-qf6c6 from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.761: INFO: 	Container kube-flannel ready: true, restart count 0
Jun  8 16:40:06.761: INFO: kube-proxy-9kw5t from kube-system started at 2023-06-08 13:33:04 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.761: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  8 16:40:06.762: INFO: kube-scheduler-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.762: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun  8 16:40:06.762: INFO: oci-cloud-controller-manager-2sdzm from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.762: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
Jun  8 16:40:06.762: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-jpq4c from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 16:40:06.762: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 16:40:06.762: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  8 16:40:06.762: INFO: 
Logging pods the apiserver thinks is on node chl8tf-worker-001 before test
Jun  8 16:40:06.776: INFO: csi-oci-node-7ww4x from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
Jun  8 16:40:06.776: INFO: 	Container csi-node-registrar ready: true, restart count 0
Jun  8 16:40:06.776: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
Jun  8 16:40:06.776: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
Jun  8 16:40:06.776: INFO: kube-flannel-ds-vg6nz from kube-system started at 2023-06-08 16:01:36 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.776: INFO: 	Container kube-flannel ready: true, restart count 0
Jun  8 16:40:06.776: INFO: kube-proxy-6kfv2 from kube-system started at 2023-06-08 13:33:27 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.776: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  8 16:40:06.776: INFO: oci-cloud-controller-manager-fchj2 from kube-system started at 2023-06-08 16:01:36 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.776: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
Jun  8 16:40:06.776: INFO: pod-projected-configmaps-381239ce-064f-4424-aebe-344c87d63d66 from projected-7076 started at 2023-06-08 16:39:58 +0000 UTC (3 container statuses recorded)
Jun  8 16:40:06.776: INFO: 	Container createcm-volume-test ready: true, restart count 0
Jun  8 16:40:06.776: INFO: 	Container delcm-volume-test ready: true, restart count 0
Jun  8 16:40:06.776: INFO: 	Container updcm-volume-test ready: true, restart count 0
Jun  8 16:40:06.776: INFO: sonobuoy from sonobuoy started at 2023-06-08 15:05:13 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.776: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  8 16:40:06.776: INFO: sonobuoy-e2e-job-e329b7fb80aa4b40 from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 16:40:06.776: INFO: 	Container e2e ready: true, restart count 0
Jun  8 16:40:06.776: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 16:40:06.776: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-rs4qz from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 16:40:06.776: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 16:40:06.776: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  8 16:40:06.776: INFO: 
Logging pods the apiserver thinks is on node chl8tf-worker-002 before test
Jun  8 16:40:06.791: INFO: csi-oci-controller-69f8b488fc-n8th6 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (5 container statuses recorded)
Jun  8 16:40:06.791: INFO: 	Container csi-attacher ready: true, restart count 1
Jun  8 16:40:06.791: INFO: 	Container csi-fss-volume-provisioner ready: true, restart count 1
Jun  8 16:40:06.791: INFO: 	Container csi-resizer ready: true, restart count 0
Jun  8 16:40:06.791: INFO: 	Container csi-volume-provisioner ready: true, restart count 0
Jun  8 16:40:06.791: INFO: 	Container oci-csi-controller-driver ready: true, restart count 0
Jun  8 16:40:06.791: INFO: csi-oci-node-thcvn from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
Jun  8 16:40:06.791: INFO: 	Container csi-node-registrar ready: true, restart count 0
Jun  8 16:40:06.791: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
Jun  8 16:40:06.791: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
Jun  8 16:40:06.791: INFO: kube-flannel-ds-74q2b from kube-system started at 2023-06-08 13:33:47 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.791: INFO: 	Container kube-flannel ready: true, restart count 0
Jun  8 16:40:06.791: INFO: kube-proxy-hjjpt from kube-system started at 2023-06-08 13:33:47 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.791: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  8 16:40:06.791: INFO: oci-cloud-controller-manager-lwnq4 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.791: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
Jun  8 16:40:06.791: INFO: agnhost from proxy-2444 started at 2023-06-08 16:40:04 +0000 UTC (1 container statuses recorded)
Jun  8 16:40:06.791: INFO: 	Container agnhost ready: true, restart count 0
Jun  8 16:40:06.791: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-6nv2x from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
Jun  8 16:40:06.791: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  8 16:40:06.791: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node chl8tf-control-plane-001 06/08/23 16:40:06.917
STEP: verifying the node has the label node chl8tf-control-plane-002 06/08/23 16:40:06.943
STEP: verifying the node has the label node chl8tf-control-plane-003 06/08/23 16:40:06.963
STEP: verifying the node has the label node chl8tf-worker-001 06/08/23 16:40:06.993
STEP: verifying the node has the label node chl8tf-worker-002 06/08/23 16:40:07.017
Jun  8 16:40:07.043: INFO: Pod coredns-55b8ccd764-56hlv requesting resource cpu=100m on Node chl8tf-control-plane-002
Jun  8 16:40:07.043: INFO: Pod coredns-55b8ccd764-jpqkc requesting resource cpu=100m on Node chl8tf-control-plane-002
Jun  8 16:40:07.043: INFO: Pod csi-oci-controller-69f8b488fc-n8th6 requesting resource cpu=0m on Node chl8tf-worker-002
Jun  8 16:40:07.043: INFO: Pod csi-oci-node-5p7f5 requesting resource cpu=0m on Node chl8tf-control-plane-001
Jun  8 16:40:07.043: INFO: Pod csi-oci-node-6zdfs requesting resource cpu=0m on Node chl8tf-control-plane-003
Jun  8 16:40:07.043: INFO: Pod csi-oci-node-7ww4x requesting resource cpu=0m on Node chl8tf-worker-001
Jun  8 16:40:07.043: INFO: Pod csi-oci-node-thcvn requesting resource cpu=0m on Node chl8tf-worker-002
Jun  8 16:40:07.043: INFO: Pod csi-oci-node-xbn7q requesting resource cpu=0m on Node chl8tf-control-plane-002
Jun  8 16:40:07.043: INFO: Pod etcd-chl8tf-control-plane-001 requesting resource cpu=100m on Node chl8tf-control-plane-001
Jun  8 16:40:07.043: INFO: Pod etcd-chl8tf-control-plane-002 requesting resource cpu=100m on Node chl8tf-control-plane-002
Jun  8 16:40:07.043: INFO: Pod etcd-chl8tf-control-plane-003 requesting resource cpu=100m on Node chl8tf-control-plane-003
Jun  8 16:40:07.043: INFO: Pod kube-apiserver-chl8tf-control-plane-001 requesting resource cpu=250m on Node chl8tf-control-plane-001
Jun  8 16:40:07.043: INFO: Pod kube-apiserver-chl8tf-control-plane-002 requesting resource cpu=250m on Node chl8tf-control-plane-002
Jun  8 16:40:07.043: INFO: Pod kube-apiserver-chl8tf-control-plane-003 requesting resource cpu=250m on Node chl8tf-control-plane-003
Jun  8 16:40:07.043: INFO: Pod kube-controller-manager-chl8tf-control-plane-001 requesting resource cpu=200m on Node chl8tf-control-plane-001
Jun  8 16:40:07.043: INFO: Pod kube-controller-manager-chl8tf-control-plane-002 requesting resource cpu=200m on Node chl8tf-control-plane-002
Jun  8 16:40:07.043: INFO: Pod kube-controller-manager-chl8tf-control-plane-003 requesting resource cpu=200m on Node chl8tf-control-plane-003
Jun  8 16:40:07.043: INFO: Pod kube-flannel-ds-6fgbp requesting resource cpu=100m on Node chl8tf-control-plane-002
Jun  8 16:40:07.043: INFO: Pod kube-flannel-ds-74q2b requesting resource cpu=100m on Node chl8tf-worker-002
Jun  8 16:40:07.043: INFO: Pod kube-flannel-ds-d5bvw requesting resource cpu=100m on Node chl8tf-control-plane-001
Jun  8 16:40:07.043: INFO: Pod kube-flannel-ds-qf6c6 requesting resource cpu=100m on Node chl8tf-control-plane-003
Jun  8 16:40:07.043: INFO: Pod kube-flannel-ds-vg6nz requesting resource cpu=100m on Node chl8tf-worker-001
Jun  8 16:40:07.043: INFO: Pod kube-proxy-6kfv2 requesting resource cpu=0m on Node chl8tf-worker-001
Jun  8 16:40:07.043: INFO: Pod kube-proxy-9kw5t requesting resource cpu=0m on Node chl8tf-control-plane-003
Jun  8 16:40:07.043: INFO: Pod kube-proxy-cgwfj requesting resource cpu=0m on Node chl8tf-control-plane-002
Jun  8 16:40:07.043: INFO: Pod kube-proxy-hjjpt requesting resource cpu=0m on Node chl8tf-worker-002
Jun  8 16:40:07.043: INFO: Pod kube-proxy-j2p7z requesting resource cpu=0m on Node chl8tf-control-plane-001
Jun  8 16:40:07.043: INFO: Pod kube-scheduler-chl8tf-control-plane-001 requesting resource cpu=100m on Node chl8tf-control-plane-001
Jun  8 16:40:07.043: INFO: Pod kube-scheduler-chl8tf-control-plane-002 requesting resource cpu=100m on Node chl8tf-control-plane-002
Jun  8 16:40:07.043: INFO: Pod kube-scheduler-chl8tf-control-plane-003 requesting resource cpu=100m on Node chl8tf-control-plane-003
Jun  8 16:40:07.043: INFO: Pod oci-cloud-controller-manager-2sdzm requesting resource cpu=0m on Node chl8tf-control-plane-003
Jun  8 16:40:07.043: INFO: Pod oci-cloud-controller-manager-9n2zj requesting resource cpu=0m on Node chl8tf-control-plane-001
Jun  8 16:40:07.043: INFO: Pod oci-cloud-controller-manager-dpkmx requesting resource cpu=0m on Node chl8tf-control-plane-002
Jun  8 16:40:07.043: INFO: Pod oci-cloud-controller-manager-fchj2 requesting resource cpu=0m on Node chl8tf-worker-001
Jun  8 16:40:07.043: INFO: Pod oci-cloud-controller-manager-lwnq4 requesting resource cpu=0m on Node chl8tf-worker-002
Jun  8 16:40:07.043: INFO: Pod kubernetes-dashboard-8c85c4f9-9l4pq requesting resource cpu=0m on Node chl8tf-control-plane-002
Jun  8 16:40:07.043: INFO: Pod pod-projected-configmaps-381239ce-064f-4424-aebe-344c87d63d66 requesting resource cpu=0m on Node chl8tf-worker-001
Jun  8 16:40:07.043: INFO: Pod agnhost requesting resource cpu=0m on Node chl8tf-worker-002
Jun  8 16:40:07.043: INFO: Pod sonobuoy requesting resource cpu=0m on Node chl8tf-worker-001
Jun  8 16:40:07.043: INFO: Pod sonobuoy-e2e-job-e329b7fb80aa4b40 requesting resource cpu=0m on Node chl8tf-worker-001
Jun  8 16:40:07.043: INFO: Pod sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-6nv2x requesting resource cpu=0m on Node chl8tf-worker-002
Jun  8 16:40:07.043: INFO: Pod sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-74mph requesting resource cpu=0m on Node chl8tf-control-plane-002
Jun  8 16:40:07.043: INFO: Pod sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-jpq4c requesting resource cpu=0m on Node chl8tf-control-plane-003
Jun  8 16:40:07.043: INFO: Pod sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-rs4qz requesting resource cpu=0m on Node chl8tf-worker-001
Jun  8 16:40:07.043: INFO: Pod sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-wmj9x requesting resource cpu=0m on Node chl8tf-control-plane-001
STEP: Starting Pods to consume most of the cluster CPU. 06/08/23 16:40:07.043
Jun  8 16:40:07.044: INFO: Creating a pod which consumes cpu=2275m on Node chl8tf-control-plane-001
Jun  8 16:40:07.057: INFO: Creating a pod which consumes cpu=2135m on Node chl8tf-control-plane-002
Jun  8 16:40:07.068: INFO: Creating a pod which consumes cpu=2275m on Node chl8tf-control-plane-003
Jun  8 16:40:07.081: INFO: Creating a pod which consumes cpu=2730m on Node chl8tf-worker-001
Jun  8 16:40:07.100: INFO: Creating a pod which consumes cpu=2730m on Node chl8tf-worker-002
Jun  8 16:40:07.118: INFO: Waiting up to 5m0s for pod "filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c" in namespace "sched-pred-9644" to be "running"
Jun  8 16:40:07.140: INFO: Pod "filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.939331ms
Jun  8 16:40:09.145: INFO: Pod "filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c": Phase="Running", Reason="", readiness=true. Elapsed: 2.027454492s
Jun  8 16:40:09.145: INFO: Pod "filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c" satisfied condition "running"
Jun  8 16:40:09.146: INFO: Waiting up to 5m0s for pod "filler-pod-e130400f-a2c0-41ca-a425-a183256cbb84" in namespace "sched-pred-9644" to be "running"
Jun  8 16:40:09.149: INFO: Pod "filler-pod-e130400f-a2c0-41ca-a425-a183256cbb84": Phase="Running", Reason="", readiness=true. Elapsed: 3.889276ms
Jun  8 16:40:09.149: INFO: Pod "filler-pod-e130400f-a2c0-41ca-a425-a183256cbb84" satisfied condition "running"
Jun  8 16:40:09.149: INFO: Waiting up to 5m0s for pod "filler-pod-36124d0c-03d5-4314-b9ef-567fe807c5c6" in namespace "sched-pred-9644" to be "running"
Jun  8 16:40:09.153: INFO: Pod "filler-pod-36124d0c-03d5-4314-b9ef-567fe807c5c6": Phase="Running", Reason="", readiness=true. Elapsed: 3.854897ms
Jun  8 16:40:09.153: INFO: Pod "filler-pod-36124d0c-03d5-4314-b9ef-567fe807c5c6" satisfied condition "running"
Jun  8 16:40:09.153: INFO: Waiting up to 5m0s for pod "filler-pod-3e6db3fe-a801-497b-9bc8-8b6604f483d5" in namespace "sched-pred-9644" to be "running"
Jun  8 16:40:09.157: INFO: Pod "filler-pod-3e6db3fe-a801-497b-9bc8-8b6604f483d5": Phase="Running", Reason="", readiness=true. Elapsed: 3.590264ms
Jun  8 16:40:09.157: INFO: Pod "filler-pod-3e6db3fe-a801-497b-9bc8-8b6604f483d5" satisfied condition "running"
Jun  8 16:40:09.157: INFO: Waiting up to 5m0s for pod "filler-pod-18a685d4-e91f-44a2-a3f5-9841679cf844" in namespace "sched-pred-9644" to be "running"
Jun  8 16:40:09.161: INFO: Pod "filler-pod-18a685d4-e91f-44a2-a3f5-9841679cf844": Phase="Running", Reason="", readiness=true. Elapsed: 3.790363ms
Jun  8 16:40:09.161: INFO: Pod "filler-pod-18a685d4-e91f-44a2-a3f5-9841679cf844" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 06/08/23 16:40:09.161
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c.1766bc88feb1f4f0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9644/filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c to chl8tf-control-plane-001] 06/08/23 16:40:09.166
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c.1766bc8921b8465c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/08/23 16:40:09.166
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c.1766bc893150951c], Reason = [Created], Message = [Created container filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c] 06/08/23 16:40:09.166
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c.1766bc893459f787], Reason = [Started], Message = [Started container filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c] 06/08/23 16:40:09.166
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-18a685d4-e91f-44a2-a3f5-9841679cf844.1766bc8901f8873e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9644/filler-pod-18a685d4-e91f-44a2-a3f5-9841679cf844 to chl8tf-worker-002] 06/08/23 16:40:09.167
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-18a685d4-e91f-44a2-a3f5-9841679cf844.1766bc8920a90afb], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/08/23 16:40:09.167
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-18a685d4-e91f-44a2-a3f5-9841679cf844.1766bc892dda8d43], Reason = [Created], Message = [Created container filler-pod-18a685d4-e91f-44a2-a3f5-9841679cf844] 06/08/23 16:40:09.167
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-18a685d4-e91f-44a2-a3f5-9841679cf844.1766bc892febaebd], Reason = [Started], Message = [Started container filler-pod-18a685d4-e91f-44a2-a3f5-9841679cf844] 06/08/23 16:40:09.167
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-36124d0c-03d5-4314-b9ef-567fe807c5c6.1766bc8900879640], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9644/filler-pod-36124d0c-03d5-4314-b9ef-567fe807c5c6 to chl8tf-control-plane-003] 06/08/23 16:40:09.167
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-36124d0c-03d5-4314-b9ef-567fe807c5c6.1766bc891f255007], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/08/23 16:40:09.167
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-36124d0c-03d5-4314-b9ef-567fe807c5c6.1766bc892cd86673], Reason = [Created], Message = [Created container filler-pod-36124d0c-03d5-4314-b9ef-567fe807c5c6] 06/08/23 16:40:09.167
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-36124d0c-03d5-4314-b9ef-567fe807c5c6.1766bc892f8d3bfc], Reason = [Started], Message = [Started container filler-pod-36124d0c-03d5-4314-b9ef-567fe807c5c6] 06/08/23 16:40:09.167
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3e6db3fe-a801-497b-9bc8-8b6604f483d5.1766bc890195d8b2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9644/filler-pod-3e6db3fe-a801-497b-9bc8-8b6604f483d5 to chl8tf-worker-001] 06/08/23 16:40:09.167
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3e6db3fe-a801-497b-9bc8-8b6604f483d5.1766bc8920f75719], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/08/23 16:40:09.168
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3e6db3fe-a801-497b-9bc8-8b6604f483d5.1766bc892dee5e33], Reason = [Created], Message = [Created container filler-pod-3e6db3fe-a801-497b-9bc8-8b6604f483d5] 06/08/23 16:40:09.168
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3e6db3fe-a801-497b-9bc8-8b6604f483d5.1766bc892fb4b4e0], Reason = [Started], Message = [Started container filler-pod-3e6db3fe-a801-497b-9bc8-8b6604f483d5] 06/08/23 16:40:09.168
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e130400f-a2c0-41ca-a425-a183256cbb84.1766bc88ffc13318], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9644/filler-pod-e130400f-a2c0-41ca-a425-a183256cbb84 to chl8tf-control-plane-002] 06/08/23 16:40:09.168
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e130400f-a2c0-41ca-a425-a183256cbb84.1766bc892226335e], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/08/23 16:40:09.168
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e130400f-a2c0-41ca-a425-a183256cbb84.1766bc89300633fa], Reason = [Created], Message = [Created container filler-pod-e130400f-a2c0-41ca-a425-a183256cbb84] 06/08/23 16:40:09.168
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e130400f-a2c0-41ca-a425-a183256cbb84.1766bc8936249487], Reason = [Started], Message = [Started container filler-pod-e130400f-a2c0-41ca-a425-a183256cbb84] 06/08/23 16:40:09.168
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1766bc897c0d0feb], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu. preemption: 0/5 nodes are available: 5 No preemption victims found for incoming pod..] 06/08/23 16:40:09.179
STEP: removing the label node off the node chl8tf-control-plane-002 06/08/23 16:40:10.181
STEP: verifying the node doesn't have the label node 06/08/23 16:40:10.198
STEP: removing the label node off the node chl8tf-control-plane-003 06/08/23 16:40:10.202
STEP: verifying the node doesn't have the label node 06/08/23 16:40:10.224
STEP: removing the label node off the node chl8tf-worker-001 06/08/23 16:40:10.231
STEP: verifying the node doesn't have the label node 06/08/23 16:40:10.256
STEP: removing the label node off the node chl8tf-worker-002 06/08/23 16:40:10.263
STEP: verifying the node doesn't have the label node 06/08/23 16:40:10.287
STEP: removing the label node off the node chl8tf-control-plane-001 06/08/23 16:40:10.3
STEP: verifying the node doesn't have the label node 06/08/23 16:40:10.332
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:40:10.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9644" for this suite. 06/08/23 16:40:10.356
------------------------------
• [3.713 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:40:06.664
    Jun  8 16:40:06.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename sched-pred 06/08/23 16:40:06.666
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:40:06.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:40:06.69
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jun  8 16:40:06.694: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun  8 16:40:06.710: INFO: Waiting for terminating namespaces to be deleted...
    Jun  8 16:40:06.715: INFO: 
    Logging pods the apiserver thinks is on node chl8tf-control-plane-001 before test
    Jun  8 16:40:06.728: INFO: csi-oci-node-5p7f5 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
    Jun  8 16:40:06.728: INFO: 	Container csi-node-registrar ready: true, restart count 0
    Jun  8 16:40:06.728: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
    Jun  8 16:40:06.728: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
    Jun  8 16:40:06.728: INFO: etcd-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:04 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.729: INFO: 	Container etcd ready: true, restart count 0
    Jun  8 16:40:06.729: INFO: kube-apiserver-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:21 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.729: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jun  8 16:40:06.729: INFO: kube-controller-manager-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:21 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.729: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jun  8 16:40:06.729: INFO: kube-flannel-ds-d5bvw from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.729: INFO: 	Container kube-flannel ready: true, restart count 0
    Jun  8 16:40:06.729: INFO: kube-proxy-j2p7z from kube-system started at 2023-06-08 13:31:27 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.729: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  8 16:40:06.729: INFO: kube-scheduler-chl8tf-control-plane-001 from kube-system started at 2023-06-08 13:31:04 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.729: INFO: 	Container kube-scheduler ready: true, restart count 1
    Jun  8 16:40:06.729: INFO: oci-cloud-controller-manager-9n2zj from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.729: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
    Jun  8 16:40:06.729: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-wmj9x from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 16:40:06.729: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 16:40:06.729: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  8 16:40:06.729: INFO: 
    Logging pods the apiserver thinks is on node chl8tf-control-plane-002 before test
    Jun  8 16:40:06.749: INFO: coredns-55b8ccd764-56hlv from kube-system started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.749: INFO: 	Container coredns ready: true, restart count 0
    Jun  8 16:40:06.749: INFO: coredns-55b8ccd764-jpqkc from kube-system started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.749: INFO: 	Container coredns ready: true, restart count 0
    Jun  8 16:40:06.749: INFO: csi-oci-node-xbn7q from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
    Jun  8 16:40:06.749: INFO: 	Container csi-node-registrar ready: true, restart count 0
    Jun  8 16:40:06.749: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
    Jun  8 16:40:06.749: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
    Jun  8 16:40:06.749: INFO: etcd-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.749: INFO: 	Container etcd ready: true, restart count 0
    Jun  8 16:40:06.749: INFO: kube-apiserver-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:10 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.749: INFO: 	Container kube-apiserver ready: true, restart count 1
    Jun  8 16:40:06.749: INFO: kube-controller-manager-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.749: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jun  8 16:40:06.749: INFO: kube-flannel-ds-6fgbp from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.749: INFO: 	Container kube-flannel ready: true, restart count 0
    Jun  8 16:40:06.749: INFO: kube-proxy-cgwfj from kube-system started at 2023-06-08 13:32:05 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.749: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  8 16:40:06.749: INFO: kube-scheduler-chl8tf-control-plane-002 from kube-system started at 2023-06-08 14:42:09 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.749: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jun  8 16:40:06.749: INFO: oci-cloud-controller-manager-dpkmx from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.749: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
    Jun  8 16:40:06.749: INFO: kubernetes-dashboard-8c85c4f9-9l4pq from kubernetes-dashboard started at 2023-06-08 13:33:49 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.749: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Jun  8 16:40:06.749: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-74mph from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 16:40:06.749: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 16:40:06.749: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  8 16:40:06.749: INFO: 
    Logging pods the apiserver thinks is on node chl8tf-control-plane-003 before test
    Jun  8 16:40:06.761: INFO: csi-oci-node-6zdfs from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
    Jun  8 16:40:06.761: INFO: 	Container csi-node-registrar ready: true, restart count 0
    Jun  8 16:40:06.761: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
    Jun  8 16:40:06.761: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
    Jun  8 16:40:06.761: INFO: etcd-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.761: INFO: 	Container etcd ready: true, restart count 0
    Jun  8 16:40:06.761: INFO: kube-apiserver-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.761: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jun  8 16:40:06.761: INFO: kube-controller-manager-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.761: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jun  8 16:40:06.761: INFO: kube-flannel-ds-qf6c6 from kube-system started at 2023-06-08 13:33:43 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.761: INFO: 	Container kube-flannel ready: true, restart count 0
    Jun  8 16:40:06.761: INFO: kube-proxy-9kw5t from kube-system started at 2023-06-08 13:33:04 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.761: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  8 16:40:06.762: INFO: kube-scheduler-chl8tf-control-plane-003 from kube-system started at 2023-06-08 14:42:41 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.762: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jun  8 16:40:06.762: INFO: oci-cloud-controller-manager-2sdzm from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.762: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
    Jun  8 16:40:06.762: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-jpq4c from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 16:40:06.762: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 16:40:06.762: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  8 16:40:06.762: INFO: 
    Logging pods the apiserver thinks is on node chl8tf-worker-001 before test
    Jun  8 16:40:06.776: INFO: csi-oci-node-7ww4x from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
    Jun  8 16:40:06.776: INFO: 	Container csi-node-registrar ready: true, restart count 0
    Jun  8 16:40:06.776: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
    Jun  8 16:40:06.776: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
    Jun  8 16:40:06.776: INFO: kube-flannel-ds-vg6nz from kube-system started at 2023-06-08 16:01:36 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.776: INFO: 	Container kube-flannel ready: true, restart count 0
    Jun  8 16:40:06.776: INFO: kube-proxy-6kfv2 from kube-system started at 2023-06-08 13:33:27 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.776: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  8 16:40:06.776: INFO: oci-cloud-controller-manager-fchj2 from kube-system started at 2023-06-08 16:01:36 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.776: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
    Jun  8 16:40:06.776: INFO: pod-projected-configmaps-381239ce-064f-4424-aebe-344c87d63d66 from projected-7076 started at 2023-06-08 16:39:58 +0000 UTC (3 container statuses recorded)
    Jun  8 16:40:06.776: INFO: 	Container createcm-volume-test ready: true, restart count 0
    Jun  8 16:40:06.776: INFO: 	Container delcm-volume-test ready: true, restart count 0
    Jun  8 16:40:06.776: INFO: 	Container updcm-volume-test ready: true, restart count 0
    Jun  8 16:40:06.776: INFO: sonobuoy from sonobuoy started at 2023-06-08 15:05:13 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.776: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun  8 16:40:06.776: INFO: sonobuoy-e2e-job-e329b7fb80aa4b40 from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 16:40:06.776: INFO: 	Container e2e ready: true, restart count 0
    Jun  8 16:40:06.776: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 16:40:06.776: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-rs4qz from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 16:40:06.776: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 16:40:06.776: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  8 16:40:06.776: INFO: 
    Logging pods the apiserver thinks is on node chl8tf-worker-002 before test
    Jun  8 16:40:06.791: INFO: csi-oci-controller-69f8b488fc-n8th6 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (5 container statuses recorded)
    Jun  8 16:40:06.791: INFO: 	Container csi-attacher ready: true, restart count 1
    Jun  8 16:40:06.791: INFO: 	Container csi-fss-volume-provisioner ready: true, restart count 1
    Jun  8 16:40:06.791: INFO: 	Container csi-resizer ready: true, restart count 0
    Jun  8 16:40:06.791: INFO: 	Container csi-volume-provisioner ready: true, restart count 0
    Jun  8 16:40:06.791: INFO: 	Container oci-csi-controller-driver ready: true, restart count 0
    Jun  8 16:40:06.791: INFO: csi-oci-node-thcvn from kube-system started at 2023-06-08 14:37:51 +0000 UTC (3 container statuses recorded)
    Jun  8 16:40:06.791: INFO: 	Container csi-node-registrar ready: true, restart count 0
    Jun  8 16:40:06.791: INFO: 	Container csi-node-registrar-fss ready: true, restart count 0
    Jun  8 16:40:06.791: INFO: 	Container oci-csi-node-driver ready: true, restart count 0
    Jun  8 16:40:06.791: INFO: kube-flannel-ds-74q2b from kube-system started at 2023-06-08 13:33:47 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.791: INFO: 	Container kube-flannel ready: true, restart count 0
    Jun  8 16:40:06.791: INFO: kube-proxy-hjjpt from kube-system started at 2023-06-08 13:33:47 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.791: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  8 16:40:06.791: INFO: oci-cloud-controller-manager-lwnq4 from kube-system started at 2023-06-08 14:37:51 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.791: INFO: 	Container oci-cloud-controller-manager ready: true, restart count 0
    Jun  8 16:40:06.791: INFO: agnhost from proxy-2444 started at 2023-06-08 16:40:04 +0000 UTC (1 container statuses recorded)
    Jun  8 16:40:06.791: INFO: 	Container agnhost ready: true, restart count 0
    Jun  8 16:40:06.791: INFO: sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-6nv2x from sonobuoy started at 2023-06-08 15:05:17 +0000 UTC (2 container statuses recorded)
    Jun  8 16:40:06.791: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  8 16:40:06.791: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node chl8tf-control-plane-001 06/08/23 16:40:06.917
    STEP: verifying the node has the label node chl8tf-control-plane-002 06/08/23 16:40:06.943
    STEP: verifying the node has the label node chl8tf-control-plane-003 06/08/23 16:40:06.963
    STEP: verifying the node has the label node chl8tf-worker-001 06/08/23 16:40:06.993
    STEP: verifying the node has the label node chl8tf-worker-002 06/08/23 16:40:07.017
    Jun  8 16:40:07.043: INFO: Pod coredns-55b8ccd764-56hlv requesting resource cpu=100m on Node chl8tf-control-plane-002
    Jun  8 16:40:07.043: INFO: Pod coredns-55b8ccd764-jpqkc requesting resource cpu=100m on Node chl8tf-control-plane-002
    Jun  8 16:40:07.043: INFO: Pod csi-oci-controller-69f8b488fc-n8th6 requesting resource cpu=0m on Node chl8tf-worker-002
    Jun  8 16:40:07.043: INFO: Pod csi-oci-node-5p7f5 requesting resource cpu=0m on Node chl8tf-control-plane-001
    Jun  8 16:40:07.043: INFO: Pod csi-oci-node-6zdfs requesting resource cpu=0m on Node chl8tf-control-plane-003
    Jun  8 16:40:07.043: INFO: Pod csi-oci-node-7ww4x requesting resource cpu=0m on Node chl8tf-worker-001
    Jun  8 16:40:07.043: INFO: Pod csi-oci-node-thcvn requesting resource cpu=0m on Node chl8tf-worker-002
    Jun  8 16:40:07.043: INFO: Pod csi-oci-node-xbn7q requesting resource cpu=0m on Node chl8tf-control-plane-002
    Jun  8 16:40:07.043: INFO: Pod etcd-chl8tf-control-plane-001 requesting resource cpu=100m on Node chl8tf-control-plane-001
    Jun  8 16:40:07.043: INFO: Pod etcd-chl8tf-control-plane-002 requesting resource cpu=100m on Node chl8tf-control-plane-002
    Jun  8 16:40:07.043: INFO: Pod etcd-chl8tf-control-plane-003 requesting resource cpu=100m on Node chl8tf-control-plane-003
    Jun  8 16:40:07.043: INFO: Pod kube-apiserver-chl8tf-control-plane-001 requesting resource cpu=250m on Node chl8tf-control-plane-001
    Jun  8 16:40:07.043: INFO: Pod kube-apiserver-chl8tf-control-plane-002 requesting resource cpu=250m on Node chl8tf-control-plane-002
    Jun  8 16:40:07.043: INFO: Pod kube-apiserver-chl8tf-control-plane-003 requesting resource cpu=250m on Node chl8tf-control-plane-003
    Jun  8 16:40:07.043: INFO: Pod kube-controller-manager-chl8tf-control-plane-001 requesting resource cpu=200m on Node chl8tf-control-plane-001
    Jun  8 16:40:07.043: INFO: Pod kube-controller-manager-chl8tf-control-plane-002 requesting resource cpu=200m on Node chl8tf-control-plane-002
    Jun  8 16:40:07.043: INFO: Pod kube-controller-manager-chl8tf-control-plane-003 requesting resource cpu=200m on Node chl8tf-control-plane-003
    Jun  8 16:40:07.043: INFO: Pod kube-flannel-ds-6fgbp requesting resource cpu=100m on Node chl8tf-control-plane-002
    Jun  8 16:40:07.043: INFO: Pod kube-flannel-ds-74q2b requesting resource cpu=100m on Node chl8tf-worker-002
    Jun  8 16:40:07.043: INFO: Pod kube-flannel-ds-d5bvw requesting resource cpu=100m on Node chl8tf-control-plane-001
    Jun  8 16:40:07.043: INFO: Pod kube-flannel-ds-qf6c6 requesting resource cpu=100m on Node chl8tf-control-plane-003
    Jun  8 16:40:07.043: INFO: Pod kube-flannel-ds-vg6nz requesting resource cpu=100m on Node chl8tf-worker-001
    Jun  8 16:40:07.043: INFO: Pod kube-proxy-6kfv2 requesting resource cpu=0m on Node chl8tf-worker-001
    Jun  8 16:40:07.043: INFO: Pod kube-proxy-9kw5t requesting resource cpu=0m on Node chl8tf-control-plane-003
    Jun  8 16:40:07.043: INFO: Pod kube-proxy-cgwfj requesting resource cpu=0m on Node chl8tf-control-plane-002
    Jun  8 16:40:07.043: INFO: Pod kube-proxy-hjjpt requesting resource cpu=0m on Node chl8tf-worker-002
    Jun  8 16:40:07.043: INFO: Pod kube-proxy-j2p7z requesting resource cpu=0m on Node chl8tf-control-plane-001
    Jun  8 16:40:07.043: INFO: Pod kube-scheduler-chl8tf-control-plane-001 requesting resource cpu=100m on Node chl8tf-control-plane-001
    Jun  8 16:40:07.043: INFO: Pod kube-scheduler-chl8tf-control-plane-002 requesting resource cpu=100m on Node chl8tf-control-plane-002
    Jun  8 16:40:07.043: INFO: Pod kube-scheduler-chl8tf-control-plane-003 requesting resource cpu=100m on Node chl8tf-control-plane-003
    Jun  8 16:40:07.043: INFO: Pod oci-cloud-controller-manager-2sdzm requesting resource cpu=0m on Node chl8tf-control-plane-003
    Jun  8 16:40:07.043: INFO: Pod oci-cloud-controller-manager-9n2zj requesting resource cpu=0m on Node chl8tf-control-plane-001
    Jun  8 16:40:07.043: INFO: Pod oci-cloud-controller-manager-dpkmx requesting resource cpu=0m on Node chl8tf-control-plane-002
    Jun  8 16:40:07.043: INFO: Pod oci-cloud-controller-manager-fchj2 requesting resource cpu=0m on Node chl8tf-worker-001
    Jun  8 16:40:07.043: INFO: Pod oci-cloud-controller-manager-lwnq4 requesting resource cpu=0m on Node chl8tf-worker-002
    Jun  8 16:40:07.043: INFO: Pod kubernetes-dashboard-8c85c4f9-9l4pq requesting resource cpu=0m on Node chl8tf-control-plane-002
    Jun  8 16:40:07.043: INFO: Pod pod-projected-configmaps-381239ce-064f-4424-aebe-344c87d63d66 requesting resource cpu=0m on Node chl8tf-worker-001
    Jun  8 16:40:07.043: INFO: Pod agnhost requesting resource cpu=0m on Node chl8tf-worker-002
    Jun  8 16:40:07.043: INFO: Pod sonobuoy requesting resource cpu=0m on Node chl8tf-worker-001
    Jun  8 16:40:07.043: INFO: Pod sonobuoy-e2e-job-e329b7fb80aa4b40 requesting resource cpu=0m on Node chl8tf-worker-001
    Jun  8 16:40:07.043: INFO: Pod sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-6nv2x requesting resource cpu=0m on Node chl8tf-worker-002
    Jun  8 16:40:07.043: INFO: Pod sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-74mph requesting resource cpu=0m on Node chl8tf-control-plane-002
    Jun  8 16:40:07.043: INFO: Pod sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-jpq4c requesting resource cpu=0m on Node chl8tf-control-plane-003
    Jun  8 16:40:07.043: INFO: Pod sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-rs4qz requesting resource cpu=0m on Node chl8tf-worker-001
    Jun  8 16:40:07.043: INFO: Pod sonobuoy-systemd-logs-daemon-set-67a55218d14c41a0-wmj9x requesting resource cpu=0m on Node chl8tf-control-plane-001
    STEP: Starting Pods to consume most of the cluster CPU. 06/08/23 16:40:07.043
    Jun  8 16:40:07.044: INFO: Creating a pod which consumes cpu=2275m on Node chl8tf-control-plane-001
    Jun  8 16:40:07.057: INFO: Creating a pod which consumes cpu=2135m on Node chl8tf-control-plane-002
    Jun  8 16:40:07.068: INFO: Creating a pod which consumes cpu=2275m on Node chl8tf-control-plane-003
    Jun  8 16:40:07.081: INFO: Creating a pod which consumes cpu=2730m on Node chl8tf-worker-001
    Jun  8 16:40:07.100: INFO: Creating a pod which consumes cpu=2730m on Node chl8tf-worker-002
    Jun  8 16:40:07.118: INFO: Waiting up to 5m0s for pod "filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c" in namespace "sched-pred-9644" to be "running"
    Jun  8 16:40:07.140: INFO: Pod "filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.939331ms
    Jun  8 16:40:09.145: INFO: Pod "filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c": Phase="Running", Reason="", readiness=true. Elapsed: 2.027454492s
    Jun  8 16:40:09.145: INFO: Pod "filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c" satisfied condition "running"
    Jun  8 16:40:09.146: INFO: Waiting up to 5m0s for pod "filler-pod-e130400f-a2c0-41ca-a425-a183256cbb84" in namespace "sched-pred-9644" to be "running"
    Jun  8 16:40:09.149: INFO: Pod "filler-pod-e130400f-a2c0-41ca-a425-a183256cbb84": Phase="Running", Reason="", readiness=true. Elapsed: 3.889276ms
    Jun  8 16:40:09.149: INFO: Pod "filler-pod-e130400f-a2c0-41ca-a425-a183256cbb84" satisfied condition "running"
    Jun  8 16:40:09.149: INFO: Waiting up to 5m0s for pod "filler-pod-36124d0c-03d5-4314-b9ef-567fe807c5c6" in namespace "sched-pred-9644" to be "running"
    Jun  8 16:40:09.153: INFO: Pod "filler-pod-36124d0c-03d5-4314-b9ef-567fe807c5c6": Phase="Running", Reason="", readiness=true. Elapsed: 3.854897ms
    Jun  8 16:40:09.153: INFO: Pod "filler-pod-36124d0c-03d5-4314-b9ef-567fe807c5c6" satisfied condition "running"
    Jun  8 16:40:09.153: INFO: Waiting up to 5m0s for pod "filler-pod-3e6db3fe-a801-497b-9bc8-8b6604f483d5" in namespace "sched-pred-9644" to be "running"
    Jun  8 16:40:09.157: INFO: Pod "filler-pod-3e6db3fe-a801-497b-9bc8-8b6604f483d5": Phase="Running", Reason="", readiness=true. Elapsed: 3.590264ms
    Jun  8 16:40:09.157: INFO: Pod "filler-pod-3e6db3fe-a801-497b-9bc8-8b6604f483d5" satisfied condition "running"
    Jun  8 16:40:09.157: INFO: Waiting up to 5m0s for pod "filler-pod-18a685d4-e91f-44a2-a3f5-9841679cf844" in namespace "sched-pred-9644" to be "running"
    Jun  8 16:40:09.161: INFO: Pod "filler-pod-18a685d4-e91f-44a2-a3f5-9841679cf844": Phase="Running", Reason="", readiness=true. Elapsed: 3.790363ms
    Jun  8 16:40:09.161: INFO: Pod "filler-pod-18a685d4-e91f-44a2-a3f5-9841679cf844" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 06/08/23 16:40:09.161
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c.1766bc88feb1f4f0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9644/filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c to chl8tf-control-plane-001] 06/08/23 16:40:09.166
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c.1766bc8921b8465c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/08/23 16:40:09.166
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c.1766bc893150951c], Reason = [Created], Message = [Created container filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c] 06/08/23 16:40:09.166
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c.1766bc893459f787], Reason = [Started], Message = [Started container filler-pod-0b67b66e-f612-4ef1-9e99-4feebf6aac5c] 06/08/23 16:40:09.166
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-18a685d4-e91f-44a2-a3f5-9841679cf844.1766bc8901f8873e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9644/filler-pod-18a685d4-e91f-44a2-a3f5-9841679cf844 to chl8tf-worker-002] 06/08/23 16:40:09.167
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-18a685d4-e91f-44a2-a3f5-9841679cf844.1766bc8920a90afb], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/08/23 16:40:09.167
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-18a685d4-e91f-44a2-a3f5-9841679cf844.1766bc892dda8d43], Reason = [Created], Message = [Created container filler-pod-18a685d4-e91f-44a2-a3f5-9841679cf844] 06/08/23 16:40:09.167
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-18a685d4-e91f-44a2-a3f5-9841679cf844.1766bc892febaebd], Reason = [Started], Message = [Started container filler-pod-18a685d4-e91f-44a2-a3f5-9841679cf844] 06/08/23 16:40:09.167
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-36124d0c-03d5-4314-b9ef-567fe807c5c6.1766bc8900879640], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9644/filler-pod-36124d0c-03d5-4314-b9ef-567fe807c5c6 to chl8tf-control-plane-003] 06/08/23 16:40:09.167
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-36124d0c-03d5-4314-b9ef-567fe807c5c6.1766bc891f255007], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/08/23 16:40:09.167
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-36124d0c-03d5-4314-b9ef-567fe807c5c6.1766bc892cd86673], Reason = [Created], Message = [Created container filler-pod-36124d0c-03d5-4314-b9ef-567fe807c5c6] 06/08/23 16:40:09.167
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-36124d0c-03d5-4314-b9ef-567fe807c5c6.1766bc892f8d3bfc], Reason = [Started], Message = [Started container filler-pod-36124d0c-03d5-4314-b9ef-567fe807c5c6] 06/08/23 16:40:09.167
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3e6db3fe-a801-497b-9bc8-8b6604f483d5.1766bc890195d8b2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9644/filler-pod-3e6db3fe-a801-497b-9bc8-8b6604f483d5 to chl8tf-worker-001] 06/08/23 16:40:09.167
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3e6db3fe-a801-497b-9bc8-8b6604f483d5.1766bc8920f75719], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/08/23 16:40:09.168
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3e6db3fe-a801-497b-9bc8-8b6604f483d5.1766bc892dee5e33], Reason = [Created], Message = [Created container filler-pod-3e6db3fe-a801-497b-9bc8-8b6604f483d5] 06/08/23 16:40:09.168
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3e6db3fe-a801-497b-9bc8-8b6604f483d5.1766bc892fb4b4e0], Reason = [Started], Message = [Started container filler-pod-3e6db3fe-a801-497b-9bc8-8b6604f483d5] 06/08/23 16:40:09.168
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e130400f-a2c0-41ca-a425-a183256cbb84.1766bc88ffc13318], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9644/filler-pod-e130400f-a2c0-41ca-a425-a183256cbb84 to chl8tf-control-plane-002] 06/08/23 16:40:09.168
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e130400f-a2c0-41ca-a425-a183256cbb84.1766bc892226335e], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/08/23 16:40:09.168
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e130400f-a2c0-41ca-a425-a183256cbb84.1766bc89300633fa], Reason = [Created], Message = [Created container filler-pod-e130400f-a2c0-41ca-a425-a183256cbb84] 06/08/23 16:40:09.168
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e130400f-a2c0-41ca-a425-a183256cbb84.1766bc8936249487], Reason = [Started], Message = [Started container filler-pod-e130400f-a2c0-41ca-a425-a183256cbb84] 06/08/23 16:40:09.168
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1766bc897c0d0feb], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu. preemption: 0/5 nodes are available: 5 No preemption victims found for incoming pod..] 06/08/23 16:40:09.179
    STEP: removing the label node off the node chl8tf-control-plane-002 06/08/23 16:40:10.181
    STEP: verifying the node doesn't have the label node 06/08/23 16:40:10.198
    STEP: removing the label node off the node chl8tf-control-plane-003 06/08/23 16:40:10.202
    STEP: verifying the node doesn't have the label node 06/08/23 16:40:10.224
    STEP: removing the label node off the node chl8tf-worker-001 06/08/23 16:40:10.231
    STEP: verifying the node doesn't have the label node 06/08/23 16:40:10.256
    STEP: removing the label node off the node chl8tf-worker-002 06/08/23 16:40:10.263
    STEP: verifying the node doesn't have the label node 06/08/23 16:40:10.287
    STEP: removing the label node off the node chl8tf-control-plane-001 06/08/23 16:40:10.3
    STEP: verifying the node doesn't have the label node 06/08/23 16:40:10.332
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:40:10.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9644" for this suite. 06/08/23 16:40:10.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:40:10.378
Jun  8 16:40:10.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename replication-controller 06/08/23 16:40:10.38
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:40:10.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:40:10.418
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 06/08/23 16:40:10.427
Jun  8 16:40:10.442: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7134" to be "running and ready"
Jun  8 16:40:10.452: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 10.052622ms
Jun  8 16:40:10.452: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jun  8 16:40:12.460: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.01765845s
Jun  8 16:40:12.460: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jun  8 16:40:12.460: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 06/08/23 16:40:12.466
STEP: Then the orphan pod is adopted 06/08/23 16:40:12.476
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jun  8 16:40:13.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7134" for this suite. 06/08/23 16:40:13.502
------------------------------
• [3.131 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:40:10.378
    Jun  8 16:40:10.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename replication-controller 06/08/23 16:40:10.38
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:40:10.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:40:10.418
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 06/08/23 16:40:10.427
    Jun  8 16:40:10.442: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7134" to be "running and ready"
    Jun  8 16:40:10.452: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 10.052622ms
    Jun  8 16:40:10.452: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 16:40:12.460: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.01765845s
    Jun  8 16:40:12.460: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jun  8 16:40:12.460: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 06/08/23 16:40:12.466
    STEP: Then the orphan pod is adopted 06/08/23 16:40:12.476
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:40:13.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7134" for this suite. 06/08/23 16:40:13.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:40:13.51
Jun  8 16:40:13.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename services 06/08/23 16:40:13.512
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:40:13.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:40:13.532
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 06/08/23 16:40:13.54
STEP: watching for the Service to be added 06/08/23 16:40:13.558
Jun  8 16:40:13.561: INFO: Found Service test-service-vj974 in namespace services-3855 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jun  8 16:40:13.561: INFO: Service test-service-vj974 created
STEP: Getting /status 06/08/23 16:40:13.561
Jun  8 16:40:13.565: INFO: Service test-service-vj974 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 06/08/23 16:40:13.565
STEP: watching for the Service to be patched 06/08/23 16:40:13.577
Jun  8 16:40:13.581: INFO: observed Service test-service-vj974 in namespace services-3855 with annotations: map[] & LoadBalancer: {[]}
Jun  8 16:40:13.581: INFO: Found Service test-service-vj974 in namespace services-3855 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jun  8 16:40:13.581: INFO: Service test-service-vj974 has service status patched
STEP: updating the ServiceStatus 06/08/23 16:40:13.581
Jun  8 16:40:13.594: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 06/08/23 16:40:13.594
Jun  8 16:40:13.596: INFO: Observed Service test-service-vj974 in namespace services-3855 with annotations: map[] & Conditions: {[]}
Jun  8 16:40:13.596: INFO: Observed event: &Service{ObjectMeta:{test-service-vj974  services-3855  36b28c6b-19ca-4e44-8cba-bbc11619629f 62076 0 2023-06-08 16:40:13 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-06-08 16:40:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-06-08 16:40:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.237.111,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.237.111],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jun  8 16:40:13.597: INFO: Found Service test-service-vj974 in namespace services-3855 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun  8 16:40:13.597: INFO: Service test-service-vj974 has service status updated
STEP: patching the service 06/08/23 16:40:13.597
STEP: watching for the Service to be patched 06/08/23 16:40:13.613
Jun  8 16:40:13.615: INFO: observed Service test-service-vj974 in namespace services-3855 with labels: map[test-service-static:true]
Jun  8 16:40:13.615: INFO: observed Service test-service-vj974 in namespace services-3855 with labels: map[test-service-static:true]
Jun  8 16:40:13.615: INFO: observed Service test-service-vj974 in namespace services-3855 with labels: map[test-service-static:true]
Jun  8 16:40:13.615: INFO: Found Service test-service-vj974 in namespace services-3855 with labels: map[test-service:patched test-service-static:true]
Jun  8 16:40:13.615: INFO: Service test-service-vj974 patched
STEP: deleting the service 06/08/23 16:40:13.615
STEP: watching for the Service to be deleted 06/08/23 16:40:13.643
Jun  8 16:40:13.645: INFO: Observed event: ADDED
Jun  8 16:40:13.645: INFO: Observed event: MODIFIED
Jun  8 16:40:13.645: INFO: Observed event: MODIFIED
Jun  8 16:40:13.645: INFO: Observed event: MODIFIED
Jun  8 16:40:13.646: INFO: Found Service test-service-vj974 in namespace services-3855 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jun  8 16:40:13.646: INFO: Service test-service-vj974 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  8 16:40:13.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3855" for this suite. 06/08/23 16:40:13.656
------------------------------
• [0.156 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:40:13.51
    Jun  8 16:40:13.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename services 06/08/23 16:40:13.512
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:40:13.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:40:13.532
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 06/08/23 16:40:13.54
    STEP: watching for the Service to be added 06/08/23 16:40:13.558
    Jun  8 16:40:13.561: INFO: Found Service test-service-vj974 in namespace services-3855 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jun  8 16:40:13.561: INFO: Service test-service-vj974 created
    STEP: Getting /status 06/08/23 16:40:13.561
    Jun  8 16:40:13.565: INFO: Service test-service-vj974 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 06/08/23 16:40:13.565
    STEP: watching for the Service to be patched 06/08/23 16:40:13.577
    Jun  8 16:40:13.581: INFO: observed Service test-service-vj974 in namespace services-3855 with annotations: map[] & LoadBalancer: {[]}
    Jun  8 16:40:13.581: INFO: Found Service test-service-vj974 in namespace services-3855 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jun  8 16:40:13.581: INFO: Service test-service-vj974 has service status patched
    STEP: updating the ServiceStatus 06/08/23 16:40:13.581
    Jun  8 16:40:13.594: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 06/08/23 16:40:13.594
    Jun  8 16:40:13.596: INFO: Observed Service test-service-vj974 in namespace services-3855 with annotations: map[] & Conditions: {[]}
    Jun  8 16:40:13.596: INFO: Observed event: &Service{ObjectMeta:{test-service-vj974  services-3855  36b28c6b-19ca-4e44-8cba-bbc11619629f 62076 0 2023-06-08 16:40:13 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-06-08 16:40:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-06-08 16:40:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.237.111,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.237.111],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jun  8 16:40:13.597: INFO: Found Service test-service-vj974 in namespace services-3855 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun  8 16:40:13.597: INFO: Service test-service-vj974 has service status updated
    STEP: patching the service 06/08/23 16:40:13.597
    STEP: watching for the Service to be patched 06/08/23 16:40:13.613
    Jun  8 16:40:13.615: INFO: observed Service test-service-vj974 in namespace services-3855 with labels: map[test-service-static:true]
    Jun  8 16:40:13.615: INFO: observed Service test-service-vj974 in namespace services-3855 with labels: map[test-service-static:true]
    Jun  8 16:40:13.615: INFO: observed Service test-service-vj974 in namespace services-3855 with labels: map[test-service-static:true]
    Jun  8 16:40:13.615: INFO: Found Service test-service-vj974 in namespace services-3855 with labels: map[test-service:patched test-service-static:true]
    Jun  8 16:40:13.615: INFO: Service test-service-vj974 patched
    STEP: deleting the service 06/08/23 16:40:13.615
    STEP: watching for the Service to be deleted 06/08/23 16:40:13.643
    Jun  8 16:40:13.645: INFO: Observed event: ADDED
    Jun  8 16:40:13.645: INFO: Observed event: MODIFIED
    Jun  8 16:40:13.645: INFO: Observed event: MODIFIED
    Jun  8 16:40:13.645: INFO: Observed event: MODIFIED
    Jun  8 16:40:13.646: INFO: Found Service test-service-vj974 in namespace services-3855 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jun  8 16:40:13.646: INFO: Service test-service-vj974 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:40:13.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3855" for this suite. 06/08/23 16:40:13.656
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:40:13.67
Jun  8 16:40:13.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename custom-resource-definition 06/08/23 16:40:13.671
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:40:13.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:40:13.703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jun  8 16:40:13.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:40:20.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2675" for this suite. 06/08/23 16:40:20.02
------------------------------
• [SLOW TEST] [6.359 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:40:13.67
    Jun  8 16:40:13.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename custom-resource-definition 06/08/23 16:40:13.671
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:40:13.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:40:13.703
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jun  8 16:40:13.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:40:20.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2675" for this suite. 06/08/23 16:40:20.02
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:40:20.029
Jun  8 16:40:20.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename prestop 06/08/23 16:40:20.03
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:40:20.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:40:20.047
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-2257 06/08/23 16:40:20.051
STEP: Waiting for pods to come up. 06/08/23 16:40:20.06
Jun  8 16:40:20.061: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2257" to be "running"
Jun  8 16:40:20.065: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.263882ms
Jun  8 16:40:22.071: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.010098261s
Jun  8 16:40:22.071: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-2257 06/08/23 16:40:22.075
Jun  8 16:40:22.081: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2257" to be "running"
Jun  8 16:40:22.084: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.570458ms
Jun  8 16:40:24.089: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.008601207s
Jun  8 16:40:24.089: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 06/08/23 16:40:24.089
Jun  8 16:40:29.106: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 06/08/23 16:40:29.106
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Jun  8 16:40:29.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-2257" for this suite. 06/08/23 16:40:29.128
------------------------------
• [SLOW TEST] [9.106 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:40:20.029
    Jun  8 16:40:20.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename prestop 06/08/23 16:40:20.03
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:40:20.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:40:20.047
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-2257 06/08/23 16:40:20.051
    STEP: Waiting for pods to come up. 06/08/23 16:40:20.06
    Jun  8 16:40:20.061: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2257" to be "running"
    Jun  8 16:40:20.065: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.263882ms
    Jun  8 16:40:22.071: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.010098261s
    Jun  8 16:40:22.071: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-2257 06/08/23 16:40:22.075
    Jun  8 16:40:22.081: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2257" to be "running"
    Jun  8 16:40:22.084: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.570458ms
    Jun  8 16:40:24.089: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.008601207s
    Jun  8 16:40:24.089: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 06/08/23 16:40:24.089
    Jun  8 16:40:29.106: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 06/08/23 16:40:29.106
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:40:29.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-2257" for this suite. 06/08/23 16:40:29.128
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:40:29.141
Jun  8 16:40:29.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename cronjob 06/08/23 16:40:29.143
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:40:29.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:40:29.162
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 06/08/23 16:40:29.165
STEP: Ensuring a job is scheduled 06/08/23 16:40:29.171
STEP: Ensuring exactly one is scheduled 06/08/23 16:41:01.178
STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/08/23 16:41:01.182
STEP: Ensuring the job is replaced with a new one 06/08/23 16:41:01.185
STEP: Removing cronjob 06/08/23 16:42:01.192
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jun  8 16:42:01.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2621" for this suite. 06/08/23 16:42:01.207
------------------------------
• [SLOW TEST] [92.075 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:40:29.141
    Jun  8 16:40:29.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename cronjob 06/08/23 16:40:29.143
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:40:29.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:40:29.162
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 06/08/23 16:40:29.165
    STEP: Ensuring a job is scheduled 06/08/23 16:40:29.171
    STEP: Ensuring exactly one is scheduled 06/08/23 16:41:01.178
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/08/23 16:41:01.182
    STEP: Ensuring the job is replaced with a new one 06/08/23 16:41:01.185
    STEP: Removing cronjob 06/08/23 16:42:01.192
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:42:01.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2621" for this suite. 06/08/23 16:42:01.207
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:42:01.216
Jun  8 16:42:01.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename configmap 06/08/23 16:42:01.218
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:42:01.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:42:01.238
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-a0da4132-8c91-442b-a00f-da4bb037e193 06/08/23 16:42:01.241
STEP: Creating a pod to test consume configMaps 06/08/23 16:42:01.246
Jun  8 16:42:01.255: INFO: Waiting up to 5m0s for pod "pod-configmaps-4e021792-ace1-4b10-a4c7-891d61740176" in namespace "configmap-1279" to be "Succeeded or Failed"
Jun  8 16:42:01.258: INFO: Pod "pod-configmaps-4e021792-ace1-4b10-a4c7-891d61740176": Phase="Pending", Reason="", readiness=false. Elapsed: 3.595105ms
Jun  8 16:42:03.264: INFO: Pod "pod-configmaps-4e021792-ace1-4b10-a4c7-891d61740176": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009038344s
Jun  8 16:42:05.263: INFO: Pod "pod-configmaps-4e021792-ace1-4b10-a4c7-891d61740176": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00870765s
STEP: Saw pod success 06/08/23 16:42:05.263
Jun  8 16:42:05.263: INFO: Pod "pod-configmaps-4e021792-ace1-4b10-a4c7-891d61740176" satisfied condition "Succeeded or Failed"
Jun  8 16:42:05.268: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-configmaps-4e021792-ace1-4b10-a4c7-891d61740176 container agnhost-container: <nil>
STEP: delete the pod 06/08/23 16:42:05.285
Jun  8 16:42:05.298: INFO: Waiting for pod pod-configmaps-4e021792-ace1-4b10-a4c7-891d61740176 to disappear
Jun  8 16:42:05.302: INFO: Pod pod-configmaps-4e021792-ace1-4b10-a4c7-891d61740176 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  8 16:42:05.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1279" for this suite. 06/08/23 16:42:05.308
------------------------------
• [4.101 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:42:01.216
    Jun  8 16:42:01.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename configmap 06/08/23 16:42:01.218
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:42:01.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:42:01.238
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-a0da4132-8c91-442b-a00f-da4bb037e193 06/08/23 16:42:01.241
    STEP: Creating a pod to test consume configMaps 06/08/23 16:42:01.246
    Jun  8 16:42:01.255: INFO: Waiting up to 5m0s for pod "pod-configmaps-4e021792-ace1-4b10-a4c7-891d61740176" in namespace "configmap-1279" to be "Succeeded or Failed"
    Jun  8 16:42:01.258: INFO: Pod "pod-configmaps-4e021792-ace1-4b10-a4c7-891d61740176": Phase="Pending", Reason="", readiness=false. Elapsed: 3.595105ms
    Jun  8 16:42:03.264: INFO: Pod "pod-configmaps-4e021792-ace1-4b10-a4c7-891d61740176": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009038344s
    Jun  8 16:42:05.263: INFO: Pod "pod-configmaps-4e021792-ace1-4b10-a4c7-891d61740176": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00870765s
    STEP: Saw pod success 06/08/23 16:42:05.263
    Jun  8 16:42:05.263: INFO: Pod "pod-configmaps-4e021792-ace1-4b10-a4c7-891d61740176" satisfied condition "Succeeded or Failed"
    Jun  8 16:42:05.268: INFO: Trying to get logs from node chl8tf-worker-001 pod pod-configmaps-4e021792-ace1-4b10-a4c7-891d61740176 container agnhost-container: <nil>
    STEP: delete the pod 06/08/23 16:42:05.285
    Jun  8 16:42:05.298: INFO: Waiting for pod pod-configmaps-4e021792-ace1-4b10-a4c7-891d61740176 to disappear
    Jun  8 16:42:05.302: INFO: Pod pod-configmaps-4e021792-ace1-4b10-a4c7-891d61740176 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:42:05.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1279" for this suite. 06/08/23 16:42:05.308
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:42:05.318
Jun  8 16:42:05.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename crd-publish-openapi 06/08/23 16:42:05.319
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:42:05.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:42:05.338
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Jun  8 16:42:05.341: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/08/23 16:42:07.457
Jun  8 16:42:07.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-4270 --namespace=crd-publish-openapi-4270 create -f -'
Jun  8 16:42:08.129: INFO: stderr: ""
Jun  8 16:42:08.129: INFO: stdout: "e2e-test-crd-publish-openapi-5295-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun  8 16:42:08.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-4270 --namespace=crd-publish-openapi-4270 delete e2e-test-crd-publish-openapi-5295-crds test-cr'
Jun  8 16:42:08.242: INFO: stderr: ""
Jun  8 16:42:08.242: INFO: stdout: "e2e-test-crd-publish-openapi-5295-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun  8 16:42:08.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-4270 --namespace=crd-publish-openapi-4270 apply -f -'
Jun  8 16:42:08.869: INFO: stderr: ""
Jun  8 16:42:08.869: INFO: stdout: "e2e-test-crd-publish-openapi-5295-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun  8 16:42:08.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-4270 --namespace=crd-publish-openapi-4270 delete e2e-test-crd-publish-openapi-5295-crds test-cr'
Jun  8 16:42:08.952: INFO: stderr: ""
Jun  8 16:42:08.952: INFO: stdout: "e2e-test-crd-publish-openapi-5295-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 06/08/23 16:42:08.952
Jun  8 16:42:08.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-4270 explain e2e-test-crd-publish-openapi-5295-crds'
Jun  8 16:42:09.138: INFO: stderr: ""
Jun  8 16:42:09.139: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5295-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:42:11.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4270" for this suite. 06/08/23 16:42:11.224
------------------------------
• [SLOW TEST] [5.913 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:42:05.318
    Jun  8 16:42:05.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename crd-publish-openapi 06/08/23 16:42:05.319
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:42:05.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:42:05.338
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Jun  8 16:42:05.341: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/08/23 16:42:07.457
    Jun  8 16:42:07.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-4270 --namespace=crd-publish-openapi-4270 create -f -'
    Jun  8 16:42:08.129: INFO: stderr: ""
    Jun  8 16:42:08.129: INFO: stdout: "e2e-test-crd-publish-openapi-5295-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jun  8 16:42:08.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-4270 --namespace=crd-publish-openapi-4270 delete e2e-test-crd-publish-openapi-5295-crds test-cr'
    Jun  8 16:42:08.242: INFO: stderr: ""
    Jun  8 16:42:08.242: INFO: stdout: "e2e-test-crd-publish-openapi-5295-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jun  8 16:42:08.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-4270 --namespace=crd-publish-openapi-4270 apply -f -'
    Jun  8 16:42:08.869: INFO: stderr: ""
    Jun  8 16:42:08.869: INFO: stdout: "e2e-test-crd-publish-openapi-5295-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jun  8 16:42:08.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-4270 --namespace=crd-publish-openapi-4270 delete e2e-test-crd-publish-openapi-5295-crds test-cr'
    Jun  8 16:42:08.952: INFO: stderr: ""
    Jun  8 16:42:08.952: INFO: stdout: "e2e-test-crd-publish-openapi-5295-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 06/08/23 16:42:08.952
    Jun  8 16:42:08.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=crd-publish-openapi-4270 explain e2e-test-crd-publish-openapi-5295-crds'
    Jun  8 16:42:09.138: INFO: stderr: ""
    Jun  8 16:42:09.139: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5295-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:42:11.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4270" for this suite. 06/08/23 16:42:11.224
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:42:11.232
Jun  8 16:42:11.232: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename downward-api 06/08/23 16:42:11.233
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:42:11.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:42:11.255
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 06/08/23 16:42:11.258
Jun  8 16:42:11.268: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8bfd36fe-851a-4fcb-bd13-ca51806faff0" in namespace "downward-api-2568" to be "Succeeded or Failed"
Jun  8 16:42:11.271: INFO: Pod "downwardapi-volume-8bfd36fe-851a-4fcb-bd13-ca51806faff0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.099193ms
Jun  8 16:42:13.276: INFO: Pod "downwardapi-volume-8bfd36fe-851a-4fcb-bd13-ca51806faff0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008116058s
Jun  8 16:42:15.276: INFO: Pod "downwardapi-volume-8bfd36fe-851a-4fcb-bd13-ca51806faff0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008190711s
STEP: Saw pod success 06/08/23 16:42:15.276
Jun  8 16:42:15.276: INFO: Pod "downwardapi-volume-8bfd36fe-851a-4fcb-bd13-ca51806faff0" satisfied condition "Succeeded or Failed"
Jun  8 16:42:15.280: INFO: Trying to get logs from node chl8tf-worker-002 pod downwardapi-volume-8bfd36fe-851a-4fcb-bd13-ca51806faff0 container client-container: <nil>
STEP: delete the pod 06/08/23 16:42:15.296
Jun  8 16:42:15.306: INFO: Waiting for pod downwardapi-volume-8bfd36fe-851a-4fcb-bd13-ca51806faff0 to disappear
Jun  8 16:42:15.309: INFO: Pod downwardapi-volume-8bfd36fe-851a-4fcb-bd13-ca51806faff0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  8 16:42:15.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2568" for this suite. 06/08/23 16:42:15.315
------------------------------
• [4.089 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:42:11.232
    Jun  8 16:42:11.232: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename downward-api 06/08/23 16:42:11.233
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:42:11.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:42:11.255
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 06/08/23 16:42:11.258
    Jun  8 16:42:11.268: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8bfd36fe-851a-4fcb-bd13-ca51806faff0" in namespace "downward-api-2568" to be "Succeeded or Failed"
    Jun  8 16:42:11.271: INFO: Pod "downwardapi-volume-8bfd36fe-851a-4fcb-bd13-ca51806faff0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.099193ms
    Jun  8 16:42:13.276: INFO: Pod "downwardapi-volume-8bfd36fe-851a-4fcb-bd13-ca51806faff0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008116058s
    Jun  8 16:42:15.276: INFO: Pod "downwardapi-volume-8bfd36fe-851a-4fcb-bd13-ca51806faff0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008190711s
    STEP: Saw pod success 06/08/23 16:42:15.276
    Jun  8 16:42:15.276: INFO: Pod "downwardapi-volume-8bfd36fe-851a-4fcb-bd13-ca51806faff0" satisfied condition "Succeeded or Failed"
    Jun  8 16:42:15.280: INFO: Trying to get logs from node chl8tf-worker-002 pod downwardapi-volume-8bfd36fe-851a-4fcb-bd13-ca51806faff0 container client-container: <nil>
    STEP: delete the pod 06/08/23 16:42:15.296
    Jun  8 16:42:15.306: INFO: Waiting for pod downwardapi-volume-8bfd36fe-851a-4fcb-bd13-ca51806faff0 to disappear
    Jun  8 16:42:15.309: INFO: Pod downwardapi-volume-8bfd36fe-851a-4fcb-bd13-ca51806faff0 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:42:15.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2568" for this suite. 06/08/23 16:42:15.315
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:42:15.321
Jun  8 16:42:15.321: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename pod-network-test 06/08/23 16:42:15.323
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:42:15.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:42:15.344
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-9185 06/08/23 16:42:15.347
STEP: creating a selector 06/08/23 16:42:15.347
STEP: Creating the service pods in kubernetes 06/08/23 16:42:15.347
Jun  8 16:42:15.348: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun  8 16:42:15.401: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9185" to be "running and ready"
Jun  8 16:42:15.410: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.041174ms
Jun  8 16:42:15.410: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  8 16:42:17.416: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015098316s
Jun  8 16:42:17.416: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 16:42:19.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013667729s
Jun  8 16:42:19.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 16:42:21.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.013517645s
Jun  8 16:42:21.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 16:42:23.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014150375s
Jun  8 16:42:23.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 16:42:25.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014122359s
Jun  8 16:42:25.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 16:42:27.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.01382335s
Jun  8 16:42:27.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 16:42:29.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013969949s
Jun  8 16:42:29.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 16:42:31.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014678777s
Jun  8 16:42:31.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 16:42:33.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.014004185s
Jun  8 16:42:33.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 16:42:35.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014416566s
Jun  8 16:42:35.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  8 16:42:37.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.013256943s
Jun  8 16:42:37.414: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun  8 16:42:37.414: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun  8 16:42:37.417: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9185" to be "running and ready"
Jun  8 16:42:37.420: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.14307ms
Jun  8 16:42:37.421: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun  8 16:42:37.421: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun  8 16:42:37.423: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9185" to be "running and ready"
Jun  8 16:42:37.427: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.151436ms
Jun  8 16:42:37.427: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun  8 16:42:37.427: INFO: Pod "netserver-2" satisfied condition "running and ready"
Jun  8 16:42:37.430: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-9185" to be "running and ready"
Jun  8 16:42:37.432: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 2.83014ms
Jun  8 16:42:37.432: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Jun  8 16:42:39.438: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 2.008219321s
Jun  8 16:42:39.438: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Jun  8 16:42:41.438: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 4.008670613s
Jun  8 16:42:41.438: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Jun  8 16:42:43.439: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 6.009199938s
Jun  8 16:42:43.439: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Jun  8 16:42:45.438: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 8.008720775s
Jun  8 16:42:45.438: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Jun  8 16:42:47.437: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 10.007578593s
Jun  8 16:42:47.437: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Jun  8 16:42:49.437: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 12.007663995s
Jun  8 16:42:49.437: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Jun  8 16:42:51.438: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 14.008819301s
Jun  8 16:42:51.438: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Jun  8 16:42:51.438: INFO: Pod "netserver-3" satisfied condition "running and ready"
Jun  8 16:42:51.442: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-9185" to be "running and ready"
Jun  8 16:42:51.445: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.123553ms
Jun  8 16:42:51.445: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Jun  8 16:42:51.445: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 06/08/23 16:42:51.448
Jun  8 16:42:51.458: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9185" to be "running"
Jun  8 16:42:51.461: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.893914ms
Jun  8 16:42:53.466: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007901363s
Jun  8 16:42:53.466: INFO: Pod "test-container-pod" satisfied condition "running"
Jun  8 16:42:53.469: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Jun  8 16:42:53.469: INFO: Breadth first check of 10.244.0.94 on host 100.100.237.165...
Jun  8 16:42:53.472: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.137:9080/dial?request=hostname&protocol=http&host=10.244.0.94&port=8083&tries=1'] Namespace:pod-network-test-9185 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 16:42:53.472: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 16:42:53.473: INFO: ExecWithOptions: Clientset creation
Jun  8 16:42:53.473: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9185/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.0.94%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun  8 16:42:53.565: INFO: Waiting for responses: map[]
Jun  8 16:42:53.565: INFO: reached 10.244.0.94 after 0/1 tries
Jun  8 16:42:53.565: INFO: Breadth first check of 10.244.1.89 on host 100.100.236.41...
Jun  8 16:42:53.569: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.137:9080/dial?request=hostname&protocol=http&host=10.244.1.89&port=8083&tries=1'] Namespace:pod-network-test-9185 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 16:42:53.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 16:42:53.569: INFO: ExecWithOptions: Clientset creation
Jun  8 16:42:53.569: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9185/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.89%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun  8 16:42:53.655: INFO: Waiting for responses: map[]
Jun  8 16:42:53.655: INFO: reached 10.244.1.89 after 0/1 tries
Jun  8 16:42:53.655: INFO: Breadth first check of 10.244.2.87 on host 100.100.237.235...
Jun  8 16:42:53.659: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.137:9080/dial?request=hostname&protocol=http&host=10.244.2.87&port=8083&tries=1'] Namespace:pod-network-test-9185 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 16:42:53.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 16:42:53.660: INFO: ExecWithOptions: Clientset creation
Jun  8 16:42:53.660: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9185/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.2.87%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun  8 16:42:53.751: INFO: Waiting for responses: map[]
Jun  8 16:42:53.751: INFO: reached 10.244.2.87 after 0/1 tries
Jun  8 16:42:53.751: INFO: Breadth first check of 10.244.3.136 on host 100.100.236.215...
Jun  8 16:42:53.754: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.137:9080/dial?request=hostname&protocol=http&host=10.244.3.136&port=8083&tries=1'] Namespace:pod-network-test-9185 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 16:42:53.755: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 16:42:53.755: INFO: ExecWithOptions: Clientset creation
Jun  8 16:42:53.755: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9185/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.3.136%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun  8 16:42:53.835: INFO: Waiting for responses: map[]
Jun  8 16:42:53.835: INFO: reached 10.244.3.136 after 0/1 tries
Jun  8 16:42:53.835: INFO: Breadth first check of 10.244.4.160 on host 100.100.237.90...
Jun  8 16:42:53.839: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.137:9080/dial?request=hostname&protocol=http&host=10.244.4.160&port=8083&tries=1'] Namespace:pod-network-test-9185 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  8 16:42:53.839: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
Jun  8 16:42:53.839: INFO: ExecWithOptions: Clientset creation
Jun  8 16:42:53.840: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9185/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.4.160%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun  8 16:42:53.924: INFO: Waiting for responses: map[]
Jun  8 16:42:53.924: INFO: reached 10.244.4.160 after 0/1 tries
Jun  8 16:42:53.924: INFO: Going to retry 0 out of 5 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jun  8 16:42:53.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9185" for this suite. 06/08/23 16:42:53.931
------------------------------
• [SLOW TEST] [38.615 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:42:15.321
    Jun  8 16:42:15.321: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename pod-network-test 06/08/23 16:42:15.323
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:42:15.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:42:15.344
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-9185 06/08/23 16:42:15.347
    STEP: creating a selector 06/08/23 16:42:15.347
    STEP: Creating the service pods in kubernetes 06/08/23 16:42:15.347
    Jun  8 16:42:15.348: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jun  8 16:42:15.401: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9185" to be "running and ready"
    Jun  8 16:42:15.410: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.041174ms
    Jun  8 16:42:15.410: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun  8 16:42:17.416: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015098316s
    Jun  8 16:42:17.416: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 16:42:19.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013667729s
    Jun  8 16:42:19.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 16:42:21.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.013517645s
    Jun  8 16:42:21.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 16:42:23.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014150375s
    Jun  8 16:42:23.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 16:42:25.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014122359s
    Jun  8 16:42:25.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 16:42:27.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.01382335s
    Jun  8 16:42:27.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 16:42:29.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013969949s
    Jun  8 16:42:29.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 16:42:31.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014678777s
    Jun  8 16:42:31.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 16:42:33.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.014004185s
    Jun  8 16:42:33.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 16:42:35.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014416566s
    Jun  8 16:42:35.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  8 16:42:37.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.013256943s
    Jun  8 16:42:37.414: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun  8 16:42:37.414: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun  8 16:42:37.417: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9185" to be "running and ready"
    Jun  8 16:42:37.420: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.14307ms
    Jun  8 16:42:37.421: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun  8 16:42:37.421: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun  8 16:42:37.423: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9185" to be "running and ready"
    Jun  8 16:42:37.427: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.151436ms
    Jun  8 16:42:37.427: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun  8 16:42:37.427: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Jun  8 16:42:37.430: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-9185" to be "running and ready"
    Jun  8 16:42:37.432: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 2.83014ms
    Jun  8 16:42:37.432: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Jun  8 16:42:39.438: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 2.008219321s
    Jun  8 16:42:39.438: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Jun  8 16:42:41.438: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 4.008670613s
    Jun  8 16:42:41.438: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Jun  8 16:42:43.439: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 6.009199938s
    Jun  8 16:42:43.439: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Jun  8 16:42:45.438: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 8.008720775s
    Jun  8 16:42:45.438: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Jun  8 16:42:47.437: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 10.007578593s
    Jun  8 16:42:47.437: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Jun  8 16:42:49.437: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 12.007663995s
    Jun  8 16:42:49.437: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Jun  8 16:42:51.438: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 14.008819301s
    Jun  8 16:42:51.438: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Jun  8 16:42:51.438: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Jun  8 16:42:51.442: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-9185" to be "running and ready"
    Jun  8 16:42:51.445: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.123553ms
    Jun  8 16:42:51.445: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Jun  8 16:42:51.445: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 06/08/23 16:42:51.448
    Jun  8 16:42:51.458: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9185" to be "running"
    Jun  8 16:42:51.461: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.893914ms
    Jun  8 16:42:53.466: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007901363s
    Jun  8 16:42:53.466: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun  8 16:42:53.469: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Jun  8 16:42:53.469: INFO: Breadth first check of 10.244.0.94 on host 100.100.237.165...
    Jun  8 16:42:53.472: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.137:9080/dial?request=hostname&protocol=http&host=10.244.0.94&port=8083&tries=1'] Namespace:pod-network-test-9185 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 16:42:53.472: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 16:42:53.473: INFO: ExecWithOptions: Clientset creation
    Jun  8 16:42:53.473: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9185/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.0.94%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun  8 16:42:53.565: INFO: Waiting for responses: map[]
    Jun  8 16:42:53.565: INFO: reached 10.244.0.94 after 0/1 tries
    Jun  8 16:42:53.565: INFO: Breadth first check of 10.244.1.89 on host 100.100.236.41...
    Jun  8 16:42:53.569: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.137:9080/dial?request=hostname&protocol=http&host=10.244.1.89&port=8083&tries=1'] Namespace:pod-network-test-9185 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 16:42:53.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 16:42:53.569: INFO: ExecWithOptions: Clientset creation
    Jun  8 16:42:53.569: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9185/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.89%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun  8 16:42:53.655: INFO: Waiting for responses: map[]
    Jun  8 16:42:53.655: INFO: reached 10.244.1.89 after 0/1 tries
    Jun  8 16:42:53.655: INFO: Breadth first check of 10.244.2.87 on host 100.100.237.235...
    Jun  8 16:42:53.659: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.137:9080/dial?request=hostname&protocol=http&host=10.244.2.87&port=8083&tries=1'] Namespace:pod-network-test-9185 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 16:42:53.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 16:42:53.660: INFO: ExecWithOptions: Clientset creation
    Jun  8 16:42:53.660: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9185/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.2.87%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun  8 16:42:53.751: INFO: Waiting for responses: map[]
    Jun  8 16:42:53.751: INFO: reached 10.244.2.87 after 0/1 tries
    Jun  8 16:42:53.751: INFO: Breadth first check of 10.244.3.136 on host 100.100.236.215...
    Jun  8 16:42:53.754: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.137:9080/dial?request=hostname&protocol=http&host=10.244.3.136&port=8083&tries=1'] Namespace:pod-network-test-9185 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 16:42:53.755: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 16:42:53.755: INFO: ExecWithOptions: Clientset creation
    Jun  8 16:42:53.755: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9185/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.3.136%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun  8 16:42:53.835: INFO: Waiting for responses: map[]
    Jun  8 16:42:53.835: INFO: reached 10.244.3.136 after 0/1 tries
    Jun  8 16:42:53.835: INFO: Breadth first check of 10.244.4.160 on host 100.100.237.90...
    Jun  8 16:42:53.839: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.137:9080/dial?request=hostname&protocol=http&host=10.244.4.160&port=8083&tries=1'] Namespace:pod-network-test-9185 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  8 16:42:53.839: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    Jun  8 16:42:53.839: INFO: ExecWithOptions: Clientset creation
    Jun  8 16:42:53.840: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9185/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.4.160%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun  8 16:42:53.924: INFO: Waiting for responses: map[]
    Jun  8 16:42:53.924: INFO: reached 10.244.4.160 after 0/1 tries
    Jun  8 16:42:53.924: INFO: Going to retry 0 out of 5 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:42:53.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9185" for this suite. 06/08/23 16:42:53.931
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:42:53.938
Jun  8 16:42:53.938: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename projected 06/08/23 16:42:53.94
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:42:53.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:42:53.96
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-c9a70cb5-ce58-4798-8ca7-c977fcc7104a 06/08/23 16:42:53.963
STEP: Creating secret with name secret-projected-all-test-volume-da1bef47-a2cd-4dc6-ba9c-bbd21f418433 06/08/23 16:42:53.969
STEP: Creating a pod to test Check all projections for projected volume plugin 06/08/23 16:42:53.973
Jun  8 16:42:53.981: INFO: Waiting up to 5m0s for pod "projected-volume-971719b0-8bc4-4569-9337-2c43b1790462" in namespace "projected-2896" to be "Succeeded or Failed"
Jun  8 16:42:53.984: INFO: Pod "projected-volume-971719b0-8bc4-4569-9337-2c43b1790462": Phase="Pending", Reason="", readiness=false. Elapsed: 3.147561ms
Jun  8 16:42:55.989: INFO: Pod "projected-volume-971719b0-8bc4-4569-9337-2c43b1790462": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007614602s
Jun  8 16:42:57.990: INFO: Pod "projected-volume-971719b0-8bc4-4569-9337-2c43b1790462": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008581543s
STEP: Saw pod success 06/08/23 16:42:57.99
Jun  8 16:42:57.990: INFO: Pod "projected-volume-971719b0-8bc4-4569-9337-2c43b1790462" satisfied condition "Succeeded or Failed"
Jun  8 16:42:57.993: INFO: Trying to get logs from node chl8tf-worker-001 pod projected-volume-971719b0-8bc4-4569-9337-2c43b1790462 container projected-all-volume-test: <nil>
STEP: delete the pod 06/08/23 16:42:58.01
Jun  8 16:42:58.023: INFO: Waiting for pod projected-volume-971719b0-8bc4-4569-9337-2c43b1790462 to disappear
Jun  8 16:42:58.026: INFO: Pod projected-volume-971719b0-8bc4-4569-9337-2c43b1790462 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Jun  8 16:42:58.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2896" for this suite. 06/08/23 16:42:58.031
------------------------------
• [4.099 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:42:53.938
    Jun  8 16:42:53.938: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename projected 06/08/23 16:42:53.94
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:42:53.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:42:53.96
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-c9a70cb5-ce58-4798-8ca7-c977fcc7104a 06/08/23 16:42:53.963
    STEP: Creating secret with name secret-projected-all-test-volume-da1bef47-a2cd-4dc6-ba9c-bbd21f418433 06/08/23 16:42:53.969
    STEP: Creating a pod to test Check all projections for projected volume plugin 06/08/23 16:42:53.973
    Jun  8 16:42:53.981: INFO: Waiting up to 5m0s for pod "projected-volume-971719b0-8bc4-4569-9337-2c43b1790462" in namespace "projected-2896" to be "Succeeded or Failed"
    Jun  8 16:42:53.984: INFO: Pod "projected-volume-971719b0-8bc4-4569-9337-2c43b1790462": Phase="Pending", Reason="", readiness=false. Elapsed: 3.147561ms
    Jun  8 16:42:55.989: INFO: Pod "projected-volume-971719b0-8bc4-4569-9337-2c43b1790462": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007614602s
    Jun  8 16:42:57.990: INFO: Pod "projected-volume-971719b0-8bc4-4569-9337-2c43b1790462": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008581543s
    STEP: Saw pod success 06/08/23 16:42:57.99
    Jun  8 16:42:57.990: INFO: Pod "projected-volume-971719b0-8bc4-4569-9337-2c43b1790462" satisfied condition "Succeeded or Failed"
    Jun  8 16:42:57.993: INFO: Trying to get logs from node chl8tf-worker-001 pod projected-volume-971719b0-8bc4-4569-9337-2c43b1790462 container projected-all-volume-test: <nil>
    STEP: delete the pod 06/08/23 16:42:58.01
    Jun  8 16:42:58.023: INFO: Waiting for pod projected-volume-971719b0-8bc4-4569-9337-2c43b1790462 to disappear
    Jun  8 16:42:58.026: INFO: Pod projected-volume-971719b0-8bc4-4569-9337-2c43b1790462 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:42:58.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2896" for this suite. 06/08/23 16:42:58.031
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:42:58.038
Jun  8 16:42:58.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename webhook 06/08/23 16:42:58.039
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:42:58.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:42:58.057
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/08/23 16:42:58.072
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 16:42:58.523
STEP: Deploying the webhook pod 06/08/23 16:42:58.534
STEP: Wait for the deployment to be ready 06/08/23 16:42:58.545
Jun  8 16:42:58.552: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 06/08/23 16:43:00.563
STEP: Verifying the service has paired with the endpoint 06/08/23 16:43:00.577
Jun  8 16:43:01.577: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 06/08/23 16:43:01.636
STEP: Creating a configMap that does not comply to the validation webhook rules 06/08/23 16:43:01.674
STEP: Deleting the collection of validation webhooks 06/08/23 16:43:01.707
STEP: Creating a configMap that does not comply to the validation webhook rules 06/08/23 16:43:01.756
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  8 16:43:01.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6355" for this suite. 06/08/23 16:43:01.816
STEP: Destroying namespace "webhook-6355-markers" for this suite. 06/08/23 16:43:01.83
------------------------------
• [3.812 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:42:58.038
    Jun  8 16:42:58.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename webhook 06/08/23 16:42:58.039
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:42:58.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:42:58.057
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/08/23 16:42:58.072
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/08/23 16:42:58.523
    STEP: Deploying the webhook pod 06/08/23 16:42:58.534
    STEP: Wait for the deployment to be ready 06/08/23 16:42:58.545
    Jun  8 16:42:58.552: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 06/08/23 16:43:00.563
    STEP: Verifying the service has paired with the endpoint 06/08/23 16:43:00.577
    Jun  8 16:43:01.577: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 06/08/23 16:43:01.636
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/08/23 16:43:01.674
    STEP: Deleting the collection of validation webhooks 06/08/23 16:43:01.707
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/08/23 16:43:01.756
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:43:01.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6355" for this suite. 06/08/23 16:43:01.816
    STEP: Destroying namespace "webhook-6355-markers" for this suite. 06/08/23 16:43:01.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/08/23 16:43:01.861
Jun  8 16:43:01.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
STEP: Building a namespace api object, basename kubectl 06/08/23 16:43:01.862
STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:43:01.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:43:01.904
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Jun  8 16:43:01.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-1928 version'
Jun  8 16:43:02.015: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jun  8 16:43:02.015: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.5\", GitCommit:\"890a139214b4de1f01543d15003b5bda71aae9c7\", GitTreeState:\"clean\", BuildDate:\"2023-05-17T14:14:46Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.5+2.el8\", GitCommit:\"0e72dd8308bbcd03f944be8293f1cf087a2c1c0d\", GitTreeState:\"clean\", BuildDate:\"2023-06-02T09:41:29Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  8 16:43:02.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1928" for this suite. 06/08/23 16:43:02.023
------------------------------
• [0.171 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/08/23 16:43:01.861
    Jun  8 16:43:01.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2392066542
    STEP: Building a namespace api object, basename kubectl 06/08/23 16:43:01.862
    STEP: Waiting for a default service account to be provisioned in namespace 06/08/23 16:43:01.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/08/23 16:43:01.904
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Jun  8 16:43:01.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2392066542 --namespace=kubectl-1928 version'
    Jun  8 16:43:02.015: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jun  8 16:43:02.015: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.5\", GitCommit:\"890a139214b4de1f01543d15003b5bda71aae9c7\", GitTreeState:\"clean\", BuildDate:\"2023-05-17T14:14:46Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.5+2.el8\", GitCommit:\"0e72dd8308bbcd03f944be8293f1cf087a2c1c0d\", GitTreeState:\"clean\", BuildDate:\"2023-06-02T09:41:29Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  8 16:43:02.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1928" for this suite. 06/08/23 16:43:02.023
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Jun  8 16:43:02.034: INFO: Running AfterSuite actions on node 1
Jun  8 16:43:02.034: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Jun  8 16:43:02.034: INFO: Running AfterSuite actions on node 1
    Jun  8 16:43:02.034: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.162 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5847.804 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h37m28.348765007s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

